# Comparing `tmp/megadetector-5.0.8.tar.gz` & `tmp/megadetector-5.0.9.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "megadetector-5.0.8.tar", last modified: Fri Apr 19 23:55:36 2024, max compression
+gzip compressed data, was "megadetector-5.0.9.tar", last modified: Sun May  5 02:13:16 2024, max compression
```

## Comparing `megadetector-5.0.8.tar` & `megadetector-5.0.9.tar`

### file list

```diff
@@ -1,250 +1,271 @@
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.236348 megadetector-5.0.8/
--rw-rw-r--   0 user      (1000) user      (1000)     1100 2023-05-22 23:49:44.000000 megadetector-5.0.8/LICENSE
--rw-r--r--   0 user      (1000) user      (1000)     7373 2024-04-19 23:55:36.236348 megadetector-5.0.8/PKG-INFO
--rw-rw-r--   0 user      (1000) user      (1000)     4811 2024-04-19 23:54:16.000000 megadetector-5.0.8/README-package.md
--rw-rw-r--   0 user      (1000) user      (1000)    20819 2024-04-19 23:54:16.000000 megadetector-5.0.8/README.md
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/batch_processing/
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/batch_processing/api_core/
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/batch_processing/api_core/batch_service/
--rw-rw-r--   0 user      (1000) user      (1000)    17347 2023-05-21 20:45:21.000000 megadetector-5.0.8/api/batch_processing/api_core/batch_service/score.py
--rw-rw-r--   0 user      (1000) user      (1000)    11668 2023-05-21 18:12:32.000000 megadetector-5.0.8/api/batch_processing/api_core/server.py
--rw-rw-r--   0 user      (1000) user      (1000)     3318 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/batch_processing/api_core/server_api_config.py
--rw-rw-r--   0 user      (1000) user      (1000)     1860 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/batch_processing/api_core/server_app_config.py
--rw-rw-r--   0 user      (1000) user      (1000)    10324 2023-05-21 18:12:32.000000 megadetector-5.0.8/api/batch_processing/api_core/server_batch_job_manager.py
--rw-rw-r--   0 user      (1000) user      (1000)     6240 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/batch_processing/api_core/server_job_status_table.py
--rw-rw-r--   0 user      (1000) user      (1000)    17003 2023-05-21 18:12:32.000000 megadetector-5.0.8/api/batch_processing/api_core/server_orchestration.py
--rw-rw-r--   0 user      (1000) user      (1000)     3314 2023-05-21 18:12:32.000000 megadetector-5.0.8/api/batch_processing/api_core/server_utils.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/batch_processing/api_core_support/
--rw-rw-r--   0 user      (1000) user      (1000)     2275 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/batch_processing/api_core_support/aggregate_results_manually.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/batch_processing/api_support/
--rw-rw-r--   0 user      (1000) user      (1000)     5383 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/batch_processing/api_support/summarize_daily_activity.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/batch_processing/data_preparation/
--rw-rw-r--   0 user      (1000) user      (1000)    89186 2024-03-31 18:21:07.000000 megadetector-5.0.8/api/batch_processing/data_preparation/manage_local_batch.py
--rw-rw-r--   0 user      (1000) user      (1000)     9668 2024-01-28 18:41:58.000000 megadetector-5.0.8/api/batch_processing/data_preparation/manage_video_batch.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/batch_processing/integration/
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.224348 megadetector-5.0.8/api/batch_processing/integration/digiKam/
--rw-rw-r--   0 user      (1000) user      (1000)      203 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/batch_processing/integration/digiKam/setup.py
--rw-rw-r--   0 user      (1000) user      (1000)    17775 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/batch_processing/integration/digiKam/xmp_integration.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/batch_processing/integration/eMammal/
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.224348 megadetector-5.0.8/api/batch_processing/integration/eMammal/test_scripts/
--rw-rw-r--   0 user      (1000) user      (1000)       73 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/batch_processing/integration/eMammal/test_scripts/config_template.py
--rw-rw-r--   0 user      (1000) user      (1000)     3608 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/batch_processing/integration/eMammal/test_scripts/push_annotations_to_emammal.py
--rw-rw-r--   0 user      (1000) user      (1000)     1369 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/batch_processing/integration/eMammal/test_scripts/select_images_for_testing.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.224348 megadetector-5.0.8/api/batch_processing/postprocessing/
--rw-rw-r--   0 user      (1000) user      (1000)     1530 2023-12-06 01:51:04.000000 megadetector-5.0.8/api/batch_processing/postprocessing/add_max_conf.py
--rw-rw-r--   0 user      (1000) user      (1000)     4877 2023-10-17 20:51:54.000000 megadetector-5.0.8/api/batch_processing/postprocessing/categorize_detections_by_size.py
--rw-rw-r--   0 user      (1000) user      (1000)     8237 2024-03-18 22:03:38.000000 megadetector-5.0.8/api/batch_processing/postprocessing/combine_api_outputs.py
--rw-rw-r--   0 user      (1000) user      (1000)    34652 2024-02-25 01:09:49.000000 megadetector-5.0.8/api/batch_processing/postprocessing/compare_batch_results.py
--rw-rw-r--   0 user      (1000) user      (1000)    13600 2024-04-09 13:56:32.000000 megadetector-5.0.8/api/batch_processing/postprocessing/convert_output_format.py
--rw-rw-r--   0 user      (1000) user      (1000)     6895 2024-03-09 00:34:45.000000 megadetector-5.0.8/api/batch_processing/postprocessing/load_api_results.py
--rw-rw-r--   0 user      (1000) user      (1000)    10139 2023-11-08 23:56:54.000000 megadetector-5.0.8/api/batch_processing/postprocessing/md_to_coco.py
--rw-rw-r--   0 user      (1000) user      (1000)     9779 2024-03-31 18:21:07.000000 megadetector-5.0.8/api/batch_processing/postprocessing/md_to_labelme.py
--rw-rw-r--   0 user      (1000) user      (1000)    17192 2024-03-18 22:03:38.000000 megadetector-5.0.8/api/batch_processing/postprocessing/merge_detections.py
--rw-rw-r--   0 user      (1000) user      (1000)    74165 2024-03-19 20:27:38.000000 megadetector-5.0.8/api/batch_processing/postprocessing/postprocess_batch_results.py
--rw-rw-r--   0 user      (1000) user      (1000)     6042 2024-04-19 23:54:16.000000 megadetector-5.0.8/api/batch_processing/postprocessing/remap_detection_categories.py
--rw-rw-r--   0 user      (1000) user      (1000)    25047 2024-02-25 02:14:50.000000 megadetector-5.0.8/api/batch_processing/postprocessing/render_detection_confusion_matrix.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.224348 megadetector-5.0.8/api/batch_processing/postprocessing/repeat_detection_elimination/
--rw-rw-r--   0 user      (1000) user      (1000)     9053 2024-01-28 18:41:58.000000 megadetector-5.0.8/api/batch_processing/postprocessing/repeat_detection_elimination/find_repeat_detections.py
--rw-rw-r--   0 user      (1000) user      (1000)     2189 2023-12-28 01:45:41.000000 megadetector-5.0.8/api/batch_processing/postprocessing/repeat_detection_elimination/remove_repeat_detections.py
--rw-rw-r--   0 user      (1000) user      (1000)    62880 2024-03-31 18:21:07.000000 megadetector-5.0.8/api/batch_processing/postprocessing/repeat_detection_elimination/repeat_detections_core.py
--rw-rw-r--   0 user      (1000) user      (1000)    28720 2023-10-02 01:42:03.000000 megadetector-5.0.8/api/batch_processing/postprocessing/separate_detections_into_folders.py
--rw-rw-r--   0 user      (1000) user      (1000)    26368 2024-02-02 00:27:22.000000 megadetector-5.0.8/api/batch_processing/postprocessing/subset_json_detector_output.py
--rw-rw-r--   0 user      (1000) user      (1000)     5476 2023-12-06 01:57:03.000000 megadetector-5.0.8/api/batch_processing/postprocessing/top_folders_to_bottom.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/synchronous/
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/synchronous/api_core/
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.224348 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/
--rw-rw-r--   0 user      (1000) user      (1000)     4934 2023-05-21 20:45:21.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/api_backend.py
--rw-rw-r--   0 user      (1000) user      (1000)    10690 2023-05-21 20:45:21.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/api_frontend.py
--rw-rw-r--   0 user      (1000) user      (1000)     1017 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/config.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.220348 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/data_management/
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.224348 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/data_management/annotations/
--rw-r--r--   0 user      (1000) user      (1000)     1566 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/data_management/annotations/annotation_constants.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.224348 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.224348 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/detector_training/
--rw-rw-r--   0 user      (1000) user      (1000)     1091 2023-07-13 00:36:13.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/detector_training/copy_checkpoints.py
--rw-rw-r--   0 user      (1000) user      (1000)     4936 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/detector_training/model_main_tf2.py
--rw-rw-r--   0 user      (1000) user      (1000)    22089 2023-09-22 21:33:40.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/process_video.py
--rw-rw-r--   0 user      (1000) user      (1000)    11505 2023-09-24 22:24:49.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/pytorch_detector.py
--rw-rw-r--   0 user      (1000) user      (1000)    23821 2023-09-24 22:27:02.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/run_detector.py
--rw-rw-r--   0 user      (1000) user      (1000)    41849 2023-11-08 23:08:09.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/run_detector_batch.py
--rw-rw-r--   0 user      (1000) user      (1000)    22118 2023-09-22 20:29:38.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/run_inference_with_yolov5_val.py
--rw-rw-r--   0 user      (1000) user      (1000)    28785 2023-09-02 21:43:14.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/run_tiled_inference.py
--rw-rw-r--   0 user      (1000) user      (1000)     6760 2023-09-22 20:29:38.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/tf_detector.py
--rw-rw-r--   0 user      (1000) user      (1000)    18317 2023-09-30 15:19:08.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/video_utils.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.224348 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/
--rw-r--r--   0 user      (1000) user      (1000)     6243 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/azure_utils.py
--rw-r--r--   0 user      (1000) user      (1000)     7600 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/ct_utils.py
--rw-r--r--   0 user      (1000) user      (1000)     9598 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/directory_listing.py
--rw-r--r--   0 user      (1000) user      (1000)     2005 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/matlab_porting_tools.py
--rw-r--r--   0 user      (1000) user      (1000)    12397 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/path_utils.py
--rw-r--r--   0 user      (1000) user      (1000)     3049 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/process_utils.py
--rw-r--r--   0 user      (1000) user      (1000)    16955 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/sas_blob_utils.py
--rw-r--r--   0 user      (1000) user      (1000)     1277 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/string_utils.py
--rw-r--r--   0 user      (1000) user      (1000)     4598 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/url_utils.py
--rw-r--r--   0 user      (1000) user      (1000)     7573 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/write_html_image_list.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.224348 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_visualization/
--rw-r--r--   0 user      (1000) user      (1000)    31680 2023-10-10 14:44:28.000000 megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_visualization/visualization_utils.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.224348 megadetector-5.0.8/api/synchronous/api_core/tests/
--rw-rw-r--   0 user      (1000) user      (1000)     3386 2023-05-20 21:19:48.000000 megadetector-5.0.8/api/synchronous/api_core/tests/load_test.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.228348 megadetector-5.0.8/classification/
--rw-rw-r--   0 user      (1000) user      (1000)     3488 2023-10-10 14:50:39.000000 megadetector-5.0.8/classification/aggregate_classifier_probs.py
--rw-rw-r--   0 user      (1000) user      (1000)     8463 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/analyze_failed_images.py
--rw-rw-r--   0 user      (1000) user      (1000)     6400 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/cache_batchapi_outputs.py
--rw-rw-r--   0 user      (1000) user      (1000)    25578 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/create_classification_dataset.py
--rw-rw-r--   0 user      (1000) user      (1000)    20510 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/crop_detections.py
--rw-rw-r--   0 user      (1000) user      (1000)     6088 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/csv_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)    37158 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/detect_and_crop.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.228348 megadetector-5.0.8/classification/efficientnet/
--rw-rw-r--   0 user      (1000) user      (1000)      182 2023-05-20 21:19:48.000000 megadetector-5.0.8/classification/efficientnet/__init__.py
--rwxrwxr-x   0 user      (1000) user      (1000)    17040 2023-05-20 21:19:48.000000 megadetector-5.0.8/classification/efficientnet/model.py
--rwxrwxr-x   0 user      (1000) user      (1000)    24846 2023-07-05 16:59:21.000000 megadetector-5.0.8/classification/efficientnet/utils.py
--rw-rw-r--   0 user      (1000) user      (1000)    19378 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/evaluate_model.py
--rw-rw-r--   0 user      (1000) user      (1000)     5111 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/identify_mislabeled_candidates.py
--rw-rw-r--   0 user      (1000) user      (1000)     1670 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/json_to_azcopy_list.py
--rw-rw-r--   0 user      (1000) user      (1000)    26618 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/json_validator.py
--rw-rw-r--   0 user      (1000) user      (1000)    10738 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/map_classification_categories.py
--rw-rw-r--   0 user      (1000) user      (1000)    20123 2023-09-22 20:28:52.000000 megadetector-5.0.8/classification/merge_classification_detection_output.py
--rw-rw-r--   0 user      (1000) user      (1000)     5952 2023-12-06 01:57:03.000000 megadetector-5.0.8/classification/prepare_classification_script.py
--rw-rw-r--   0 user      (1000) user      (1000)     7440 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/prepare_classification_script_mc.py
--rw-rw-r--   0 user      (1000) user      (1000)     9363 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/run_classifier.py
--rw-rw-r--   0 user      (1000) user      (1000)     3466 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/save_mislabeled.py
--rw-rw-r--   0 user      (1000) user      (1000)    32329 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/train_classifier.py
--rw-rw-r--   0 user      (1000) user      (1000)    28096 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/train_classifier_tf.py
--rw-rw-r--   0 user      (1000) user      (1000)    11355 2023-07-13 00:36:13.000000 megadetector-5.0.8/classification/train_utils.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.228348 megadetector-5.0.8/data_management/
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.228348 megadetector-5.0.8/data_management/annotations/
--rw-rw-r--   0 user      (1000) user      (1000)     1566 2023-06-27 22:04:42.000000 megadetector-5.0.8/data_management/annotations/annotation_constants.py
--rw-rw-r--   0 user      (1000) user      (1000)     2416 2023-10-10 14:50:39.000000 megadetector-5.0.8/data_management/cct_json_to_filename_json.py
--rw-rw-r--   0 user      (1000) user      (1000)    12767 2024-03-09 00:34:45.000000 megadetector-5.0.8/data_management/cct_json_utils.py
--rw-rw-r--   0 user      (1000) user      (1000)     3859 2023-10-10 14:50:39.000000 megadetector-5.0.8/data_management/cct_to_csv.py
--rw-rw-r--   0 user      (1000) user      (1000)     4485 2023-08-19 17:11:37.000000 megadetector-5.0.8/data_management/cct_to_md.py
--rw-rw-r--   0 user      (1000) user      (1000)     8336 2023-05-22 23:05:38.000000 megadetector-5.0.8/data_management/cct_to_wi.py
--rw-rw-r--   0 user      (1000) user      (1000)     8077 2024-04-19 23:54:16.000000 megadetector-5.0.8/data_management/coco_to_labelme.py
--rw-rw-r--   0 user      (1000) user      (1000)    26042 2024-02-05 18:22:23.000000 megadetector-5.0.8/data_management/coco_to_yolo.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.228348 megadetector-5.0.8/data_management/databases/
--rw-rw-r--   0 user      (1000) user      (1000)      619 2023-06-27 21:58:14.000000 megadetector-5.0.8/data_management/databases/add_width_and_height_to_db.py
--rw-rw-r--   0 user      (1000) user      (1000)     6888 2023-10-10 14:50:39.000000 megadetector-5.0.8/data_management/databases/combine_coco_camera_traps_files.py
--rw-rw-r--   0 user      (1000) user      (1000)    14887 2024-04-19 23:54:16.000000 megadetector-5.0.8/data_management/databases/integrity_check_json_db.py
--rw-rw-r--   0 user      (1000) user      (1000)     6138 2023-10-10 14:50:39.000000 megadetector-5.0.8/data_management/databases/remove_corrupted_images_from_db.py
--rw-rw-r--   0 user      (1000) user      (1000)     2749 2024-03-18 22:03:38.000000 megadetector-5.0.8/data_management/databases/subset_json_db.py
--rw-rw-r--   0 user      (1000) user      (1000)     4283 2023-12-06 01:57:03.000000 megadetector-5.0.8/data_management/generate_crops_from_cct.py
--rw-rw-r--   0 user      (1000) user      (1000)     4961 2024-04-19 23:54:16.000000 megadetector-5.0.8/data_management/get_image_sizes.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.232348 megadetector-5.0.8/data_management/importers/
--rw-rw-r--   0 user      (1000) user      (1000)     1172 2023-09-21 01:58:08.000000 megadetector-5.0.8/data_management/importers/add_nacti_sizes.py
--rw-rw-r--   0 user      (1000) user      (1000)     2459 2023-11-05 20:26:38.000000 megadetector-5.0.8/data_management/importers/add_timestamps_to_icct.py
--rw-rw-r--   0 user      (1000) user      (1000)     4884 2024-03-18 22:03:38.000000 megadetector-5.0.8/data_management/importers/animl_results_to_md_results.py
--rw-rw-r--   0 user      (1000) user      (1000)    12910 2023-11-09 00:05:19.000000 megadetector-5.0.8/data_management/importers/auckland_doc_test_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     5952 2023-11-08 23:59:23.000000 megadetector-5.0.8/data_management/importers/auckland_doc_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     5307 2023-11-09 00:06:34.000000 megadetector-5.0.8/data_management/importers/awc_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     7909 2023-11-09 00:06:34.000000 megadetector-5.0.8/data_management/importers/bellevue_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)    28621 2023-09-22 23:29:41.000000 megadetector-5.0.8/data_management/importers/cacophony-thermal-importer.py
--rw-rw-r--   0 user      (1000) user      (1000)     7827 2023-11-09 00:03:29.000000 megadetector-5.0.8/data_management/importers/carrizo_shrubfree_2018.py
--rw-rw-r--   0 user      (1000) user      (1000)     8830 2023-11-09 00:05:19.000000 megadetector-5.0.8/data_management/importers/carrizo_trail_cam_2017.py
--rw-rw-r--   0 user      (1000) user      (1000)     1351 2023-11-09 00:07:28.000000 megadetector-5.0.8/data_management/importers/cct_field_adjustments.py
--rw-rw-r--   0 user      (1000) user      (1000)    29483 2023-11-09 00:05:19.000000 megadetector-5.0.8/data_management/importers/channel_islands_to_cct.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.232348 megadetector-5.0.8/data_management/importers/eMammal/
--rw-rw-r--   0 user      (1000) user      (1000)     6080 2023-05-20 21:19:48.000000 megadetector-5.0.8/data_management/importers/eMammal/copy_and_unzip_emammal.py
--rw-rw-r--   0 user      (1000) user      (1000)     6838 2023-05-20 21:19:48.000000 megadetector-5.0.8/data_management/importers/eMammal/eMammal_helpers.py
--rw-rw-r--   0 user      (1000) user      (1000)     6987 2023-07-05 17:01:56.000000 megadetector-5.0.8/data_management/importers/eMammal/make_eMammal_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     8248 2023-11-09 00:05:19.000000 megadetector-5.0.8/data_management/importers/ena24_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)    10526 2023-09-21 01:58:08.000000 megadetector-5.0.8/data_management/importers/filenames_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     8701 2023-11-09 00:05:19.000000 megadetector-5.0.8/data_management/importers/helena_to_cct.py
--rw-rw-r--   0 user      (1000) user      (1000)    54058 2023-11-09 00:05:19.000000 megadetector-5.0.8/data_management/importers/idaho-camera-traps.py
--rw-rw-r--   0 user      (1000) user      (1000)     8166 2023-11-09 00:05:19.000000 megadetector-5.0.8/data_management/importers/idfg_iwildcam_lila_prep.py
--rw-rw-r--   0 user      (1000) user      (1000)     3730 2023-11-09 00:06:34.000000 megadetector-5.0.8/data_management/importers/jb_csv_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     6718 2023-09-21 01:58:08.000000 megadetector-5.0.8/data_management/importers/mcgill_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)    14840 2023-11-09 00:01:34.000000 megadetector-5.0.8/data_management/importers/missouri_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     2045 2023-09-21 01:58:08.000000 megadetector-5.0.8/data_management/importers/nacti_fieldname_adjustments.py
--rw-rw-r--   0 user      (1000) user      (1000)     5147 2024-02-03 20:07:26.000000 megadetector-5.0.8/data_management/importers/noaa_seals_2019.py
--rw-rw-r--   0 user      (1000) user      (1000)    10718 2023-11-09 00:06:34.000000 megadetector-5.0.8/data_management/importers/pc_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     3787 2023-09-21 01:58:08.000000 megadetector-5.0.8/data_management/importers/plot_wni_giraffes.py
--rw-rw-r--   0 user      (1000) user      (1000)    12774 2023-11-08 23:59:39.000000 megadetector-5.0.8/data_management/importers/prepare-noaa-fish-data-for-lila.py
--rw-rw-r--   0 user      (1000) user      (1000)     3754 2023-11-09 00:06:34.000000 megadetector-5.0.8/data_management/importers/prepare_zsl_imerit.py
--rw-rw-r--   0 user      (1000) user      (1000)     9834 2023-11-09 00:07:41.000000 megadetector-5.0.8/data_management/importers/rspb_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)    10639 2023-11-09 00:05:19.000000 megadetector-5.0.8/data_management/importers/save_the_elephants_survey_A.py
--rw-rw-r--   0 user      (1000) user      (1000)    11167 2023-11-09 00:05:19.000000 megadetector-5.0.8/data_management/importers/save_the_elephants_survey_B.py
--rw-rw-r--   0 user      (1000) user      (1000)    23591 2023-11-09 00:06:52.000000 megadetector-5.0.8/data_management/importers/snapshot_safari_importer.py
--rw-rw-r--   0 user      (1000) user      (1000)    22996 2023-07-13 19:18:01.000000 megadetector-5.0.8/data_management/importers/snapshot_safari_importer_reprise.py
--rw-rw-r--   0 user      (1000) user      (1000)    33854 2023-11-09 00:07:01.000000 megadetector-5.0.8/data_management/importers/snapshot_serengeti_lila.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.232348 megadetector-5.0.8/data_management/importers/snapshotserengeti/
--rw-rw-r--   0 user      (1000) user      (1000)     4126 2023-05-20 21:19:48.000000 megadetector-5.0.8/data_management/importers/snapshotserengeti/make_full_SS_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     4316 2023-05-20 21:19:48.000000 megadetector-5.0.8/data_management/importers/snapshotserengeti/make_per_season_SS_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     2088 2023-09-21 01:58:08.000000 megadetector-5.0.8/data_management/importers/sulross_get_exif.py
--rw-rw-r--   0 user      (1000) user      (1000)    15902 2023-11-09 00:06:34.000000 megadetector-5.0.8/data_management/importers/timelapse_csv_set_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)    14869 2023-11-09 00:05:19.000000 megadetector-5.0.8/data_management/importers/ubc_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)    16177 2023-11-09 00:05:19.000000 megadetector-5.0.8/data_management/importers/umn_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     7671 2023-11-08 23:58:56.000000 megadetector-5.0.8/data_management/importers/wellington_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)    13656 2023-11-09 00:00:11.000000 megadetector-5.0.8/data_management/importers/wi_to_json.py
--rw-rw-r--   0 user      (1000) user      (1000)     5594 2023-12-06 01:45:18.000000 megadetector-5.0.8/data_management/importers/zamba_results_to_md_results.py
--rw-rw-r--   0 user      (1000) user      (1000)    18344 2024-04-19 23:54:16.000000 megadetector-5.0.8/data_management/labelme_to_coco.py
--rw-rw-r--   0 user      (1000) user      (1000)    10041 2024-04-19 23:54:16.000000 megadetector-5.0.8/data_management/labelme_to_yolo.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.232348 megadetector-5.0.8/data_management/lila/
--rw-rw-r--   0 user      (1000) user      (1000)     2561 2023-11-08 23:58:29.000000 megadetector-5.0.8/data_management/lila/add_locations_to_island_camera_traps.py
--rw-rw-r--   0 user      (1000) user      (1000)     5017 2023-10-28 15:19:33.000000 megadetector-5.0.8/data_management/lila/add_locations_to_nacti.py
--rw-rw-r--   0 user      (1000) user      (1000)    19764 2024-02-04 15:39:49.000000 megadetector-5.0.8/data_management/lila/create_lila_blank_set.py
--rw-rw-r--   0 user      (1000) user      (1000)     4825 2023-12-31 18:59:10.000000 megadetector-5.0.8/data_management/lila/create_lila_test_set.py
--rw-rw-r--   0 user      (1000) user      (1000)     3916 2023-12-06 01:45:44.000000 megadetector-5.0.8/data_management/lila/create_links_to_md_results_files.py
--rw-rw-r--   0 user      (1000) user      (1000)     7531 2024-04-09 13:56:32.000000 megadetector-5.0.8/data_management/lila/download_lila_subset.py
--rw-rw-r--   0 user      (1000) user      (1000)    17352 2024-02-25 01:09:49.000000 megadetector-5.0.8/data_management/lila/generate_lila_per_image_labels.py
--rw-rw-r--   0 user      (1000) user      (1000)     5490 2023-12-31 18:59:10.000000 megadetector-5.0.8/data_management/lila/get_lila_annotation_counts.py
--rw-rw-r--   0 user      (1000) user      (1000)     3625 2023-06-26 23:52:53.000000 megadetector-5.0.8/data_management/lila/get_lila_image_counts.py
--rw-rw-r--   0 user      (1000) user      (1000)     7691 2024-02-04 15:39:49.000000 megadetector-5.0.8/data_management/lila/lila_common.py
--rw-rw-r--   0 user      (1000) user      (1000)     3966 2023-12-31 18:59:10.000000 megadetector-5.0.8/data_management/lila/test_lila_metadata_urls.py
--rw-rw-r--   0 user      (1000) user      (1000)    29920 2023-11-05 20:26:38.000000 megadetector-5.0.8/data_management/ocr_tools.py
--rw-rw-r--   0 user      (1000) user      (1000)    21446 2024-04-19 23:54:16.000000 megadetector-5.0.8/data_management/read_exif.py
--rw-rw-r--   0 user      (1000) user      (1000)     3002 2024-04-19 23:54:16.000000 megadetector-5.0.8/data_management/remap_coco_categories.py
--rw-rw-r--   0 user      (1000) user      (1000)     1555 2023-07-13 00:36:13.000000 megadetector-5.0.8/data_management/remove_exif.py
--rw-rw-r--   0 user      (1000) user      (1000)     6100 2024-04-19 23:54:16.000000 megadetector-5.0.8/data_management/resize_coco_dataset.py
--rw-rw-r--   0 user      (1000) user      (1000)     7704 2024-03-09 00:34:45.000000 megadetector-5.0.8/data_management/wi_download_csv_to_coco.py
--rw-rw-r--   0 user      (1000) user      (1000)    16313 2024-01-28 18:41:58.000000 megadetector-5.0.8/data_management/yolo_output_to_md_output.py
--rw-rw-r--   0 user      (1000) user      (1000)    15196 2024-04-19 23:54:16.000000 megadetector-5.0.8/data_management/yolo_to_coco.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.232348 megadetector-5.0.8/detection/
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.232348 megadetector-5.0.8/detection/detector_training/
--rw-rw-r--   0 user      (1000) user      (1000)     1091 2023-07-13 00:36:13.000000 megadetector-5.0.8/detection/detector_training/copy_checkpoints.py
--rw-rw-r--   0 user      (1000) user      (1000)     4936 2023-05-20 21:19:48.000000 megadetector-5.0.8/detection/detector_training/model_main_tf2.py
--rw-rw-r--   0 user      (1000) user      (1000)    26825 2024-02-02 00:27:22.000000 megadetector-5.0.8/detection/process_video.py
--rw-rw-r--   0 user      (1000) user      (1000)    11992 2024-01-28 18:41:58.000000 megadetector-5.0.8/detection/pytorch_detector.py
--rw-rw-r--   0 user      (1000) user      (1000)    26036 2023-12-21 01:23:34.000000 megadetector-5.0.8/detection/run_detector.py
--rw-rw-r--   0 user      (1000) user      (1000)    47426 2024-03-31 18:21:07.000000 megadetector-5.0.8/detection/run_detector_batch.py
--rw-rw-r--   0 user      (1000) user      (1000)    33975 2024-03-31 18:21:07.000000 megadetector-5.0.8/detection/run_inference_with_yolov5_val.py
--rw-rw-r--   0 user      (1000) user      (1000)    33931 2024-02-25 01:09:49.000000 megadetector-5.0.8/detection/run_tiled_inference.py
--rw-rw-r--   0 user      (1000) user      (1000)     6769 2024-03-31 18:21:07.000000 megadetector-5.0.8/detection/tf_detector.py
--rw-rw-r--   0 user      (1000) user      (1000)    19501 2024-04-09 14:11:23.000000 megadetector-5.0.8/detection/video_utils.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.236348 megadetector-5.0.8/md_utils/
--rw-rw-r--   0 user      (1000) user      (1000)     6243 2023-05-22 23:02:57.000000 megadetector-5.0.8/md_utils/azure_utils.py
--rw-rw-r--   0 user      (1000) user      (1000)    13311 2024-04-13 19:10:46.000000 megadetector-5.0.8/md_utils/ct_utils.py
--rw-rw-r--   0 user      (1000) user      (1000)     9615 2023-10-10 14:50:39.000000 megadetector-5.0.8/md_utils/directory_listing.py
--rw-rw-r--   0 user      (1000) user      (1000)    33520 2024-03-19 20:27:38.000000 megadetector-5.0.8/md_utils/md_tests.py
--rw-rw-r--   0 user      (1000) user      (1000)    23176 2024-04-19 23:54:16.000000 megadetector-5.0.8/md_utils/path_utils.py
--rw-rw-r--   0 user      (1000) user      (1000)     4316 2024-01-28 18:41:58.000000 megadetector-5.0.8/md_utils/process_utils.py
--rw-rw-r--   0 user      (1000) user      (1000)    16955 2023-07-05 17:03:19.000000 megadetector-5.0.8/md_utils/sas_blob_utils.py
--rw-rw-r--   0 user      (1000) user      (1000)     9241 2023-11-22 03:22:33.000000 megadetector-5.0.8/md_utils/split_locations_into_train_val.py
--rw-rw-r--   0 user      (1000) user      (1000)     1600 2024-02-02 00:27:22.000000 megadetector-5.0.8/md_utils/string_utils.py
--rw-rw-r--   0 user      (1000) user      (1000)     6824 2024-03-31 18:21:07.000000 megadetector-5.0.8/md_utils/url_utils.py
--rw-rw-r--   0 user      (1000) user      (1000)     7532 2024-03-19 20:27:38.000000 megadetector-5.0.8/md_utils/write_html_image_list.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.236348 megadetector-5.0.8/md_visualization/
--rw-rw-r--   0 user      (1000) user      (1000)    10712 2023-10-25 02:02:08.000000 megadetector-5.0.8/md_visualization/plot_utils.py
--rw-rw-r--   0 user      (1000) user      (1000)    10391 2023-07-05 17:03:56.000000 megadetector-5.0.8/md_visualization/render_images_with_thumbnails.py
--rw-rw-r--   0 user      (1000) user      (1000)    46496 2024-03-31 18:21:07.000000 megadetector-5.0.8/md_visualization/visualization_utils.py
--rw-rw-r--   0 user      (1000) user      (1000)    19616 2024-04-19 23:54:16.000000 megadetector-5.0.8/md_visualization/visualize_db.py
--rw-rw-r--   0 user      (1000) user      (1000)    15814 2024-01-13 01:05:31.000000 megadetector-5.0.8/md_visualization/visualize_detector_output.py
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.236348 megadetector-5.0.8/megadetector.egg-info/
--rw-r--r--   0 user      (1000) user      (1000)     7373 2024-04-19 23:55:36.000000 megadetector-5.0.8/megadetector.egg-info/PKG-INFO
--rw-rw-r--   0 user      (1000) user      (1000)    10305 2024-04-19 23:55:36.000000 megadetector-5.0.8/megadetector.egg-info/SOURCES.txt
--rw-rw-r--   0 user      (1000) user      (1000)        1 2024-04-19 23:55:36.000000 megadetector-5.0.8/megadetector.egg-info/dependency_links.txt
--rw-rw-r--   0 user      (1000) user      (1000)      238 2024-04-19 23:55:36.000000 megadetector-5.0.8/megadetector.egg-info/requires.txt
--rw-rw-r--   0 user      (1000) user      (1000)      101 2024-04-19 23:55:36.000000 megadetector-5.0.8/megadetector.egg-info/top_level.txt
--rw-rw-r--   0 user      (1000) user      (1000)     1688 2024-04-19 23:55:26.000000 megadetector-5.0.8/pyproject.toml
--rw-rw-r--   0 user      (1000) user      (1000)       38 2024-04-19 23:55:36.236348 megadetector-5.0.8/setup.cfg
-drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-04-19 23:55:36.236348 megadetector-5.0.8/taxonomy_mapping/
--rw-rw-r--   0 user      (1000) user      (1000)    16517 2023-12-06 01:57:03.000000 megadetector-5.0.8/taxonomy_mapping/map_lila_taxonomy_to_wi_taxonomy.py
--rw-rw-r--   0 user      (1000) user      (1000)     4324 2024-01-09 23:45:10.000000 megadetector-5.0.8/taxonomy_mapping/map_new_lila_datasets.py
--rw-rw-r--   0 user      (1000) user      (1000)     4357 2024-01-09 23:45:10.000000 megadetector-5.0.8/taxonomy_mapping/prepare_lila_taxonomy_release.py
--rw-rw-r--   0 user      (1000) user      (1000)    20144 2023-12-31 18:59:10.000000 megadetector-5.0.8/taxonomy_mapping/preview_lila_taxonomy.py
--rw-rw-r--   0 user      (1000) user      (1000)     1992 2023-06-26 19:00:42.000000 megadetector-5.0.8/taxonomy_mapping/retrieve_sample_image.py
--rw-rw-r--   0 user      (1000) user      (1000)     6874 2023-07-05 17:04:26.000000 megadetector-5.0.8/taxonomy_mapping/simple_image_download.py
--rw-rw-r--   0 user      (1000) user      (1000)    28303 2023-12-31 18:59:10.000000 megadetector-5.0.8/taxonomy_mapping/species_lookup.py
--rw-rw-r--   0 user      (1000) user      (1000)     4874 2023-12-31 18:59:10.000000 megadetector-5.0.8/taxonomy_mapping/taxonomy_csv_checker.py
--rw-rw-r--   0 user      (1000) user      (1000)    12361 2023-07-13 00:36:13.000000 megadetector-5.0.8/taxonomy_mapping/taxonomy_graph.py
--rw-rw-r--   0 user      (1000) user      (1000)     2314 2023-06-26 17:52:32.000000 megadetector-5.0.8/taxonomy_mapping/validate_lila_category_mappings.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.306155 megadetector-5.0.9/
+-rw-rw-r--   0 user      (1000) user      (1000)     1100 2023-05-22 23:49:44.000000 megadetector-5.0.9/LICENSE
+-rw-r--r--   0 user      (1000) user      (1000)     7653 2024-05-05 02:13:16.306155 megadetector-5.0.9/PKG-INFO
+-rw-rw-r--   0 user      (1000) user      (1000)     5027 2024-05-05 02:10:08.000000 megadetector-5.0.9/README-package.md
+-rw-rw-r--   0 user      (1000) user      (1000)    20942 2024-05-05 02:10:08.000000 megadetector-5.0.9/README.md
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.234155 megadetector-5.0.9/api/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/__init__.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.234155 megadetector-5.0.9/api/batch_processing/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/batch_processing/__init__.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.234155 megadetector-5.0.9/api/batch_processing/api_core/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/batch_processing/api_core/__init__.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.238155 megadetector-5.0.9/api/batch_processing/api_core/batch_service/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/batch_processing/api_core/batch_service/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)    17346 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/api_core/batch_service/score.py
+-rw-rw-r--   0 user      (1000) user      (1000)    11668 2023-05-21 18:12:32.000000 megadetector-5.0.9/api/batch_processing/api_core/server.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3318 2023-05-20 21:19:48.000000 megadetector-5.0.9/api/batch_processing/api_core/server_api_config.py
+-rw-rw-r--   0 user      (1000) user      (1000)     1860 2023-05-20 21:19:48.000000 megadetector-5.0.9/api/batch_processing/api_core/server_app_config.py
+-rw-rw-r--   0 user      (1000) user      (1000)    10324 2023-05-21 18:12:32.000000 megadetector-5.0.9/api/batch_processing/api_core/server_batch_job_manager.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6239 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/api_core/server_job_status_table.py
+-rw-rw-r--   0 user      (1000) user      (1000)    17003 2023-05-21 18:12:32.000000 megadetector-5.0.9/api/batch_processing/api_core/server_orchestration.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3314 2023-05-21 18:12:32.000000 megadetector-5.0.9/api/batch_processing/api_core/server_utils.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.238155 megadetector-5.0.9/api/batch_processing/api_core_support/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/batch_processing/api_core_support/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)     2274 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/api_core_support/aggregate_results_manually.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.238155 megadetector-5.0.9/api/batch_processing/api_support/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/batch_processing/api_support/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5382 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/api_support/summarize_daily_activity.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.238155 megadetector-5.0.9/api/batch_processing/data_preparation/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/batch_processing/data_preparation/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)    89072 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/data_preparation/manage_local_batch.py
+-rw-rw-r--   0 user      (1000) user      (1000)     9649 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/batch_processing/data_preparation/manage_video_batch.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.234155 megadetector-5.0.9/api/batch_processing/integration/
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.238155 megadetector-5.0.9/api/batch_processing/integration/digiKam/
+-rw-rw-r--   0 user      (1000) user      (1000)      203 2023-05-20 21:19:48.000000 megadetector-5.0.9/api/batch_processing/integration/digiKam/setup.py
+-rw-rw-r--   0 user      (1000) user      (1000)    17774 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/integration/digiKam/xmp_integration.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.234155 megadetector-5.0.9/api/batch_processing/integration/eMammal/
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.242155 megadetector-5.0.9/api/batch_processing/integration/eMammal/test_scripts/
+-rw-rw-r--   0 user      (1000) user      (1000)       73 2023-05-20 21:19:48.000000 megadetector-5.0.9/api/batch_processing/integration/eMammal/test_scripts/config_template.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3607 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/integration/eMammal/test_scripts/push_annotations_to_emammal.py
+-rw-rw-r--   0 user      (1000) user      (1000)     1369 2023-05-20 21:19:48.000000 megadetector-5.0.9/api/batch_processing/integration/eMammal/test_scripts/select_images_for_testing.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.246155 megadetector-5.0.9/api/batch_processing/postprocessing/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/batch_processing/postprocessing/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)     1504 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/batch_processing/postprocessing/add_max_conf.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5677 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/categorize_detections_by_size.py
+-rw-rw-r--   0 user      (1000) user      (1000)     8524 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/combine_api_outputs.py
+-rw-rw-r--   0 user      (1000) user      (1000)    38211 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/compare_batch_results.py
+-rw-rw-r--   0 user      (1000) user      (1000)    14977 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/convert_output_format.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6851 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/load_api_results.py
+-rw-rw-r--   0 user      (1000) user      (1000)    11019 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/md_to_coco.py
+-rw-rw-r--   0 user      (1000) user      (1000)    11670 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/md_to_labelme.py
+-rw-rw-r--   0 user      (1000) user      (1000)    17163 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/batch_processing/postprocessing/merge_detections.py
+-rw-rw-r--   0 user      (1000) user      (1000)    77286 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/postprocess_batch_results.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6630 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/remap_detection_categories.py
+-rw-rw-r--   0 user      (1000) user      (1000)    27342 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/render_detection_confusion_matrix.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.246155 megadetector-5.0.9/api/batch_processing/postprocessing/repeat_detection_elimination/
+-rw-rw-r--   0 user      (1000) user      (1000)     9840 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/repeat_detection_elimination/find_repeat_detections.py
+-rw-rw-r--   0 user      (1000) user      (1000)     2836 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/repeat_detection_elimination/remove_repeat_detections.py
+-rw-rw-r--   0 user      (1000) user      (1000)    66477 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/repeat_detection_elimination/repeat_detections_core.py
+-rw-rw-r--   0 user      (1000) user      (1000)    31181 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/separate_detections_into_folders.py
+-rw-rw-r--   0 user      (1000) user      (1000)    26404 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/subset_json_detector_output.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6273 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/batch_processing/postprocessing/top_folders_to_bottom.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.246155 megadetector-5.0.9/api/synchronous/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/synchronous/__init__.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.234155 megadetector-5.0.9/api/synchronous/api_core/
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.246155 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4932 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/api_backend.py
+-rw-rw-r--   0 user      (1000) user      (1000)    10420 2024-05-05 02:10:08.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/api_frontend.py
+-rw-rw-r--   0 user      (1000) user      (1000)      982 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/config.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.234155 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/data_management/
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.246155 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/data_management/annotations/
+-rw-r--r--   0 user      (1000) user      (1000)     1566 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/data_management/annotations/annotation_constants.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.254155 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.254155 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/detector_training/
+-rw-rw-r--   0 user      (1000) user      (1000)     1091 2023-07-13 00:36:13.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/detector_training/copy_checkpoints.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4936 2023-05-20 21:19:48.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/detector_training/model_main_tf2.py
+-rw-rw-r--   0 user      (1000) user      (1000)    22089 2023-09-22 21:33:40.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/process_video.py
+-rw-rw-r--   0 user      (1000) user      (1000)    11505 2023-09-24 22:24:49.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/pytorch_detector.py
+-rw-rw-r--   0 user      (1000) user      (1000)    23821 2023-09-24 22:27:02.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/run_detector.py
+-rw-rw-r--   0 user      (1000) user      (1000)    41849 2023-11-08 23:08:09.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/run_detector_batch.py
+-rw-rw-r--   0 user      (1000) user      (1000)    22118 2023-09-22 20:29:38.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/run_inference_with_yolov5_val.py
+-rw-rw-r--   0 user      (1000) user      (1000)    28785 2023-09-02 21:43:14.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/run_tiled_inference.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6760 2023-09-22 20:29:38.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/tf_detector.py
+-rw-rw-r--   0 user      (1000) user      (1000)    18317 2023-09-30 15:19:08.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/video_utils.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.258155 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/
+-rw-r--r--   0 user      (1000) user      (1000)     6243 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/azure_utils.py
+-rw-r--r--   0 user      (1000) user      (1000)     7600 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/ct_utils.py
+-rw-r--r--   0 user      (1000) user      (1000)     9598 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/directory_listing.py
+-rw-r--r--   0 user      (1000) user      (1000)     2005 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/matlab_porting_tools.py
+-rw-r--r--   0 user      (1000) user      (1000)    12397 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/path_utils.py
+-rw-r--r--   0 user      (1000) user      (1000)     3049 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/process_utils.py
+-rw-r--r--   0 user      (1000) user      (1000)    16955 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/sas_blob_utils.py
+-rw-r--r--   0 user      (1000) user      (1000)     1277 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/string_utils.py
+-rw-r--r--   0 user      (1000) user      (1000)     4598 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/url_utils.py
+-rw-r--r--   0 user      (1000) user      (1000)     7573 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/write_html_image_list.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.258155 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_visualization/
+-rw-r--r--   0 user      (1000) user      (1000)    31680 2023-10-10 14:44:28.000000 megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_visualization/visualization_utils.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.258155 megadetector-5.0.9/api/synchronous/api_core/tests/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/synchronous/api_core/tests/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3277 2024-04-22 22:31:42.000000 megadetector-5.0.9/api/synchronous/api_core/tests/load_test.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.266155 megadetector-5.0.9/classification/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3429 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/aggregate_classifier_probs.py
+-rw-rw-r--   0 user      (1000) user      (1000)     8413 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/analyze_failed_images.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6297 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/cache_batchapi_outputs.py
+-rw-rw-r--   0 user      (1000) user      (1000)    25454 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/create_classification_dataset.py
+-rw-rw-r--   0 user      (1000) user      (1000)    20427 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/crop_detections.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5889 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/csv_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)    36970 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/detect_and_crop.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.266155 megadetector-5.0.9/classification/efficientnet/
+-rw-rw-r--   0 user      (1000) user      (1000)      182 2023-05-20 21:19:48.000000 megadetector-5.0.9/classification/efficientnet/__init__.py
+-rwxrwxr-x   0 user      (1000) user      (1000)    17040 2023-05-20 21:19:48.000000 megadetector-5.0.9/classification/efficientnet/model.py
+-rwxrwxr-x   0 user      (1000) user      (1000)    24846 2023-07-05 16:59:21.000000 megadetector-5.0.9/classification/efficientnet/utils.py
+-rw-rw-r--   0 user      (1000) user      (1000)    19307 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/evaluate_model.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5032 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/identify_mislabeled_candidates.py
+-rw-rw-r--   0 user      (1000) user      (1000)     1647 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/json_to_azcopy_list.py
+-rw-rw-r--   0 user      (1000) user      (1000)    26492 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/json_validator.py
+-rw-rw-r--   0 user      (1000) user      (1000)    10666 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/map_classification_categories.py
+-rw-rw-r--   0 user      (1000) user      (1000)    20011 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/merge_classification_detection_output.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6510 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/prepare_classification_script.py
+-rw-rw-r--   0 user      (1000) user      (1000)     7190 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/prepare_classification_script_mc.py
+-rw-rw-r--   0 user      (1000) user      (1000)     9325 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/run_classifier.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3408 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/save_mislabeled.py
+-rw-rw-r--   0 user      (1000) user      (1000)    32300 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/train_classifier.py
+-rw-rw-r--   0 user      (1000) user      (1000)    28067 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/train_classifier_tf.py
+-rw-rw-r--   0 user      (1000) user      (1000)    11332 2024-04-22 22:31:42.000000 megadetector-5.0.9/classification/train_utils.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.274155 megadetector-5.0.9/data_management/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/__init__.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.274155 megadetector-5.0.9/data_management/annotations/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/annotations/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)      953 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/annotations/annotation_constants.py
+-rw-rw-r--   0 user      (1000) user      (1000)     8592 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/camtrap_dp_to_coco.py
+-rw-rw-r--   0 user      (1000) user      (1000)    14783 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/cct_json_utils.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5060 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/cct_to_md.py
+-rw-rw-r--   0 user      (1000) user      (1000)     9771 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/cct_to_wi.py
+-rw-rw-r--   0 user      (1000) user      (1000)     8304 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/coco_to_labelme.py
+-rw-rw-r--   0 user      (1000) user      (1000)    28111 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/coco_to_yolo.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.274155 megadetector-5.0.9/data_management/databases/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/databases/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)      749 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/databases/add_width_and_height_to_db.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6693 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/databases/combine_coco_camera_traps_files.py
+-rw-rw-r--   0 user      (1000) user      (1000)    16285 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/databases/integrity_check_json_db.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3219 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/databases/subset_json_db.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4383 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/generate_crops_from_cct.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6581 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/get_image_sizes.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.290155 megadetector-5.0.9/data_management/importers/
+-rw-rw-r--   0 user      (1000) user      (1000)     1156 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/add_nacti_sizes.py
+-rw-rw-r--   0 user      (1000) user      (1000)     2364 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/add_timestamps_to_icct.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4698 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/animl_results_to_md_results.py
+-rw-rw-r--   0 user      (1000) user      (1000)    12893 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/auckland_doc_test_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5936 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/auckland_doc_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5292 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/awc_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     7886 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/bellevue_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)    28600 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/cacophony-thermal-importer.py
+-rw-rw-r--   0 user      (1000) user      (1000)     7811 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/carrizo_shrubfree_2018.py
+-rw-rw-r--   0 user      (1000) user      (1000)     8814 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/carrizo_trail_cam_2017.py
+-rw-rw-r--   0 user      (1000) user      (1000)     1334 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/cct_field_adjustments.py
+-rw-rw-r--   0 user      (1000) user      (1000)    29465 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/channel_islands_to_cct.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.290155 megadetector-5.0.9/data_management/importers/eMammal/
+-rw-rw-r--   0 user      (1000) user      (1000)     6085 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/importers/eMammal/copy_and_unzip_emammal.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6838 2023-05-20 21:19:48.000000 megadetector-5.0.9/data_management/importers/eMammal/eMammal_helpers.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6987 2023-07-05 17:01:56.000000 megadetector-5.0.9/data_management/importers/eMammal/make_eMammal_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     8233 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/ena24_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)    10510 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/filenames_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     8686 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/helena_to_cct.py
+-rw-rw-r--   0 user      (1000) user      (1000)    54043 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/idaho-camera-traps.py
+-rw-rw-r--   0 user      (1000) user      (1000)     8148 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/idfg_iwildcam_lila_prep.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3713 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/jb_csv_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6702 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/mcgill_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)    14814 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/missouri_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     2027 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/nacti_fieldname_adjustments.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5132 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/noaa_seals_2019.py
+-rw-rw-r--   0 user      (1000) user      (1000)    10703 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/pc_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3772 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/plot_wni_giraffes.py
+-rw-rw-r--   0 user      (1000) user      (1000)    12402 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/prepare-noaa-fish-data-for-lila.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3739 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/prepare_zsl_imerit.py
+-rw-rw-r--   0 user      (1000) user      (1000)     9818 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/rspb_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)    10623 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/save_the_elephants_survey_A.py
+-rw-rw-r--   0 user      (1000) user      (1000)    11152 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/save_the_elephants_survey_B.py
+-rw-rw-r--   0 user      (1000) user      (1000)    23557 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/snapshot_safari_importer.py
+-rw-rw-r--   0 user      (1000) user      (1000)    22313 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/snapshot_safari_importer_reprise.py
+-rw-rw-r--   0 user      (1000) user      (1000)    33832 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/snapshot_serengeti_lila.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.290155 megadetector-5.0.9/data_management/importers/snapshotserengeti/
+-rw-rw-r--   0 user      (1000) user      (1000)     4126 2023-05-20 21:19:48.000000 megadetector-5.0.9/data_management/importers/snapshotserengeti/make_full_SS_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4316 2023-05-20 21:19:48.000000 megadetector-5.0.9/data_management/importers/snapshotserengeti/make_per_season_SS_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     2071 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/importers/sulross_get_exif.py
+-rw-rw-r--   0 user      (1000) user      (1000)    15883 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/timelapse_csv_set_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)    14848 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/ubc_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)    16162 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/umn_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     7655 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/wellington_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)    13639 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/wi_to_json.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5381 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/importers/zamba_results_to_md_results.py
+-rw-rw-r--   0 user      (1000) user      (1000)    21179 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/labelme_to_coco.py
+-rw-rw-r--   0 user      (1000) user      (1000)    10024 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/labelme_to_yolo.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.294155 megadetector-5.0.9/data_management/lila/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/lila/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)     2540 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/lila/add_locations_to_island_camera_traps.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4849 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/lila/add_locations_to_nacti.py
+-rw-rw-r--   0 user      (1000) user      (1000)    19736 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/lila/create_lila_blank_set.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4806 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/lila/create_lila_test_set.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3793 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/lila/create_links_to_md_results_files.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5293 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/lila/download_lila_subset.py
+-rw-rw-r--   0 user      (1000) user      (1000)    18016 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/lila/generate_lila_per_image_labels.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5588 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/lila/get_lila_annotation_counts.py
+-rw-rw-r--   0 user      (1000) user      (1000)     3601 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/lila/get_lila_image_counts.py
+-rw-rw-r--   0 user      (1000) user      (1000)    10091 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/lila/lila_common.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4415 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/lila/test_lila_metadata_urls.py
+-rw-rw-r--   0 user      (1000) user      (1000)    32502 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/ocr_tools.py
+-rw-rw-r--   0 user      (1000) user      (1000)    22745 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/read_exif.py
+-rw-rw-r--   0 user      (1000) user      (1000)     2902 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/remap_coco_categories.py
+-rw-rw-r--   0 user      (1000) user      (1000)     1665 2024-04-22 22:31:42.000000 megadetector-5.0.9/data_management/remove_exif.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6770 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/resize_coco_dataset.py
+-rw-rw-r--   0 user      (1000) user      (1000)     8307 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/wi_download_csv_to_coco.py
+-rw-rw-r--   0 user      (1000) user      (1000)    17261 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/yolo_output_to_md_output.py
+-rw-rw-r--   0 user      (1000) user      (1000)    25080 2024-05-05 02:10:08.000000 megadetector-5.0.9/data_management/yolo_to_coco.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.294155 megadetector-5.0.9/detection/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/detection/__init__.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.294155 megadetector-5.0.9/detection/detector_training/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/detection/detector_training/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4936 2023-05-20 21:19:48.000000 megadetector-5.0.9/detection/detector_training/model_main_tf2.py
+-rw-rw-r--   0 user      (1000) user      (1000)    29829 2024-05-05 02:10:08.000000 megadetector-5.0.9/detection/process_video.py
+-rw-rw-r--   0 user      (1000) user      (1000)    12537 2024-05-05 02:10:08.000000 megadetector-5.0.9/detection/pytorch_detector.py
+-rw-rw-r--   0 user      (1000) user      (1000)    29687 2024-05-05 02:10:08.000000 megadetector-5.0.9/detection/run_detector.py
+-rw-rw-r--   0 user      (1000) user      (1000)    52059 2024-05-05 02:10:08.000000 megadetector-5.0.9/detection/run_detector_batch.py
+-rw-rw-r--   0 user      (1000) user      (1000)    36888 2024-05-05 02:10:08.000000 megadetector-5.0.9/detection/run_inference_with_yolov5_val.py
+-rw-rw-r--   0 user      (1000) user      (1000)    37834 2024-05-05 02:10:08.000000 megadetector-5.0.9/detection/run_tiled_inference.py
+-rw-rw-r--   0 user      (1000) user      (1000)     7533 2024-05-05 02:10:08.000000 megadetector-5.0.9/detection/tf_detector.py
+-rw-rw-r--   0 user      (1000) user      (1000)    22753 2024-05-05 02:10:08.000000 megadetector-5.0.9/detection/video_utils.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.234155 megadetector-5.0.9/docs/
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.294155 megadetector-5.0.9/docs/source/
+-rw-rw-r--   0 user      (1000) user      (1000)     1158 2024-05-05 02:10:08.000000 megadetector-5.0.9/docs/source/conf.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.298155 megadetector-5.0.9/md_utils/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/md_utils/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6223 2024-04-22 22:31:42.000000 megadetector-5.0.9/md_utils/azure_utils.py
+-rw-rw-r--   0 user      (1000) user      (1000)    17847 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_utils/ct_utils.py
+-rw-rw-r--   0 user      (1000) user      (1000)     9658 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_utils/directory_listing.py
+-rw-rw-r--   0 user      (1000) user      (1000)    35870 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_utils/md_tests.py
+-rw-rw-r--   0 user      (1000) user      (1000)    35948 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_utils/path_utils.py
+-rw-rw-r--   0 user      (1000) user      (1000)     5226 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_utils/process_utils.py
+-rw-rw-r--   0 user      (1000) user      (1000)    16917 2024-04-22 22:31:42.000000 megadetector-5.0.9/md_utils/sas_blob_utils.py
+-rw-rw-r--   0 user      (1000) user      (1000)    10091 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_utils/split_locations_into_train_val.py
+-rw-rw-r--   0 user      (1000) user      (1000)     2050 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_utils/string_utils.py
+-rw-rw-r--   0 user      (1000) user      (1000)    11476 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_utils/url_utils.py
+-rw-rw-r--   0 user      (1000) user      (1000)     8628 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_utils/write_html_image_list.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.298155 megadetector-5.0.9/md_visualization/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/md_visualization/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)    10671 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_visualization/plot_utils.py
+-rw-rw-r--   0 user      (1000) user      (1000)    10559 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_visualization/render_images_with_thumbnails.py
+-rw-rw-r--   0 user      (1000) user      (1000)    62096 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_visualization/visualization_utils.py
+-rw-rw-r--   0 user      (1000) user      (1000)    20761 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_visualization/visualize_db.py
+-rw-rw-r--   0 user      (1000) user      (1000)    17043 2024-05-05 02:10:08.000000 megadetector-5.0.9/md_visualization/visualize_detector_output.py
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.306155 megadetector-5.0.9/megadetector.egg-info/
+-rw-r--r--   0 user      (1000) user      (1000)     7653 2024-05-05 02:13:16.000000 megadetector-5.0.9/megadetector.egg-info/PKG-INFO
+-rw-rw-r--   0 user      (1000) user      (1000)    10955 2024-05-05 02:13:16.000000 megadetector-5.0.9/megadetector.egg-info/SOURCES.txt
+-rw-rw-r--   0 user      (1000) user      (1000)        1 2024-05-05 02:13:16.000000 megadetector-5.0.9/megadetector.egg-info/dependency_links.txt
+-rw-rw-r--   0 user      (1000) user      (1000)      238 2024-05-05 02:13:16.000000 megadetector-5.0.9/megadetector.egg-info/requires.txt
+-rw-rw-r--   0 user      (1000) user      (1000)      106 2024-05-05 02:13:16.000000 megadetector-5.0.9/megadetector.egg-info/top_level.txt
+-rw-rw-r--   0 user      (1000) user      (1000)     1745 2024-05-05 02:12:23.000000 megadetector-5.0.9/pyproject.toml
+-rw-rw-r--   0 user      (1000) user      (1000)       38 2024-05-05 02:13:16.306155 megadetector-5.0.9/setup.cfg
+drwxrwxr-x   0 user      (1000) user      (1000)        0 2024-05-05 02:13:16.302155 megadetector-5.0.9/taxonomy_mapping/
+-rw-rw-r--   0 user      (1000) user      (1000)        0 2024-04-22 22:31:42.000000 megadetector-5.0.9/taxonomy_mapping/__init__.py
+-rw-rw-r--   0 user      (1000) user      (1000)    17914 2024-04-22 22:31:42.000000 megadetector-5.0.9/taxonomy_mapping/map_lila_taxonomy_to_wi_taxonomy.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4151 2024-04-22 22:31:42.000000 megadetector-5.0.9/taxonomy_mapping/map_new_lila_datasets.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4676 2024-04-22 22:31:42.000000 megadetector-5.0.9/taxonomy_mapping/prepare_lila_taxonomy_release.py
+-rw-rw-r--   0 user      (1000) user      (1000)    19532 2024-04-22 22:31:42.000000 megadetector-5.0.9/taxonomy_mapping/preview_lila_taxonomy.py
+-rw-rw-r--   0 user      (1000) user      (1000)     1966 2024-04-22 22:31:42.000000 megadetector-5.0.9/taxonomy_mapping/retrieve_sample_image.py
+-rw-rw-r--   0 user      (1000) user      (1000)     6851 2024-04-22 22:31:42.000000 megadetector-5.0.9/taxonomy_mapping/simple_image_download.py
+-rw-rw-r--   0 user      (1000) user      (1000)    28281 2024-04-22 22:31:42.000000 megadetector-5.0.9/taxonomy_mapping/species_lookup.py
+-rw-rw-r--   0 user      (1000) user      (1000)     4837 2024-04-22 22:31:42.000000 megadetector-5.0.9/taxonomy_mapping/taxonomy_csv_checker.py
+-rw-rw-r--   0 user      (1000) user      (1000)    12263 2024-04-22 22:31:42.000000 megadetector-5.0.9/taxonomy_mapping/taxonomy_graph.py
+-rw-rw-r--   0 user      (1000) user      (1000)     2514 2024-04-22 22:31:42.000000 megadetector-5.0.9/taxonomy_mapping/validate_lila_category_mappings.py
```

### Comparing `megadetector-5.0.8/LICENSE` & `megadetector-5.0.9/LICENSE`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/PKG-INFO` & `megadetector-5.0.9/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: megadetector
-Version: 5.0.8
+Version: 5.0.9
 Summary: MegaDetector is an AI model that helps conservation folks spend less time doing boring things with camera trap images.
 Author-email: Your friendly neighborhood MegaDetector team <cameratraps@lila.science>
 Maintainer-email: Your friendly neighborhood MegaDetector team <cameratraps@lila.science>
 License:     MIT License
         
             Permission is hereby granted, free of charge, to any person obtaining a copy
             of this software and associated documentation files (the "Software"), to deal
@@ -21,14 +21,15 @@
             FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
             AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
             LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
             OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
             SOFTWARE.
         
 Project-URL: Homepage, https://github.com/agentmorris/MegaDetector
+Project-URL: Documentation, https://megadetector.readthedocs.io
 Project-URL: Bug Reports, https://github.com/agentmorris/MegaDetector/issues
 Project-URL: Source, https://github.com/agentmorris/MegaDetector
 Keywords: camera traps,conservation,wildlife,ai
 Classifier: Development Status :: 3 - Alpha
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Programming Language :: Python :: 3
 Requires-Python: <3.12,>=3.9
@@ -47,24 +48,24 @@
 Requires-Dist: scikit-learn>=1.3.1
 Requires-Dist: pandas>=2.1.1
 Requires-Dist: PyYAML>=6.0.1
 Requires-Dist: ultralytics-yolov5==0.1.1
 
 # MegaDetector
 
-This package is a pip-installable version of the support/inference code for [MegaDetector](https://github.com/agentmorris/MegaDetector), an object detection model that helps conservation biologists spend less time doing boring things with camera trap images.
+This package is a pip-installable version of the support/inference code for [MegaDetector](https://github.com/agentmorris/MegaDetector), an object detection model that helps conservation biologists spend less time doing boring things with camera trap images.  Complete documentation for this Python package is available at <megadetector.readthedocs.io>.
 
-If you want to learn more about what MegaDetector is all about, head over to the [MegaDetector repo](https://github.com/agentmorris/MegaDetector).
+If you aren't looking for the Python package specificaly, and you just want to learn more about what MegaDetector is all about, head over to the [MegaDetector repo](https://github.com/agentmorris/MegaDetector).
 
 
 ## Reasons you probably aren't looking for this package
 
 ### If you are an ecologist...
 
-If you are an ecologist looking to use MegaDetector to help you get through your camera trap images, you probably don't want this package.  We recommend starting with our "[Getting started with MegaDetector](https://github.com/agentmorris/MegaDetector/blob/main/collaborations.md)" page, then digging in to the [MegaDetector User Guide](https://github.com/agentmorris/MegaDetector/blob/main/megadetector.md), which will walk you through the process of using MegaDetector.  That journey will <i>not</i> involve this Python package.
+If you are an ecologist looking to use MegaDetector to help you get through your camera trap images, you probably don't want this package.  We recommend starting with our "[Getting started with MegaDetector](https://github.com/agentmorris/MegaDetector/blob/main/getting-started.md)" page, then digging in to the [MegaDetector User Guide](https://github.com/agentmorris/MegaDetector/blob/main/megadetector.md), which will walk you through the process of using MegaDetector.  That journey will <i>not</i> involve this Python package.
 
 ### If you are a computer-vision-y type...
 
 If you are a computer-vision-y person looking to run or fine-tune MegaDetector programmatically, you still probably don't want this package.  MegaDetector is just a fine-tuned version of [YOLOv5](https://github.com/ultralytics/yolov5), and the [ultralytics](https://github.com/ultralytics/ultralytics/) package (from the developers of YOLOv5) has a zillion bells and whistles for both inference and fine-tuning that this package doesn't.
 
 ## Reasons you might want to use this package
 
@@ -76,17 +77,22 @@
 
 To install:
 
 `pip install megadetector`
 
 MegaDetector model weights aren't downloaded at pip-install time, but they will be (optionally) automatically downloaded the first time you run the model.
 
-### Examples of things you can do with this package
+## Package reference
 
-#### Run MegaDetector on one image and count the number of detections
+See <megadetector.readthedocs.io>.
+
+
+## Examples of things you can do with this package
+
+### Run MegaDetector on one image and count the number of detections
 
 ```
 from md_utils import url_utils
 from md_visualization import visualization_utils as vis_utils
 from detection import run_detector
 
 # This is the image at the bottom of this page, it has one animal in it
@@ -100,15 +106,15 @@
 
 result = model.generate_detections_one_image(image)
 
 detections_above_threshold = [d for d in result['detections'] if d['conf'] > 0.2]
 print('Found {} detections above threshold'.format(len(detections_above_threshold)))
 ```
 
-#### Run MegaDetector on a folder of images
+### Run MegaDetector on a folder of images
 
 ```
 from detection.run_detector_batch import load_and_run_detector_batch,write_results_to_file
 from md_utils import path_utils
 import os
 
 # Pick a folder to run MD on recursively, and an output file
```

### Comparing `megadetector-5.0.8/README-package.md` & `megadetector-5.0.9/README-package.md`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # MegaDetector
 
-This package is a pip-installable version of the support/inference code for [MegaDetector](https://github.com/agentmorris/MegaDetector), an object detection model that helps conservation biologists spend less time doing boring things with camera trap images.
+This package is a pip-installable version of the support/inference code for [MegaDetector](https://github.com/agentmorris/MegaDetector), an object detection model that helps conservation biologists spend less time doing boring things with camera trap images.  Complete documentation for this Python package is available at <megadetector.readthedocs.io>.
 
-If you want to learn more about what MegaDetector is all about, head over to the [MegaDetector repo](https://github.com/agentmorris/MegaDetector).
+If you aren't looking for the Python package specificaly, and you just want to learn more about what MegaDetector is all about, head over to the [MegaDetector repo](https://github.com/agentmorris/MegaDetector).
 
 
 ## Reasons you probably aren't looking for this package
 
 ### If you are an ecologist...
 
-If you are an ecologist looking to use MegaDetector to help you get through your camera trap images, you probably don't want this package.  We recommend starting with our "[Getting started with MegaDetector](https://github.com/agentmorris/MegaDetector/blob/main/collaborations.md)" page, then digging in to the [MegaDetector User Guide](https://github.com/agentmorris/MegaDetector/blob/main/megadetector.md), which will walk you through the process of using MegaDetector.  That journey will <i>not</i> involve this Python package.
+If you are an ecologist looking to use MegaDetector to help you get through your camera trap images, you probably don't want this package.  We recommend starting with our "[Getting started with MegaDetector](https://github.com/agentmorris/MegaDetector/blob/main/getting-started.md)" page, then digging in to the [MegaDetector User Guide](https://github.com/agentmorris/MegaDetector/blob/main/megadetector.md), which will walk you through the process of using MegaDetector.  That journey will <i>not</i> involve this Python package.
 
 ### If you are a computer-vision-y type...
 
 If you are a computer-vision-y person looking to run or fine-tune MegaDetector programmatically, you still probably don't want this package.  MegaDetector is just a fine-tuned version of [YOLOv5](https://github.com/ultralytics/yolov5), and the [ultralytics](https://github.com/ultralytics/ultralytics/) package (from the developers of YOLOv5) has a zillion bells and whistles for both inference and fine-tuning that this package doesn't.
 
 ## Reasons you might want to use this package
 
@@ -25,17 +25,22 @@
 
 To install:
 
 `pip install megadetector`
 
 MegaDetector model weights aren't downloaded at pip-install time, but they will be (optionally) automatically downloaded the first time you run the model.
 
-### Examples of things you can do with this package
+## Package reference
 
-#### Run MegaDetector on one image and count the number of detections
+See <megadetector.readthedocs.io>.
+
+
+## Examples of things you can do with this package
+
+### Run MegaDetector on one image and count the number of detections
 
 ```
 from md_utils import url_utils
 from md_visualization import visualization_utils as vis_utils
 from detection import run_detector
 
 # This is the image at the bottom of this page, it has one animal in it
@@ -49,15 +54,15 @@
 
 result = model.generate_detections_one_image(image)
 
 detections_above_threshold = [d for d in result['detections'] if d['conf'] > 0.2]
 print('Found {} detections above threshold'.format(len(detections_above_threshold)))
 ```
 
-#### Run MegaDetector on a folder of images
+### Run MegaDetector on a folder of images
 
 ```
 from detection.run_detector_batch import load_and_run_detector_batch,write_results_to_file
 from md_utils import path_utils
 import os
 
 # Pick a folder to run MD on recursively, and an output file
```

### Comparing `megadetector-5.0.8/README.md` & `megadetector-5.0.9/README.md`

 * *Files 1% similar despite different names*

```diff
@@ -19,25 +19,25 @@
 
 ![Red bounding box on fox](images/detector_example.jpg)<br/>Image credit University of Washington.
 
 
 ## How do I get started with MegaDetector?
 
 * If you are looking for a convenient tool to run MegaDetector, you don't need anything from this repository: check out [EcoAssist](https://github.com/PetervanLunteren/EcoAssist?tab=readme-ov-file).
-* If you're just <i>considering</i> the use of AI in your workflow, and you aren't even sure yet whether MegaDetector would be useful to you, we recommend reading the "[getting started with MegaDetector](collaborations.md)" page.
+* If you're just <i>considering</i> the use of AI in your workflow, and you aren't even sure yet whether MegaDetector would be useful to you, we recommend reading the "[getting started with MegaDetector](getting-started.md)" page.
 * If you're already familiar with MegaDetector and you're ready to run it on your data, see the [MegaDetector User Guide](megadetector.md) for instructions on running MegaDetector.
 * If you're a programmer-type looking to use tools from this repo, check out the [Python package](https://pypi.org/project/megadetector/) that provides access to everything in this repo (yes, you guessed it, "pip install megadetector").
 * If you have any questions, or you want to tell us that MegaDetector was amazing/terrible on your images, <a href="mailto:cameratraps@lila.science">email us</a>!
 
 MegaDetector is just one of many tools that aims to make conservation biologists more efficient with AI.  If you want to learn about other ways to use AI to accelerate camera trap workflows, check out our of the field, affectionately titled &ldquo;[Everything I know about machine learning and camera traps](https://agentmorris.github.io/camera-trap-ml-survey/)&rdquo;.
 
 
 ## Who is using MegaDetector?
 
-We work with ecologists all over the world to help them spend less time annotating images and more time thinking about conservation.  You can read a little more about how this works on our [getting started with MegaDetector](collaborations.md) page.
+We work with ecologists all over the world to help them spend less time annotating images and more time thinking about conservation.  You can read a little more about how this works on our [getting started with MegaDetector](getting-started.md) page.
 
 Here are a few of the organizations that have used MegaDetector... we're only listing organizations who (a) we know about and (b) have given us permission to refer to them here (or have posted publicly about their use of MegaDetector), so if you're using MegaDetector or other tools from this repo and would like to be added to this list, <a href="mailto:cameratraps@lila.science">email us</a>!
 
 * [Arizona Department of Environmental Quality](http://azdeq.gov/)
 * [Biometrio.earth](https://biometrio.earth/)
 * [Blackbird Environmental](https://blackbirdenv.com/)
 * [Camelot](https://camelotproject.org/)
@@ -81,14 +81,15 @@
 * [Ecology and Conservation of Amazonian Vertebrates Research Group](https://www.researchgate.net/lab/Fernanda-Michalski-Lab-4), Federal University of Amap
 * [Gola Forest Programma](https://www.rspb.org.uk/our-work/conservation/projects/scientific-support-for-the-gola-forest-programme/), Royal Society for the Protection of Birds (RSPB)
 * [Graeme Shannon's Research Group](https://wildliferesearch.co.uk/group-1), Bangor University 
 * [Grizzly Bear Recovery Program](https://www.fws.gov/office/grizzly-bear-recovery-program), U.S. Fish & Wildlife Service
 * [Hamaarag](https://hamaarag.org.il/), The Steinhardt Museum of Natural History, Tel Aviv University
 * [Institut des Science de la Fort Tempre](https://isfort.uqo.ca/) (ISFORT), Universit du Qubec en Outaouais
 * [Lab of Dr. Bilal Habib](https://bhlab.in/about), the Wildlife Institute of India
+* [Landscape Ecology Lab](https://www.concordia.ca/artsci/geography-planning-environment/research/labs/lel.html), Concordia University
 * [Mammal Spatial Ecology and Conservation Lab](https://labs.wsu.edu/dthornton/), Washington State University
 * [McLoughlin Lab in Population Ecology](http://mcloughlinlab.ca/lab/), University of Saskatchewan
 * [National Wildlife Refuge System, Southwest Region](https://www.fws.gov/about/region/southwest), U.S. Fish & Wildlife Service
 * [Northern Great Plains Program](https://nationalzoo.si.edu/news/restoring-americas-prairie), Smithsonian
 * [Polar Ecology Group](https://polarecologygroup.wordpress.com), University of Gdansk
 * [Quantitative Ecology Lab](https://depts.washington.edu/sefsqel/), University of Washington
 * [Santa Monica Mountains Recreation Area](https://www.nps.gov/samo/index.htm), National Park Service
@@ -180,15 +181,15 @@
 * Converting frequently-used metadata formats to [COCO Camera Traps](https://github.com/agentmorris/MegaDetector/blob/main/data_management/README.md#coco-cameratraps-format) format
 * Converting the output of AI models (especially [YOLOv5](https://github.com/agentmorris/MegaDetector/blob/main/api/batch_processing/postprocessing/convert_output_format.py)) to the format used for AI results throughout this repo
 * Creating, visualizing, and  editing COCO Camera Traps .json databases
 
 
 ### detection
 
-Code for training, running, and evaluating MegaDetector.
+Code for running models, especially MegaDetector.
 
 
 ### envs
 
 Environment files... specifically .yml files for mamba/conda environments (these are what we recommend in our [MegaDetector User Guide](megadetector.md)), and a requirements.txt for the pip-inclined.
 
 
@@ -200,15 +201,15 @@
 ### md_utils
 
 Small utility functions for string manipulation, filename manipulation, downloading files from URLs, etc.  Mostly adapted from the [ai4eutils](https://github.com/microsoft/ai4eutils) repo.
 
 
 ### md_visualization
 
-Shared tools for visualizing images with ground truth and/or predicted annotations.
+Tools for visualizing images with ground truth and/or predicted annotations.
 
 
 ### sandbox
 
 Random things that don't fit in any other directory, but aren't quite deprecated.  Mostly postprocessing scripts that were built for a single use case but could potentially be useful in the future.
```

#### html2text {}

```diff
@@ -12,15 +12,15 @@
 fox](images/detector_example.jpg)
 Image credit University of Washington. ## How do I get started with
 MegaDetector? * If you are looking for a convenient tool to run MegaDetector,
 you don't need anything from this repository: check out [EcoAssist](https://
 github.com/PetervanLunteren/EcoAssist?tab=readme-ov-file). * If you're just
 considering the use of AI in your workflow, and you aren't even sure yet
 whether MegaDetector would be useful to you, we recommend reading the "[getting
-started with MegaDetector](collaborations.md)" page. * If you're already
+started with MegaDetector](getting-started.md)" page. * If you're already
 familiar with MegaDetector and you're ready to run it on your data, see the
 [MegaDetector User Guide](megadetector.md) for instructions on running
 MegaDetector. * If you're a programmer-type looking to use tools from this
 repo, check out the [Python package](https://pypi.org/project/megadetector/
 ) that provides access to everything in this repo (yes, you guessed it, "pip
 install megadetector"). * If you have any questions, or you want to tell us
 that MegaDetector was amazing/terrible on your images, _e_m_a_i_l_ _u_s! MegaDetector
@@ -28,15 +28,15 @@
 efficient with AI. If you want to learn about other ways to use AI to
 accelerate camera trap workflows, check out our of the field, affectionately
 titled [Everything I know about machine learning and camera traps](https://
 agentmorris.github.io/camera-trap-ml-survey/). ## Who is using MegaDetector?
 We work with ecologists all over the world to help them spend less time
 annotating images and more time thinking about conservation. You can read a
 little more about how this works on our [getting started with MegaDetector]
-(collaborations.md) page. Here are a few of the organizations that have used
+(getting-started.md) page. Here are a few of the organizations that have used
 MegaDetector... we're only listing organizations who (a) we know about and (b)
 have given us permission to refer to them here (or have posted publicly about
 their use of MegaDetector), so if you're using MegaDetector or other tools from
 this repo and would like to be added to this list, _e_m_a_i_l_ _u_s! * [Arizona
 Department of Environmental Quality](http://azdeq.gov/) * [Biometrio.earth]
 (https://biometrio.earth/) * [Blackbird Environmental](https://
 blackbirdenv.com/) * [Camelot](https://camelotproject.org/) * [Canadian Parks
@@ -87,43 +87,45 @@
 Society for the Protection of Birds (RSPB) * [Graeme Shannon's Research Group]
 (https://wildliferesearch.co.uk/group-1), Bangor University * [Grizzly Bear
 Recovery Program](https://www.fws.gov/office/grizzly-bear-recovery-program),
 U.S. Fish & Wildlife Service * [Hamaarag](https://hamaarag.org.il/), The
 Steinhardt Museum of Natural History, Tel Aviv University * [Institut des
 Science de la Fort Tempre](https://isfort.uqo.ca/) (ISFORT), Universit
 du Qubec en Outaouais * [Lab of Dr. Bilal Habib](https://bhlab.in/about), the
-Wildlife Institute of India * [Mammal Spatial Ecology and Conservation Lab]
-(https://labs.wsu.edu/dthornton/), Washington State University * [McLoughlin
-Lab in Population Ecology](http://mcloughlinlab.ca/lab/), University of
-Saskatchewan * [National Wildlife Refuge System, Southwest Region](https://
-www.fws.gov/about/region/southwest), U.S. Fish & Wildlife Service * [Northern
-Great Plains Program](https://nationalzoo.si.edu/news/restoring-americas-
-prairie), Smithsonian * [Polar Ecology Group](https://
-polarecologygroup.wordpress.com), University of Gdansk * [Quantitative Ecology
-Lab](https://depts.washington.edu/sefsqel/), University of Washington * [Santa
-Monica Mountains Recreation Area](https://www.nps.gov/samo/index.htm), National
-Park Service * [Seattle Urban Carnivore Project](https://www.zoo.org/
-seattlecarnivores), Woodland Park Zoo * [Serra dos rgos National Park]
-(https://www.icmbio.gov.br/parnaserradosorgaos/), ICMBio * [Snapshot USA]
-(https://emammal.si.edu/snapshot-usa), Smithsonian * [TROPECOLNET project]
-(https://www.anabenitezlopez.com/research/global-change-biology/tropecolnet/),
-Museo Nacional de Ciencias Naturales * [Wildlife Coexistence Lab](https://
-wildlife.forestry.ubc.ca/), University of British Columbia * [Wildlife
-Research](https://www.dfw.state.or.us/wildlife/research/index.asp), Oregon
-Department of Fish and Wildlife * [Wildlife Division](https://www.michigan.gov/
-dnr/about/contact/wildlife), Michigan Department of Natural Resources *
-Department of Ecology, TU Berlin * Ghost Cat Analytics * Protected Areas Unit,
-Canadian Wildlife Service * [School of Natural Sciences](https://
-www.utas.edu.au/natural-sciences), University of Tasmania ([story](https://
-www.utas.edu.au/about/news-and-stories/articles/2022/1204-innovative-camera-
-network-keeps-close-eye-on-tassie-wildlife)) * [Kenai National Wildlife Refuge]
-(https://www.fws.gov/refuge/kenai), U.S. Fish & Wildlife Service ([story]
-(https://www.peninsulaclarion.com/sports/refuge-notebook-new-technology-
-increases-efficiency-of-refuge-cameras/)) * [Idaho Department of Fish and Game]
-(https://idfg.idaho.gov/) ([fancy PBS video](https://www.youtube.com/
+Wildlife Institute of India * [Landscape Ecology Lab](https://www.concordia.ca/
+artsci/geography-planning-environment/research/labs/lel.html), Concordia
+University * [Mammal Spatial Ecology and Conservation Lab](https://
+labs.wsu.edu/dthornton/), Washington State University * [McLoughlin Lab in
+Population Ecology](http://mcloughlinlab.ca/lab/), University of Saskatchewan *
+[National Wildlife Refuge System, Southwest Region](https://www.fws.gov/about/
+region/southwest), U.S. Fish & Wildlife Service * [Northern Great Plains
+Program](https://nationalzoo.si.edu/news/restoring-americas-prairie),
+Smithsonian * [Polar Ecology Group](https://polarecologygroup.wordpress.com),
+University of Gdansk * [Quantitative Ecology Lab](https://depts.washington.edu/
+sefsqel/), University of Washington * [Santa Monica Mountains Recreation Area]
+(https://www.nps.gov/samo/index.htm), National Park Service * [Seattle Urban
+Carnivore Project](https://www.zoo.org/seattlecarnivores), Woodland Park Zoo *
+[Serra dos rgos National Park](https://www.icmbio.gov.br/
+parnaserradosorgaos/), ICMBio * [Snapshot USA](https://emammal.si.edu/snapshot-
+usa), Smithsonian * [TROPECOLNET project](https://www.anabenitezlopez.com/
+research/global-change-biology/tropecolnet/), Museo Nacional de Ciencias
+Naturales * [Wildlife Coexistence Lab](https://wildlife.forestry.ubc.ca/),
+University of British Columbia * [Wildlife Research](https://
+www.dfw.state.or.us/wildlife/research/index.asp), Oregon Department of Fish and
+Wildlife * [Wildlife Division](https://www.michigan.gov/dnr/about/contact/
+wildlife), Michigan Department of Natural Resources * Department of Ecology, TU
+Berlin * Ghost Cat Analytics * Protected Areas Unit, Canadian Wildlife Service
+* [School of Natural Sciences](https://www.utas.edu.au/natural-sciences),
+University of Tasmania ([story](https://www.utas.edu.au/about/news-and-stories/
+articles/2022/1204-innovative-camera-network-keeps-close-eye-on-tassie-
+wildlife)) * [Kenai National Wildlife Refuge](https://www.fws.gov/refuge/
+kenai), U.S. Fish & Wildlife Service ([story](https://www.peninsulaclarion.com/
+sports/refuge-notebook-new-technology-increases-efficiency-of-refuge-cameras/))
+* [Idaho Department of Fish and Game](https://idfg.idaho.gov/) ([fancy PBS
+video](https://www.youtube.com/
 watch?v=uEsL8EZKpbA&t=261s&ab_channel=OutdoorIdaho)) * [Australian Wildlife
 Conservancy](https://www.australianwildlife.org/) (blog posts [1](https://
 www.australianwildlife.org/cutting-edge-technology-delivering-efficiency-gains-
 in-conservation/), [2](https://www.australianwildlife.org/efficiency-gains-at-
 the-cutting-edge-of-technology/), [3](https://www.australianwildlife.org/
 federal-grant-to-fund-ai-supported-wildlife-recognisers)) * [Bavarian Forest
 National Park](https://www.nationalpark-bayerischer-wald.bayern.de/english/
@@ -229,22 +231,22 @@
 data_management Code for: * Converting frequently-used metadata formats to
 [COCO Camera Traps](https://github.com/agentmorris/MegaDetector/blob/main/
 data_management/README.md#coco-cameratraps-format) format * Converting the
 output of AI models (especially [YOLOv5](https://github.com/agentmorris/
 MegaDetector/blob/main/api/batch_processing/postprocessing/
 convert_output_format.py)) to the format used for AI results throughout this
 repo * Creating, visualizing, and editing COCO Camera Traps .json databases ###
-detection Code for training, running, and evaluating MegaDetector. ### envs
+detection Code for running models, especially MegaDetector. ### envs
 Environment files... specifically .yml files for mamba/conda environments
 (these are what we recommend in our [MegaDetector User Guide]
 (megadetector.md)), and a requirements.txt for the pip-inclined. ### images
 Media used in documentation. ### md_utils Small utility functions for string
 manipulation, filename manipulation, downloading files from URLs, etc. Mostly
 adapted from the [ai4eutils](https://github.com/microsoft/ai4eutils) repo. ###
-md_visualization Shared tools for visualizing images with ground truth and/or
+md_visualization Tools for visualizing images with ground truth and/or
 predicted annotations. ### sandbox Random things that don't fit in any other
 directory, but aren't quite deprecated. Mostly postprocessing scripts that were
 built for a single use case but could potentially be useful in the future. ###
 taxonomy_mapping Code to facilitate mapping data-set-specific category names
 (e.g. "lion", which means very different things in Idaho vs. South Africa) to a
 standard taxonomy. ### test_images A handful of images from [LILA](https://
 lila.science) that facilitate testing and debugging. ## Contact For questions
```

### Comparing `megadetector-5.0.8/api/batch_processing/api_core/batch_service/score.py` & `megadetector-5.0.9/api/batch_processing/api_core/batch_service/score.py`

 * *Files 0% similar despite different names*

```diff
@@ -431,10 +431,9 @@
         print(f'score.py, main(), score_images() duration: {duration}')
     except Exception as e:
         raise RuntimeError(f'score.py, main(), exception in score_images(): {e}')
 
     with open(task_output_path, 'w', encoding='utf-8') as f:
         json.dump(detections, f, ensure_ascii=False)
 
-
 if __name__ == '__main__':
     main()
```

### Comparing `megadetector-5.0.8/api/batch_processing/api_core/server.py` & `megadetector-5.0.9/api/batch_processing/api_core/server.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/batch_processing/api_core/server_api_config.py` & `megadetector-5.0.9/api/batch_processing/api_core/server_api_config.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/batch_processing/api_core/server_app_config.py` & `megadetector-5.0.9/api/batch_processing/api_core/server_app_config.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/batch_processing/api_core/server_batch_job_manager.py` & `megadetector-5.0.9/api/batch_processing/api_core/server_batch_job_manager.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/batch_processing/api_core/server_job_status_table.py` & `megadetector-5.0.9/api/batch_processing/api_core/server_job_status_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -144,10 +144,9 @@
 
     def test_read_invalid_id(self):
         table = JobStatusTable(TestJobStatusTable.api_instance)
         job_id = uuid.uuid4().hex  # should not be in the database
         item_read = table.read_job_status(job_id)
         self.assertIsNone(item_read)
 
-
 if __name__ == '__main__':
     unittest.main()
```

### Comparing `megadetector-5.0.8/api/batch_processing/api_core/server_orchestration.py` & `megadetector-5.0.9/api/batch_processing/api_core/server_orchestration.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/batch_processing/api_core/server_utils.py` & `megadetector-5.0.9/api/batch_processing/api_core/server_utils.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/batch_processing/api_core_support/aggregate_results_manually.py` & `megadetector-5.0.9/api/batch_processing/api_core_support/aggregate_results_manually.py`

 * *Files 0% similar despite different names*

```diff
@@ -38,10 +38,9 @@
                              request_name=args.request_name,
                              request_submission_timestamp='',
                              model_version=args.model_version)
     output_file_urls = aml_monitor.aggregate_results()
     output_file_urls_str = json.dumps(output_file_urls)
     print(output_file_urls_str)
 
-
 if __name__ == '__main__':
     main()
```

### Comparing `megadetector-5.0.8/api/batch_processing/api_support/summarize_daily_activity.py` & `megadetector-5.0.9/api/batch_processing/api_support/summarize_daily_activity.py`

 * *Files 1% similar despite different names*

```diff
@@ -144,10 +144,9 @@
     time.sleep(duration.seconds)
 
     while True:
         print(f'Woke up at {datetime.utcnow()}')
         send_message()
         time.sleep(24 * 60 * 60)
 
-
 if __name__ == '__main__':
     main()
```

### Comparing `megadetector-5.0.8/api/batch_processing/data_preparation/manage_local_batch.py` & `megadetector-5.0.9/api/batch_processing/data_preparation/manage_local_batch.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,68 +1,68 @@
-########
-#
-# manage_local_batch.py
-#    
-# Semi-automated process for managing a local MegaDetector job, including
-# standard postprocessing steps.
-#
-# This script is not intended to be run from top to bottom like a typical Python script,
-# it's a notebook disguised with a .py extension.  It's the Bestest Most Awesome way to
-# run MegaDetector, but it's also pretty subtle; if you want to play with this, you might
-# want to check in with cameratraps@lila.science for some tips.  Otherwise... YMMV.
-#
-# Some general notes on using this script, which I do in Spyder, though everything will be
-# the same if you are reading this in Jupyter Notebook (using the .ipynb version of the 
-# script):
-#
-# * Typically when I have a MegaDetector job to run, I make a copy of this script.  Let's 
-#   say I'm running a job for an organization called "bibblebop"; I have a big folder of
-#   job-specific copies of this script, and I might save a new one called "bibblebop-2023-07-26.py" 
-#   (the filename doesn't matter, it just helps me keep these organized).
-#
-# * There are three variables you need to set in this script before you start running code:
-#   "input_path", "organization_name_short", and "job_date".  You will get a sensible error if you forget 
-#   to set any of these.  In this case I might set those to "/data/bibblebobcamerastuff",
-#   "bibblebop", and "2023-07-26", respectively.
-#
-# * The defaults assume you want to split the job into two tasks (this is the default because I have 
-#   two GPUs).  Nothing bad will happen if you do this on a zero-GPU or single-GPU machine, but if you
-#   want everything to run in one logical task, change "n_gpus" and "n_jobs" to 1 (instead of 2).
-#
-# * After setting the required variables, I run the first few cells - up to and including the one 
-#   called "Generate commands" - which collectively take basically zero seconds.  After you run the
-#   "Generate commands" cell, you will have a folder that looks something like:
-#
-#   ~/postprocessing/bibblebop/bibblebop-2023-07-06-mdv5a/
-#  
-#   On Windows, this means:
-#
-#   ~/postprocessing/bibblebop/bibblebop-2023-07-06-mdv5a/    
-#
-#   Everything related to this job - scripts, outputs, intermediate stuff - will be in this folder.
-#   Specifically, after the "Generate commands" cell, you'll have scripts in that folder called something
-#   like:
-#
-#   run_chunk_000_gpu_00.sh (or .bat on Windows)
-#
-#   Personally, I like to run that script directly in a command prompt (I just leave Spyder open, though 
-#   it's OK if Spyder gets shut down while MD is running).  
-#
-#   At this point, once you get the hang of it, you've invested about zero seconds of human time,
-#   but possibly several days of unattended compute time, depending on the size of your job.
-#   
-# * Then when the jobs are done, back to the interactive environment!  I run the next few cells,
-#   which make sure the job finished OK, and the cell called "Post-processing (pre-RDE)", which 
-#   generates an HTML preview of the results.  You are very plausibly done at this point, and can ignore
-#   all the remaining cells.  If you want to do things like repeat detection elimination, or running 
-#   a classifier, or splitting your results file up in specialized ways, there are cells for all of those
-#   things, but now you're in power-user territory, so I'm going to leave this guide here.  Email
-#   cameratraps@lila.science with questions about the fancy stuff.
-#
-########
+"""
+
+manage_local_batch.py
+   
+Semi-automated process for managing a local MegaDetector job, including
+standard postprocessing steps.
+
+This script is not intended to be run from top to bottom like a typical Python script,
+it's a notebook disguised with a .py extension.  It's the Bestest Most Awesome way to
+run MegaDetector, but it's also pretty subtle; if you want to play with this, you might
+want to check in with cameratraps@lila.science for some tips.  Otherwise... YMMV.
+
+Some general notes on using this script, which I do in Spyder, though everything will be
+the same if you are reading this in Jupyter Notebook (using the .ipynb version of the 
+script):
+
+* Typically when I have a MegaDetector job to run, I make a copy of this script.  Let's 
+  say I'm running a job for an organization called "bibblebop"; I have a big folder of
+  job-specific copies of this script, and I might save a new one called "bibblebop-2023-07-26.py" 
+  (the filename doesn't matter, it just helps me keep these organized).
+
+* There are three variables you need to set in this script before you start running code:
+  "input_path", "organization_name_short", and "job_date".  You will get a sensible error if you forget 
+  to set any of these.  In this case I might set those to "/data/bibblebobcamerastuff",
+  "bibblebop", and "2023-07-26", respectively.
+
+* The defaults assume you want to split the job into two tasks (this is the default because I have 
+  two GPUs).  Nothing bad will happen if you do this on a zero-GPU or single-GPU machine, but if you
+  want everything to run in one logical task, change "n_gpus" and "n_jobs" to 1 (instead of 2).
+
+* After setting the required variables, I run the first few cells - up to and including the one 
+  called "Generate commands" - which collectively take basically zero seconds.  After you run the
+  "Generate commands" cell, you will have a folder that looks something like:
+
+  ~/postprocessing/bibblebop/bibblebop-2023-07-06-mdv5a/
+ 
+  On Windows, this means:
+
+  ~/postprocessing/bibblebop/bibblebop-2023-07-06-mdv5a/    
+
+  Everything related to this job - scripts, outputs, intermediate stuff - will be in this folder.
+  Specifically, after the "Generate commands" cell, you'll have scripts in that folder called something
+  like:
+
+  run_chunk_000_gpu_00.sh (or .bat on Windows)
+
+  Personally, I like to run that script directly in a command prompt (I just leave Spyder open, though 
+  it's OK if Spyder gets shut down while MD is running).  
+
+  At this point, once you get the hang of it, you've invested about zero seconds of human time,
+  but possibly several days of unattended compute time, depending on the size of your job.
+  
+* Then when the jobs are done, back to the interactive environment!  I run the next few cells,
+  which make sure the job finished OK, and the cell called "Post-processing (pre-RDE)", which 
+  generates an HTML preview of the results.  You are very plausibly done at this point, and can ignore
+  all the remaining cells.  If you want to do things like repeat detection elimination, or running 
+  a classifier, or splitting your results file up in specialized ways, there are cells for all of those
+  things, but now you're in power-user territory, so I'm going to leave this guide here.  Email
+  cameratraps@lila.science with questions about the fancy stuff.
+
+"""
 
 #%% Imports and constants
 
 import json
 import os
 import stat
 import time
@@ -799,15 +799,15 @@
     base_task_name + '_{:.3f}'.format(options.confidence_threshold))
 if render_animals_only:
     output_base = output_base + '_animals_only'
 
 os.makedirs(output_base, exist_ok=True)
 print('Processing to {}'.format(output_base))
 
-options.api_output_file = combined_api_output_file
+options.md_results_file = combined_api_output_file
 options.output_dir = output_base
 ppresults = process_batch_results(options)
 html_output_file = ppresults.output_html_file
 path_utils.open_file(html_output_file,attempt_to_open_in_wsl_host=True,browser_name='chrome')
 # import clipboard; clipboard.copy(html_output_file)
 
 
@@ -935,15 +935,15 @@
 
 if render_animals_only:
     output_base = output_base + '_render_animals_only'
 os.makedirs(output_base, exist_ok=True)
 
 print('Processing post-RDE to {}'.format(output_base))
 
-options.api_output_file = filtered_output_filename
+options.md_results_file = filtered_output_filename
 options.output_dir = output_base
 ppresults = process_batch_results(options)
 html_output_file = ppresults.output_html_file
 
 path_utils.open_file(html_output_file,attempt_to_open_in_wsl_host=True,browser_name='chrome')
 # import clipboard; clipboard.copy(html_output_file)
 
@@ -2010,15 +2010,15 @@
 folder_token = folder_token.replace('.json','_seqsmoothing')
 
 output_base = os.path.join(postprocessing_output_folder, folder_token + \
     base_task_name + '_{:.3f}'.format(options.confidence_threshold))
 os.makedirs(output_base, exist_ok=True)
 print('Processing {} to {}'.format(base_task_name, output_base))
 
-options.api_output_file = sequence_smoothed_classification_file
+options.md_results_file = sequence_smoothed_classification_file
 options.output_dir = output_base
 ppresults = process_batch_results(options)
 path_utils.open_file(ppresults.output_html_file,attempt_to_open_in_wsl_host=True,browser_name='chrome')
 # import clipboard; clipboard.copy(ppresults.output_html_file)
 
 #% Zip .json files
 
@@ -2130,15 +2130,15 @@
 #%% Preview large boxes
 
 output_base_large_boxes = os.path.join(postprocessing_output_folder, 
     base_task_name + '_{}_{:.3f}_size_separated_boxes'.format(rde_string, options.confidence_threshold))    
 os.makedirs(output_base_large_boxes, exist_ok=True)
 print('Processing post-RDE, post-size-separation to {}'.format(output_base_large_boxes))
 
-options.api_output_file = size_separated_file
+options.md_results_file = size_separated_file
 options.output_dir = output_base_large_boxes
 
 ppresults = process_batch_results(options)
 html_output_file = ppresults.output_html_file
 path_utils.open_file(html_output_file,attempt_to_open_in_wsl_host=True,browser_name='chrome')
```

### Comparing `megadetector-5.0.8/api/batch_processing/data_preparation/manage_video_batch.py` & `megadetector-5.0.9/api/batch_processing/data_preparation/manage_video_batch.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# manage_video_batch.py
-#
-# Notebook-esque script to manage the process of running a local batch of videos
-# through MD.  Defers most of the heavy lifting to manage_local_batch.py .
-#
-########
+"""
+
+manage_video_batch.py
+
+Notebook-esque script to manage the process of running a local batch of videos
+through MD.  Defers most of the heavy lifting to manage_local_batch.py .
+
+"""
 
 #%% Imports and constants
 
 import os
 
 from md_utils import path_utils
 from detection import video_utils
```

### Comparing `megadetector-5.0.8/api/batch_processing/integration/digiKam/xmp_integration.py` & `megadetector-5.0.9/api/batch_processing/integration/digiKam/xmp_integration.py`

 * *Files 0% similar despite different names*

```diff
@@ -456,11 +456,10 @@
         assert options.rename_conf is None, 'Command-line argument specified in GUI mode'
         assert options.rename_cat is None, 'Command-line argument specified in GUI mode'
         assert options.num_threads == 1, 'Command-line argument specified in GUI mode'
         create_gui(options)    
     else:
         process_input_data(options)
 
-
 if __name__ == '__main__':
     
     main()
```

### Comparing `megadetector-5.0.8/api/batch_processing/integration/eMammal/test_scripts/push_annotations_to_emammal.py` & `megadetector-5.0.9/api/batch_processing/integration/eMammal/test_scripts/push_annotations_to_emammal.py`

 * *Files 1% similar despite different names*

```diff
@@ -117,11 +117,10 @@
                       AND wild_id.image.raw_name = '{}' """.format(fn)
 
             
             print(sql)
             update_data(sql)
             mysql_connection.commit()
 
-
 if __name__ == '__main__':
     main()
```

### Comparing `megadetector-5.0.8/api/batch_processing/integration/eMammal/test_scripts/select_images_for_testing.py` & `megadetector-5.0.9/api/batch_processing/integration/eMammal/test_scripts/select_images_for_testing.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/add_max_conf.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/add_max_conf.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-########
-#
-# add_max_conf.py
-#
-# The MD output format included a "max_detection_conf" field with each image
-# up to and including version 1.2; it was removed as of version 1.3 (it's
-# redundant with the individual detection confidence values).
-#
-# Just in case someone took a dependency on that field, this script allows you
-# to add it back to an existing .json file.
-#
-########
+"""
+
+add_max_conf.py
+
+The MD output format included a "max_detection_conf" field with each image
+up to and including version 1.2; it was removed as of version 1.3 (it's
+redundant with the individual detection confidence values).
+
+Just in case someone took a dependency on that field, this script allows you
+to add it back to an existing .json file.
+
+"""
 
 #%% Imports and constants
 
 import os
 import json
 from md_utils import ct_utils
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/combine_api_outputs.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/combine_api_outputs.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,58 +1,60 @@
-########
-#
-# combine_api_outputs.py
-#
-# Merges two or more .json files in batch API output format, optionally
-# writing the results to another .json file.
-#
-# * Concatenates image lists, erroring if images are not unique.
-# * Errors if class lists are conflicting; errors on unrecognized fields.
-# * Checks compatibility in info structs, within reason.
-# 
-# File format:
-# 
-# https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing#batch-processing-api-output-format
-# 
-# Command-line use:
-# 
-# combine_api_outputs input1.json input2.json ... inputN.json output.json
-# 
-# Also see combine_api_shard_files() (not exposed via the command line yet) to
-# combine the intermediate files created by the API.
-# 
-# This does no checking for redundancy; if you are looking to ensemble
-# the results of multiple model versions, see merge_detections.py.
-# 
-########
+"""
+
+combine_api_outputs.py
+
+Merges two or more .json files in batch API output format, optionally
+writing the results to another .json file.
+
+* Concatenates image lists, erroring if images are not unique.
+* Errors if class lists are conflicting; errors on unrecognized fields.
+* Checks compatibility in info structs, within reason.
+
+File format:
+
+https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing#batch-processing-api-output-format
+
+Command-line use:
+
+combine_api_outputs input1.json input2.json ... inputN.json output.json
+
+Also see combine_api_shard_files() (not exposed via the command line yet) to
+combine the intermediate files created by the API.
+
+This does no checking for redundancy; if you are looking to ensemble
+the results of multiple model versions, see merge_detections.py.
+
+"""
 
 #%% Constants and imports
 
 import argparse
 import sys
 import json
-from typing import Any, Dict, Iterable, Mapping, List, Optional
 
 
 #%% Merge functions
 
-def combine_api_output_files(input_files: List[str],
-                             output_file: Optional[str] = None,
-                             require_uniqueness: bool = True,
-                             verbose: bool = True
-                             ) -> Dict[str, Any]:
+def combine_api_output_files(input_files,
+                             output_file=None,
+                             require_uniqueness=True,
+                             verbose=True):
     """
-    Merges list of JSON API detection files *input_files* into a single
-    dictionary, optionally writing the result to *output_file*.
+    Merges the list of MD results files [input_files] into a single
+    dictionary, optionally writing the result to [output_file].
 
     Args:
-        input_files: list of str, paths to JSON detection files
-        output_file: optional str, path to write merged JSON
-        require_uniqueness: bool, whether to require that the images in
+        input_files (list of str): paths to JSON detection files
+        output_file (str, optional): path to write merged JSON
+        require_uniqueness (bool): whether to require that the images in
             each list of images be unique
+            
+    Returns:
+        dict: merged dictionaries loaded from [input_files], identical to what's 
+        written to [output_file] if [output_file] is not None
     """
     
     def print_if_verbose(s):
         if verbose:
             print(s)
             
     input_dicts = []
@@ -69,35 +71,35 @@
     if output_file is not None:
         with open(output_file, 'w') as f:
             json.dump(merged_dict, f, indent=1)
 
     return merged_dict
 
 
-def combine_api_output_dictionaries(input_dicts: Iterable[Mapping[str, Any]],
-                                    require_uniqueness: bool = True
-                                    ) -> Dict[str, Any]:
+def combine_api_output_dictionaries(input_dicts, require_uniqueness=True):
     """
-    Merges the list of API detection dictionaries *input_dicts*.  See header
-    comment for details on merge rules.
+    Merges the list of MD results dictionaries [input_dicts] into a single dict.
+    See module header comment for details on merge rules.
 
     Args:
-        input_dicts: list of dicts, each dict is the JSON of the detections
-            output file from the Batch Processing API
-        require_uniqueness: bool, whether to require that the images in
-            each input dict be unique
+        input_dicts (list of dicts): list of dicts in which each dict represents the 
+            contents of a MD output file
+        require_uniqueness (bool): whether to require that the images in
+            each input dict be unique; if this is True and image filenames are
+            not unique, an error is raised.
 
-    Returns: dict, represents the merged JSON
+    Returns
+        dict: merged MD results
     """
     
     # Map image filenames to detections, we'll convert to a list later
     images = {}
-    info: Dict[str, str] = {}
-    detection_categories: Dict[str, str] = {}
-    classification_categories: Dict[str, str] = {}
+    info = {}
+    detection_categories = {}
+    classification_categories = {}
     n_redundant_images = 0
     n_images = 0
 
     known_fields = ['info', 'detection_categories', 'classification_categories',
                     'images']
 
     for input_dict in input_dicts:
@@ -178,16 +180,28 @@
     return merged_dict
 
 # ...combine_api_output_files()
 
 
 def combine_api_shard_files(input_files, output_file=None):
     """
-    Merges the list of .json-formatted API shard files *input_files* into a single
-    list of dictionaries, optionally writing the result to *output_file*.
+    Merges the list of .json-formatted API shard files [input_files] into a single
+    list of dictionaries, optionally writing the result to [output_file].
+    
+    This operates on mostly-deprecated API shard files, not MegaDetector results files.
+    If you don't know what an API shard file is, you don't want this function.
+    
+    Args:
+        input_files (list of str): files to merge
+        output_file (str, optiona): file to which we should write merged results
+        
+    Returns:
+        dict: merged results
+        
+    :meta private:
     """
 
     input_lists = []
     print('Loading input files')
     for fn in input_files:
         input_lists.append(json.load(open(fn)))
 
@@ -211,14 +225,15 @@
 
 # ...combine_api_shard_files()
 
 
 #%% Command-line driver
 
 def main():
+    
     parser = argparse.ArgumentParser()
     parser.add_argument(
         'input_paths', nargs='+',
         help='List of input .json files')
     parser.add_argument(
         'output_path',
         help='Output .json file')
@@ -226,10 +241,9 @@
     if len(sys.argv[1:]) == 0:
         parser.print_help()
         parser.exit()
 
     args = parser.parse_args()
     combine_api_output_files(args.input_paths, args.output_path)
 
-
 if __name__ == '__main__':
     main()
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/compare_batch_results.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/compare_batch_results.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-########
-#
-# compare_batch_results.py
-# 
-# Compare sets of batch results; typically used to compare:
-#
-# * Results from different MegaDetector versions
-# * Results before/after RDE
-# * Results with/without augmentation
-#
-# Makes pairwise comparisons, but can take lists of results files (will perform 
-# all pairwise comparisons).  Results are written to an HTML page that shows the number
-# and nature of disagreements (in the sense of each image being a detection or non-detection), 
-# with sample images for each category.
-# 
-########
+"""
+
+compare_batch_results.py
+
+Compare sets of batch results; typically used to compare:
+
+* Results from different MegaDetector versions
+* Results before/after RDE
+* Results with/without augmentation
+
+Makes pairwise comparisons, but can take lists of results files (will perform 
+all pairwise comparisons).  Results are written to an HTML page that shows the number
+and nature of disagreements (in the sense of each image being a detection or non-detection), 
+with sample images for each category.
+
+"""
 
 #%% Imports
 
 import json
 import os
 import random
 import copy
@@ -39,96 +39,135 @@
     
 class PairwiseBatchComparisonOptions:
     """
     Defines the options used for a single pairwise comparison; a list of these
     pairwise options sets is stored in the BatchComparisonsOptions class.
     """
     
+    #: First filename to compare
     results_filename_a = None
+    
+    #: Second filename to compare
     results_filename_b = None
     
+    #: Description to use in the output HTML for filename A
     results_description_a = None
+    
+    #: Description to use in the output HTML for filename B
     results_description_b = None
     
+    #: Per-class detection thresholds to use for filename A (including a 'default' threshold)
     detection_thresholds_a = {'animal':0.15,'person':0.15,'vehicle':0.15,'default':0.15}
+    
+    #: Per-class detection thresholds to use for filename B (including a 'default' threshold)
     detection_thresholds_b = {'animal':0.15,'person':0.15,'vehicle':0.15,'default':0.15}
 
+    #: Rendering threshold to use for all categories for filename A
     rendering_confidence_threshold_a = 0.1
+    
+    #: Rendering threshold to use for all categories for filename B
     rendering_confidence_threshold_b = 0.1
 
 # ...class PairwiseBatchComparisonOptions
 
 
 class BatchComparisonOptions:
     """
     Defines the options for a set of (possibly many) pairwise comparisons.
     """
     
+    #: Folder to which we should write HTML output
     output_folder = None
+    
+    #: Base folder for images (which are specified as relative files)
     image_folder = None
+    
+    #: Job name to use in the HTML output file
     job_name = ''
     
+    #: Maximum number of images to render for each category, where a "category" here is
+    #: "detections_a_only", "detections_b_only", etc., or None to render all images.
     max_images_per_category = 1000
+    
+    #: Maximum number of images per HTML page (paginates if a category page goes beyond this),
+    #: or None to disable pagination.
     max_images_per_page = None
+    
+    #: Colormap to use for detections in file A (maps detection categories to colors)
     colormap_a = ['Red']
+    
+    #: Colormap to use for detections in file B (maps detection categories to colors)
     colormap_b = ['RoyalBlue']
 
-    # Process-based parallelization isn't supported yet; this must be "True"
+    #: Process-based parallelization isn't supported yet; this must be "True"
     parallelize_rendering_with_threads = True
     
-    # List of filenames to include in the comparison, or None to use all files
+    #: List of filenames to include in the comparison, or None to use all files
     filenames_to_include = None
     
-    # Compare only detections/non-detections, ignore categories (still renders categories)
+    #: Compare only detections/non-detections, ignore categories (still renders categories)
     class_agnostic_comparison = False
     
+    #: Width of images to render in the output HTML
     target_width = 800
+    
+    #: Number of workers to use for rendering, or <=1 to disable parallelization
     n_rendering_workers = 20
+    
+    #: Random seed for image sampling (not used if max_images_per_category is None)
     random_seed = 0
     
-    # Default to sorting by filename
+    #: Whether to sort results by confidence; if this is False, sorts by filename
     sort_by_confidence = False
     
+    #: The expectation is that all results sets being compared will refer to the same images; if this
+    #: is True (default), we'll error if that's not the case, otherwise non-matching lists will just be
+    #: a warning.
     error_on_non_matching_lists = True
     
+    #: List of PairwiseBatchComparisonOptions that defines the comparisons we'll render.
     pairwise_options = []
     
 # ...class BatchComparisonOptions
     
 
 class PairwiseBatchComparisonResults:
     """
     The results from a single pairwise comparison.
     """
     
+    #: String of HTML content suitable for rendering to an HTML file
     html_content = None
+    
+    #: Possibly-modified version of the PairwiseBatchComparisonOptions supplied as input.
     pairwise_options = None
     
-    # A dictionary with keys including:
-    #
-    # common_detections
-    # common_non_detections
-    # detections_a_only
-    # detections_b_only
-    # class_transitions
+    #: A dictionary with keys including:
+    #:
+    #: common_detections
+    #: common_non_detections
+    #: detections_a_only
+    #: detections_b_only
+    #: class_transitions
     #
-    # Each of these maps a filename to a two-element list (the image in set A, the image in set B).
+    #: Each of these maps a filename to a two-element list (the image in set A, the image in set B).
     categories_to_image_pairs = None
 
 # ...class PairwiseBatchComparisonResults
     
     
 class BatchComparisonResults:
     """
     The results from a set of pairwise comparisons
     """
     
+    #: Filename containing HTML output
     html_output_file = None
     
-    # An list of PairwiseBatchComparisonResults
+    #: A list of PairwiseBatchComparisonResults
     pairwise_results = None
     
 # ...class BatchComparisonResults    
 
 
 main_page_style_header = """<head>
     <style type="text/css">
@@ -140,17 +179,28 @@
 
 main_page_header = '<html>\n{}\n<body>\n'.format(main_page_style_header)
 main_page_footer = '<br/><br/><br/></body></html>\n'
 
 
 #%% Comparison functions
 
-def render_image_pair(fn,image_pairs,category_folder,options,pairwise_options):
+def _render_image_pair(fn,image_pairs,category_folder,options,pairwise_options):
     """
     Render two sets of results (i.e., a comparison) for a single image.
+    
+    Args:
+        fn (str): image filename
+        image_pairs (dict): dict mapping filenames to pairs of image dicts
+        category_folder (str): folder to which to render this image, typically 
+            "detections_a_only", "detections_b_only", etc.
+        options (BatchComparisonOptions): job options
+        pairwise_options (PairwiseBatchComparisonOptions): pairwise comparison options
+            
+    Returns:
+        str: rendered image filename            
     """
     
     input_image_path = os.path.join(options.image_folder,fn)
     assert os.path.isfile(input_image_path), 'Image {} does not exist'.format(input_image_path)
     
     im = visualization_utils.open_image(input_image_path)
     image_pair = image_pairs[fn]
@@ -190,28 +240,30 @@
         custom_strings=custom_strings_b)
 
     output_image_fn = path_utils.flatten_path(fn)
     output_image_path = os.path.join(category_folder,output_image_fn)
     im.save(output_image_path)
     return output_image_path
 
-# ...def render_image_pair()
+# ...def _render_image_pair()
 
 
-def pairwise_compare_batch_results(options,output_index,pairwise_options):
+def _pairwise_compare_batch_results(options,output_index,pairwise_options):
     """
     The main entry point for this module is compare_batch_results(), which calls 
     this function for each pair of comparisons the caller has requested.  Generates an
     HTML page for this comparison.  Returns a BatchComparisonResults object.
     
-    options: an instance of BatchComparisonOptions
-    
-    output_index: a numeric index used for generating HTML titles
-    
-    pairwise_options: an instance of PairwiseBatchComparisonOptions    
+    Args:
+        options (BatchComparisonOptions): overall job options for this comparison group
+        output_index (int): a numeric index used for generating HTML titles    
+        pairwise_options (PairwiseBatchComparisonOptions): job options for this comparison
+        
+    Returns:
+        PairwiseBatchComparisonResults: the results of this pairwise comparison
     """
     
     # pairwise_options is passed as a parameter here, and should not be specified
     # in the options object.
     assert options.pairwise_options is None
     
     if options.random_seed is not None:
@@ -459,19 +511,19 @@
         category_folder = os.path.join(local_output_folder,category)
         os.makedirs(category_folder,exist_ok=True)
         
         # fn = image_filenames[0]
         if options.n_rendering_workers <= 1:
             output_image_paths = []
             for fn in tqdm(image_filenames):        
-                output_image_paths.append(render_image_pair(fn,image_pairs,category_folder,
+                output_image_paths.append(_render_image_pair(fn,image_pairs,category_folder,
                                                             options,pairwise_options))
         else:            
             output_image_paths = list(tqdm(pool.imap(
-                partial(render_image_pair, image_pairs=image_pairs, 
+                partial(_render_image_pair, image_pairs=image_pairs, 
                         category_folder=category_folder,options=options,
                         pairwise_options=pairwise_options),
                 image_filenames), 
                 total=len(image_filenames)))
         
         return output_image_paths
     
@@ -640,22 +692,28 @@
     
     pairwise_results.html_content = html_output_string
     pairwise_results.pairwise_options = pairwise_options
     pairwise_results.categories_to_image_pairs = categories_to_image_pairs
             
     return pairwise_results
         
-# ...def pairwise_compare_batch_results()
+# ...def _pairwise_compare_batch_results()
 
 
 def compare_batch_results(options):
     """
     The main entry point for this module.  Runs one or more batch results comparisons, 
-    writing results to an html page.  Most of the work is deferred to
-    pairwise_compare_batch_results().
+    writing results to an html page.  Most of the work is deferred to _pairwise_compare_batch_results().
+    
+    Args:
+        options (BatchComparisonOptions): job options to use for this comparison task, including the
+            list of specific pairswise comparisons to make (in the pairwise_options field)
+            
+    Returns:
+        BatchComparisonResults: the results of this comparison task
     """
     
     assert options.output_folder is not None
     assert options.image_folder is not None
     assert options.pairwise_options is not None
 
     options = copy.deepcopy(options)
@@ -671,15 +729,15 @@
     html_content = ''
     all_pairwise_results = []
     
     # i_comparison = 0; pairwise_options = pairwise_options_list[i_comparison]
     for i_comparison,pairwise_options in enumerate(pairwise_options_list):
         print('Running comparison {} of {}'.format(i_comparison,n_comparisons))
         pairwise_results = \
-            pairwise_compare_batch_results(options,i_comparison,pairwise_options)
+            _pairwise_compare_batch_results(options,i_comparison,pairwise_options)
         html_content += pairwise_results.html_content
         all_pairwise_results.append(pairwise_results)
 
     html_output_string = main_page_header
     job_name_string = ''
     if len(options.job_name) > 0:
         job_name_string = ' for {}'.format(options.job_name)
@@ -698,14 +756,26 @@
     return results
 
 
 def n_way_comparison(filenames,options,detection_thresholds=None,rendering_thresholds=None):
     """
     Performs N pairwise comparisons for the list of results files in [filenames], by generating
     sets of pairwise options and calling compare_batch_results.
+    
+    Args:
+        filenames (list): list of MD results filenames to compare
+        options (BatchComparisonOptions): task options set in which pairwise_options is still 
+            empty; that will get populated from [filenames]
+        detection_thresholds (list, optional): list of detection thresholds with the same length
+            as [filenames], or None to use sensible defaults
+        rendering_thresholds (list, optional): list of rendering thresholds with the same length
+            as [filenames], or None to use sensible defaults
+    
+    Returns:
+        BatchComparisonResults: the results of this comparison task
     """
     
     if detection_thresholds is None:
         detection_thresholds = [0.15] * len(filenames)
     assert len(detection_thresholds) == len(filenames)
 
     if rendering_thresholds is not None:
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/convert_output_format.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/convert_output_format.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,19 @@
-########
-#
-# convert_output_format.py
-#
-# Converts between file formats output by our batch processing API.  Currently
-# supports json <--> csv conversion, but this should be the landing place for any
-# conversion - including between hypothetical alternative .json versions - that we support 
-# in the future.
-#
-########
+"""
+
+convert_output_format.py
+
+Converts between file formats output by our batch processing API.  Currently
+supports json <--> csv conversion, but this should be the landing place for any
+conversion - including between hypothetical alternative .json versions - that we support 
+in the future.
+
+The .csv format is largely obsolete, don't use it unless you're super-duper sure you need it.
+
+"""
 
 #%% Constants and imports
 
 import argparse
 import json
 import csv
 import sys
@@ -29,21 +31,35 @@
 
 #%% Conversion functions
 
 def convert_json_to_csv(input_path,output_path=None,min_confidence=None,
                         omit_bounding_boxes=False,output_encoding=None,
                         overwrite=True):
     """
-    Convert .json to .csv
+    Converts a MD results .json file to a totally non-standard .csv format.
     
-    If output_path is None, will convert x.json to x.csv.
+    If [output_path] is None, will convert x.json to x.csv.
     
     TODO: this function should obviously be using Pandas or some other sensible structured
     representation of tabular data.  Even a list of dicts.  This implementation is quite
     brittle and depends on adding fields to every row in exactly the right order.
+    
+    Args:
+        input_path (str): the input .json file to convert
+        output_path (str, optional): the output .csv file to generate; if this is None, uses 
+            [input_path].csv
+        min_confidence (float, optional): the minimum-confidence detection we should include 
+            in the "detections" column; has no impact on the other columns
+        omit_bounding_boxes (bool): whether to leave out the json-formatted bounding boxes
+            that make up the "detections" column, which are not generally useful for someone who
+            wants to consume this data as a .csv file
+        output_encoding (str, optional): encoding to use for the .csv file
+        overwrite (bool): whether to overwrite an existing .csv file; if this is False and the
+            output file exists, no-ops and returns
+           
     """
     
     if output_path is None:
         output_path = os.path.splitext(input_path)[0]+'.csv'
         
     if os.path.isfile(output_path) and (not overwrite):
         print('File {} exists, skipping json --> csv conversion'.format(output_path))
@@ -54,19 +70,20 @@
 
     rows = []
     
     fixed_columns = ['image_path', 'max_confidence', 'detections']
     
     # We add an output column for each class other than 'empty', 
     # containing the maximum probability of  that class for each image
-    n_non_empty_detection_categories = len(annotation_constants.annotation_bbox_categories) - 1
+    # n_non_empty_detection_categories = len(annotation_constants.annotation_bbox_categories) - 1
+    n_non_empty_detection_categories = annotation_constants.NUM_DETECTOR_CATEGORIES
     detection_category_column_names = []
-    assert annotation_constants.annotation_bbox_category_id_to_name[0] == 'empty'
+    assert annotation_constants.detector_bbox_categories[0] == 'empty'
     for cat_id in range(1,n_non_empty_detection_categories+1):
-        cat_name = annotation_constants.annotation_bbox_category_id_to_name[cat_id]
+        cat_name = annotation_constants.detector_bbox_categories[cat_id]
         detection_category_column_names.append('max_conf_' + cat_name)
     
     n_classification_categories = 0
     
     if 'classification_categories' in json_output.keys():
         classification_category_id_to_name = json_output['classification_categories']
         classification_category_ids = list(classification_category_id_to_name.keys())
@@ -202,14 +219,22 @@
 
 # ...def convert_json_to_csv(...)
 
     
 def convert_csv_to_json(input_path,output_path=None,overwrite=True):
     """
     Convert .csv to .json.  If output_path is None, will convert x.csv to x.json.
+    
+    Args:
+        input_path (str): .csv filename to convert to .json
+        output_path (str, optional): the output .json file to generate; if this is None, uses 
+            [input_path].json
+        overwrite (bool): whether to overwrite an existing .json file; if this is False and the
+            output file exists, no-ops and returns    
+        
     """
     
     if output_path is None:
         output_path = os.path.splitext(input_path)[0]+'.json'
         
     if os.path.isfile(output_path) and (not overwrite):
         print('File {} exists, skipping csv --> json conversion'.format(output_path))
@@ -227,15 +252,15 @@
         "detector": "unknown",
         "detection_completion_time" : "unknown",
         "classifier": "unknown",
         "classification_completion_time": "unknown"
     }
     
     classification_categories = {}
-    detection_categories = annotation_constants.annotation_bbox_category_id_to_name
+    detection_categories = annotation_constants.detector_bbox_categories
 
     images = []
     
     # iFile = 0; row = df.iloc[iFile]
     for iFile,row in df.iterrows():
         
         image = {}
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/load_api_results.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/load_api_results.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-########
-#
-# load_api_results.py
-#
-# DEPRECATED
-#
-# As of 2023.12, this module is used in postprocessing and RDE.  Not recommended
-# for new code.
-#
-# Loads the output of the batch processing API (json) into a Pandas dataframe.
-#
-# Includes functions to read/write the (very very old) .csv results format.
-#
-########
+"""
+
+load_api_results.py
+
+DEPRECATED
+
+As of 2023.12, this module is used in postprocessing and RDE.  Not recommended
+for new code.
+
+Loads the output of the batch processing API (json) into a Pandas dataframe.
+
+Includes functions to read/write the (very very old) .csv results format.
+
+"""
 
 #%% Imports
 
 import json
 import os
 
 from typing import Dict, Mapping, Optional, Tuple
@@ -27,29 +27,28 @@
 
 #%% Functions for loading .json results into a Pandas DataFrame, and writing back to .json
 
 def load_api_results(api_output_path: str, normalize_paths: bool = True,
                      filename_replacements: Optional[Mapping[str, str]] = None,
                      force_forward_slashes: bool = True
                      ) -> Tuple[pd.DataFrame, Dict]:
-    """
+    r"""
     Loads json-formatted MegaDetector results to a Pandas DataFrame.
 
     Args:
         api_output_path: path to the output json file
         normalize_paths: whether to apply os.path.normpath to the 'file' field
             in each image entry in the output file
         filename_replacements: replace some path tokens to match local paths to
             the original blob structure
         force_forward_slashes: whether to convert backslashes to forward slashes
             in filenames
 
     Returns:
-        detection_results: pd.DataFrame, contains at least the columns:
-                ['file', 'detections','failure']
+        detection_results: pd.DataFrame, contains at least the columns ['file', 'detections','failure']
         other_fields: a dict containing fields in the results other than 'images'
     """
     
     print('Loading results from {}'.format(api_output_path))
 
     with open(api_output_path) as f:
         detection_results = json.load(f)
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/md_to_coco.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/md_to_coco.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-########
-#
-# md_to_coco.py
-#
-# "Converts" MegaDetector output files to COCO format.  "Converts" is in quotes because
-# this is an opinionated transformation that requires a confidence threshold.
-#
-# Does not currently handle classification information.
-#
-########
+"""
+
+md_to_coco.py
+
+"Converts" MegaDetector output files to COCO format.  "Converts" is in quotes because
+this is an opinionated transformation that requires a confidence threshold.
+
+Does not currently handle classification information.
+
+"""
 
 #%% Constants and imports
 
 import os
 import json
 import uuid
 
@@ -34,26 +34,36 @@
                include_failed_images=True):
     """
     "Converts" MegaDetector output files to COCO format.  "Converts" is in quotes because
     this is an opinionated transformation that requires a confidence threshold.
     
     A folder of images is required if width and height information are not available 
     in the MD results file.
+
+    Args:
+        md_results_file (str): MD results .json file to convert to COCO format
+        coco_output_file (str, optional): COCO .json file to write; if this is None, we'll return 
+            a COCO-formatted dict, but won't write it to disk
+        image_folder (str, optional): folder of images, required if 'width' and 'height' are not
+            present in the MD results file (they are not required by the format)
+        confidence_threshold (float, optional): boxes below this confidence threshold will not be
+            included in the output data
+        validate_image_sizes (bool, optional): if this is True, we'll check the image sizes 
+            regardless of whether "width" and "height" are present in the MD results file.
+        info (dict, optional): arbitrary metadata to include in an "info" field in the COCO-formatted
+            output
+        preserve_nonstandard_metadata (bool, optional): if this is True, confidence will be preserved in a 
+            non-standard "conf" field in each annotation, and any random fields present in each image's data
+            (e.g. EXIF metadata) will be propagated to COCO output    
+        include_failed_images (boo, optional): if this is True, failed images will be propagated to COCO output 
+            with a non-empty "failure" field and no other fields, otherwise failed images will be skipped.
     
-    If validate_image_sizes is True, we'll check the image sizes regardless of whether width
-    and height are present in the MD results file.
-    
-    If preserve_nonstandard_metadata is True, confidence will be preserved in a non-standard 
-    "conf" field in each annotation, and any random fields present in each image's data (e.g.
-    EXIF metadata) will be propagated to COCO output.
-    
-    If include_failed_images is True, failed images will be propagated to COCO output with
-    a non-empty "failure" field and no other fields, otherwise failed images will be skipped.
-    
-    Returns the COCO json dict.
+    Returns:
+        dict: the COCO data dict, identical to what's written to [coco_output_file] if [coco_output_file]
+        is not None.
     """
 
     with open(md_results_file,'r') as f:    
         md_results = json.load(f)    
         
     coco_images = []
     coco_annotations = []
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/md_to_labelme.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/md_to_labelme.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,54 +1,70 @@
-########
-#
-# md_to_labelme.py
-#
-# "Converts" a MegaDetector output .json file to labelme format (one .json per image
-# file).  "Convert" is in quotes because this is an opinionated transformation that 
-# requires a confidence threshold.
-#
-# TODO:
-#    
-# * support variable confidence thresholds across classes
-# * support classification data
-#
-########
+"""
+
+md_to_labelme.py
+
+"Converts" a MegaDetector output .json file to labelme format (one .json per image
+file).  "Convert" is in quotes because this is an opinionated transformation that 
+requires a confidence threshold.
+
+TODO:
+   
+* support variable confidence thresholds across classes
+* support classification data
+
+"""
 
 #%% Imports and constants
 
 import os
 import json
 
 from tqdm import tqdm
 
 from multiprocessing.pool import Pool
 from multiprocessing.pool import ThreadPool
 from functools import partial
 
 from md_visualization.visualization_utils import open_image
 from md_utils.ct_utils import truncate_float
+from detection.run_detector import DEFAULT_DETECTOR_LABEL_MAP
 
 output_precision = 3
 default_confidence_threshold = 0.15
 
 
 #%% Functions
 
-def get_labelme_dict_for_image(im,image_base_name,category_id_to_name,
+def get_labelme_dict_for_image(im,image_base_name=None,category_id_to_name=None,
                                info=None,confidence_threshold=None):
     """
     For the given image struct in MD results format, reformat the detections into
-    labelme format.  Returns a dict.
+    labelme format.
     
-    'height' and 'width' are required in [im].
+    Args:
+        im (dict): MegaDetector-formatted results dict, must include 'height' and 'width' fields
+        image_base_name (str, optional): written directly to the 'imagePath' field in the output; 
+            defaults to os.path.basename(im['file']).
+        category_id_to_name (dict, optional): maps string-int category IDs to category names, defaults
+            to the standard MD categories
+        info (dict, optional): arbitrary metadata to write to the "detector_info" field in the output
+            dict
+        confidence_threshold (float, optional): only detections at or above this confidence threshold
+            will be included in the output dict
     
-    image_base_name is written directly to the 'imagePath' field in the output; it should generally be
-    os.path.basename(your_image_file).
+    Return:
+        dict: labelme-formatted dictionary, suitable for writing directly to a labelme-formatted .json file
     """
     
+    if image_base_name is None:
+        image_base_name = os.path.basename(im['file'])
+        
+    if category_id_to_name:
+        category_id_to_name = DEFAULT_DETECTOR_LABEL_MAP
+        
     if confidence_threshold is None:        
         confidence_threshold = -1.0
      
     output_dict = {}
     if info is not None:
         output_dict['detector_info'] = info
     output_dict['version'] = '5.3.0a0'
@@ -126,15 +142,30 @@
                   overwrite=False,extension_prefix='',n_workers=1,
                   use_threads=False,bypass_image_size_read=False,
                   verbose=False):
     """
     For all the images in [results_file], write a .json file in labelme format alongside the
     corresponding relative path within image_base.
     
-    If non-empty, "extension_prefix" will be inserted before the .json extension.
+    Args:
+        results_file (str): MD results .json file to convert to Labelme format
+        image_base (str): folder of images; filenames in [results_file] should be relative to
+            this folder
+        confidence_threshold (float, optional): only detections at or above this confidence threshold
+            will be included in the output dict
+        overwrite (bool, optional): whether to overwrite existing output files; if this is False
+            and the output file for an image exists, we'll skip that image
+        extension_prefix (str, optional): if non-empty, "extension_prefix" will be inserted before the .json 
+            extension
+        n_workers (int, optional): enables multiprocessing if > 1
+        use_threads (bool, optional): if [n_workers] > 1, determines whether we parallelize via threads (True)
+            or processes (False)
+        bypass_image_size_read (bool, optional): if True, skips reading image sizes and trusts whatever is in
+            the MD results file (don't set this to "True" if your MD results file doesn't contain image sizes)
+        verbose (bool, optional): enables additionald ebug output    
     """
     
     if extension_prefix is None:
         extension_prefix = ''
         
     # Load MD results if necessary
     if isinstance(results_file,dict):
@@ -290,11 +321,10 @@
     if len(sys.argv[1:]) == 0:
         parser.print_help()
         parser.exit()
 
     args = parser.parse_args()
 
     md_to_labelme(args.results_file,args.image_base,args.confidence_threshold,args.overwrite)
-    
-    
+
 if __name__ == '__main__':
     main()
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/merge_detections.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/merge_detections.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-########
-#
-# merge_detections.py
-#
-# Merge high-confidence detections from one or more results files into another 
-# file.  Typically used to combine results from MDv5b and/or MDv4 into a "primary"
-# results file from MDv5a.
-#
-# Detection categories must be the same in both files; if you want to first remap
-# one file's category mapping to be the same as another's, see remap_detection_categories.
-#
-# If you want to literally merge two .json files, see combine_api_outputs.py.
-#
-########
+"""
+
+merge_detections.py
+
+Merge high-confidence detections from one or more results files into another 
+file.  Typically used to combine results from MDv5b and/or MDv4 into a "primary"
+results file from MDv5a.
+
+Detection categories must be the same in both files; if you want to first remap
+one file's category mapping to be the same as another's, see remap_detection_categories.
+
+If you want to literally merge two .json files, see combine_api_outputs.py.
+
+"""
 
 #%% Constants and imports
 
 import argparse
 import sys
 import json
 import os
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/postprocess_batch_results.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/postprocess_batch_results.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,24 +1,25 @@
-########
-#
-# postprocess_batch_results.py
-#
-# Given a .json or .csv file representing the output from the batch detection API,
-# do one or more of the following:
-# 
-# * Evaluate detector precision/recall, optionally rendering results (requires
-#   ground truth)
-# * Sample true/false positives/negatives and render to HTML (requires ground
-#   truth)
-# * Sample detections/non-detections and render to HTML (when ground truth isn't
-#   available)
-# 
-# Ground truth, if available, must be in the COCO Camera Traps format.
-#
-########
+"""
+
+postprocess_batch_results.py
+
+Given a .json or .csv file containing MD results, do one or more of the following:
+
+* Sample detections/non-detections and render to HTML (when ground truth isn't
+  available) (this is 99.9% of what this module is for)
+* Evaluate detector precision/recall, optionally rendering results (requires
+  ground truth)
+* Sample true/false positives/negatives and render to HTML (requires ground
+  truth)
+
+Ground truth, if available, must be in COCO Camera Traps format:
+    
+https://github.com/agentmorris/MegaDetector/blob/main/data_management/README.md#coco-camera-traps-format
+
+"""
 
 #%% Constants and imports
 
 import argparse
 import collections
 import copy
 import errno
@@ -26,15 +27,14 @@
 import os
 import sys
 import time
 import uuid
 import warnings
 import random
 
-from typing import Any, Dict, Iterable, Optional, Tuple
 from enum import IntEnum
 from multiprocessing.pool import ThreadPool
 from multiprocessing.pool import Pool
 from functools import partial
 
 import matplotlib.pyplot as plt
 import numpy as np
@@ -47,164 +47,193 @@
 import md_visualization.visualization_utils as vis_utils
 import md_visualization.plot_utils as plot_utils
 
 from md_utils.write_html_image_list import write_html_image_list
 from md_utils import path_utils
 from data_management.cct_json_utils import (CameraTrapJsonUtils, IndexedJsonDb)
 from api.batch_processing.postprocessing.load_api_results import load_api_results
-from md_utils.ct_utils import args_to_object
+from md_utils.ct_utils import args_to_object, sets_overlap
 
 from detection.run_detector import get_typical_confidence_threshold_from_results
 
 warnings.filterwarnings('ignore', '(Possibly )?corrupt EXIF data', UserWarning)
 
 
 #%% Options
 
 DEFAULT_NEGATIVE_CLASSES = ['empty']
 DEFAULT_UNKNOWN_CLASSES = ['unknown', 'unlabeled', 'ambiguous']
 
-
-def has_overlap(set1: Iterable, set2: Iterable) -> bool:
-    """
-    Check whether two sets overlap.
-    """
-    
-    return not set(set1).isdisjoint(set(set2))
-
-
 # Make sure there is no overlap between the two sets, because this will cause
 # issues in the code
-assert not has_overlap(DEFAULT_NEGATIVE_CLASSES, DEFAULT_UNKNOWN_CLASSES), (
+assert not sets_overlap(DEFAULT_NEGATIVE_CLASSES, DEFAULT_UNKNOWN_CLASSES), (
         'Default negative and unknown classes cannot overlap.')
 
 
 class PostProcessingOptions:
-
+    """
+    Options used to parameterize process_batch_results().
+    """
+    
     ### Required inputs
 
-    api_output_file = ''
+    #: MD results .json file to process
+    md_results_file = ''
+    
+    #: Folder to which we should write HTML output
     output_dir = ''
 
     ### Options
 
-    # Can be a folder or a SAS URL
+    #: Folder where images live (filenames in [md_results_file] should be relative to this folder)
     image_base_dir = '.'
 
-    ground_truth_json_file = ''
-
     ## These apply only when we're doing ground-truth comparisons
     
-    # Classes we'll treat as negative
-    #
-    # Include the token "#NO_LABELS#" to indicate that an image with no annotations
-    # should be considered empty.
+    #: Optional .json file containing ground truth information
+    ground_truth_json_file = ''
+
+    #: Classes we'll treat as negative
+    #:
+    #: Include the token "#NO_LABELS#" to indicate that an image with no annotations
+    #: should be considered empty.
     negative_classes = DEFAULT_NEGATIVE_CLASSES
     
-    # Classes we'll treat as neither positive nor negative
+    #: Classes we'll treat as neither positive nor negative
     unlabeled_classes = DEFAULT_UNKNOWN_CLASSES
 
-    # A list of output sets that we should count, but not render images for.
-    #
-    # Typically used to preview sets with lots of empties, where you don't want to
-    # subset but also don't want to render 100,000 empty images.
-    #
-    # detections, non_detections
-    # detections_animal, detections_person, detections_vehicle
+    #: A list of output sets that we should count, but not render images for.
+    #:
+    #: Typically used to preview sets with lots of empties, where you don't want to
+    #: subset but also don't want to render 100,000 empty images.
+    #:
+    #: detections, non_detections
+    #: detections_animal, detections_person, detections_vehicle
     rendering_bypass_sets = []
 
-    # If this is None, choose a confidence threshold based on the detector version.
-    #
-    # This can either be a float or a dictionary mapping category names (not IDs) to 
-    # thresholds.  The category "default" can be used to specify thresholds for 
-    # other categories.  Currently the use of a dict here is not supported when 
-    # ground truth is supplied.
+    #: If this is None, choose a confidence threshold based on the detector version.
+    #:
+    #: This can either be a float or a dictionary mapping category names (not IDs) to 
+    #: thresholds.  The category "default" can be used to specify thresholds for 
+    #: other categories.  Currently the use of a dict here is not supported when 
+    #: ground truth is supplied.
     confidence_threshold = None
     
-    # Confidence threshold to apply to classification (not detection) results
-    #
-    # Only a float is supported here (unlike the "confidence_threshold" parameter, which
-    # can be a dict).    
+    #: Confidence threshold to apply to classification (not detection) results
+    #:
+    #: Only a float is supported here (unlike the "confidence_threshold" parameter, which
+    #: can be a dict).    
     classification_confidence_threshold = 0.5
 
-    # Used for summary statistics only
+    #: Used for summary statistics only
     target_recall = 0.9
 
-    # Number of images to sample, -1 for "all images"
+    #: Number of images to sample, -1 for "all images"
     num_images_to_sample = 500
 
-    # Random seed for sampling, or None
-    sample_seed: Optional[int] = 0 # None
+    #: Random seed for sampling, or None
+    sample_seed = 0 # None
 
+    #: Image width for images in the HTML output
     viz_target_width = 800
 
+    #: Line width (in pixels) for rendering detections
     line_thickness = 4
+    
+    #: Box expansion (in pixels) for rendering detections
     box_expansion = 0
 
+    #: Job name to include in big letters in the output HTML
     job_name_string = None
+    
+    #: Model version string to include in the output HTML
     model_version_string = None
     
-    # Sort order for the output, should be one of "filename", "confidence", or "random"
+    #: Sort order for the output, should be one of "filename", "confidence", or "random"
     html_sort_order = 'filename'
    
+    #: If True, images in the output HTML will be links back to the original images
     link_images_to_originals = True
     
-    # Optionally separate detections into categories (animal/vehicle/human)
-    # 
-    # Currently only supported when ground truth is unavailable
+    #: Optionally separate detections into categories (animal/vehicle/human)
+    #: 
+    #: Currently only supported when ground truth is unavailable
     separate_detections_by_category = True
 
-    # Optionally replace one or more strings in filenames with other strings;
-    # useful for taking a set of results generated for one folder structure
-    # and applying them to a slightly different folder structure.
+    #: Optionally replace one or more strings in filenames with other strings;
+    #: useful for taking a set of results generated for one folder structure
+    #: and applying them to a slightly different folder structure.
     api_output_filename_replacements = {}
+    
+    #: Optionally replace one or more strings in filenames with other strings;
+    #: useful for taking a set of results generated for one folder structure
+    #: and applying them to a slightly different folder structure.
     ground_truth_filename_replacements = {}
 
-    # Allow bypassing API output loading when operating on previously-loaded
-    # results
-    api_detection_results: Optional[pd.DataFrame] = None
-    api_other_fields: Optional[Dict[str, Any]] = None
-
-    # Should we also split out a separate report about the detections that were
-    # just below our main confidence threshold?
-    #
-    # Currently only supported when ground truth is unavailable
+    #: Allow bypassing API output loading when operating on previously-loaded
+    #: results.  If present, this is a Pandas DataFrame.  Almost never useful.
+    api_detection_results = None
+    
+    #: Allow bypassing API output loading when operating on previously-loaded
+    #: results.  If present, this is a str --> obj dict.  Almost never useful.
+    api_other_fields = None
+
+    #: Should we also split out a separate report about the detections that were
+    #: just below our main confidence threshold?
+    #:
+    #: Currently only supported when ground truth is unavailable.
     include_almost_detections = False
     
-    # Only a float is supported here (unlike the "confidence_threshold" parameter, which
-    # can be a dict).
+    #: Only a float is supported here (unlike the "confidence_threshold" parameter, which
+    #: can be a dict).
     almost_detection_confidence_threshold = None
 
-    # Control rendering parallelization
-    parallelize_rendering_n_cores: Optional[int] = 100
-    parallelize_rendering_with_threads = True
+    #: Enable/disable rendering parallelization    
     parallelize_rendering = False
     
+    #: Number of threads/processes to use for rendering parallelization
+    parallelize_rendering_n_cores = 25
+    
+    #: Whether to use threads (True) or processes (False) for rendering parallelization
+    parallelize_rendering_with_threads = True
+    
+    #: When classification results are present, should be sort alphabetically by class name (False)
+    #: or in descending order by frequency (True)?
     sort_classification_results_by_count = False    
     
-    # Should we split individual pages up into smaller pages if there are more than
-    # N images?
+    #: Should we split individual pages up into smaller pages if there are more than
+    #: N images?
     max_figures_per_html_file = None
     
 # ...PostProcessingOptions
 
 
 class PostProcessingResults:
-
+    """
+    Return format from process_batch_results
+    """
+    
+    #: HTML file to which preview information was written
     output_html_file = ''
-    api_detection_results: Optional[pd.DataFrame] = None
-    api_other_fields: Optional[Dict[str, Any]] = None
+    
+    #: Pandas Dataframe containing detection results
+    api_detection_results = None
+    
+    #: str --> obj dictionary containing other information loaded from the results file
+    api_other_fields = None
 
 
 ##%% Helper classes and functions
 
 class DetectionStatus(IntEnum):
     """
     Flags used to mark images as positive or negative for P/R analysis
     (according to ground truth and/or detector output)
+        
+    :meta private:
     """
     
     DS_NEGATIVE = 0
     DS_POSITIVE = 1
 
     # Anything greater than this isn't clearly positive or negative
     DS_MAX_DEFINITIVE_VALUE = DS_POSITIVE
@@ -219,19 +248,17 @@
     DS_UNASSIGNED = 4
 
     # In some analyses, we add an additional class that lets us look at
     # detections just below our main confidence threshold
     DS_ALMOST = 5
 
 
-def mark_detection_status(
-        indexed_db: IndexedJsonDb,
-        negative_classes: Iterable[str] = DEFAULT_NEGATIVE_CLASSES,
-        unknown_classes: Iterable[str] = DEFAULT_UNKNOWN_CLASSES
-        ) -> Tuple[int, int, int, int]:
+def _mark_detection_status(indexed_db,
+                           negative_classes=DEFAULT_NEGATIVE_CLASSES,
+                           unknown_classes=DEFAULT_UNKNOWN_CLASSES):
     """
     For each image in indexed_db.db['images'], add a '_detection_status' field
     to indicate whether to treat this image as positive, negative, ambiguous,
     or unknown.
 
     Makes modifications in-place.
 
@@ -255,16 +282,16 @@
         categories = [ann['category_id'] for ann in annotations]
         category_names = set(indexed_db.cat_id_to_name[cat] for cat in categories)
 
         # Check whether this image has:
         # - unknown / unassigned-type labels
         # - negative-type labels
         # - positive labels (i.e., labels that are neither unknown nor negative)
-        has_unknown_labels = has_overlap(category_names, unknown_classes)
-        has_negative_labels = has_overlap(category_names, negative_classes)
+        has_unknown_labels = sets_overlap(category_names, unknown_classes)
+        has_negative_labels = sets_overlap(category_names, negative_classes)
         has_positive_labels = 0 < len(category_names - (unknown_classes | negative_classes))
         # assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)
 
         # If there are no image annotations...
         if len(categories) == 0:
                         
             if '#NO_LABELS#' in negative_classes:
@@ -311,31 +338,35 @@
         else:
             raise Exception('Invalid detection state')
 
     # ...for each image
 
     return n_negative, n_positive, n_unknown, n_ambiguous
 
-# ...mark_detection_status()
+# ..._mark_detection_status()
 
 
-def is_sas_url(s: str) -> bool:
+def is_sas_url(s) -> bool:
     """
     Placeholder for a more robust way to verify that a link is a SAS URL.
     99.999% of the time this will suffice for what we're using it for right now.
+    
+    :meta private:
     """
     
     return (s.startswith(('http://', 'https://')) and ('core.windows.net' in s)
             and ('?' in s))
 
 
-def relative_sas_url(folder_url: str, relative_path: str) -> Optional[str]:
+def relative_sas_url(folder_url, relative_path):
     """
     Given a container-level or folder-level SAS URL, create a SAS URL to the
     specified relative path.
+    
+    :meta private:
     """
     
     relative_path = relative_path.replace('%','%25')
     relative_path = relative_path.replace('#','%23')
     relative_path = relative_path.replace(' ','%20')
 
     if not is_sas_url(folder_url):
@@ -345,26 +376,29 @@
     if not tokens[0].endswith('/'):
         tokens[0] = tokens[0] + '/'
     if relative_path.startswith('/'):
         relative_path = relative_path[1:]
     return tokens[0] + relative_path + '?' + tokens[1]
 
 
-def render_bounding_boxes(
+def _render_bounding_boxes(
         image_base_dir,
         image_relative_path,
         display_name,
         detections,
         res,
         ground_truth_boxes=None,
         detection_categories=None,
         classification_categories=None,
         options=None):
     """
     Renders detection bounding boxes on a single image.
+    
+    This is an internal function; if you want tools for rendering boxes on images, see 
+    md_visualization.visualization_utils.
 
     The source image is:
 
         image_base_dir / image_relative_path
 
     The target image is, for example:
 
@@ -375,14 +409,16 @@
     "res" is a result type, e.g. "detections", "non-detections"; this determines the
     output folder for the rendered image.
     
     Only very preliminary support is provided for ground truth box rendering.
     
     Returns the html info struct for this image in the format that's used for
     write_html_image_list.
+    
+    :meta private:
     """
 
     if options is None:
         options = PostProcessingOptions()
 
     # Leaving code in place for reading from blob storage, may support this
     # in the future.
@@ -444,15 +480,15 @@
             else:
                 category_ids = set()
                 for d in detections:
                     category_ids.add(d['category'])
                 rendering_confidence_threshold = {}
                 for category_id in category_ids:
                     rendering_confidence_threshold[category_id] = \
-                        get_threshold_for_category_id(category_id, options, detection_categories)
+                        _get_threshold_for_category_id(category_id, options, detection_categories)
                 
             vis_utils.render_detection_bounding_boxes(
                 detections, image,
                 label_map=detection_categories,
                 classification_label_map=classification_categories,
                 confidence_threshold=rendering_confidence_threshold,
                 thickness=options.line_thickness,
@@ -489,18 +525,18 @@
         # link_target = image_full_path.replace('\\','/')
         # link_target  = urllib.parse.quote(link_target)
         link_target = image_full_path
         info['linkTarget'] = link_target 
         
     return info
 
-# ...render_bounding_boxes
+# ..._render_bounding_boxes
 
 
-def prepare_html_subpages(images_html, output_dir, options=None):
+def _prepare_html_subpages(images_html, output_dir, options=None):
     """
     Write out a series of html image lists, e.g. the "detections" or "non-detections"
     pages.
 
     image_html is a dictionary mapping an html page name (e.g. "detections_animal") to 
     a list of image structs friendly to write_html_image_list.
     
@@ -558,19 +594,21 @@
             write_html_image_list(
                 filename=os.path.join(output_dir, '{}.html'.format(res)),
                 images=array,
                 options=html_image_list_options)
 
     return image_counts
 
-# ...prepare_html_subpages()
+# ..._prepare_html_subpages()
 
 
-# Determine the confidence threshold we should use for a specific category name
-def get_threshold_for_category_name(category_name,options):
+def _get_threshold_for_category_name(category_name,options):
+    """
+    Determines the confidence threshold we should use for a specific category name.
+    """
     
     if isinstance(options.confidence_threshold,float):
         return options.confidence_threshold
     else:
         assert isinstance(options.confidence_threshold,dict), \
             'confidence_threshold must either be a float or a dict'
                            
@@ -581,87 +619,96 @@
     else:
         assert 'default' in options.confidence_threshold, \
             'category {} not in confidence_threshold dict, and no default supplied'.format(
                 category_name)
         return options.confidence_threshold['default']
 
     
-# Determine the confidence threshold we should use for a specific category ID
-#
-# detection_categories is a dict mapping category IDs to names.
-def get_threshold_for_category_id(category_id,options,detection_categories):
+def _get_threshold_for_category_id(category_id,options,detection_categories):
+    """
+    Determines the confidence threshold we should use for a specific category ID.
+    
+    [detection_categories] is a dict mapping category IDs to names.
+    """
     
     if isinstance(options.confidence_threshold,float):        
         return options.confidence_threshold
     
     assert category_id in detection_categories, \
         'Invalid category ID {}'.format(category_id)
         
     category_name = detection_categories[category_id]
     
-    return get_threshold_for_category_name(category_name,options)
+    return _get_threshold_for_category_name(category_name,options)
+    
+    
+def _get_positive_categories(detections,options,detection_categories):
+    """
+    Gets a sorted list of unique categories (as string IDs) above the threshold for this image
     
+    [detection_categories] is a dict mapping category IDs to names.
+    """
     
-# Get a sorted list of unique categories (as string IDs) above the threshold for this image
-#
-# "detection_categories" is a dict mapping category IDs to names.
-def get_positive_categories(detections,options,detection_categories):
     positive_categories = set()
     for d in detections:
-        threshold = get_threshold_for_category_id(d['category'], options, detection_categories)
+        threshold = _get_threshold_for_category_id(d['category'], options, detection_categories)
         if d['conf'] >= threshold:
             positive_categories.add(d['category'])
     return sorted(positive_categories)
 
 
-# Determine whether any positive detections are present in the detection list
-# [detections].
-def has_positive_detection(detections,options,detection_categories):
+def _has_positive_detection(detections,options,detection_categories):
+    """
+    Determines whether any positive detections are present in the detection list
+    [detections].
+    """    
     
     found_positive_detection = False
     for d in detections:
-        threshold = get_threshold_for_category_id(d['category'], options, detection_categories)
+        threshold = _get_threshold_for_category_id(d['category'], options, detection_categories)
         if d['conf'] >= threshold:
             found_positive_detection = True
             break
     return found_positive_detection
         
 
-# Render an image (with no ground truth information)
-#
-# Returns a list of rendering structs, where the first item is a category (e.g. "detections_animal"), 
-# and  the second is a dict of information needed for rendering.  E.g.:
-#
-# [['detections_animal', 
-# {
-#  'filename': 'detections_animal/detections_animal_blah~01060415.JPG', 
-#  'title': '<b>Result type</b>: detections_animal, 
-#            <b>Image</b>: blah\\01060415.JPG,
-#            <b>Max conf</b>: 0.897',
-#   'textStyle': 'font-family:verdana,arial,calibri;font-size:80%;text-align:left;margin-top:20;margin-bottom:5',
-#   'linkTarget': 'full_path_to_%5C01060415.JPG'
-# }]]
-# 
-# When no classification data is present, this list will always be length-1.  When
-# classification data is present, an image may appear in multiple categories.
-#
-# Populates the 'max_conf' field of the first element of the list.
-#
-# Returns None if there are any errors.
-def render_image_no_gt(file_info,detection_categories_to_results_name,
+def _render_image_no_gt(file_info,detection_categories_to_results_name,
                        detection_categories,classification_categories,
                        options):
-
+    """
+    Renders an image (with no ground truth information)
+    
+    Returns a list of rendering structs, where the first item is a category (e.g. "detections_animal"), 
+    and  the second is a dict of information needed for rendering.  E.g.:
+        
+    [['detections_animal', 
+    {
+     'filename': 'detections_animal/detections_animal_blah~01060415.JPG', 
+     'title': '<b>Result type</b>: detections_animal, 
+               <b>Image</b>: blah\\01060415.JPG,
+               <b>Max conf</b>: 0.897',
+      'textStyle': 'font-family:verdana,arial,calibri;font-size:80%;text-align:left;margin-top:20;margin-bottom:5',
+      'linkTarget': 'full_path_to_%5C01060415.JPG'
+    }]]
+     
+    When no classification data is present, this list will always be length-1.  When
+    classification data is present, an image may appear in multiple categories.
+    
+    Populates the 'max_conf' field of the first element of the list.
+    
+    Returns None if there are any errors.
+    """
+    
     image_relative_path = file_info[0]
     max_conf = file_info[1]
     detections = file_info[2]
 
     # Determine whether any positive detections are present (using a threshold that
     # may vary by category)
-    found_positive_detection = has_positive_detection(detections,options,detection_categories)
+    found_positive_detection = _has_positive_detection(detections,options,detection_categories)
         
     detection_status = DetectionStatus.DS_UNASSIGNED
     if found_positive_detection:
         detection_status = DetectionStatus.DS_POSITIVE
     else:
         if options.include_almost_detections:
             if max_conf >= options.almost_detection_confidence_threshold:
@@ -669,15 +716,15 @@
             else:
                 detection_status = DetectionStatus.DS_NEGATIVE
         else:
             detection_status = DetectionStatus.DS_NEGATIVE
 
     if detection_status == DetectionStatus.DS_POSITIVE:
         if options.separate_detections_by_category:
-            positive_categories = tuple(get_positive_categories(detections,options,detection_categories))
+            positive_categories = tuple(_get_positive_categories(detections,options,detection_categories))
             if positive_categories not in detection_categories_to_results_name:
                 raise ValueError('Error: {} not in category mapping (file {})'.format(
                     str(positive_categories),image_relative_path))
             res = detection_categories_to_results_name[positive_categories]
         else:
             res = 'detections'
 
@@ -691,15 +738,15 @@
         res, image_relative_path, max_conf)
 
     rendering_options = copy.copy(options)
     if detection_status == DetectionStatus.DS_ALMOST:
         rendering_options.confidence_threshold = \
             rendering_options.almost_detection_confidence_threshold
             
-    rendered_image_html_info = render_bounding_boxes(
+    rendered_image_html_info = _render_bounding_boxes(
         image_base_dir=options.image_base_dir,
         image_relative_path=image_relative_path,
         display_name=display_name,
         detections=detections,
         res=res,
         ground_truth_boxes=None,
         detection_categories=detection_categories,
@@ -739,26 +786,28 @@
 
             # ...if this detection has classification info
 
         # ...for each detection
 
         image_result[0][1]['max_conf'] = max_conf
         
-    # ...if we got valid rendering info back from render_bounding_boxes()
+    # ...if we got valid rendering info back from _render_bounding_boxes()
     
     return image_result
 
-# ...def render_image_no_gt()
+# ...def _render_image_no_gt()
     
 
-# Render an image with ground truth information.  See render_image_no_gt for return
-# data format.
-def render_image_with_gt(file_info,ground_truth_indexed_db,
+def _render_image_with_gt(file_info,ground_truth_indexed_db,
                          detection_categories,classification_categories,options):
-
+    """
+    Render an image with ground truth information.  See _render_image_no_gt for return
+    data format.
+    """
+    
     image_relative_path = file_info[0]
     max_conf = file_info[1]
     detections = file_info[2]
 
     # This should already have been normalized to either '/' or '\'
 
     image_id = ground_truth_indexed_db.filename_to_id.get(image_relative_path, None)
@@ -776,24 +825,24 @@
             ground_truth_box.append(ann['category_id'])
             ground_truth_boxes.append(ground_truth_box)
     
     gt_status = image['_detection_status']
 
     gt_presence = bool(gt_status)
 
-    gt_classes = CameraTrapJsonUtils.annotations_to_classnames(
+    gt_classes = CameraTrapJsonUtils.annotations_to_class_names(
         annotations, ground_truth_indexed_db.cat_id_to_name)
     gt_class_summary = ','.join(gt_classes)
 
     if gt_status > DetectionStatus.DS_MAX_DEFINITIVE_VALUE:
         print(f'Skipping image {image_id}, does not have a definitive '
               f'ground truth status (status: {gt_status}, classes: {gt_class_summary})')
         return None
 
-    detected = has_positive_detection(detections, options, detection_categories)
+    detected = _has_positive_detection(detections, options, detection_categories)
 
     if gt_presence and detected:
         if '_classification_accuracy' not in image.keys():
             res = 'tp'
         elif np.isclose(1, image['_classification_accuracy']):
             res = 'tpc'
         else:
@@ -805,15 +854,15 @@
     else:
         res = 'tn'
 
     display_name = '<b>Result type</b>: {}, <b>Presence</b>: {}, <b>Class</b>: {}, <b>Max conf</b>: {:0.3f}%, <b>Image</b>: {}'.format(
         res.upper(), str(gt_presence), gt_class_summary,
         max_conf * 100, image_relative_path)
 
-    rendered_image_html_info = render_bounding_boxes(
+    rendered_image_html_info = _render_bounding_boxes(
         image_base_dir=options.image_base_dir,
         image_relative_path=image_relative_path,
         display_name=display_name,
         detections=detections,
         res=res,
         ground_truth_boxes=ground_truth_boxes,
         detection_categories=detection_categories,
@@ -824,22 +873,43 @@
     if len(rendered_image_html_info) > 0:
         image_result = [[res, rendered_image_html_info]]
         for gt_class in gt_classes:
             image_result.append(['class_{}'.format(gt_class), rendered_image_html_info])
 
     return image_result
 
-# ...def render_image_with_gt()
+# ...def _render_image_with_gt()
 
     
 #%% Main function
 
-def process_batch_results(options: PostProcessingOptions
-                          ) -> PostProcessingResults:
+def process_batch_results(options):
 
+    """
+    Given a .json or .csv file containing MD results, do one or more of the following:
+
+    * Sample detections/non-detections and render to HTML (when ground truth isn't
+      available) (this is 99.9% of what this module is for)
+    * Evaluate detector precision/recall, optionally rendering results (requires
+      ground truth)
+    * Sample true/false positives/negatives and render to HTML (requires ground
+      truth)
+
+    Ground truth, if available, must be in COCO Camera Traps format:
+        
+    https://github.com/agentmorris/MegaDetector/blob/main/data_management/README.md#coco-camera-traps-format
+
+    Args:
+        options (PostProcessingOptions): everything we need to render a preview/analysis for
+            this set of results; see the PostProcessingOptions class for details.
+           
+    Returns:
+        PostProcessingResults: information about the results/preview, most importantly the HTML filename
+            of the output.  See the PostProcessingResults class for details.
+    """
     ppresults = PostProcessingResults()
 
     ##%% Expand some options for convenience
 
     output_dir = options.output_dir
 
 
@@ -848,15 +918,15 @@
     os.makedirs(output_dir, exist_ok=True)
 
 
     ##%% Load ground truth if available
 
     ground_truth_indexed_db = None
 
-    if (options.ground_truth_json_file is not None):
+    if (options.ground_truth_json_file is not None) and (len(options.ground_truth_json_file) > 0):
         assert (options.confidence_threshold is None) or (isinstance(options.confidence_threshold,float)), \
             'Variable confidence thresholds are not supported when supplying ground truth'
             
     if (options.ground_truth_json_file is not None) and (len(options.ground_truth_json_file) > 0):
 
         if options.separate_detections_by_category:
             print("Warning: I don't know how to separate categories yet when doing " + \
@@ -864,15 +934,15 @@
             options.separate_detections_by_category = False
 
         ground_truth_indexed_db = IndexedJsonDb(
             options.ground_truth_json_file, b_normalize_paths=True,
             filename_replacements=options.ground_truth_filename_replacements)
 
         # Mark images in the ground truth as positive or negative
-        n_negative, n_positive, n_unknown, n_ambiguous = mark_detection_status(
+        n_negative, n_positive, n_unknown, n_ambiguous = _mark_detection_status(
             ground_truth_indexed_db, negative_classes=options.negative_classes,
             unknown_classes=options.unlabeled_classes)
         print(f'Finished loading and indexing ground truth: {n_negative} '
               f'negative, {n_positive} positive, {n_unknown} unknown, '
               f'{n_ambiguous} ambiguous')
 
 
@@ -896,15 +966,18 @@
     
     if options.confidence_threshold is None:
         options.confidence_threshold = \
             get_typical_confidence_threshold_from_results(other_fields)
         print('Choosing default confidence threshold of {} based on MD version'.format(
             options.confidence_threshold))    
             
-    if options.almost_detection_confidence_threshold is None:
+    if options.almost_detection_confidence_threshold is None and options.include_almost_detections:
+        assert isinstance(options.confidence_threshold,float), \
+            'If you are using a dictionary of confidence thresholds and almost-detections are enabled, ' + \
+            'you need to supply a threshold for almost detections.'
         options.almost_detection_confidence_threshold = options.confidence_threshold - 0.05
         if options.almost_detection_confidence_threshold < 0:
             options.almost_detection_confidence_threshold = 0
             
     # Remove rows with inference failures (typically due to corrupt images)
     n_failures = 0
     if 'failure' in detections_df.columns:
@@ -930,15 +1003,15 @@
     n_positives = 0
     n_almosts = 0
     
     for i_row,row in tqdm(detections_df.iterrows(),total=len(detections_df)):
         
         detections = row['detections']
         max_conf = row['max_detection_conf']
-        if has_positive_detection(detections, options, detection_categories):
+        if _has_positive_detection(detections, options, detection_categories):
             n_positives += 1
         elif (options.almost_detection_confidence_threshold is not None) and \
              (max_conf >= options.almost_detection_confidence_threshold):
             n_almosts += 1        
         
     print(f'Finished loading and preprocessing {len(detections_df)} rows '
           f'from detector output, predicted {n_positives} positives.')
@@ -1280,23 +1353,23 @@
                 else:
                     pool = Pool(options.parallelize_rendering_n_cores)
                     worker_string = 'processes'
                 print('Rendering images with {} {}'.format(options.parallelize_rendering_n_cores,
                                                            worker_string))
                 
             rendering_results = list(tqdm(pool.imap(
-                partial(render_image_with_gt,
+                partial(_render_image_with_gt,
                         ground_truth_indexed_db=ground_truth_indexed_db,
                         detection_categories=detection_categories,
                         classification_categories=classification_categories,
                         options=options), 
                 files_to_render), total=len(files_to_render)))
         else:
             for file_info in tqdm(files_to_render):
-                rendering_results.append(render_image_with_gt(
+                rendering_results.append(_render_image_with_gt(
                     file_info,ground_truth_indexed_db,
                     detection_categories,classification_categories,
                     options=options))
         elapsed = time.time() - start_time
 
         # Map all the rendering results in the list rendering_results into the
         # dictionary images_html, which maps category names to lists of results
@@ -1305,15 +1378,15 @@
             if rendering_result is None:
                 continue
             image_rendered_count += 1
             for assignment in rendering_result:
                 images_html[assignment[0]].append(assignment[1])
 
         # Prepare the individual html image files
-        image_counts = prepare_html_subpages(images_html, output_dir, options)
+        image_counts = _prepare_html_subpages(images_html, output_dir, options)
 
         print('{} images rendered (of {})'.format(image_rendered_count,image_count))
 
         # Write index.html
         all_tp_count = image_counts['tp'] + image_counts['tpc'] + image_counts['tpi']
         total_count = all_tp_count + image_counts['tn'] + image_counts['fp'] + image_counts['fn']
 
@@ -1465,15 +1538,15 @@
             used_combinations = set()
             
             # row = images_to_visualize.iloc[0]
             for i_row, row in images_to_visualize.iterrows():
                 detections_this_row = row['detections']
                 above_threshold_category_ids_this_row = set()
                 for detection in detections_this_row:
-                    threshold = get_threshold_for_category_id(detection['category'], options, detection_categories)
+                    threshold = _get_threshold_for_category_id(detection['category'], options, detection_categories)
                     if detection['conf'] >= threshold:
                         above_threshold_category_ids_this_row.add(detection['category'])
                 if len(above_threshold_category_ids_this_row) == 0:
                     continue
                 sorted_categories_this_row = tuple(sorted(above_threshold_category_ids_this_row))
                 used_combinations.add(sorted_categories_this_row)
             
@@ -1528,27 +1601,27 @@
                     worker_string = 'threads'
                 else:
                     pool = Pool(options.parallelize_rendering_n_cores)
                     worker_string = 'processes'
                 print('Rendering images with {} {}'.format(options.parallelize_rendering_n_cores,
                                                            worker_string))
                 
-            # render_image_no_gt(file_info,detection_categories_to_results_name,
+            # _render_image_no_gt(file_info,detection_categories_to_results_name,
             # detection_categories,classification_categories)
 
             rendering_results = list(tqdm(pool.imap(
-                partial(render_image_no_gt, 
+                partial(_render_image_no_gt, 
                         detection_categories_to_results_name=detection_categories_to_results_name,
                         detection_categories=detection_categories,
                         classification_categories=classification_categories,
                         options=options),
                         files_to_render), total=len(files_to_render)))
         else:
             for file_info in tqdm(files_to_render):
-                rendering_results.append(render_image_no_gt(file_info,
+                rendering_results.append(_render_image_no_gt(file_info,
                                                             detection_categories_to_results_name,
                                                             detection_categories,
                                                             classification_categories,
                                                             options=options))
                 
         elapsed = time.time() - start_time
 
@@ -1564,15 +1637,15 @@
             image_rendered_count += 1
             for assignment in rendering_result:
                 if 'class' in assignment[0]:
                     has_classification_info = True
                 images_html[assignment[0]].append(assignment[1])
 
         # Prepare the individual html image files
-        image_counts = prepare_html_subpages(images_html, output_dir, options)
+        image_counts = _prepare_html_subpages(images_html, output_dir, options)
         
         if image_rendered_count == 0:
             seconds_per_image = 0.0
         else:
             seconds_per_image = elapsed/image_rendered_count
 
         print('Rendered {} images (of {}) in {} ({} per image)'.format(image_rendered_count,
@@ -1719,26 +1792,25 @@
 
 #%% Interactive driver(s)
 
 if False:
 
     #%%
 
-    base_dir = r'G:\temp\md'
+    base_dir = r'g:\temp'
     options = PostProcessingOptions()
     options.image_base_dir = base_dir
-    options.output_dir = os.path.join(base_dir, 'postprocessing')
-    options.api_output_filename_replacements = {} # {'20190430cameratraps\\':''}
-    options.ground_truth_filename_replacements = {} # {'\\data\\blob\\':''}
+    options.output_dir = os.path.join(base_dir, 'preview')
     options.api_output_file = os.path.join(base_dir, 'results.json')
-    options.ground_truth_json_file = os.path.join(base_dir, 'gt.json')
-    # options.unlabeled_classes = ['human']
+    options.confidence_threshold = {'person':0.5,'animal':0.5,'vehicle':0.01}
+    options.include_almost_detections = True
+    options.almost_detection_confidence_threshold = 0.001
 
     ppresults = process_batch_results(options)
-    # os.start(ppresults.output_html_file)
+    # from md_utils.path_utils import open_file; open_file(ppresults.output_html_file)
 
 
 #%% Command-line driver
 
 def main():
     
     options = PostProcessingOptions()
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/remap_detection_categories.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/remap_detection_categories.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-########
-#
-# remap_detection_categories.py
-#
-# Given a MegaDetector results file, remap the category IDs according to a specified 
-# dictionary, writing the results to a new file.
-#
-# Currently only supports remapping detection categories, not classification categories.
-#
-########
+"""
+
+remap_detection_categories.py
+
+Given a MegaDetector results file, remap the category IDs according to a specified 
+dictionary, writing the results to a new file.
+
+Currently only supports remapping detection categories, not classification categories.
+
+"""
 
 #%% Constants and imports
 
 import json
 import os
 
 from tqdm import tqdm
@@ -23,35 +23,43 @@
 
 def remap_detection_categories(input_file,
                                output_file,
                                target_category_map,
                                extra_category_handling='error',
                                overwrite=False):
     """
-    Given a MD results file [input_file], remap the category IDs according to the dictionary
+    Given a MegaDetector results file [input_file], remap the category IDs according to the dictionary
     [target_category_map], writing the results to [output_file].  The remapped dictionary needs to have 
     the same category names as the input file's detection_categories dictionary.
     
-    Currently only supports remapping detection categories, not classification categories.
-    
-    target_category_map can also be a MD results file, in which case we'll use that file's
-    detection_categories dictionary.
+    Typically used to map, e.g., a variety of species to the class "mammal" or the class "animal".
     
-    [extra_category_handling] specifies what we should do if categories are present in the source file 
-    that are not present in the target mapping.
-    
-    'error' == Error in this case.
-    'drop_if_unused' == Don't include these in the output file's category mappings if they are unused,
-       error if they are.
-    'remap' == Remap to unused category IDs.  This is reserved for future use, not currently implemented.
+    Currently only supports remapping detection categories, not classification categories.
     
+    Args:
+        input_file (str): the MD .json results file to remap
+        output_file (str): the remapped .json file to write
+        target_category_map (dict): the category mapping that should be used in the output file.
+            This can also be a MD results file, in which case we'll use that file's
+            detection_categories dictionary.
+        extra_category_handling (str, optional): specifies what we should do if categories are present
+            in the source file that are not present in the target mapping:
+            
+            * 'error' == Error in this case.
+            * 'drop_if_unused' == Don't include these in the output file's category mappings if they are 
+              unused, error if they are.
+            * 'remap' == Remap to unused category IDs.  This is reserved for future use, not currently
+              implemented.
+        overwrite (bool, optional): whether to overwrite [output_file] if it exists; if this is True and
+            [output_file] exists, this function is a no-op
+               
     """
     
     if os.path.exists(output_file) and (not overwrite):
-        print('File {} exists, bypassing remapping'.format(output_file))
+        print('File {} exists, bypassing remapping'.format(output_file))        
         return
     
     assert os.path.isfile(input_file), \
         'File {} does not exist'.format(input_file)
 
     # If "target_category_map" is passed as a filename, load the "detection_categories"
     # dict.
@@ -126,15 +134,15 @@
     input_data['detection_categories'] = target_category_map
     
     with open(output_file,'w') as f:
         json.dump(input_data,f,indent=1)
     
     
     print('Saved remapped results to {}'.format(output_file))
-
+    
 
 #%% Interactive driver
 
 if False:
     
     pass
 
@@ -156,8 +164,7 @@
                                extra_category_handling=extra_category_handling,                               
                                overwrite=overwrite)
 
 
 #%% Command-line driver
 
 # TODO
-
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/render_detection_confusion_matrix.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/render_detection_confusion_matrix.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-########
-#
-# render_detection_confusion_matrix.py
-#
-# Given a CCT-formatted ground truth file and a MegaDetector-formatted results file,
-# render an HTML confusion matrix.  Typically used for multi-class detectors.  Currently
-# assumes a single class per image.
-#
-########
+"""
+
+render_detection_confusion_matrix.py
+
+Given a CCT-formatted ground truth file and a MegaDetector-formatted results file,
+render an HTML confusion matrix.  Typically used for multi-class detectors.  Currently
+assumes a single class per image.
+
+"""
 
 #%% Imports and constants
 
 import os
 import json
 
 import matplotlib.pyplot as plt
@@ -29,38 +29,44 @@
 
 from multiprocessing.pool import ThreadPool
 from multiprocessing.pool import Pool
 
 
 #%% Support functions
 
-def image_to_output_file(im,preview_images_folder):
+def _image_to_output_file(im,preview_images_folder):
+    """
+    Produces a clean filename from im (if [im] is a str) or im['file'] (if [im] is a dict).
+    """
     
     if isinstance(im,str):
         filename_relative = im
     else:
         filename_relative = im['file']
         
     fn_clean = flatten_path(filename_relative).replace(' ','_')
     return os.path.join(preview_images_folder,fn_clean)
 
 
-def render_image(im,render_image_constants):
+def _render_image(im,render_image_constants):
+    """
+    Internal function for rendering a single image to the confusion matrix preview folder.
+    """
     
     filename_to_ground_truth_im = render_image_constants['filename_to_ground_truth_im']
     image_folder = render_image_constants['image_folder']
     preview_images_folder = render_image_constants['preview_images_folder']
     force_render_images = render_image_constants['force_render_images']
     results_category_id_to_name = render_image_constants['results_category_id_to_name']
     rendering_confidence_thresholds = render_image_constants['rendering_confidence_thresholds']
     target_image_size = render_image_constants['target_image_size']
     
     assert im['file'] in filename_to_ground_truth_im
     
-    output_file = image_to_output_file(im,preview_images_folder)
+    output_file = _image_to_output_file(im,preview_images_folder)
     if os.path.isfile(output_file) and not force_render_images:
         return output_file
     
     input_file = os.path.join(image_folder,im['file'])
     assert os.path.isfile(input_file)
                           
     detections_to_render = []
@@ -101,46 +107,73 @@
     Given a CCT-formatted ground truth file and a MegaDetector-formatted results file,
     render an HTML confusion matrix in [preview_folder.  Typically used for multi-class detectors. 
     Currently assumes a single class per image.
     
     confidence_thresholds and rendering_confidence_thresholds are dictionaries mapping
     class names to thresholds.  "default" is a special token that will be used for all
     classes not otherwise assigned thresholds.
+    
+    Args:
+        ground_truth_file (str): the CCT-formatted .json file with ground truth information
+        results_file (str): the MegaDetector results .json file
+        image_folder (str): the folder where images live; filenames in [ground_truth_file] and
+            [results_file] should be relative to this folder.
+        preview_folder (str): the output folder, i.e. the folder in which we'll create our nifty
+            HTML stuff.
+        force_rendering_images (bool, optional): if False, skips images that already exist
+        confidence_thresholds (dict, optional): a dictionary mapping class names to thresholds;
+            all classes not explicitly named here will use the threshold for the "default" category.
+        rendering_thresholds (dict, optional): a dictionary mapping class names to thresholds;
+            all classes not explicitly named here will use the threshold for the "default" category.
+        target_image_size (tuple, optional): output image size, as a pair of ints (width,height).  If one 
+            value is -1 and the other is not, aspect ratio is preserved.  If both are -1, the original image
+            sizes are preserved.
+        parallelize_rendering (bool, optional): enable (default) or disable parallelization when rendering
+        parallelize_rendering_n_core (int, optional): number of threads or processes to use for rendering, only
+            used if parallelize_rendering is True
+        parallelize_rendering_with_threads: whether to use threads (True) or processes (False) when rendering,
+            only used if parallelize_rendering is True
+        job_name (str, optional): job name to include in big letters in the output file
+        model_file (str, optional) model filename to include in HTML output
+        empty_category_name (str, optional): special category name that we should treat as empty, typically
+            "empty"
+        html_image_list_options (dict, optional): options listed passed along to write_html_image_list; 
+            see write_html_image_list for documentation.            
     """
     
-    #%% Argument and path handling
+    ##%% Argument and path handling
     
     preview_images_folder = os.path.join(preview_folder,'images')
     os.makedirs(preview_images_folder,exist_ok=True)
 
     if confidence_thresholds is None:
         confidence_thresholds = {'default':0.5}
     if rendering_confidence_thresholds is None:
         rendering_confidence_thresholds = {'default':0.4}
     
 
-    #%% Load ground truth 
+    ##%% Load ground truth 
         
     with open(ground_truth_file,'r') as f:
         ground_truth_data_cct = json.load(f)
     
     filename_to_ground_truth_im = {}
     for im in ground_truth_data_cct['images']:
         assert im['file_name'] not in filename_to_ground_truth_im
         filename_to_ground_truth_im[im['file_name']] = im
     
     
-    #%% Confirm that the ground truth images are present in the image folder
+    ##%% Confirm that the ground truth images are present in the image folder
     
     ground_truth_images = find_images(image_folder,return_relative_paths=True,recursive=True)
     assert len(ground_truth_images) == len(ground_truth_data_cct['images'])
     del ground_truth_images
     
     
-    #%% Map images to categories
+    ##%% Map images to categories
     
     # gt_image_id_to_image = {im['id']:im for im in ground_truth_data_cct['images']}
     gt_image_id_to_annotations = defaultdict(list)
     
     ground_truth_category_id_to_name = {}
     for c in ground_truth_data_cct['categories']:
         ground_truth_category_id_to_name[c['id']] = c['name']
@@ -171,23 +204,23 @@
         if empty_category_name in category_names_this_file:
             assert len(category_names_this_file) == 1, \
               'Empty category assigned along with another category for {}'.format(filename)
         assert len(category_names_this_file) > 0, \
             'No ground truth category assigned to {}'.format(filename)
     
         
-    #%% Load results
+    ##%% Load results
     
     with open(results_file,'r') as f:
         md_formatted_results = json.load(f)
     
     results_category_id_to_name = md_formatted_results['detection_categories']
     
     
-    #%% Render images with detections    
+    ##%% Render images with detections    
     
     render_image_constants = {}
     render_image_constants['filename_to_ground_truth_im'] = filename_to_ground_truth_im
     render_image_constants['image_folder'] = image_folder
     render_image_constants['preview_images_folder'] = preview_images_folder
     render_image_constants['force_render_images'] = force_render_images
     render_image_constants['results_category_id_to_name'] = results_category_id_to_name
@@ -207,26 +240,26 @@
                 worker_string = 'threads'
             else:
                 pool = Pool(parallelize_rendering_n_cores)
                 worker_string = 'processes'
             print('Rendering images with {} {}'.format(parallelize_rendering_n_cores,
                                                        worker_string))
             
-        _ = list(tqdm(pool.imap(partial(render_image,render_image_constants=render_image_constants),
+        _ = list(tqdm(pool.imap(partial(_render_image,render_image_constants=render_image_constants),
                                 md_formatted_results['images']),
                                 total=len(md_formatted_results['images'])))        
     
     else:
         
         # im = md_formatted_results['images'][0]
         for im in tqdm(md_formatted_results['images']):    
-            render_image(im,render_image_constants)
+            _render_image(im,render_image_constants)
     
     
-    #%% Map images to predicted categories, and vice-versa
+    ##%% Map images to predicted categories, and vice-versa
     
     filename_to_predicted_categories = defaultdict(set)
     predicted_category_name_to_filenames = defaultdict(set)
     
     # im = md_formatted_results['images'][0]
     for im in tqdm(md_formatted_results['images']):
         
@@ -243,15 +276,15 @@
                 predicted_category_name_to_filenames[category_name].add(im['file'])
                 
         # ...for each detection
     
     # ...for each image
     
     
-    #%% Create TP/TN/FP/FN lists
+    ##%% Create TP/TN/FP/FN lists
     
     category_name_to_image_lists = {}
     
     sub_page_tokens = ['fn','tn','fp','tp']
     
     for category_name in ground_truth_category_names:
         
@@ -297,15 +330,15 @@
                         assignment = 'tn'        
                             
             category_name_to_image_lists[category_name][assignment].append(filename)
                         
     # ...for each filename
     
     
-    #%% Create confusion matrix
+    ##%% Create confusion matrix
     
     gt_category_name_to_category_index = {}
     
     for i_category,category_name in enumerate(ground_truth_category_names):
         gt_category_name_to_category_index[category_name] = i_category
     
     n_categories = len(gt_category_name_to_category_index)    
@@ -379,15 +412,15 @@
     # fig.show()
     fig.savefig(cm_figure_fn_abs,dpi=100)
     plt.close(fig)
     
     # open_file(cm_figure_fn_abs)
     
     
-    #%% Create HTML confusion matrix
+    ##%% Create HTML confusion matrix
     
     html_confusion_matrix = '<table class="result-table">\n'
     html_confusion_matrix += '<tr>\n'
     html_confusion_matrix += '<td>{}</td>\n'.format('True category')
     for category_name in ground_truth_category_names:
         html_confusion_matrix += '<td>{}</td>\n'.format('&nbsp;')
     html_confusion_matrix += '</tr>\n'
@@ -419,15 +452,15 @@
                         max_conf = 0
                     else:
                         max_conf = max([d['conf'] for d in detections])
                     
                     title = '<b>Image</b>: {}, <b>Max conf</b>: {:0.3f}'.format(
                         image_filename_relative, max_conf)
                     image_link = 'images/' + os.path.basename(
-                        image_to_output_file(image_filename_relative,preview_images_folder))
+                        _image_to_output_file(image_filename_relative,preview_images_folder))
                     html_image_info = {
                         'filename': image_link,
                         'title': title,
                         'textStyle':\
                          'font-family:verdana,arial,calibri;font-size:80%;' + \
                              'text-align:left;margin-top:20;margin-bottom:5'
                     }                
@@ -523,15 +556,15 @@
                         max_conf = 0
                     else:
                         max_conf = max([d['conf'] for d in detections])
                     
                     title = '<b>Image</b>: {}, <b>Max conf</b>: {:0.3f}'.format(
                         image_filename_relative, max_conf)
                     image_link = 'images/' + os.path.basename(
-                        image_to_output_file(image_filename_relative,preview_images_folder))
+                        _image_to_output_file(image_filename_relative,preview_images_folder))
                     html_image_info = {
                         'filename': image_link,
                         'title': title,
                         'linkTarget': source_file,
                         'textStyle':\
                          'font-family:verdana,arial,calibri;font-size:80%;' + \
                              'text-align:left;margin-top:20;margin-bottom:5'
@@ -614,15 +647,15 @@
     
     target_html_file = os.path.join(preview_folder,'index.html')
     
     with open(target_html_file,'w') as f:
         f.write(html)
     
     
-    #%% Prepare return data
+    ##%% Prepare return data
     
     confusion_matrix_info = {}
     confusion_matrix_info['html_file'] = target_html_file
     
     return confusion_matrix_info
 
 # ...render_detection_confusion_matrix(...)
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/repeat_detection_elimination/find_repeat_detections.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/repeat_detection_elimination/find_repeat_detections.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,28 +1,30 @@
-########
-#
-# find_repeat_detections.py
-#
-# If you want to use this script, we recommend that you read the user's guide:
-#
-# https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination
-#
-# Really, don't try to run this script without reading the user's guide, you'll think 
-# it's more magical than it is. 
-#
-# This script looks through a sequence of detections in the API output json file, and finds 
-# candidates that might be "repeated false positives", i.e. that random branch that the 
-# detector thinks is an animal/person/vehicle.
-#
-# Typically after running this script, you would do a manual step to remove 
-# true positives, then run remove_repeat_detections to produce a final output file.
-#
-# There's no way that statement was self-explanatory; see the user's guide.
-#
-########
+r"""
+
+find_repeat_detections.py
+
+If you want to use this script, we recommend that you read the RDE user's guide:
+
+https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination
+
+Really, don't try to run this script without reading the user's guide, you'll think 
+it's more magical than it is. 
+
+This script looks through a sequence of detections in the API output json file, and finds 
+candidates that might be "repeated false positives", i.e. that random branch that the 
+detector thinks is an animal/person/vehicle.
+
+Typically after running this script, you would do a manual step to remove 
+true positives, then run remove_repeat_detections to produce a final output file.
+
+There's no way that statement was self-explanatory; see the user's guide.
+
+This script is just a command-line driver for repeat_detections_core.py.
+
+"""
 
 #%% Constants and imports
 
 import argparse
 import os
 import sys
 
@@ -47,15 +49,15 @@
     options.confidenceMin = 0.15
     options.confidenceMax = 1.01
     options.iouThreshold = 0.85
     options.occurrenceThreshold = 8
     options.maxSuspiciousDetectionSize = 0.2
 
     options.filterFileToLoad = ''
-    options.filterFileToLoad = os.path.join(baseDir,'...\detectionIndex.json')
+    options.filterFileToLoad = os.path.join(baseDir,r'..\detectionIndex.json')
 
     options.debugMaxDir = -1
     options.debugMaxRenderDir = -1
     options.debugMaxRenderDetection = -1
     options.debugMaxRenderInstance = -1
 
     options.bParallelizeComparisons = False
@@ -71,23 +73,24 @@
 #%% Command-line driver
 
 def main():
     
     defaultOptions = repeat_detections_core.RepeatDetectionOptions()
 
     parser = argparse.ArgumentParser()
-    parser.add_argument('inputFile')
+    
+    parser.add_argument('inputFile', type=str, help='MD results .json file to process')
     
     parser.add_argument('--outputFile', action='store', type=str, default=None,
-                        help=".json file to write filtered results to... don't use this " + \
-                            "if you're going to do manual review of the repeat detection images")
+                        help='.json file to write filtered results to... do not use this if you are going to ' + \
+                             'do manual review of the repeat detection images (which you should)')
         
     parser.add_argument('--imageBase', action='store', type=str, default='',
-                        help='Image base dir, relevant if renderHtml is True or if " + \
-                            "omitFilteringFolder is not set')
+                        help='Image base dir, relevant if renderHtml is True or if ' + \
+                             '"omitFilteringFolder" is not set')
                             
     parser.add_argument('--outputBase', action='store', type=str, default='',
                         help='HTML or filtering folder output dir')
     
     parser.add_argument('--confidenceMin', action='store', type=float,
                         default=defaultOptions.confidenceMin,
                         help='Detection confidence threshold; don\'t process anything below this')
@@ -95,30 +98,30 @@
     parser.add_argument('--confidenceMax', action='store', type=float,
                         default=defaultOptions.confidenceMax,
                         help='Detection confidence threshold; don\'t process anything above this')
     
     parser.add_argument('--iouThreshold', action='store', type=float,
                         default=defaultOptions.iouThreshold,
                         help='Detections with IOUs greater than this are considered ' + \
-                            '"the same detection"')
+                             '"the same detection"')
         
     parser.add_argument('--occurrenceThreshold', action='store', type=int,
                         default=defaultOptions.occurrenceThreshold,
                         help='More than this many near-identical detections in a group ' + \
-                            '(e.g. a folder) is considered suspicious')
+                             '(e.g. a folder) is considered suspicious')
         
     parser.add_argument('--minSuspiciousDetectionSize', action='store', type=float,
                         default=defaultOptions.minSuspiciousDetectionSize,
                         help='Detections smaller than this fraction of image area are not ' + \
-                            'considered suspicious')
+                             'considered suspicious')
 
     parser.add_argument('--maxSuspiciousDetectionSize', action='store', type=float,
                         default=defaultOptions.maxSuspiciousDetectionSize,
                         help='Detections larger than this fraction of image area are not ' + \
-                            'considered suspicious')
+                             'considered suspicious')
 
     parser.add_argument('--maxImagesPerFolder', action='store', type=int,
                         default=defaultOptions.maxImagesPerFolder,
                         help='Ignore folders with more than this many images in them')
     
     parser.add_argument('--excludeClasses', action='store', nargs='+', type=int,
                         default=None,
@@ -134,56 +137,63 @@
     
     parser.add_argument('--parallelizationUsesProcesses', action='store_false', 
                         dest='parallelizationUsesThreads',
                         help='Parallelize with processes (defaults to threads)')    
     
     parser.add_argument('--filterFileToLoad', action='store', type=str, default='',  
                         help='Path to detectionIndex.json, which should be inside a ' + \
-                            'folder of images that are manually verified to _not_ ' + \
-                            'contain valid animals')
+                             'folder of images that are manually verified to _not_ ' + \
+                             'contain valid animals')
 
     parser.add_argument('--omitFilteringFolder', action='store_false',
                         dest='bWriteFilteringFolder',
                         help='Should we create a folder of rendered detections for post-filtering?')
     
-    parser.add_argument('--debugMaxDir', action='store', type=int, default=-1)
-    parser.add_argument('--debugMaxRenderDir', action='store', type=int, default=-1)
-    parser.add_argument('--debugMaxRenderDetection', action='store', type=int, default=-1)
-    parser.add_argument('--debugMaxRenderInstance', action='store', type=int, default=-1)
+    parser.add_argument('--debugMaxDir', action='store', type=int, default=-1, 
+                        help='For debugging only, limit the number of directories we process')
+    parser.add_argument('--debugMaxRenderDir', action='store', type=int, default=-1,
+                        help='For debugging only, limit the number of directories we render')
+    parser.add_argument('--debugMaxRenderDetection', action='store', type=int, default=-1,
+                        help='For debugging only, limit the number of detections we process per folder')
+    parser.add_argument('--debugMaxRenderInstance', action='store', type=int, default=-1,
+                        help='For debugging only, limit the number of instances we process per detection')
 
     parser.add_argument('--forceSerialComparisons', action='store_false',
-                        dest='bParallelizeComparisons')
+                        dest='bParallelizeComparisons',
+                        help='Disable parallelization during the comparison stage')
     parser.add_argument('--forceSerialRendering', action='store_false',
-                        dest='bParallelizeRendering')
+                        dest='bParallelizeRendering',
+                        help='Disable parallelization during the rendering stage')
     
     parser.add_argument('--maxOutputImageWidth', action='store', type=int,
                         default=defaultOptions.maxOutputImageWidth,
-                        help='Maximum output size for thumbnail images')    
+                        help='Maximum output size for thumbnail images') 
     
     parser.add_argument('--lineThickness', action='store', type=int,
                         default=defaultOptions.lineThickness,
                         help='Line thickness thumbnail images')    
     
     parser.add_argument('--boxExpansion', action='store', type=int,
                         default=defaultOptions.boxExpansion,
                         help='Box expansion for thumbnail images')
         
     parser.add_argument('--nDirLevelsFromLeaf', type=int,
                         default=defaultOptions.nDirLevelsFromLeaf,
                         help='Number of levels from the leaf folders to use for repeat ' + \
-                            'detection (0 == leaves)')
+                             'detection (0 == leaves)')
 
     parser.add_argument('--bRenderOtherDetections', action='store_true',
                         help='Show non-target detections in light gray on each image')
     
     parser.add_argument('--bRenderDetectionTiles', action='store_true',
-                        help='Should we render a grid showing every instance for each detection?')
+                        help='Should we render a grid showing every instance (up to a limit) for each detection?')
     
     parser.add_argument('--detectionTilesPrimaryImageWidth', type=int,
-                        default=defaultOptions.detectionTilesPrimaryImageWidth)                        
+                        default=defaultOptions.detectionTilesPrimaryImageWidth,
+                        help='The width of the main image when rendering images with detection tiles')
 
     parser.add_argument('--renderHtml', action='store_true',
                         dest='bRenderHtml', help='Should we render HTML output?')
     
     if len(sys.argv[1:]) == 0:
         parser.print_help()
         parser.exit()
@@ -193,10 +203,9 @@
     # Convert to an options object
     options = repeat_detections_core.RepeatDetectionOptions()
 
     ct_utils.args_to_object(args, options)
 
     repeat_detections_core.find_repeat_detections(args.inputFile, args.outputFile, options)
 
-
 if __name__ == '__main__':
     main()
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/repeat_detection_elimination/remove_repeat_detections.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/repeat_detection_elimination/remove_repeat_detections.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,31 +1,43 @@
-########
-#
-# remove_repeat_detections.py
-#
-# Used after running find_repeat_detections, then manually filtering the results,
-# to create a final filtered output file.
-#
-# If you want to use this script, we recommend that you read the user's guide:
-#
-# https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination
-#
-########
+"""
+
+remove_repeat_detections.py
+
+Used after running find_repeat_detections, then manually filtering the results,
+to create a final filtered output file.
+
+If you want to use this script, we recommend that you read the RDE user's guide:
+
+https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination
+
+"""
 
 #%% Constants and imports
 
 import argparse
 import os
 from api.batch_processing.postprocessing.repeat_detection_elimination import repeat_detections_core
 
 
 #%% Main function
 
 def remove_repeat_detections(inputFile,outputFile,filteringDir):
-
+    """
+    Given an index file that was produced in a first pass through find_repeat_detections,
+    and a folder of images (from which the user has deleted images they don't want removed),
+    remove the identified repeat detections from a set of MD results and write to a new file.
+    
+    Args:
+        inputFile (str): .json file of MD results, from which we should remove repeat detections
+        outputFile (str): output .json file to which we should write MD results (with repeat 
+            detections removed)
+        filteringDir (str): the folder produced by find_repeat_detections, containing a 
+            detectionIndex.json file        
+    """
+    
     assert os.path.isfile(inputFile), "Can't find file {}".format(inputFile)
     assert os.path.isdir(filteringDir), "Can't find folder {}".format(filteringDir)
     options = repeat_detections_core.RepeatDetectionOptions()
     if os.path.isfile(filteringDir):
         options.filterFileToLoad = filteringDir
     else:
         assert os.path.isdir(filteringDir), '{} is not a valid folder'.format(filteringDir)
@@ -62,10 +74,9 @@
     if len(sys.argv[1:]) == 0:
         parser.print_help()
         parser.exit()
 
     args = parser.parse_args()
     remove_repeat_detections(args.inputFile, args.outputFile, args.filteringDir)
 
-
 if __name__ == '__main__':
     main()
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/repeat_detection_elimination/repeat_detections_core.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/repeat_detection_elimination/repeat_detections_core.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,14 +1,19 @@
-########
-#
-# repeat_detections_core.py
-#
-# Core utilities shared by find_repeat_detections and remove_repeat_detections.
-#
-########
+"""
+
+repeat_detections_core.py
+
+Core utilities shared by find_repeat_detections and remove_repeat_detections.
+
+Nothing in this file (in fact nothing in this subpackage) will make sense until you read
+the RDE user's guide:
+    
+https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination
+
+"""
 
 #%% Imports and environment
 
 import os
 import copy
 import warnings
 import sklearn.cluster
@@ -58,261 +63,336 @@
 #%% Classes
 
 class RepeatDetectionOptions:
     """
     Options that control the behavior of repeat detection elimination
     """
     
-    # Relevant for rendering the folder of images for filtering
-    #
-    # imageBase can also be a SAS URL, in which case some error-checking is
-    # disabled.
+    #: Folder where images live; filenames in the MD results .json file should
+    #: be relative to this folder.
+    #:
+    #: imageBase can also be a SAS URL, in which case some error-checking is
+    #: disabled.
     imageBase = ''
+    
+    #: Folder where we should write temporary output.
     outputBase = ''
 
-    # Don't consider detections with confidence lower than this as suspicious
+    #: Don't consider detections with confidence lower than this as suspicious
     confidenceMin = 0.1
 
-    # Don't consider detections with confidence higher than this as suspicious
+    #: Don't consider detections with confidence higher than this as suspicious
     confidenceMax = 1.0
 
-    # What's the IOU threshold for considering two boxes the same?
+    #: What's the IOU threshold for considering two boxes the same?
     iouThreshold = 0.9
 
-    # How many occurrences of a single location (as defined by the IOU threshold)
-    # are required before we declare it suspicious?
+    #: How many occurrences of a single location (as defined by the IOU threshold)
+    #: are required before we declare it suspicious?
     occurrenceThreshold = 20
     
-    # Ignore "suspicious" detections smaller than some size
+    #: Ignore "suspicious" detections smaller than some size
     minSuspiciousDetectionSize = 0.0
 
-    # Ignore "suspicious" detections larger than some size; these are often animals
-    # taking up the whole image.  This is expressed as a fraction of the image size.
+    #: Ignore "suspicious" detections larger than some size; these are often animals
+    #: taking up the whole image.  This is expressed as a fraction of the image size.
     maxSuspiciousDetectionSize = 0.2
 
-    # Ignore folders with more than this many images in them
+    #: Ignore folders with more than this many images in them
     maxImagesPerFolder = None
     
-    # A list of classes we don't want to treat as suspicious. Each element is an int.
-    excludeClasses = []  # [annotation_constants.detector_bbox_category_name_to_id['person']]
-
-    # For very large sets of results, passing chunks of results to and from workers as 
-    # parameters ('memory') can be memory-intensive, so we can serialize to intermediate
-    # files instead ('file').
-    #
-    # The use of 'file' here is still experimental.
+    #: A list of category IDs (ints) that we don't want consider as candidate repeat detections.
+    #:
+    #: Typically used to say, e.g., "don't bother analyzing people or vehicles for repeat 
+    #: detections", which you could do by saying excludeClasses = [2,3].
+    excludeClasses = []
+
+    #: For very large sets of results, passing chunks of results to and from workers as 
+    #: parameters ('memory') can be memory-intensive, so we can serialize to intermediate
+    #: files instead ('file').
+    #:
+    #: The use of 'file' here is still experimental.
     pass_detections_to_processes_method = 'memory'
     
+    #: Number of workers to use for parallel operations
     nWorkers = 10
     
-    # Should we use threads or processes for parallelization?
+    #: Should we use threads (True) or processes (False) for parallelization?
+    #:
+    #: Not relevant if nWorkers <= 1, or if bParallelizeComparisons and 
+    #: bParallelizeRendering are both False.
     parallelizationUsesThreads = True
 
-    # Load detections from a filter file rather than finding them from the detector output
-
-    # .json file containing detections, generally this is the detectionIndex.json file in 
-    # the filtering_* folder produced in the first pass
+    #: If this is not empty, we'll load detections from a filter file rather than finding them 
+    #: from the detector output.  This should be a .json file containing detections, generally this 
+    #: is the detectionIndex.json file in the filtering_* folder produced by find_repeat_detections().
     filterFileToLoad = ''
 
-    # (optional) List of filenames remaining after deletion of identified 
-    # repeated detections that are actually animals.  This should be a flat
-    # text file, one relative filename per line.  See enumerate_images().
-    #
-    # This is a pretty esoteric code path and a candidate for removal.
-    #
-    # The scenario where I see it being most useful is the very hypothetical one
-    # where we use an external tool for image handling that allows us to do something
-    # smarter and less destructive than deleting images to mark them as non-false-positives.
+    #: (optional) List of filenames remaining after deletion of identified 
+    #: repeated detections that are actually animals.  This should be a flat
+    #: text file, one relative filename per line.
+    #:
+    #: This is a pretty esoteric code path and a candidate for removal.
+    #:
+    #: The scenario where I see it being most useful is the very hypothetical one
+    #: where we use an external tool for image handling that allows us to do something
+    #: smarter and less destructive than deleting images to mark them as non-false-positives.
     filteredFileListToLoad = None
 
-    # Turn on/off optional outputs
+    #: Should we write the folder of images used to manually review repeat detections?
     bWriteFilteringFolder = True
 
+    #: For debugging: limit comparisons to a specific number of folders
     debugMaxDir = -1
+    
+    #: For debugging: limit rendering to a specific number of folders
     debugMaxRenderDir = -1
+    
+    #: For debugging: limit comparisons to a specific number of detections
     debugMaxRenderDetection = -1
+    
+    #: For debugging: limit comparisons to a specific number of instances
     debugMaxRenderInstance = -1
+    
+    #: Should we parallelize (across cameras) comparisons to find repeat detections?
     bParallelizeComparisons = True
+    
+    #: Should we parallelize image rendering?
     bParallelizeRendering = True
     
-    # If this is False (default), a detection from class A is not considered to be "the same"
-    # as a detection from class B, even if they're at the same location.
+    #: If this is False (default), a detection from class A is *not* considered to be "the same"
+    #: as a detection from class B, even if they're at the same location.
     categoryAgnosticComparisons = False
     
-    # Determines whether bounding-box rendering errors (typically network errors) should
-    # be treated as failures    
+    #: Determines whether bounding-box rendering errors (typically network errors) should
+    #: be treated as failures    
     bFailOnRenderError = False
     
+    #: Should we print a warning if images referred to in the MD results file are missing?
     bPrintMissingImageWarnings = True
+    
+    #: If bPrintMissingImageWarnings is True, should we print a warning about missing images
+    #: just once ('once') or every time ('all')?
     missingImageWarningType = 'once'  # 'all'
 
-    # This does *not* include the tile image grid
+    #: Image width for rendered images (it's called "max" because we don't resize smaller images).
+    #:
+    #: Original size is preserved if this is None.
+    #:
+    #: This does *not* include the tile image grid.
     maxOutputImageWidth = None
     
-    # Box rendering options
+    #: Line thickness (in pixels) for box rendering
     lineThickness = 10
+    
+    #: Box expansion (in pixels)
     boxExpansion = 2
     
-    # State variables
+    #: Progress bar used during comparisons and rendering.  Do not set externally.
+    #:
+    #: :meta private:
     pbar = None
 
-    # Replace filename tokens after reading, useful when the directory structure
-    # has changed relative to the structure the detector saw
+    #: Replace filename tokens after reading, useful when the directory structure
+    #: has changed relative to the structure the detector saw.
     filenameReplacements = {}
 
-    # How many folders up from the leaf nodes should we be going to aggregate images?
+    #: How many folders up from the leaf nodes should we be going to aggregate images into 
+    #: cameras?
+    #:
+    #: If this is zero, each leaf folder is treated as a camera.
     nDirLevelsFromLeaf = 0
     
-    # An optional function that takes a string (an image file name) and returns
-    # a string (the corresponding  folder ID), typically used when multiple folders
-    # actually correspond to the same camera in a manufacturer-specific way (e.g.
-    # a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).
+    #: An optional function that takes a string (an image file name) and returns
+    #: a string (the corresponding  folder ID), typically used when multiple folders
+    #: actually correspond to the same camera in a manufacturer-specific way (e.g.
+    #: a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).
+    #:
+    #: See ct_utils for a common replacement function that handles most common
+    #: manufacturer folder names.
     customDirNameFunction = None
     
-    # Include/exclude specific folders... only one of these may be
-    # specified; "including" folders includes *only* those folders.
+    #: Include only specific folders, mutually exclusive with [excludeFolders]
     includeFolders = None
+    
+    #: Exclude specific folders, mutually exclusive with [includeFolders]
     excludeFolders = None
 
-    # Optionally show *other* detections (i.e., detections other than the
-    # one the user is evaluating) in a light gray
+    #: Optionally show *other* detections (i.e., detections other than the
+    #: one the user is evaluating), typically in a light gray.
     bRenderOtherDetections = False
+    
+    #: Threshold to use for *other* detections
     otherDetectionsThreshold = 0.2    
+    
+    #: Line width (in pixels) for *other* detections
     otherDetectionsLineWidth = 1
     
-    # Optionally show a grid that includes a sample image for the detection, plus
-    # the top N additional detections
+    #: Optionally show a grid that includes a sample image for the detection, plus
+    #: the top N additional detections
     bRenderDetectionTiles = True
     
-    # If this is None, we'll render at the width of the original image
+    #: Width of the original image (within the larger output image) when bRenderDetectionTiles
+    #: is True.
+    #:
+    #: If this is None, we'll render the original image in the detection tile image
+    #: at its original width.
     detectionTilesPrimaryImageWidth = None
     
-    # Can be a width in pixels, or a number from 0 to 1 representing a fraction
-    # of the primary image width.
-    #
-    # If you want to render the grid at exactly 1 pixel wide, I guess you're out
-    # of luck.
+    #: Width to use for the grid of detection instances.
+    #:
+    #: Can be a width in pixels, or a number from 0 to 1 representing a fraction
+    #: of the primary image width.
+    #:
+    #: If you want to render the grid at exactly 1 pixel wide, I guess you're out
+    #: of luck.    
     detectionTilesCroppedGridWidth = 0.6
-    detectionTilesPrimaryImageLocation='right'
+    
+    #: Location of the primary image within the mosaic ('right' or 'left)
+    detectionTilesPrimaryImageLocation = 'right'
+    
+    #: Maximum number of individual detection instances to include in the mosaic
     detectionTilesMaxCrops = 250
     
-    # If bRenderOtherDetections is True, what color should we use to render the
-    # (hopefully pretty subtle) non-target detections?
-    # 
-    # In theory I'd like these "other detection" rectangles to be partially 
-    # transparent, but this is not straightforward, and the alpha is ignored
-    # here.  But maybe if I leave it here and wish hard enough, someday it 
-    # will work.
-    #
-    # otherDetectionsColors = ['dimgray']
+    #: If bRenderOtherDetections is True, what color should we use to render the
+    #: (hopefully pretty subtle) non-target detections?
+    #: 
+    #: In theory I'd like these "other detection" rectangles to be partially 
+    #: transparent, but this is not straightforward, and the alpha is ignored
+    #: here.  But maybe if I leave it here and wish hard enough, someday it 
+    #: will work.
+    #:
+    #: otherDetectionsColors = ['dimgray']
     otherDetectionsColors = [(105,105,105,100)]
     
-    # Sort detections within a directory so nearby detections are adjacent
-    # in the list, for faster review.
-    #
-    # Can be None, 'xsort', or 'clustersort'
-    #
-    # * None sorts detections chronologically by first occurrence
-    # * 'xsort' sorts detections from left to right
-    # * 'clustersort' clusters detections and sorts by cluster
+    #: Sort detections within a directory so nearby detections are adjacent
+    #: in the list, for faster review.
+    #:
+    #: Can be None, 'xsort', or 'clustersort'
+    #:
+    #: * None sorts detections chronologically by first occurrence
+    #: * 'xsort' sorts detections from left to right
+    #: * 'clustersort' clusters detections and sorts by cluster
     smartSort = 'xsort'
     
-    # Only relevant if smartSort == 'clustersort'
+    #: Only relevant if smartSort == 'clustersort'
     smartSortDistanceThreshold = 0.1
     
     
 class RepeatDetectionResults:
     """
     The results of an entire repeat detection analysis
     """
 
-    # The data table (Pandas DataFrame), as loaded from the input json file via 
-    # load_api_results().  Has columns ['file', 'detections','failure'].
+    #: The data table (Pandas DataFrame), as loaded from the input json file via 
+    #: load_api_results().  Has columns ['file', 'detections','failure'].
     detectionResults = None
 
-    # The other fields in the input json file, loaded via load_api_results()
+    #: The other fields in the input json file, loaded via load_api_results()
     otherFields = None
 
-    # The data table after modification
+    #: The data table after modification
     detectionResultsFiltered = None
 
-    # dict mapping folder names to whole rows from the data table
+    #: dict mapping folder names to whole rows from the data table
     rowsByDirectory = None
 
-    # dict mapping filenames to rows in the master table
+    #: dict mapping filenames to rows in the master table
     filenameToRow = None
 
-    # An array of length nDirs, where each element is a list of DetectionLocation 
-    # objects for that directory that have been flagged as suspicious
+    #: An array of length nDirs, where each element is a list of DetectionLocation 
+    #: objects for that directory that have been flagged as suspicious
     suspiciousDetections = None
 
+    #: The location of the .json file written with information about the RDE
+    #: review images (typically detectionIndex.json)
     filterFile = None
 
 
 class IndexedDetection:
     """
     A single detection event on a single image
     """
 
     def __init__(self, iDetection=-1, filename='', bbox=[], confidence=-1, category='unknown'):
-        """
-        Args:
-            iDetection: order in API output file
-            filename: path to the image of this detection
-            bbox: [x_min, y_min, width_of_box, height_of_box]
-        """
+        
         assert isinstance(iDetection,int)
         assert isinstance(filename,str)
         assert isinstance(bbox,list)
         assert isinstance(category,str)
         
+        #: index of this detection within all detections for this filename
         self.iDetection = iDetection
+        
+        #: path to the image corresponding to this detection
         self.filename = filename
+        
+        #: [x_min, y_min, width_of_box, height_of_box]
         self.bbox = bbox
+        
+        #: confidence value of this detection
         self.confidence = confidence
+        
+        #: category ID (not name) of this detection            
         self.category = category
 
     def __repr__(self):
         s = ct_utils.pretty_print_object(self, False)
         return s
 
 
 class DetectionLocation:
     """
     A unique-ish detection location, meaningful in the context of one
     directory.  All detections within an IoU threshold of self.bbox
-    will be stored in "instances".
+    will be stored in IndexedDetection objects.
     """
 
     def __init__(self, instance, detection, relativeDir, category, id=None):
         
         assert isinstance(detection,dict)
         assert isinstance(instance,IndexedDetection)
         assert isinstance(relativeDir,str)
         assert isinstance(category,str)
         
-        self.instances = [instance]  # list of IndexedDetections
+        #: list of IndexedDetections that match this detection
+        self.instances = [instance]
+        
+        #: category ID (not name) for this detection
         self.category = category
+        
+        #: bbox as x,y,w,h
         self.bbox = detection['bbox']
+        
+        #: relative folder (i.e., camera name) in which this detectin was found
         self.relativeDir = relativeDir
+        
+        #: relative path to the canonical image representing this detection
         self.sampleImageRelativeFileName = ''        
+        
+        #: list of detections on that canonical image that match this detection
         self.sampleImageDetections = None
         
-        # This ID is only guaranteed to be unique within a directory
+        #: ID for this detection; this ID is only guaranteed to be unique within a directory
         self.id = id
+        
+        #: only used when doing cluster-based sorting
         self.clusterLabel = None
 
     def __repr__(self):
         s = ct_utils.pretty_print_object(self, False)
         return s
     
     def to_api_detection(self):
         """
-        Converts to a 'detection' dictionary, making the semi-arbitrary assumption that
-        the first instance is representative of confidence.
+        Converts this detection to a 'detection' dictionary, making the semi-arbitrary 
+        assumption that the first instance is representative of confidence.
+        
+        Returns:
+            dict: dictionary in the format used to store detections in MD results
         """
         
         # This is a bit of a hack right now, but for future-proofing, I don't want to call this
         # to retrieve anything other than the highest-confidence detection, and I'm assuming this 
         # is already sorted, so assert() that.
         confidences = [i.confidence for i in self.instances]
         assert confidences[0] == max(confidences), \
@@ -324,64 +404,55 @@
         detection = {'conf':self.instances[0].confidence,
                      'bbox':self.bbox,'category':self.instances[0].category}
         return detection
 
 
 #%% Support functions
 
-def enumerate_images(dirName,outputFileName=None):
+def _render_bounding_box(detection, inputFileName, outputFileName, lineWidth=5, 
+                        expansion=0):
     """
-    Non-recursively enumerates all image files in *dirName* to the text file 
-    *outputFileName*, as relative paths.  This is used to produce a file list
-    after removing true positives from the image directory.
-    
-    Not used directly in this module, but provides a consistent way to enumerate
-    files in the format expected by this module.
+    Rendering the detection [detection] on the image [inputFileName], writing the result
+    to [outputFileName].
     """
     
-    imageList = path_utils.find_images(dirName)
-    imageList = [os.path.basename(fn) for fn in imageList]
-    
-    if outputFileName is not None:
-        with open(outputFileName,'w') as f:
-            for s in imageList:
-                f.write(s + '\n')
-            
-    return imageList
-    
-
-def render_bounding_box(detection, inputFileName, outputFileName, lineWidth=5, 
-                        expansion=0):
-    
     im = open_image(inputFileName)
     d = detection.to_api_detection()
     render_detection_bounding_boxes([d],im,thickness=lineWidth,expansion=expansion,
                                     confidence_threshold=-10)
     im.save(outputFileName)
 
 
-def detection_rect_to_rtree_rect(detection_rect):
-    # We store detections as x/y/w/h, rtree and pyqtree use l/b/r/t
+def _detection_rect_to_rtree_rect(detection_rect):
+    """
+    We store detections as x/y/w/h, rtree and pyqtree use l/b/r/t.  Convert from
+    our representation to rtree's.
+    """
+    
     l = detection_rect[0]
     b = detection_rect[1]
     r = detection_rect[0] + detection_rect[2]
     t = detection_rect[1] + detection_rect[3]
     return (l,b,r,t)
 
 
-def rtree_rect_to_detection_rect(rtree_rect):
-    # We store detections as x/y/w/h, rtree and pyqtree use l/b/r/t
+def _rtree_rect_to_detection_rect(rtree_rect):
+    """
+    We store detections as x/y/w/h, rtree and pyqtree use l/b/r/t.  Convert from
+    rtree's representation to ours.
+    """
+    
     x = rtree_rect[0]
     y = rtree_rect[1]
     w = rtree_rect[2] - rtree_rect[0]
     h = rtree_rect[3] - rtree_rect[1]
     return (x,y,w,h)
     
 
-def sort_detections_for_directory(candidateDetections,options):
+def _sort_detections_for_directory(candidateDetections,options):
     """
     candidateDetections is a list of DetectionLocation objects.  Sorts them to
     put nearby detections next to each other, for easier visual review.  Returns
     a sorted copy of candidateDetections, does not sort in-place.
     """
  
     if len(candidateDetections) <= 1 or options.smartSort is None:
@@ -470,22 +541,23 @@
         
         return candidateDetectionsSorted
         
     else:
         raise ValueError('Unrecognized sort method {}'.format(
             options.smartSort))
         
-# ...def sort_detections_for_directory(...)
+# ...def _sort_detections_for_directory(...)
 
 
-def find_matches_in_directory(dirNameAndRows, options):
+def _find_matches_in_directory(dirNameAndRows, options):
     """
     dirNameAndRows is a tuple of (name,rows).
     
-    "name" is a location name, typically a folder name.
+    "name" is a location name, typically a folder name, though this may be an arbitrary
+    location identifier.
     
     "rows" is a Pandas dataframe with one row per image in this location, with columns:
         
         * 'file': relative file name
         * 'detections': a list of MD detection objects, i.e. dicts with keys ['category','conf','bbox']
         * 'max_detection_conf': maximum confidence of any detection, in any category
     
@@ -544,15 +616,15 @@
     i_iteration = -1
     n_boxes_evaluated = 0
     
     for iDirectoryRow, row in rows.iterrows():
 
         i_iteration += 1
         filename = row['file']
-        if not ct_utils.is_image_file(filename):
+        if not path_utils.is_image_file(filename):
             continue
 
         if 'max_detection_conf' not in row or 'detections' not in row or \
             row['detections'] is None:
             print('Skipping row {}'.format(iDirectoryRow))
             continue
 
@@ -639,15 +711,15 @@
             
             instance = IndexedDetection(iDetection=iDetection,
                                         filename=row['file'], bbox=bbox, 
                                         confidence=confidence, category=category)
 
             bFoundSimilarDetection = False
 
-            rtree_rect = detection_rect_to_rtree_rect(bbox)
+            rtree_rect = _detection_rect_to_rtree_rect(bbox)
             
             # This will return candidates of all classes
             overlappingCandidateDetections =\
                 candidateDetectionsIndex.intersect(rtree_rect)
             
             overlappingCandidateDetections.sort(
                 key=lambda x: x.id, reverse=False)
@@ -719,18 +791,18 @@
         with open(location_results_file,'w') as f:
             f.write(s)            
             # json.dump(candidateDetections,f,indent=1)
         return location_results_file
     else:
         return candidateDetections
 
-# ...def find_matches_in_directory(...)
+# ...def _find_matches_in_directory(...)
 
 
-def update_detection_table(repeatDetectionResults, options, outputFilename=None):
+def _update_detection_table(repeatDetectionResults, options, outputFilename=None):
     """
     Changes confidence values in repeatDetectionResults.detectionResults so that detections
     deemed to be possible false positives are given negative confidence values.
     
     repeatDetectionResults is an object of type RepeatDetectionResults, with a pandas
     dataframe (detectionResults) containing all the detections loaded from the .json file,
     and a list of detections for each location (suspiciousDetections) that are deemed to
@@ -866,18 +938,18 @@
 
     print(
         'Finished updating detection table\nChanged {} detections that impacted {} maxPs ({} to negative) ({} across confidence threshold)'.format(
             nBboxChanges, nProbChanges, nProbChangesToNegative, nProbChangesAcrossThreshold))
 
     return detectionResults
 
-# ...def update_detection_table(...)
+# ...def _update_detection_table(...)
 
 
-def render_sample_image_for_detection(detection,filteringDir,options):
+def _render_sample_image_for_detection(detection,filteringDir,options):
     """
     Render a sample image for one unique detection, possibly containing lightly-colored
     high-confidence detections from elsewhere in the sample image.            
     
     "detections" is a DetectionLocation object.
     
     Depends on having already sorted instances within this detection by confidence, and
@@ -950,15 +1022,15 @@
                                             expansion=options.boxExpansion,
                                             confidence_threshold=-10)
         
             im.save(outputFullPath)
             
         else:
             
-            render_bounding_box(detection, inputFullPath, outputFullPath,
+            _render_bounding_box(detection, inputFullPath, outputFullPath,
                 lineWidth=options.lineThickness, expansion=options.boxExpansion)
         
         # ...if we are/aren't rendering other bounding boxes
         
         # If we're rendering detection tiles, we'll re-load and re-write the image we
         # just wrote to outputFullPath
         if options.bRenderDetectionTiles:
@@ -999,35 +1071,47 @@
                 primary_image_filename=outputFullPath,
                 primary_image_width=primaryImageWidth,
                 secondary_image_filename_list=secondaryImageFilenameList,
                 secondary_image_bounding_box_list=secondaryImageBoundingBoxList,
                 cropped_grid_width=croppedGridWidth,
                 output_image_filename=outputFullPath,
                 primary_image_location=options.detectionTilesPrimaryImageLocation)
-        
-            # bDetectionTilesPrimaryImageWidth = None
-            # bDetectionTilesCroppedGridWidth = 0.6
-            # bDetectionTilesPrimaryImageLocation='right'
-        
+                    
         # ...if we are/aren't rendering detection tiles
     
     except Exception as e:
         
         stack_trace = traceback.format_exc()
         print('Warning: error rendering bounding box from {} to {}: {} ({})'.format(
             inputFullPath,outputFullPath,e,stack_trace))
         if options.bFailOnRenderError:
             raise                    
 
-# ...def render_sample_image_for_detection(...)
+# ...def _render_sample_image_for_detection(...)
 
 
 #%% Main entry point
 
 def find_repeat_detections(inputFilename, outputFilename=None, options=None):
+    """
+    Find detections in a MD results file that occur repeatedly and are likely to be 
+    rocks/sticks.
+    
+    Args:
+        inputFilename (str): the MD results .json file to analyze
+        outputFilename (str, optional): the filename to which we should write results 
+            with repeat detections removed, typically set to None during the first
+            part of the RDE process.
+        options (RepeatDetectionOptions): all the interesting options controlling this
+            process; see RepeatDetectionOptions for details.
+    
+    Returns:
+        RepeatDetectionResults: results of the RDE process; see RepeatDetectionResults
+        for details.
+    """
     
     ##%% Input handling
 
     if options is None:
         
         options = RepeatDetectionOptions()
 
@@ -1199,15 +1283,15 @@
 
             options.pbar = None
             for iDir, dirName in tqdm(enumerate(dirsToSearch)):
                 dirNameAndRow = dirNameAndRows[iDir]
                 assert dirNameAndRow[0] == dirName
                 print('Processing dir {} of {}: {}'.format(iDir,len(dirsToSearch),dirName))
                 allCandidateDetections[iDir] = \
-                    find_matches_in_directory(dirNameAndRow, options)
+                    _find_matches_in_directory(dirNameAndRow, options)
 
         else:            
             
             n_workers = options.nWorkers
             if n_workers > len(dirNameAndRows):
                 print('Pool of {} requested, but only {} folders available, reducing pool to {}'.\
                       format(n_workers,len(dirNameAndRows),len(dirNameAndRows)))
@@ -1267,15 +1351,15 @@
                     dirNameAndIntermediateFile.append((location_name,intermediate_results_file))
                     
                     
                 ##%% Find detections in each directory
                               
                 options.pbar = None                
                 allCandidateDetectionFiles = list(pool.imap(
-                    partial(find_matches_in_directory,options=options), dirNameAndIntermediateFile))
+                    partial(_find_matches_in_directory,options=options), dirNameAndIntermediateFile))
             
                 
                 ##%% Load into a combined list of candidate detections
                 
                 allCandidateDetections = []
                 
                 # candidate_detection_file = allCandidateDetectionFiles[0]
@@ -1294,19 +1378,19 @@
                 
                 # We get slightly nicer progress bar behavior using threads, by passing a pbar 
                 # object and letting it get updated.  We can't serialize this object across 
                 # processes.
                 if options.parallelizationUsesThreads:
                     options.pbar = tqdm(total=len(dirNameAndRows))
                     allCandidateDetections = list(pool.imap(
-                        partial(find_matches_in_directory,options=options), dirNameAndRows))
+                        partial(_find_matches_in_directory,options=options), dirNameAndRows))
                 else:
                     options.pbar = None                
                     allCandidateDetections = list(tqdm(pool.imap(
-                        partial(find_matches_in_directory,options=options), dirNameAndRows)))
+                        partial(_find_matches_in_directory,options=options), dirNameAndRows)))
 
         print('\nFinished looking for similar detections')
 
         
         ##%% Mark suspicious locations based on match results
 
         print('Marking repeat detections...')
@@ -1338,15 +1422,15 @@
 
                 suspiciousDetectionsThisDir.append(candidateLocation)
 
             suspiciousDetections[iDir] = suspiciousDetectionsThisDir
 
             # Sort the above-threshold detections for easier review
             if options.smartSort is not None:
-                suspiciousDetections[iDir] = sort_detections_for_directory(
+                suspiciousDetections[iDir] = _sort_detections_for_directory(
                     suspiciousDetections[iDir],options)
                 
             print('Found {} suspicious detections in directory {} ({})'.format(
                   len(suspiciousDetections[iDir]),iDir,dirsToSearch[iDir]))
         
         # ...for each directory
         
@@ -1423,15 +1507,15 @@
         print('Removed {} of {} total detections via manual filtering'.\
               format(nDetectionsRemoved, nDetectionsLoaded))
 
     # ...if we are/aren't finding detections (vs. loading from file)
 
     toReturn.suspiciousDetections = suspiciousDetections
 
-    toReturn.allRowsFiltered = update_detection_table(toReturn, options, outputFilename)
+    toReturn.allRowsFiltered = _update_detection_table(toReturn, options, outputFilename)
     
     
     ##%% Create filtering directory
     
     if options.bWriteFilteringFolder:
 
         print('Creating filtering folder...')
@@ -1497,27 +1581,27 @@
             
             # We get slightly nicer progress bar behavior using threads, by passing a pbar 
             # object and letting it get updated.  We can't serialize this object across 
             # processes.
             if options.parallelizationUsesThreads:
                 options.pbar = tqdm(total=len(allSuspiciousDetections))
                 allCandidateDetections = list(pool.imap(
-                    partial(render_sample_image_for_detection,filteringDir=filteringDir,
+                    partial(_render_sample_image_for_detection,filteringDir=filteringDir,
                             options=options), allSuspiciousDetections))
             else:
                 options.pbar = None                
                 allCandidateDetections = list(tqdm(pool.imap(
-                    partial(render_sample_image_for_detection,filteringDir=filteringDir,
+                    partial(_render_sample_image_for_detection,filteringDir=filteringDir,
                             options=options), allSuspiciousDetections)))
                 
         else:
             
             # Serial loop over detections
             for detection in allSuspiciousDetections:
-                render_sample_image_for_detection(detection,filteringDir,options)
+                _render_sample_image_for_detection(detection,filteringDir,options)
             
         # Delete (large) temporary data from the list of suspicious detections
         for detection in allSuspiciousDetections:
             detection.sampleImageDetections = None            
             
         # Write out the detection index
         detectionIndexFileName = os.path.join(filteringDir, detection_index_file_name_base)
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/separate_detections_into_folders.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/separate_detections_into_folders.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,95 +1,95 @@
-########
-#
-# separate_detections_into_folders.py
-#
-### Overview
-#
-# Given a .json file with batch processing results, separate the files in that
-# set of results into folders that contain animals/people/vehicles/nothing, 
-# according to per-class thresholds.
-#
-# Image files are copied, not moved.
-#
-### Output structure
-#
-# Preserves relative paths within each of those folders; cannot be used with .json
-# files that have absolute paths in them.
-#
-# For example, if your .json file has these images:
-#
-# a/b/c/1.jpg
-# a/b/d/2.jpg
-# a/b/e/3.jpg
-# a/b/f/4.jpg
-# a/x/y/5.jpg
-#
-# And let's say:
-#
-# * The results say that the first three images are empty/person/vehicle, respectively
-# * The fourth image is above threshold for "animal" and "person"
-# * The fifth image contains an animal
-#
-# * You specify an output base folder of c:\out
-#
-# You will get the following files:
-#
-# c:\out\empty\a\b\c\1.jpg
-# c:\out\people\a\b\d\2.jpg
-# c:\out\vehicles\a\b\e\3.jpg
-# c:\out\animal_person\a\b\f\4.jpg
-# c:\out\animals\a\x\y\5.jpg
-#
-### Rendering bounding boxes
-#
-# By default, images are just copied to the target output folder.  If you specify --render_boxes,
-# bounding boxes will be rendered on the output images.  Because this is no longer strictly
-# a copy operation, this may result in the loss of metadata.  More accurately, this *may*
-# result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.
-#
-# Rendering boxes also makes this script a lot slower.
-#
-### Classification-based separation
-#
-# If you have a results file with classification data, you can also specify classes to put
-# in their own folders, within the "animals" folder, like this:
-#
-# --classification_thresholds "deer=0.75,cow=0.75"
-#
-# So, e.g., you might get:
-#
-# c:\out\animals\deer\a\x\y\5.jpg
-#
-# In this scenario, the folders within "animals" will be:
-#
-# deer, cow, multiple, unclassified
-#
-# "multiple" in this case only means "deer and cow"; if an image is classified as containing a 
-# bird and a bear, that would end up in "unclassified", since the folder separation is based only
-# on the categories you provide at the command line.
-#
-# No classification-based separation is done within the animal_person, animal_vehicle, or 
-# animal_person_vehicle folders.
-#
-########
+r"""
+
+separate_detections_into_folders.py
+
+**Overview**
+
+Given a .json file with batch processing results, separate the files in that
+set of results into folders that contain animals/people/vehicles/nothing, 
+according to per-class thresholds.
+
+Image files are copied, not moved.
+
+**Output structure**
+
+Preserves relative paths within each of those folders; cannot be used with .json
+files that have absolute paths in them.
+
+For example, if your .json file has these images:
+
+* a/b/c/1.jpg
+* a/b/d/2.jpg
+* a/b/e/3.jpg
+* a/b/f/4.jpg
+* a/x/y/5.jpg
+
+And let's say:
+
+* The results say that the first three images are empty/person/vehicle, respectively
+* The fourth image is above threshold for "animal" and "person"
+* The fifth image contains an animal
+
+* You specify an output base folder of c:/out
+
+You will get the following files:
+
+* c:/out/empty/a/b/c/1.jpg
+* c:/out/people/a/b/d/2.jpg
+* c:/out/vehicles/a/b/e/3.jpg
+* c:/out/animal_person/a/b/f/4.jpg
+* c:/out/animals/a/x/y/5.jpg
+
+**Rendering bounding boxes**
+
+By default, images are just copied to the target output folder.  If you specify --render_boxes,
+bounding boxes will be rendered on the output images.  Because this is no longer strictly
+a copy operation, this may result in the loss of metadata.  More accurately, this *may*
+result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.
+
+Rendering boxes also makes this script a lot slower.
+
+**Classification-based separation**
+
+If you have a results file with classification data, you can also specify classes to put
+in their own folders, within the "animals" folder, like this:
+
+``--classification_thresholds "deer=0.75,cow=0.75"``
+
+So, e.g., you might get:
+
+c:/out/animals/deer/a/x/y/5.jpg
+
+In this scenario, the folders within "animals" will be:
+
+deer, cow, multiple, unclassified
+
+"multiple" in this case only means "deer and cow"; if an image is classified as containing a 
+bird and a bear, that would end up in "unclassified", since the folder separation is based only
+on the categories you provide at the command line.
+
+No classification-based separation is done within the animal_person, animal_vehicle, or 
+animal_person_vehicle folders.
+
+"""
    
 #%% Constants and imports
 
 import argparse
 import json
 import os
 import shutil
 import sys
 import itertools
 
 from multiprocessing.pool import ThreadPool
 from functools import partial
 from tqdm import tqdm
 
-from md_utils.ct_utils import args_to_object
+from md_utils.ct_utils import args_to_object, is_float
 from detection.run_detector import get_typical_confidence_threshold_from_results
 
 import md_visualization.visualization_utils as vis_utils
 
 friendly_folder_names = {'animal':'animals','person':'people','vehicle':'vehicles'}
 
 # Occasionally we have near-zero confidence detections associated with COCO classes that
@@ -100,77 +100,111 @@
 default_line_thickness = 8
 default_box_expansion = 3
 
 
 #%% Options class
 
 class SeparateDetectionsIntoFoldersOptions:
-
+    """
+    Options used to parameterize separate_detections_into_folders()
+    """
+    
     def __init__(self,threshold=None):
         
+        #: Default threshold for categories not specified in category_name_to_threshold
         self.threshold = None
         
+        #: Dict mapping category names to thresholds; for example, an image with only a detection of class 
+        #: "animal" whose confidence is greater than or equal to category_name_to_threshold['animal']
+        #: will be put in the "animal" folder.
         self.category_name_to_threshold = {
             'animal': self.threshold,
             'person': self.threshold,
             'vehicle': self.threshold
         }
         
+        #: Number of workers to use, set to <= 1 to disable parallelization
         self.n_threads = 1
         
+        #: By default, this function errors if you try to output to an existing folder
         self.allow_existing_directory = False
+        
+        #: By default, this function errors if any of the images specified in the results file don't
+        #: exist in the source folder.
         self.allow_missing_files = False
+        
+        #: Whether to overwrite images that already exist in the target folder; only relevant if
+        #: [allow_existing_directory] is True
         self.overwrite = True
+        
+        #: Whether to skip empty images; if this is False, empty images (i.e., images with no detections
+        #: above the corresponding threshold) will be copied to an "empty" folder.
         self.skip_empty_images = False
         
+        #: The MD results .json file to process
         self.results_file = None
+        
+        #: The folder containing source images; filenames in [results_file] should be relative to this
+        #: folder.
         self.base_input_folder = None
+        
+        #: The folder to which we should write output images; see the module header comment for information
+        #: about how that folder will be structured.
         self.base_output_folder = None
                   
-        # Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders
-        self.category_name_to_folder = None
-        self.category_id_to_category_name = None
-        self.debug_max_images = None
+        #: Should we move rather than copy?
+        self.move_images = False
         
-        # Populated only when using classification results
-        self.classification_category_id_to_name = None
-        self.classification_categories = None
-                
+        #: Should we render boxes on the output images?  Makes everything a lot slower.
         self.render_boxes = False
+        
+        #: Line thickness in pixels; only relevant if [render_boxes] is True
         self.line_thickness = default_line_thickness
-        self.box_expansion = default_box_expansion
         
-        # Should we move rather than copy?
-        self.move_images = False
+        #: Box expansion in pixels; only relevant if [render_boxes] is True
+        self.box_expansion = default_box_expansion
         
-        # Originally specified as a string, converted to a dict mapping name:threshold
+        #: Originally specified as a string that looks like this:
+        #:
+        #: deer=0.75,cow=0.75
+        #:
+        #: Converted internally to a dict mapping name:threshold 
         self.classification_thresholds = None
         
+        ## Debug or internal attributes
+        
+        #: Do not set explicitly; populated from data when using classification results
+        self.classification_category_id_to_name = None
+        
+        #: Do not set explicitly; populated from data when using classification results
+        self.classification_categories = None
+                
+        #: Used to test this script; sets a limit on the number of images to process.
+        self.debug_max_images = None
+        
+        #: Do not set explicitly; this gets created based on [results_file]
+        #:
+        #:Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders
+        self.category_name_to_folder = None
+        
+        #: Do not set explicitly; this gets loaded from [results_file]
+        self.category_id_to_category_name = None
+        
     # ...__init__()
     
 # ...class SeparateDetectionsIntoFoldersOptions        
         
     
 #%% Support functions
     
-def path_is_abs(p): return (len(p) > 1) and (p[0] == '/' or p[1] == ':')
-
-def is_float(v):
-    """
-    Determines whether v is either a float or a string representation of a float.
-    """
-    try:
-        _ = float(v)
-        return True
-    except ValueError:
-        return False
+def _path_is_abs(p): return (len(p) > 1) and (p[0] == '/' or p[1] == ':')
 
 printed_missing_file_warning = False
     
-def process_detections(im,options):
+def _process_detections(im,options):
     """
     Process all detections for a single image
     
     May modify *im*.
     """
 
     global printed_missing_file_warning
@@ -389,21 +423,32 @@
         # ...for each category
         
         # Try to preserve EXIF data and image quality when saving
         vis_utils.exif_preserving_save(pil_image,target_path)        
         
     # ...if we don't/do need to render boxes
     
-# ...def process_detections()
+# ...def _process_detections()
     
     
 #%% Main function
 
 def separate_detections_into_folders(options):
-
+    """
+    Given a .json file with batch processing results, separate the files in that
+    set of results into folders that contain animals/people/vehicles/nothing, 
+    according to per-class thresholds.  See the header comment of this module for 
+    more details about the output folder structure.
+    
+    Args:
+        options (SeparateDetectionsIntoFoldersOptions): parameters guiding image
+        separation, see the SeparateDetectionsIntoFoldersOptions documentation for specific
+        options.
+    """
+    
     # Input validation
     
     # Currently we don't support moving (instead of copying) when we're also rendering
     # bounding boxes.
     assert not (options.render_boxes and options.move_images), \
         'Cannot specify both render_boxes and move_images'
                 
@@ -420,15 +465,15 @@
     # Load detection results    
     print('Loading detection results')
     results = json.load(open(options.results_file))
     images = results['images']
     
     for im in images:
         fn = im['file']
-        assert not path_is_abs(fn), 'Cannot process results with absolute image paths'
+        assert not _path_is_abs(fn), 'Cannot process results with absolute image paths'
         
     print('Processing detections for {} images'.format(len(images)))
     
     default_threshold = options.threshold
     
     if default_threshold is None:        
         default_threshold = get_typical_confidence_threshold_from_results(results)        
@@ -528,23 +573,23 @@
     
     if options.n_threads <= 1 or options.debug_max_images is not None:
     
         # i_image = 14; im = images[i_image]; im
         for i_image,im in enumerate(tqdm(images)):
             if options.debug_max_images is not None and i_image > options.debug_max_images:
                 break
-            process_detections(im,options)
+            _process_detections(im,options)
         # ...for each image
         
     else:
         
         print('Starting a pool with {} threads'.format(options.n_threads))
         pool = ThreadPool(options.n_threads)
-        process_detections_with_options = partial(process_detections, options=options)
-        results = list(tqdm(pool.imap(process_detections_with_options, images), total=len(images)))
+        process_detections_with_options = partial(_process_detections, options=options)
+        _ = list(tqdm(pool.imap(process_detections_with_options, images), total=len(images)))
         
 #  ...def separate_detections_into_folders
 
 
 #%% Interactive driver
         
 if False:
@@ -591,15 +636,15 @@
     # With boxes, with classification
     python separate_detections_into_folders.py ~/data/ena24-2022-06-15-v5a.0.0_megaclassifier.json ~/data/ENA24/images ~/data/ENA24-separated --threshold 0.17 --animal_threshold 0.2 --n_threads 10 --allow_existing_directory --render_boxes --line_thickness 10 --box_expansion 10 --classification_thresholds "deer=0.75,cow=0.75,bird=0.75"
     
     # No boxes, with classification
     python separate_detections_into_folders.py ~/data/ena24-2022-06-15-v5a.0.0_megaclassifier.json ~/data/ENA24/images ~/data/ENA24-separated --threshold 0.17 --animal_threshold 0.2 --n_threads 10 --allow_existing_directory --classification_thresholds "deer=0.75,cow=0.75,bird=0.75"
     """    
     
-#%% Command-line driver   
+#%% Command-line driver
 
 def main():
     
     parser = argparse.ArgumentParser()
     parser.add_argument('results_file', type=str, help='Input .json filename')
     parser.add_argument('base_input_folder', type=str, help='Input image folder')
     parser.add_argument('base_output_folder', type=str, help='Output image folder')
@@ -616,35 +661,37 @@
                         help='Confidence threshold for vehicle category')
     parser.add_argument('--classification_thresholds', type=str, default=None,
                         help='List of classification thresholds to use for species-based folder ' + \
                              'separation, formatted as, e.g., "deer=0.75,cow=0.75"')
     
     parser.add_argument('--n_threads', type=int, default=1,
                         help='Number of threads to use for parallel operation (default=1)')
+    
     parser.add_argument('--allow_existing_directory', action='store_true', 
                         help='Proceed even if the target directory exists and is not empty')
     parser.add_argument('--no_overwrite', action='store_true', 
                         help='Skip images that already exist in the target folder, must also ' + \
                              'specify --allow_existing_directory')    
     parser.add_argument('--skip_empty_images', action='store_true',
-                        help='Don\'t copy empty images to the output folder')
+                        help='Do not copy empty images to the output folder')
     parser.add_argument('--move_images', action='store_true',
-                        help='Move images (rather than coping) (we don\'t recommend this if you haven\'t ' + \
+                        help='Move images (rather than copying) (not recommended this if you have not ' + \
                              'backed up your data!)')
+    
     parser.add_argument('--render_boxes', action='store_true',
                         help='Render bounding boxes on output images; may result in some ' + \
                              'metadata not being transferred')
     parser.add_argument('--line_thickness', type=int, default=default_line_thickness,
                         help='Line thickness (in pixels) for rendering, only meaningful if ' + \
                              'using render_boxes (defaults to {})'.format(
-                            default_line_thickness))
+                             default_line_thickness))
     parser.add_argument('--box_expansion', type=int, default=default_line_thickness,
                         help='Box expansion (in pixels) for rendering, only meaningful if ' + \
-                              'using render_boxes (defaults to {})'.format(
-                              default_box_expansion))
+                             'using render_boxes (defaults to {})'.format(
+                             default_box_expansion))
         
     if len(sys.argv[1:])==0:
         parser.print_help()
         parser.exit()
         
     args = parser.parse_args()    
     
@@ -676,11 +723,9 @@
     options.category_name_to_threshold['person'] = args.human_threshold
     options.category_name_to_threshold['vehicle'] = args.vehicle_threshold
     
     options.overwrite = (not args.no_overwrite)
         
     separate_detections_into_folders(options)
     
-if __name__ == '__main__':
-    
+if __name__ == '__main__':    
     main()
-
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/subset_json_detector_output.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/subset_json_detector_output.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,152 +1,150 @@
-########
-#
-# subset_json_detector_output.py
-#
-# Creates one or more subsets of a detector API output file (.json), doing either
-# or both of the following (if both are requested, they happen in this order):
-#
-# 1) Retrieve all elements where filenames contain a specified query string, 
-#    optionally replacing that query with a replacement token. If the query is blank, 
-#    can also be used to prepend content to all filenames.
-#
-#    Does not support regex's, but supports a special case of ^string to indicate "must start with
-#    to match".
-#
-# 2) Create separate .jsons for each unique path, optionally making the filenames 
-#    in those .json's relative paths.  In this case, you specify an output directory, 
-#    rather than an output path.  All images in the folder blah\foo\bar will end up 
-#    in a .json file called blah_foo_bar.json.
-#
-# Can also apply a confidence threshold.
-#
-# Can also subset by categories above a threshold (programmatic invocation only, this is
-# not supported at the command line yet).
-#
-###
-#
-# Sample invocations (splitting into multiple json's):
-#
-# Read from "1800_idfg_statewide_wolf_detections_w_classifications.json", split up into 
-# individual .jsons in 'd:\temp\idfg\output', making filenames relative to their individual
-# folders:
-#
-# python subset_json_detector_output.py "d:\temp\idfg\1800_idfg_statewide_wolf_detections_w_classifications.json" "d:\temp\idfg\output" --split_folders --make_folder_relative
-#
-# Now do the same thing, but instead of writing .json's to d:\temp\idfg\output, write them to *subfolders*
-# corresponding to the subfolders for each .json file.
-#
-# python subset_json_detector_output.py "d:\temp\idfg\1800_detections_S2.json" "d:\temp\idfg\output_to_folders" --split_folders --make_folder_relative --copy_jsons_to_folders
-#
-###
-#
-# Sample invocations (creating a single subset matching a query):
-#
-# Read from "1800_detections.json", write to "1800_detections_2017.json"
-#
-# Include only images matching "2017", and change "2017" to "blah"
-#
-# python subset_json_detector_output.py "d:\temp\1800_detections.json" "d:\temp\1800_detections_2017_blah.json" --query 2017 --replacement blah
-#
-# Include all images, prepend with "prefix/"
-#
-# python subset_json_detector_output.py "d:\temp\1800_detections.json" "d:\temp\1800_detections_prefix.json" --replacement "prefix/"
-#
-###
-#
-# To subset a COCO Camera Traps .json database, see subset_json_db.py
-#
-########
+r"""
+
+subset_json_detector_output.py
+
+Creates one or more subsets of a detector results file (.json), doing either
+or both of the following (if both are requested, they happen in this order):
+
+1) Retrieve all elements where filenames contain a specified query string, 
+   optionally replacing that query with a replacement token. If the query is blank, 
+   can also be used to prepend content to all filenames.
+
+   Does not support regex's, but supports a special case of ^string to indicate "must start with
+   to match".
+
+2) Create separate .jsons for each unique path, optionally making the filenames 
+   in those .json's relative paths.  In this case, you specify an output directory, 
+   rather than an output path.  All images in the folder blah/foo/bar will end up 
+   in a .json file called blah_foo_bar.json.
+
+Can also apply a confidence threshold.
+
+Can also subset by categories above a threshold (programmatic invocation only, this is
+not supported at the command line yet).
+
+To subset a COCO Camera Traps .json database, see subset_json_db.py
+
+**Sample invocation (splitting into multiple json's)**
+
+Read from "1800_idfg_statewide_wolf_detections_w_classifications.json", split up into 
+individual .jsons in 'd:/temp/idfg/output', making filenames relative to their individual
+folders:
+
+python subset_json_detector_output.py "d:/temp/idfg/1800_idfg_statewide_wolf_detections_w_classifications.json" "d:/temp/idfg/output" --split_folders --make_folder_relative
+
+Now do the same thing, but instead of writing .json's to d:/temp/idfg/output, write them to *subfolders*
+corresponding to the subfolders for each .json file.
+
+python subset_json_detector_output.py "d:/temp/idfg/1800_detections_S2.json" "d:/temp/idfg/output_to_folders" --split_folders --make_folder_relative --copy_jsons_to_folders
+
+**Sample invocation (creating a single subset matching a query)**
+
+Read from "1800_detections.json", write to "1800_detections_2017.json"
+
+Include only images matching "2017", and change "2017" to "blah"
+
+python subset_json_detector_output.py "d:/temp/1800_detections.json" "d:/temp/1800_detections_2017_blah.json" --query 2017 --replacement blah
+
+Include all images, prepend with "prefix/"
+
+python subset_json_detector_output.py "d:/temp/1800_detections.json" "d:/temp/1800_detections_prefix.json" --replacement "prefix/"
+
+"""
 
 #%% Constants and imports
 
 import argparse
 import sys
 import copy
 import json
 import os
 import re
 
 from tqdm import tqdm
 
-from md_utils.ct_utils import args_to_object
-from md_utils.ct_utils import get_max_conf
-from md_utils.ct_utils import invert_dictionary
+from md_utils.ct_utils import args_to_object, get_max_conf, invert_dictionary
+from md_utils.path_utils import top_level_folder
 
 
 #%% Helper classes
 
 class SubsetJsonDetectorOutputOptions:
+    """
+    Options used to parameterize subset_json_detector_output()
+    """
     
-    # Only process files containing the token 'query'
+    #: Only process files containing the token 'query'
     query = None
     
-    # Replace 'query' with 'replacement' if 'replacement' is not None.  If 'query' is None,
-    # prepend 'replacement'
+    #: Replace 'query' with 'replacement' if 'replacement' is not None.  If 'query' is None,
+    #: prepend 'replacement'
     replacement = None
     
-    # Should we split output into individual .json files for each folder?
+    #: Should we split output into individual .json files for each folder?
     split_folders = False
     
-    # Folder level to use for splitting ['bottom','top','n_from_bottom','n_from_top','dict']
-    #
-    # 'dict' requires 'split_folder_param' to be a dictionary mapping each filename
-    # to a token.
+    #: Folder level to use for splitting ['bottom','top','n_from_bottom','n_from_top','dict']
+    #:
+    #: 'dict' requires 'split_folder_param' to be a dictionary mapping each filename
+    #: to a token.
     split_folder_mode = 'bottom'  # 'top'
     
-    # When using the 'n_from_bottom' parameter to define folder splitting, this
-    # defines the number of directories from the bottom.  'n_from_bottom' with
-    # a parameter of zero is the same as 'bottom'.
-    #
-    # Same story with 'n_from_top'.
-    #
-    # When 'split_folder_mode' is 'dict', this should be a dictionary mapping each filename
-    # to a token.
+    #: When using the 'n_from_bottom' parameter to define folder splitting, this
+    #: defines the number of directories from the bottom.  'n_from_bottom' with
+    #: a parameter of zero is the same as 'bottom'.
+    #:
+    #: Same story with 'n_from_top'.
+    #:
+    #: When 'split_folder_mode' is 'dict', this should be a dictionary mapping each filename
+    #: to a token.
     split_folder_param = 0
     
-    # Only meaningful if split_folders is True: should we convert pathnames to be relative
-    # the folder for each .json file?
+    #: Only meaningful if split_folders is True: should we convert pathnames to be relative
+    #: the folder for each .json file?
     make_folder_relative = False
     
-    # Only meaningful if split_folders and make_folder_relative are True: if not None, 
-    # will copy .json files to their corresponding output directories, relative to 
-    # output_filename
+    #: Only meaningful if split_folders and make_folder_relative are True: if not None, 
+    #: will copy .json files to their corresponding output directories, relative to 
+    #: output_filename
     copy_jsons_to_folders = False
     
-    # Should we over-write .json files?
+    #: Should we over-write .json files?
     overwrite_json_files = False
     
-    # If copy_jsons_to_folders is true, do we require that directories already exist?
+    #: If copy_jsons_to_folders is true, do we require that directories already exist?
     copy_jsons_to_folders_directories_must_exist = True
     
-    # Threshold on confidence
+    #: Optional confidence threshold; if not None, detections below this confidence won't be
+    #: included in the output.
     confidence_threshold = None
     
-    # Should we remove failed images?
+    #: Should we remove failed images?
     remove_failed_images = False
     
-    # Either a list of category IDs (as string-ints) (not names), or a dictionary mapping category *IDs* 
-    # (as string-ints) (not names) to thresholds.  Removes  non-matching detections, does not 
-    # remove images.  Not technically mutually exclusize with category_names_to_keep, but it's an esoteric 
-    # scenario indeed where you would want to specify both.
+    #: Either a list of category IDs (as string-ints) (not names), or a dictionary mapping category *IDs* 
+    #: (as string-ints) (not names) to thresholds.  Removes  non-matching detections, does not 
+    #: remove images.  Not technically mutually exclusize with category_names_to_keep, but it's an esoteric 
+    #: scenario indeed where you would want to specify both.
     categories_to_keep = None
     
-    # Either a list of category names (not IDs), or a dictionary mapping category *names* (not IDs) to thresholds.  
-    # Removes non-matching detections, does not remove images.  Not technically mutually exclusize with 
-    # category_ids_to_keep, but it's an esoteric scenario indeed where you would want to specify both.
+    #: Either a list of category names (not IDs), or a dictionary mapping category *names* (not IDs) to thresholds.  
+    #: Removes non-matching detections, does not remove images.  Not technically mutually exclusize with 
+    #: category_ids_to_keep, but it's an esoteric scenario indeed where you would want to specify both.
     category_names_to_keep = None
     
+    #: Set to >0 during testing to limit the number of images that get processed.
     debug_max_images = -1
     
     
 #%% Main function
 
-def write_detection_results(data, output_filename, options):
+def _write_detection_results(data, output_filename, options):
     """
-    Write the detector-output-formatted dict *data* to *output_filename*.
+    Writes the detector-output-formatted dict *data* to *output_filename*.
     """
     
     if (not options.overwrite_json_files) and os.path.isfile(output_filename):
         raise ValueError('File {} exists'.format(output_filename))
     
     basedir = os.path.dirname(output_filename)
     
@@ -156,20 +154,27 @@
     else:
         os.makedirs(basedir, exist_ok=True)
     
     print('Writing detection output to {}'.format(output_filename))
     with open(output_filename, 'w') as f:
         json.dump(data,f,indent=1)
 
-# ...write_detection_results()
+# ..._write_detection_results()
 
 
 def subset_json_detector_output_by_confidence(data, options):
     """
-    Remove all detections below options.confidence_threshold, update max confidences accordingly.
+    Removes all detections below options.confidence_threshold.
+    
+    Args:
+        data (dict): data loaded from a MD results file
+        options (SubsetJsonDetectorOutputOptions): parameters for subsetting
+    
+    Returns:
+        dict: Possibly-modified version of data (also modifies in place)
     """
     
     if options.confidence_threshold is None:
         return data
     
     images_in = data['images']
     images_out = []    
@@ -228,15 +233,22 @@
     return data
 
 # ...subset_json_detector_output_by_confidence()
 
 
 def subset_json_detector_output_by_categories(data, options):
     """
-    Remove all detections without detections above a threshold for specific categories.
+    Removes all detections without detections above a threshold for specific categories.
+    
+    Args:
+        data (dict): data loaded from a MD results file
+        options (SubsetJsonDetectorOutputOptions): parameters for subsetting
+    
+    Returns:
+        dict: Possibly-modified version of data (also modifies in place)
     """
     
     # If categories_to_keep is supplied as a list, convert to a dict
     if options.categories_to_keep is not None:
         if not isinstance(options.categories_to_keep, dict):
             dict_categories_to_keep = {}
             for category_id in options.categories_to_keep:
@@ -330,14 +342,21 @@
 
 # ...subset_json_detector_output_by_categories()
 
 
 def remove_failed_images(data,options):
     """
     Removed failed images from [data]
+    
+    Args:
+        data (dict): data loaded from a MD results file
+        options (SubsetJsonDetectorOutputOptions): parameters for subsetting
+    
+    Returns:
+        dict: Possibly-modified version of data (also modifies in place)
     """
     
     images_in = data['images']
     images_out = []    
     
     if not options.remove_failed_images:
         return data
@@ -361,16 +380,23 @@
     return data
 
 # ...remove_failed_images()
 
 
 def subset_json_detector_output_by_query(data, options):
     """
-    Subset to images whose filename matches options.query; replace all instances of 
-    options.query with options.replacement.
+    Subsets to images whose filename matches options.query; replace all instances of 
+    options.query with options.replacement.  No-op if options.query_string is None or ''.
+    
+    Args:
+        data (dict): data loaded from a MD results file
+        options (SubsetJsonDetectorOutputOptions): parameters for subsetting
+    
+    Returns:
+        dict: Possibly-modified version of data (also modifies in place)
     """
     
     images_in = data['images']
     images_out = []    
     
     print('Subsetting by query {}, replacement {}...'.format(options.query, options.replacement), end='')
     
@@ -411,82 +437,35 @@
     data['images'] = images_out    
     print('done, found {} matches (of {})'.format(len(data['images']), len(images_in)))
     
     return data
 
 # ...subset_json_detector_output_by_query()
 
-
-def split_path(path, maxdepth=100):
-    """
-    Splits [path] into all its constituent tokens, e.g.:
-    
-    c:\blah\boo\goo.txt
-    
-    ...becomes:
-        
-    ['c:\\', 'blah', 'boo', 'goo.txt']
-    
-    http://nicks-liquid-soapbox.blogspot.com/2011/03/splitting-path-to-list-in-python.html
-    """
-    
-    (head, tail) = os.path.split(path)
-    return split_path(head, maxdepth - 1) + [tail] \
-        if maxdepth and head and head != path \
-        else [head or tail]
-
-# ...split_path()
-
-    
-def top_level_folder(p):
-    """
-    Gets the top-level folder from the path *p*; on Windows, will use the top-level folder
-    that isn't the drive.  E.g., top_level_folder(r"c:\blah\foo") returns "c:\blah".  Does not
-    include the leaf node, i.e. top_level_folder('/blah/foo') returns '/blah'.
-    """
-    
-    if p == '':
-        return ''
-    
-    # Path('/blah').parts is ('/','blah')
-    parts = split_path(p)
-    
-    if len(parts) == 1:
-        return parts[0]
-
-    # Handle paths like:
-    #
-    # /, \, /stuff, c:, c:\stuff
-    drive = os.path.splitdrive(p)[0]
-    if parts[0] == drive or parts[0] == drive + '/' or parts[0] == drive + '\\' or parts[0] in ['\\', '/']:
-        return os.path.join(parts[0], parts[1])
-    else:
-        return parts[0]
-
-# ...top_level_folder()
-
-    
-if False:  
-      
-    p = 'blah/foo/bar'; s = top_level_folder(p); print(s); assert s == 'blah'
-    p = '/blah/foo/bar'; s = top_level_folder(p); print(s); assert s == '/blah'
-    p = 'bar'; s = top_level_folder(p); print(s); assert s == 'bar'
-    p = ''; s = top_level_folder(p); print(s); assert s == ''
-    p = 'c:\\'; s = top_level_folder(p); print(s); assert s == 'c:\\'
-    p = r'c:\blah'; s = top_level_folder(p); print(s); assert s == 'c:\\blah'
-    p = r'c:\foo'; s = top_level_folder(p); print(s); assert s == 'c:\\foo'
-    p = r'c:/foo'; s = top_level_folder(p); print(s); assert s == 'c:/foo'
-    p = r'c:\foo/bar'; s = top_level_folder(p); print(s); assert s == 'c:\\foo'
-    
     
 def subset_json_detector_output(input_filename, output_filename, options, data=None):
     """
-    Main internal entry point
+    Main entry point; creates one or more subsets of a detector results file.  See the 
+    module header comment for more information about the available subsetting approaches.
         
     Makes a copy of [data] before modifying if a data dictionary is supplied.
+    
+    Args:
+        input_filename (str): filename to load and subset; can be None if [data] is supplied
+        output_filename (str): file or folder name (depending on [options]) to which we should
+            write subset results.
+        options (SubsetJsonDetectorOutputOptions): parameters for .json splitting/subsetting;
+            see SubsetJsonDetectorOutputOptions for details.
+        data (dict, optional): data loaded from a .json file; if this is not None, [input_filename]
+            will be ignored.  If supplied, this will be copied before it's modified.
+    
+    Returns:
+        dict: Results that are either loaded from [input_filename] and processed, or copied
+            from [data] and processed.
+    
     """
     
     if options is None:    
         options = SubsetJsonDetectorOutputOptions()
             
     # Input validation        
     if options.copy_jsons_to_folders:
@@ -524,15 +503,15 @@
         
     if (options.categories_to_keep is not None) or (options.category_names_to_keep is not None):
         
         data = subset_json_detector_output_by_categories(data, options)
         
     if not options.split_folders:
         
-        write_detection_results(data, output_filename, options)
+        _write_detection_results(data, output_filename, options)
         return data
     
     else:
         
         # Map images to unique folders
         print('Finding unique folders')    
         
@@ -554,15 +533,15 @@
                     dirname = os.path.dirname(dirname)
                     
             elif options.split_folder_mode == 'n_from_top':
                 
                 # Split string into folders, keeping delimiters
                 
                 # Don't use this, it removes delimiters
-                # tokens = split_path(fn)
+                # tokens = _split_path(fn)
                 tokens = re.split(r'([\\/])',fn)
                 
                 n_tokens_to_keep = ((options.split_folder_param + 1) * 2) - 1;
                 
                 if n_tokens_to_keep > len(tokens):
                     raise ValueError('Cannot walk {} folders from the top in path {}'.format(
                                 options.split_folder_param, fn))
@@ -617,15 +596,15 @@
             else:
                 json_fn = os.path.join(output_filename, json_fn)
             
             # Recycle the 'data' struct, replacing 'images' every time... medium-hacky, but 
             # forward-compatible in that I don't take dependencies on the other fields
             dir_data = data
             dir_data['images'] = folders_to_images[dirname]
-            write_detection_results(dir_data, json_fn, options)
+            _write_detection_results(dir_data, json_fn, options)
             print('Wrote {} images to {}'.format(len(dir_data['images']), json_fn))
             
         # ...for each directory
         
         data['images'] = all_images
         
         return data
@@ -709,11 +688,9 @@
     if args.create_folders:
         options.copy_jsons_to_folders_directories_must_exist = False
         
     args_to_object(args, options)
     
     subset_json_detector_output(args.input_file, args.output_file, options)
     
-
-if __name__ == '__main__':
-    
+if __name__ == '__main__':    
     main()
```

### Comparing `megadetector-5.0.8/api/batch_processing/postprocessing/top_folders_to_bottom.py` & `megadetector-5.0.9/api/batch_processing/postprocessing/top_folders_to_bottom.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,69 +1,84 @@
-########
-#
-# top_folders_to_bottom.py
-#
-# Given a base folder with files like:
-#
-# A/1/2/a.jpg
-# B/3/4/b.jpg
-#
-# ...moves the top-level folders to the bottom in a new output folder, i.e., creates:
-#
-# 1/2/A/a.jpg
-# 3/4/B/b.jpg
-#
-# In practice, this is used to make this:
-# 
-# animal/camera01/image01.jpg
-#
-# ...look like:
-#
-# camera01/animal/image01.jpg
-#
-########
+"""
+
+top_folders_to_bottom.py
+
+Given a base folder with files like:
+
+* A/1/2/a.jpg
+* B/3/4/b.jpg
+
+...moves the top-level folders to the bottom in a new output folder, i.e., creates:
+
+* 1/2/A/a.jpg
+* 3/4/B/b.jpg
+
+In practice, this is used to make this:
+
+animal/camera01/image01.jpg
+
+...look like:
+
+camera01/animal/image01.jpg
+
+"""
 
 #%% Constants and imports
 
 import os
 import sys
 import shutil
 import argparse
 
 from pathlib import Path
 from tqdm import tqdm
 
 from functools import partial
 from multiprocessing.pool import ThreadPool
 
+from md_utils.path_utils import path_is_abs
+
+
+#%% Classes
+
 class TopFoldersToBottomOptions:
+    """
+    Options used to parameterize top_folders_to_bottom()
+    """
     
     def __init__(self,input_folder,output_folder,copy=True,n_threads=1):
+        
+        #: Whether to copy (True) vs. move (False) false when re-organizing
         self.copy = copy
+        
+        #: Number of worker threads to use, or <1 to disable parallelization
         self.n_threads = n_threads
+        
+        #: Input folder
         self.input_folder = input_folder
+        
+        #: Output folder
         self.output_folder = output_folder
-        self.overwrite = False
         
+        #: If this is False and an output file exists, throw an error
+        self.overwrite = False
         
-#%% Support functions
-
-def path_is_abs(p): return (len(p) > 1) and (p[0] == '/' or p[1] == ':')
-
 
 #%% Main functions
 
-def process_file(relative_filename,options,execute=True):
+def _process_file(relative_filename,options,execute=True):
     
-    assert ('/' in relative_filename) and ('\\' not in relative_filename) and (not path_is_abs(relative_filename))
+    assert ('/' in relative_filename) and \
+        ('\\' not in relative_filename) and \
+        (not path_is_abs(relative_filename))
     
     # Find top-level folder
     tokens = relative_filename.split('/')
-    top_level_folder = tokens.pop(0)
-    tokens.insert(len(tokens)-1,top_level_folder)
+    topmost_folder = tokens.pop(0)
+    tokens.insert(len(tokens)-1,topmost_folder)
     
     # Find file/folder names
     output_relative_path = '/'.join(tokens)
     output_relative_folder = '/'.join(tokens[0:-1])
     
     output_absolute_folder = os.path.join(options.output_folder,output_relative_folder)
     output_absolute_path = os.path.join(options.output_folder,output_relative_path)
@@ -82,19 +97,43 @@
         if options.copy:
             shutil.copy(input_absolute_path, output_absolute_path)
         else:
             shutil.move(input_absolute_path, output_absolute_path)
 
     return output_absolute_path
     
-# ...def process_file()
+# ...def _process_file()
 
 
 def top_folders_to_bottom(options):
-    
+    """
+    top_folders_to_bottom.py
+
+    Given a base folder with files like:
+
+    * A/1/2/a.jpg
+    * B/3/4/b.jpg
+
+    ...moves the top-level folders to the bottom in a new output folder, i.e., creates:
+
+    * 1/2/A/a.jpg
+    * 3/4/B/b.jpg
+
+    In practice, this is used to make this:
+
+    animal/camera01/image01.jpg
+
+    ...look like:
+
+    camera01/animal/image01.jpg
+
+    Args:
+        options (TopFoldersToBottomOptions): See TopFoldersToBottomOptions for parameter details.
+
+    """
     os.makedirs(options.output_folder,exist_ok=True)
     
     # Enumerate input folder
     print('Enumerating files...')
     files = list(Path(options.input_folder).rglob('*'))
     files = [p for p in files if not p.is_dir()]
     files = [str(s) for s in files]
@@ -108,31 +147,31 @@
     
     base_files = [s for s in relative_files if '/' not in s]
     if len(base_files) > 0:
         print('Warning: ignoring {} files in the base folder'.format(len(base_files)))
         relative_files = [s for s in relative_files if '/' in s]
     
     # Make sure each input file maps to a unique output file
-    absolute_output_files = [process_file(s, options, execute=False) for s in relative_files]
+    absolute_output_files = [_process_file(s, options, execute=False) for s in relative_files]
     assert len(absolute_output_files) == len(set(absolute_output_files)),\
         "Error: input filenames don't map to unique output filenames"
         
     # relative_filename = relative_files[0]
     
     # Loop
     if options.n_threads <= 1:
         
         for relative_filename in tqdm(relative_files):
-            process_file(relative_filename,options)
+            _process_file(relative_filename,options)
     
     else:
         
         print('Starting a pool with {} threads'.format(options.n_threads))
         pool = ThreadPool(options.n_threads)
-        process_file_with_options = partial(process_file, options=options)
+        process_file_with_options = partial(_process_file, options=options)
         _ = list(tqdm(pool.imap(process_file_with_options, relative_files), total=len(relative_files)))
 
 # ...def top_folders_to_bottom()        
         
 
 #%% Interactive driver
         
@@ -176,14 +215,9 @@
     
     # Convert to an options object
     options = TopFoldersToBottomOptions(
         args.input_folder,args.output_folder,copy=args.copy,n_threads=args.n_threads)
     
     top_folders_to_bottom(options)
     
-    
-if __name__ == '__main__':
-    
+if __name__ == '__main__':    
     main()
-    
-
-
```

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/api_backend.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/api_backend.py`

 * *Files 0% similar despite different names*

```diff
@@ -146,9 +146,7 @@
     print('Running initial detection to load model...')
     test_image = PIL.Image.new(mode="RGB", size=(200, 200))
     result = detector.generate_detections_one_image(test_image, "test_image", detection_threshold=config.DEFAULT_CONFIDENCE_THRESHOLD)
     print(result)
     print('\n')
 
     detect_process() 
-
-
```

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/api_frontend.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/api_frontend.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,268 +1,266 @@
-# 
-# api_frontend.py
-#
-# Defines the Flask app, which takes requests (one or more images) from
-# remote callers and pushes the images onto the shared Redis queue, to be processed
-# by the main service in api_backend.py .
-#
-
-#%% Imports
-
-import os
-import json
-import time
-import uuid
-import redis
-import shutil
-import argparse
-import traceback
-
-from io import BytesIO
-from flask import Flask, Response, jsonify, make_response, request
-from requests_toolbelt.multipart.encoder import MultipartEncoder
-
-import md_visualization.visualization_utils as vis_utils
-import config
-
-
-#%% Initialization
-
-app = Flask(__name__)
-db = redis.StrictRedis(host=config.REDIS_HOST, port=config.REDIS_PORT)
-
-
-#%% Support functions
-
-def _make_error_object(error_code, error_message):
-    
-    # Make a dict that the request_processing_function can return to the endpoint 
-    # function to notify it of an error
-    return {
-        'error_message': error_message,
-        'error_code': error_code
-    }
-
-
-def _make_error_response(error_code, error_message):
-
-    return make_response(jsonify({'error': error_message}), error_code)
-
-
-def has_access(request):
-    
-    if not os.path.exists(config.API_KEYS_FILE):
-        return True
-    else:
-        if not request.headers.get('key'):
-            print('Key header not available')
-            return False
-        else:
-            API_key = request.headers.get('key').strip().lower()
-            with open(config.API_KEYS_FILE, "r") as f:
-                for line in f:
-                    valid_key = line.strip().lower()
-                    if valid_key == API_key:
-                        return True
-
-    return False
-
-
-def check_posted_data(request):
- 
-    files = request.files
-    params = request.args
-
-    # Verify that the content uploaded is not too big
-    #
-    # request.content_length is the length of the total payload
-    content_length = request.content_length
-    if not content_length:
-        return _make_error_object(411, 'No image(s) were sent, or content length cannot be determined.')
-    if content_length > config.MAX_CONTENT_LENGTH_IN_MB * 1024 * 1024:
-        return _make_error_object(413, ('Payload size {:.2f} MB exceeds the maximum allowed of {} MB. '
-                    'Please upload fewer or more compressed images.').format(
-            content_length / (1024 * 1024), config.MAX_CONTENT_LENGTH_IN_MB))
-
-    render_boxes = True if params.get('render', '').lower() == 'true' else False
-
-    if 'min_confidence' in params:
-        return_confidence_threshold = float(params['min_confidence'])
-        print('runserver, post_detect_sync, user specified detection confidence: ', return_confidence_threshold)
-        if return_confidence_threshold < 0.0 or return_confidence_threshold > 1.0:
-            return _make_error_object(400, 'Detection confidence threshold {} is invalid, should be between 0.0 and 1.0.'.format(
-                return_confidence_threshold))
-    else:
-        return_confidence_threshold = config.DEFAULT_CONFIDENCE_THRESHOLD
-        
-    if 'min_rendering_confidence' in params:
-        rendering_confidence_threshold = float(params['min_rendering_confidence'])
-        print('runserver, post_detect_sync, user specified rendering confidence: ', rendering_confidence_threshold) 
-        if rendering_confidence_threshold < 0.0 or rendering_confidence_threshold > 1.0:
-            return _make_error_object(400, 'Rendering confidence threshold {} is invalid, should be between 0.0 and 1.0.'.format(
-                rendering_confidence_threshold))
-    else:
-        rendering_confidence_threshold =  config.DEFAULT_RENDERING_CONFIDENCE_THRESHOLD
-    
-    # Verify that the number of images is acceptable
-    num_images = sum([1 if file.content_type in config.IMAGE_CONTENT_TYPES else 0 for file in files.values()])
-    print('runserver, post_detect_sync, number of images received: ', num_images)
-
-    if num_images > config.MAX_IMAGES_ACCEPTED:
-        return _make_error_object(413, 'Too many images. Maximum number of images that can be processed in one call is {}.'.format(config.MAX_IMAGES_ACCEPTED))
-    elif num_images == 0:
-        return _make_error_object(400, 'No image(s) of accepted types (image/jpeg, image/png, application/octet-stream) received.')
-    
-    return {
-        'render_boxes': render_boxes,
-        'return_confidence_threshold': return_confidence_threshold,
-        'rendering_confidence_threshold': rendering_confidence_threshold
-    }
-
-# ...def check_posted_data(request)
-    
-
-#%% Main loop
-
-@app.route(config.API_PREFIX + '/detect', methods = ['POST'])
-def detect_sync():
-    
-    if not has_access(request):
-        print('Access denied, please provide a valid API key')
-        return _make_error_response(403, 'Access denied, please provide a valid API key')
-
-    # Check whether the request_processing_function had an error
-    post_data = check_posted_data(request)
-    if post_data.get('error_code', None) is not None:
-        return _make_error_response(post_data.get('error_code'), post_data.get('error_message'))
-
-    render_boxes = post_data.get('render_boxes')
-    return_confidence_threshold = post_data.get('return_confidence_threshold')
-    rendering_confidence_threshold = post_data.get('rendering_confidence_threshold')
-  
-    redis_id = str(uuid.uuid4())
-    d = {'id': redis_id, 'render_boxes': render_boxes, 'return_confidence_threshold': return_confidence_threshold}
-    temp_direc = os.path.join(config.TEMP_FOLDER, redis_id)
-    
-    try:
-        
-        try:
-            # Write images to temporary files
-            #
-            # TODO: read from memory rather than using intermediate files
-            os.makedirs(temp_direc,exist_ok=True)
-            for name, file in request.files.items():
-                if file.content_type in config.IMAGE_CONTENT_TYPES:
-                    filename = request.files[name].filename
-                    image_path = os.path.join(temp_direc, filename)
-                    print('Saving image {} to {}'.format(name,image_path))
-                    file.save(image_path)
-                    assert os.path.isfile(image_path),'Error creating file {}'.format(image_path)
-        
-        except Exception as e:
-            return _make_error_object(500, 'Error saving images: ' + str(e))
-        
-        # Submit the image(s) for processing by api_backend.py, who is waiting on this queue
-        db.rpush(config.REDIS_QUEUE_NAME, json.dumps(d))
-        
-        while True:
-            
-            # TODO: convert to a blocking read and eliminate the sleep() statement in this loop
-            result = db.get(redis_id)
-            
-            if result:
-                
-                result = json.loads(result.decode())
-                print('Processing result {}'.format(str(result)))
-                
-                if result['status'] == 200:
-                    detections = result['detections']
-                    db.delete(redis_id)
-                
-                else:
-                    db.delete(redis_id)
-                    print('Detection error: ' + str(result))
-                    return _make_error_response(500, 'Detection error: ' + str(result))
-
-                try:
-                    print('detect_sync: postprocessing and sending images back...')
-                    fields = {
-                        'detection_result': ('detection_result', json.dumps(detections), 'application/json'),
-                    }
-
-                    if render_boxes and result['status'] == 200:
-
-                        print('Rendering images')
-
-                        for image_name, detections in detections.items():
-                            
-                            #image = Image.open(os.path.join(temp_direc, image_name))
-                            image = open(f'{temp_direc}/{image_name}', "rb")
-                            image = vis_utils.load_image(image)
-                            width, height = image.size
-
-                            _detections = []
-                            for d in detections:
-                                y1,x1,y2,x2 = d[0:4]
-                                width = x2 - x1
-                                height = y2 - y1
-                                bbox = [x1,y1,width,height]
-                                _detections.append({'bbox': bbox, 'conf': d[4], 'category': d[5]}) 
-                            
-                            vis_utils.render_detection_bounding_boxes(_detections, image, 
-                            confidence_threshold=rendering_confidence_threshold)
-                            
-                            output_img_stream = BytesIO()
-                            image.save(output_img_stream, format='jpeg')
-                            output_img_stream.seek(0)
-                            fields[image_name] = (image_name, output_img_stream, 'image/jpeg')
-                        print('Done rendering images')
-                        
-                    m = MultipartEncoder(fields=fields)                    
-                    return Response(m.to_string(), mimetype=m.content_type)
-
-                except Exception as e:
-                    
-                    print(traceback.format_exc())
-                    print('Error returning result or rendering the detection boxes: ' + str(e))
-
-                finally:
-                    
-                    try:
-                        print('Removing temporary files')
-                        shutil.rmtree(temp_direc)
-                    except Exception as e:
-                        print('Error removing temporary folder {}: {}'.format(temp_direc,str(e)))
-                    
-            else:
-                time.sleep(0.005)
-                
-            # ...if we do/don't have a request available on the queue
-            
-        # ...while(True)
-
-    except Exception as e:
-        
-        print(traceback.format_exc())
-        return _make_error_object(500, 'Error processing images: ' + str(e))
-
-# ...def detect_sync()
-
-
-#%% Command-line driver
-    
-if __name__ == '__main__':
-    
-    parser = argparse.ArgumentParser(description='api frontend')
-
-    # use --non-docker if you are testing without Docker
-    #
-    # python api_frontend.py --non-docker
-    parser.add_argument('--non-docker', action="store_true", default=False)
-    args = parser.parse_args()
-
-    if args.non_docker:
-        app.run(host='0.0.0.0', port=5050)
-    else:
-        app.run()
-
-
+# 
+# api_frontend.py
+#
+# Defines the Flask app, which takes requests (one or more images) from
+# remote callers and pushes the images onto the shared Redis queue, to be processed
+# by the main service in api_backend.py .
+#
+
+#%% Imports
+
+import os
+import json
+import time
+import uuid
+import redis
+import shutil
+import argparse
+import traceback
+
+from io import BytesIO
+from flask import Flask, Response, jsonify, make_response, request
+from requests_toolbelt.multipart.encoder import MultipartEncoder
+
+import md_visualization.visualization_utils as vis_utils
+import config
+
+
+#%% Initialization
+
+app = Flask(__name__)
+db = redis.StrictRedis(host=config.REDIS_HOST, port=config.REDIS_PORT)
+
+
+#%% Support functions
+
+def _make_error_object(error_code, error_message):
+    
+    # Make a dict that the request_processing_function can return to the endpoint 
+    # function to notify it of an error
+    return {
+        'error_message': error_message,
+        'error_code': error_code
+    }
+
+
+def _make_error_response(error_code, error_message):
+
+    return make_response(jsonify({'error': error_message}), error_code)
+
+
+def has_access(request):
+    
+    if not os.path.exists(config.API_KEYS_FILE):
+        return True
+    else:
+        if not request.headers.get('key'):
+            print('Key header not available')
+            return False
+        else:
+            API_key = request.headers.get('key').strip().lower()
+            with open(config.API_KEYS_FILE, "r") as f:
+                for line in f:
+                    valid_key = line.strip().lower()
+                    if valid_key == API_key:
+                        return True
+
+    return False
+
+
+def check_posted_data(request):
+ 
+    files = request.files
+    params = request.args
+
+    # Verify that the content uploaded is not too big
+    #
+    # request.content_length is the length of the total payload
+    content_length = request.content_length
+    if not content_length:
+        return _make_error_object(411, 'No image(s) were sent, or content length cannot be determined.')
+    if content_length > config.MAX_CONTENT_LENGTH_IN_MB * 1024 * 1024:
+        return _make_error_object(413, ('Payload size {:.2f} MB exceeds the maximum allowed of {} MB. '
+                    'Please upload fewer or more compressed images.').format(
+            content_length / (1024 * 1024), config.MAX_CONTENT_LENGTH_IN_MB))
+
+    render_boxes = True if params.get('render', '').lower() == 'true' else False
+
+    if 'min_confidence' in params:
+        return_confidence_threshold = float(params['min_confidence'])
+        print('runserver, post_detect_sync, user specified detection confidence: ', return_confidence_threshold)
+        if return_confidence_threshold < 0.0 or return_confidence_threshold > 1.0:
+            return _make_error_object(400, 'Detection confidence threshold {} is invalid, should be between 0.0 and 1.0.'.format(
+                return_confidence_threshold))
+    else:
+        return_confidence_threshold = config.DEFAULT_CONFIDENCE_THRESHOLD
+        
+    if 'min_rendering_confidence' in params:
+        rendering_confidence_threshold = float(params['min_rendering_confidence'])
+        print('runserver, post_detect_sync, user specified rendering confidence: ', rendering_confidence_threshold) 
+        if rendering_confidence_threshold < 0.0 or rendering_confidence_threshold > 1.0:
+            return _make_error_object(400, 'Rendering confidence threshold {} is invalid, should be between 0.0 and 1.0.'.format(
+                rendering_confidence_threshold))
+    else:
+        rendering_confidence_threshold =  config.DEFAULT_RENDERING_CONFIDENCE_THRESHOLD
+    
+    # Verify that the number of images is acceptable
+    num_images = sum([1 if file.content_type in config.IMAGE_CONTENT_TYPES else 0 for file in files.values()])
+    print('runserver, post_detect_sync, number of images received: ', num_images)
+
+    if num_images > config.MAX_IMAGES_ACCEPTED:
+        return _make_error_object(413, 'Too many images. Maximum number of images that can be processed in one call is {}.'.format(config.MAX_IMAGES_ACCEPTED))
+    elif num_images == 0:
+        return _make_error_object(400, 'No image(s) of accepted types (image/jpeg, image/png, application/octet-stream) received.')
+    
+    return {
+        'render_boxes': render_boxes,
+        'return_confidence_threshold': return_confidence_threshold,
+        'rendering_confidence_threshold': rendering_confidence_threshold
+    }
+
+# ...def check_posted_data(request)
+    
+
+#%% Main loop
+
+@app.route(config.API_PREFIX + '/detect', methods = ['POST'])
+def detect_sync():
+    
+    if not has_access(request):
+        print('Access denied, please provide a valid API key')
+        return _make_error_response(403, 'Access denied, please provide a valid API key')
+
+    # Check whether the request_processing_function had an error
+    post_data = check_posted_data(request)
+    if post_data.get('error_code', None) is not None:
+        return _make_error_response(post_data.get('error_code'), post_data.get('error_message'))
+
+    render_boxes = post_data.get('render_boxes')
+    return_confidence_threshold = post_data.get('return_confidence_threshold')
+    rendering_confidence_threshold = post_data.get('rendering_confidence_threshold')
+  
+    redis_id = str(uuid.uuid4())
+    d = {'id': redis_id, 'render_boxes': render_boxes, 'return_confidence_threshold': return_confidence_threshold}
+    temp_direc = os.path.join(config.TEMP_FOLDER, redis_id)
+    
+    try:
+        
+        try:
+            # Write images to temporary files
+            #
+            # TODO: read from memory rather than using intermediate files
+            os.makedirs(temp_direc,exist_ok=True)
+            for name, file in request.files.items():
+                if file.content_type in config.IMAGE_CONTENT_TYPES:
+                    filename = request.files[name].filename
+                    image_path = os.path.join(temp_direc, filename)
+                    print('Saving image {} to {}'.format(name,image_path))
+                    file.save(image_path)
+                    assert os.path.isfile(image_path),'Error creating file {}'.format(image_path)
+        
+        except Exception as e:
+            return _make_error_object(500, 'Error saving images: ' + str(e))
+        
+        # Submit the image(s) for processing by api_backend.py, who is waiting on this queue
+        db.rpush(config.REDIS_QUEUE_NAME, json.dumps(d))
+        
+        while True:
+            
+            # TODO: convert to a blocking read and eliminate the sleep() statement in this loop
+            result = db.get(redis_id)
+            
+            if result:
+                
+                result = json.loads(result.decode())
+                print('Processing result {}'.format(str(result)))
+                
+                if result['status'] == 200:
+                    detections = result['detections']
+                    db.delete(redis_id)
+                
+                else:
+                    db.delete(redis_id)
+                    print('Detection error: ' + str(result))
+                    return _make_error_response(500, 'Detection error: ' + str(result))
+
+                try:
+                    print('detect_sync: postprocessing and sending images back...')
+                    fields = {
+                        'detection_result': ('detection_result', json.dumps(detections), 'application/json'),
+                    }
+
+                    if render_boxes and result['status'] == 200:
+
+                        print('Rendering images')
+
+                        for image_name, detections in detections.items():
+                            
+                            #image = Image.open(os.path.join(temp_direc, image_name))
+                            image = open(f'{temp_direc}/{image_name}', "rb")
+                            image = vis_utils.load_image(image)
+                            width, height = image.size
+
+                            _detections = []
+                            for d in detections:
+                                y1,x1,y2,x2 = d[0:4]
+                                width = x2 - x1
+                                height = y2 - y1
+                                bbox = [x1,y1,width,height]
+                                _detections.append({'bbox': bbox, 'conf': d[4], 'category': d[5]}) 
+                            
+                            vis_utils.render_detection_bounding_boxes(_detections, image, 
+                            confidence_threshold=rendering_confidence_threshold)
+                            
+                            output_img_stream = BytesIO()
+                            image.save(output_img_stream, format='jpeg')
+                            output_img_stream.seek(0)
+                            fields[image_name] = (image_name, output_img_stream, 'image/jpeg')
+                        print('Done rendering images')
+                        
+                    m = MultipartEncoder(fields=fields)                    
+                    return Response(m.to_string(), mimetype=m.content_type)
+
+                except Exception as e:
+                    
+                    print(traceback.format_exc())
+                    print('Error returning result or rendering the detection boxes: ' + str(e))
+
+                finally:
+                    
+                    try:
+                        print('Removing temporary files')
+                        shutil.rmtree(temp_direc)
+                    except Exception as e:
+                        print('Error removing temporary folder {}: {}'.format(temp_direc,str(e)))
+                    
+            else:
+                time.sleep(0.005)
+                
+            # ...if we do/don't have a request available on the queue
+            
+        # ...while(True)
+
+    except Exception as e:
+        
+        print(traceback.format_exc())
+        return _make_error_object(500, 'Error processing images: ' + str(e))
+
+# ...def detect_sync()
+
+
+#%% Command-line driver
+    
+if __name__ == '__main__':
+    
+    parser = argparse.ArgumentParser(description='api frontend')
+
+    # use --non-docker if you are testing without Docker
+    #
+    # python api_frontend.py --non-docker
+    parser.add_argument('--non-docker', action="store_true", default=False)
+    args = parser.parse_args()
+
+    if args.non_docker:
+        app.run(host='0.0.0.0', port=5050)
+    else:
+        app.run()
```

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/config.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/config.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-## Camera trap real-time API configuration
-
-REDIS_HOST = 'localhost'
-
-REDIS_PORT = 6379
-
-# Full path to the temporary folder for image storage, only meaningful 
-# within the Docker container
-TEMP_FOLDER = '/app/temp'
-
-REDIS_QUEUE_NAME = 'camera-trap-queue'
-
-# Upper limit on total content length (all images and parameters)
-MAX_CONTENT_LENGTH_IN_MB = 5 * 8  # 5MB per image * number of images allowed
-
-MAX_IMAGES_ACCEPTED = 8
-
-IMAGE_CONTENT_TYPES = ['image/png', 'application/octet-stream', 'image/jpeg']
-
-DETECTOR_MODEL_PATH = '/app/animal_detection_api/model/md_v5a.0.0.pt'
-
-DETECTOR_MODEL_VERSION = 'v5a.0.0'
-
-# Minimum confidence threshold for detections
-DEFAULT_CONFIDENCE_THRESHOLD = 0.01
-
-# Minimum confidence threshold for showing a bounding box on the output image
-DEFAULT_RENDERING_CONFIDENCE_THRESHOLD = 0.2
-
-API_PREFIX = '/v1/camera-trap/sync'
-
-API_KEYS_FILE = 'allowed_keys.txt'
-
-# Use this when testing without Docker
-DETECTOR_MODEL_PATH_DEBUG = 'model/md_v5a.0.0.pt'
+## Camera trap real-time API configuration
+
+REDIS_HOST = 'localhost'
+
+REDIS_PORT = 6379
+
+# Full path to the temporary folder for image storage, only meaningful 
+# within the Docker container
+TEMP_FOLDER = '/app/temp'
+
+REDIS_QUEUE_NAME = 'camera-trap-queue'
+
+# Upper limit on total content length (all images and parameters)
+MAX_CONTENT_LENGTH_IN_MB = 5 * 8  # 5MB per image * number of images allowed
+
+MAX_IMAGES_ACCEPTED = 8
+
+IMAGE_CONTENT_TYPES = ['image/png', 'application/octet-stream', 'image/jpeg']
+
+DETECTOR_MODEL_PATH = '/app/animal_detection_api/model/md_v5a.0.0.pt'
+
+DETECTOR_MODEL_VERSION = 'v5a.0.0'
+
+# Minimum confidence threshold for detections
+DEFAULT_CONFIDENCE_THRESHOLD = 0.01
+
+# Minimum confidence threshold for showing a bounding box on the output image
+DEFAULT_RENDERING_CONFIDENCE_THRESHOLD = 0.2
+
+API_PREFIX = '/v1/camera-trap/sync'
+
+API_KEYS_FILE = 'allowed_keys.txt'
+
+# Use this when testing without Docker
+DETECTOR_MODEL_PATH_DEBUG = 'model/md_v5a.0.0.pt'
```

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/data_management/annotations/annotation_constants.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/data_management/annotations/annotation_constants.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/detector_training/copy_checkpoints.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/detector_training/copy_checkpoints.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/detector_training/model_main_tf2.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/detector_training/model_main_tf2.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/process_video.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/process_video.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/pytorch_detector.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/pytorch_detector.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/run_detector.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/run_detector.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/run_detector_batch.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/run_detector_batch.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/run_inference_with_yolov5_val.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/run_inference_with_yolov5_val.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/run_tiled_inference.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/run_tiled_inference.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/tf_detector.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/tf_detector.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/detection/video_utils.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/detection/video_utils.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/azure_utils.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/azure_utils.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/ct_utils.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/ct_utils.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/directory_listing.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/directory_listing.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/matlab_porting_tools.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/matlab_porting_tools.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/path_utils.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/path_utils.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/process_utils.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/process_utils.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/sas_blob_utils.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/sas_blob_utils.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/string_utils.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/string_utils.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/url_utils.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/url_utils.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_utils/write_html_image_list.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_utils/write_html_image_list.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/animal_detection_api/md_visualization/visualization_utils.py` & `megadetector-5.0.9/api/synchronous/api_core/animal_detection_api/md_visualization/visualization_utils.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/api/synchronous/api_core/tests/load_test.py` & `megadetector-5.0.9/api/synchronous/api_core/tests/load_test.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,110 +1,110 @@
-
-import os
-import json
-import io
-import random
-import requests
-
-from PIL import Image
-from multiprocessing import Pool
-from datetime import datetime
-from requests_toolbelt import MultipartEncoder
-from requests_toolbelt.multipart import decoder
-
-
-ip_address = '100.100.200.200'
-port = 5050
-
-base_url = 'http://{}:{}/v1/camera-trap/sync/'.format(ip_address, port)  
-
-
-def call_api(args):
-    start = datetime.now()
-    
-    index, url, params, data, headers = args['index'],args['url'], args['params'], args['data'], args['headers']
-    print('calling api: {} starttime: {}'.format(index, start))
-
-    response = requests.post(url, params=params, data=data, headers=headers)
-    elapsed_time = datetime.now() - start
-    print('\napi {} status code: {}, elapsed time in seconds {}'.format(index, response.status_code, elapsed_time.total_seconds()))
-    
-    get_detections(response)   
-    return response
-
-def get_detections(response):
-    results = decoder.MultipartDecoder.from_response(response)
-    text_results = {}
-    images = {}
-    for part in results.parts:
-        # part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later includes 'name' and 'filename' info
-        headers = {}
-        for k, v in part.headers.items():
-            headers[k.decode(part.encoding)] = v.decode(part.encoding)
-       
-        if headers.get('Content-Type', None) == 'application/json':
-            text_result = json.loads(part.content.decode())
-
-    print(text_result)
-
-
-def test_load(num_requests, params, max_images=1):
-    requests = []
-    
-    # read the images anew for each request
-    index = 0
-    for i in range(num_requests):
-        index += 1
-        files = {}
-        sample_input_dir = '../../../api/synchronous/sample_input/test_images'
-
-        image_files = os.listdir(sample_input_dir)
-        random.shuffle(image_files)
-
-        num_images = 0
-        for i, image_name in enumerate(image_files):
-            if not image_name.lower().endswith('.jpg'):
-                continue
-
-            if num_images >= max_images:
-                break
-            else:
-                num_images += 1
-
-            img_path = os.path.join(sample_input_dir, image_name)
-            with open(img_path, 'rb') as f:
-                content = f.read()
-            files[image_name] = (image_name, content, 'image/jpeg')
-
-        m = MultipartEncoder(fields=files)
-        args = {
-            'index': index,
-            'url': base_url + 'detect',
-            'params': params,
-            'data': m,
-            'headers': {'Content-Type': m.content_type}
-        }
-        requests.append(args)
-    
-    print('starting', num_requests, 'threads...')
-    # images are read and in each request by the time we call the API in map()
-    with Pool(num_requests) as pool:
-        results = pool.map(call_api, requests)
-
-    return results
-
-
-if __name__ == "__main__":
-    params = {
-    'min_confidence': 0.05,
-    'min_rendering_confidence': 0.2,
-    'render': True
-    }
-    
-    num_requests = 10
-    max_images = 1
-
-    start = datetime.now()
-    responses = test_load(num_requests, params, max_images=max_images)
-    end = datetime.now()
-    total_time = end - start
+
+import os
+import json
+import io
+import random
+import requests
+
+from PIL import Image
+from multiprocessing import Pool
+from datetime import datetime
+from requests_toolbelt import MultipartEncoder
+from requests_toolbelt.multipart import decoder
+
+
+ip_address = '100.100.200.200'
+port = 5050
+
+base_url = 'http://{}:{}/v1/camera-trap/sync/'.format(ip_address, port)  
+
+
+def call_api(args):
+    start = datetime.now()
+    
+    index, url, params, data, headers = args['index'],args['url'], args['params'], args['data'], args['headers']
+    print('calling api: {} starttime: {}'.format(index, start))
+
+    response = requests.post(url, params=params, data=data, headers=headers)
+    elapsed_time = datetime.now() - start
+    print('\napi {} status code: {}, elapsed time in seconds {}'.format(index, response.status_code, elapsed_time.total_seconds()))
+    
+    get_detections(response)   
+    return response
+
+def get_detections(response):
+    results = decoder.MultipartDecoder.from_response(response)
+    text_results = {}
+    images = {}
+    for part in results.parts:
+        # part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later includes 'name' and 'filename' info
+        headers = {}
+        for k, v in part.headers.items():
+            headers[k.decode(part.encoding)] = v.decode(part.encoding)
+       
+        if headers.get('Content-Type', None) == 'application/json':
+            text_result = json.loads(part.content.decode())
+
+    print(text_result)
+
+
+def test_load(num_requests, params, max_images=1):
+    requests = []
+    
+    # read the images anew for each request
+    index = 0
+    for i in range(num_requests):
+        index += 1
+        files = {}
+        sample_input_dir = '../../../api/synchronous/sample_input/test_images'
+
+        image_files = os.listdir(sample_input_dir)
+        random.shuffle(image_files)
+
+        num_images = 0
+        for i, image_name in enumerate(image_files):
+            if not image_name.lower().endswith('.jpg'):
+                continue
+
+            if num_images >= max_images:
+                break
+            else:
+                num_images += 1
+
+            img_path = os.path.join(sample_input_dir, image_name)
+            with open(img_path, 'rb') as f:
+                content = f.read()
+            files[image_name] = (image_name, content, 'image/jpeg')
+
+        m = MultipartEncoder(fields=files)
+        args = {
+            'index': index,
+            'url': base_url + 'detect',
+            'params': params,
+            'data': m,
+            'headers': {'Content-Type': m.content_type}
+        }
+        requests.append(args)
+    
+    print('starting', num_requests, 'threads...')
+    # images are read and in each request by the time we call the API in map()
+    with Pool(num_requests) as pool:
+        results = pool.map(call_api, requests)
+
+    return results
+
+
+if __name__ == "__main__":
+    params = {
+    'min_confidence': 0.05,
+    'min_rendering_confidence': 0.2,
+    'render': True
+    }
+    
+    num_requests = 10
+    max_images = 1
+
+    start = datetime.now()
+    responses = test_load(num_requests, params, max_images=max_images)
+    end = datetime.now()
+    total_time = end - start
     print('Total time for {} requests: {}'.format(num_requests, total_time))
```

### Comparing `megadetector-5.0.8/classification/aggregate_classifier_probs.py` & `megadetector-5.0.9/classification/aggregate_classifier_probs.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,55 +1,54 @@
-########
-#
-# aggregate_classifier_probs.py
-#
-# Aggregate probabilities from a classifier's outputs according to a mapping
-# from the desired (target) categories to the classifier's categories.
-#
-# Using the mapping, create a new version of the classifier output CSV with
-# probabilities summed within each target category. Also output a new
-# "index-to-name" JSON file which identifies the sequential order of the target
-# categories.
-# 
-########
+"""
 
-#%%  Example usage
+aggregate_classifier_probs.py
 
-"""
-python aggregate_classifier_probs.py \
-    classifier_output.csv.gz \
-    --target-mapping target_to_classifier_labels.json \
-    --output-csv classifier_output_remapped.csv.gz \
-    --output-label-index label_index_remapped.json
-"""
+Aggregate probabilities from a classifier's outputs according to a mapping
+from the desired (target) categories to the classifier's categories.
 
+Using the mapping, create a new version of the classifier output CSV with
+probabilities summed within each target category. Also output a new
+"index-to-name" JSON file which identifies the sequential order of the target
+categories.
+
+"""
 
 #%% Imports
 
 from __future__ import annotations
 
 import argparse
 import json
 
 import pandas as pd
 from tqdm import tqdm
 
+#%%  Example usage
+
+"""
+python aggregate_classifier_probs.py \
+    classifier_output.csv.gz \
+    --target-mapping target_to_classifier_labels.json \
+    --output-csv classifier_output_remapped.csv.gz \
+    --output-label-index label_index_remapped.json
+"""
 
 #%% Main function
 
 def main(classifier_results_csv_path: str,
          target_mapping_json_path: str,
          output_csv_path: str,
          output_label_index_json_path: str) -> None:
     """
     Main function.
 
     Because the output CSV is often very large, we process it in chunks of 1000
     rows at a time.
     """
+    
     chunked_df_iterator = pd.read_csv(
         classifier_results_csv_path, chunksize=1000, float_precision='high',
         index_col='path')
 
     with open(target_mapping_json_path, 'r') as f:
         target_mapping = json.load(f)
     target_names = sorted(target_mapping.keys())
@@ -77,17 +76,15 @@
     with open(output_label_index_json_path, 'w') as f:
         json.dump(dict(enumerate(target_names)), f, indent=1)
 
 
 #%% Command-line driver
 
 def _parse_args() -> argparse.Namespace:
-    """
-    Parses arguments.
-    """
+    
     parser = argparse.ArgumentParser(
         formatter_class=argparse.ArgumentDefaultsHelpFormatter,
         description='Aggregate classifier probabilities to target classes.')
     parser.add_argument(
         'classifier_results_csv',
         help='path to CSV with classifier probabilities')
     parser.add_argument(
```

### Comparing `megadetector-5.0.8/classification/analyze_failed_images.py` & `megadetector-5.0.9/classification/analyze_failed_images.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,15 @@
-########
-#
-# analyze_failed_images.py
-#
-########
+"""
 
-#%% Example usage
+analyze_failed_images.py
 
 """
-    python analyze_failed_images.py failed.json \
-        -a ACCOUNT -c CONTAINER -s SAS_TOKEN
-"""
 
 #%% Imports and constants
 
-from __future__ import annotations
-
 import argparse
 from collections.abc import Mapping, Sequence
 from concurrent import futures
 import json
 from pprint import pprint
 import threading
 from typing import Any, Optional
@@ -27,14 +18,22 @@
 import requests
 from tqdm import tqdm
 
 from data_management.megadb.megadb_utils import MegadbUtils
 from md_utils import path_utils
 from md_utils import sas_blob_utils
 
+
+#%% Example usage
+
+"""
+    python analyze_failed_images.py failed.json \
+        -a ACCOUNT -c CONTAINER -s SAS_TOKEN
+"""
+
 ImageFile.LOAD_TRUNCATED_IMAGES = False
 
 
 #%% Support functions
 
 def check_image_condition(img_path: str,
                           truncated_images_lock: threading.Lock,
@@ -187,16 +186,15 @@
     for status, img_list in mapping.items():
         print(f'{status}: {len(img_list)}')
         pprint(sorted(img_list))
 
 
 #%% Command-line driver
 
-def _parse_args() -> argparse.Namespace:
-    
+def _parse_args() -> argparse.Namespace:    
     
     parser = argparse.ArgumentParser(
         description='Analyze a list of images that failed to download or crop.')
     parser.add_argument(
         'failed_images', metavar='URL_OR_PATH',
         help='URL or path to text or JSON file containing list of image paths')
     parser.add_argument(
```

### Comparing `megadetector-5.0.8/classification/cache_batchapi_outputs.py` & `megadetector-5.0.9/classification/cache_batchapi_outputs.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,58 +1,58 @@
-########
-#
-# cache_batchapi_outputs.py
-#
-# Script to cache Batch Detection API outputs.
-# 
-# This script can handle either the Batch Detection API JSON Response or the
-# detections JSON.
-# 
-# Batch Detection API Response format:
-# 
-#     {
-#         "Status": {
-#             "request_status": "completed",
-#             "message": {
-#                 "num_failed_shards": 0,
-#                 "output_file_urls": {
-#                     "detections": "https://url/to/detections.json",
-#                     "failed_images": "https://url/to/failed_images.json",
-#                     "images": https://url/to/images.json",
-#                 }
-#             },
-#         },
-#         "Endpoint": "/v3/camera-trap/detection-batch/request_detections",
-#         "TaskId": "ea26326e-7e0d-4524-a9ea-f57a5799d4ba"
-#     }
-# 
-# Detections JSON format:
-#
-#     {
-#         "info": {...}
-#         "detection_categories": {...}
-#         "classification_categories": {...}
-#         "images": [
-#             {
-#                 "file": "path/from/base/dir/image1.jpg",
-#                 "max_detection_conf": 0.926,
-#                 "detections": [{
-#                         "category": "1",
-#                         "conf": 0.061,
-#                         "bbox": [0.0451, 0.1849, 0.3642, 0.4636]
-#                 }]
-#             }
-#         ]
-#     }
-# 
-# Batch Detection API Output Format:
-#
-# github.com/agentmorris/MegaDetector/tree/master/api/batch_processing#api-outputs
-#
-########
+"""
+
+cache_batchapi_outputs.py
+
+Script to cache Batch Detection API outputs.
+
+This script can handle either the Batch Detection API JSON Response or the
+detections JSON.
+
+Batch Detection API Response format:
+
+    {
+        "Status": {
+            "request_status": "completed",
+            "message": {
+                "num_failed_shards": 0,
+                "output_file_urls": {
+                    "detections": "https://url/to/detections.json",
+                    "failed_images": "https://url/to/failed_images.json",
+                    "images": https://url/to/images.json",
+                }
+            },
+        },
+        "Endpoint": "/v3/camera-trap/detection-batch/request_detections",
+        "TaskId": "ea26326e-7e0d-4524-a9ea-f57a5799d4ba"
+    }
+
+Detections JSON format:
+
+    {
+        "info": {...}
+        "detection_categories": {...}
+        "classification_categories": {...}
+        "images": [
+            {
+                "file": "path/from/base/dir/image1.jpg",
+                "max_detection_conf": 0.926,
+                "detections": [{
+                        "category": "1",
+                        "conf": 0.061,
+                        "bbox": [0.0451, 0.1849, 0.3642, 0.4636]
+                }]
+            }
+        ]
+    }
+
+Batch Detection API Output Format:
+
+github.com/agentmorris/MegaDetector/tree/master/api/batch_processing#api-outputs
+
+"""
 
 #%% Imports
 
 from __future__ import annotations
 
 import argparse
 from collections.abc import Mapping
```

### Comparing `megadetector-5.0.8/classification/create_classification_dataset.py` & `megadetector-5.0.9/classification/create_classification_dataset.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,77 +1,67 @@
-########
-#
-# create_classification_dataset.py
-# 
-# Creates a classification dataset CSV with a corresponding JSON file determining
-# the train/val/test split.
-# 
-# This script takes as input a "queried images" JSON file whose keys are paths to
-# images and values are dictionaries containing information relevant for training
-# a classifier, including labels and (optionally) ground-truth bounding boxes.
-# The image paths are in the format `<dataset-name>/<blob-name>` where we assume
-# that the dataset name does not contain '/'.
-# 
-# {
-#     "caltech/cct_images/59f79901-23d2-11e8-a6a3-ec086b02610b.jpg": {
-#         "dataset": "caltech",
-#         "location": 13,
-#         "class": "mountain_lion",  # class from dataset
-#         "bbox": [{"category": "animal",
-#                   "bbox": [0, 0.347, 0.237, 0.257]}],   # ground-truth bbox
-#         "label": ["monutain_lion"]  # labels to use in classifier
-#     },
-#     "caltech/cct_images/59f5fe2b-23d2-11e8-a6a3-ec086b02610b.jpg": {
-#         "dataset": "caltech",
-#         "location": 13,
-#         "class": "mountain_lion",  # class from dataset
-#         "label": ["monutain_lion"]  # labels to use in classifier
-#     },
-#     ...
-# }
-# 
-# We assume that the tuple (dataset, location) identifies a unique location. In
-# other words, we assume that no two datasets have overlapping locations. This
-# probably isn't 100% true, but it's pretty much the best we can do in terms of
-# avoiding overlapping locations between the train/val/test splits.
-# 
-# This script outputs 3 files to <output_dir>:
-# 
-# 1) classification_ds.csv, contains columns:
-#    
-#     - 'path': str, path to cropped images
-#     - 'dataset': str, name of dataset
-#     - 'location': str, location that image was taken, as saved in MegaDB
-#     - 'dataset_class': str, original class assigned to image, as saved in MegaDB
-#     - 'confidence': float, confidence that this crop is of an actual animal,
-#         1.0 if the crop is a "ground truth bounding box" (i.e., from MegaDB),
-#         <= 1.0 if the bounding box was detected by MegaDetector
-#     - 'label': str, comma-separated list of label(s) assigned to this crop for
-#         the sake of classification
-# 
-# 2) label_index.json: maps integer to label name
-#
-#     - keys are string representations of Python integers (JSON requires keys to
-#       be strings), numbered from 0 to num_labels-1
-#     - values are strings, label names
-# 
-# 3) splits.json: serialization of a Python dict that maps each split
-#    ['train', 'val', 'test'] to a list of length-2 lists, where each inner list
-#    is [<dataset>, <location>]
-#
-########
+"""
 
-#%% Example usage
+create_classification_dataset.py
+
+Creates a classification dataset CSV with a corresponding JSON file determining
+the train/val/test split.
+
+This script takes as input a "queried images" JSON file whose keys are paths to
+images and values are dictionaries containing information relevant for training
+a classifier, including labels and (optionally) ground-truth bounding boxes.
+The image paths are in the format `<dataset-name>/<blob-name>` where we assume
+that the dataset name does not contain '/'.
+
+{
+    "caltech/cct_images/59f79901-23d2-11e8-a6a3-ec086b02610b.jpg": {
+        "dataset": "caltech",
+        "location": 13,
+        "class": "mountain_lion",  # class from dataset
+        "bbox": [{"category": "animal",
+                  "bbox": [0, 0.347, 0.237, 0.257]}],   # ground-truth bbox
+        "label": ["monutain_lion"]  # labels to use in classifier
+    },
+    "caltech/cct_images/59f5fe2b-23d2-11e8-a6a3-ec086b02610b.jpg": {
+        "dataset": "caltech",
+        "location": 13,
+        "class": "mountain_lion",  # class from dataset
+        "label": ["monutain_lion"]  # labels to use in classifier
+    },
+    ...
+}
+
+We assume that the tuple (dataset, location) identifies a unique location. In
+other words, we assume that no two datasets have overlapping locations. This
+probably isn't 100% true, but it's pretty much the best we can do in terms of
+avoiding overlapping locations between the train/val/test splits.
+
+This script outputs 3 files to <output_dir>:
+
+1) classification_ds.csv, contains columns:
+   
+    - 'path': str, path to cropped images
+    - 'dataset': str, name of dataset
+    - 'location': str, location that image was taken, as saved in MegaDB
+    - 'dataset_class': str, original class assigned to image, as saved in MegaDB
+    - 'confidence': float, confidence that this crop is of an actual animal,
+        1.0 if the crop is a "ground truth bounding box" (i.e., from MegaDB),
+        <= 1.0 if the bounding box was detected by MegaDetector
+    - 'label': str, comma-separated list of label(s) assigned to this crop for
+        the sake of classification
+
+2) label_index.json: maps integer to label name
+
+    - keys are string representations of Python integers (JSON requires keys to
+      be strings), numbered from 0 to num_labels-1
+    - values are strings, label names
+
+3) splits.json: serialization of a Python dict that maps each split
+   ['train', 'val', 'test'] to a list of length-2 lists, where each inner list
+   is [<dataset>, <location>]
 
-"""
-    python create_classification_dataset.py \
-        run_idfg2 \
-        --queried-images-json run_idfg2/queried_images.json \
-        --cropped-images-dir /ssd/crops_sq \
-        -d $HOME/classifier-training/mdcache -v "4.1" -t 0.8
 """
 
 #%% Imports and constants
 
 from __future__ import annotations
 
 import argparse
@@ -83,14 +73,25 @@
 import numpy as np
 import pandas as pd
 from tqdm import tqdm
 
 from classification import detect_and_crop
 
 
+#%% Example usage
+
+"""
+    python create_classification_dataset.py \
+        run_idfg2 \
+        --queried-images-json run_idfg2/queried_images.json \
+        --cropped-images-dir /ssd/crops_sq \
+        -d $HOME/classifier-training/mdcache -v "4.1" -t 0.8
+"""
+
+
 DATASET_FILENAME = 'classification_ds.csv'
 LABEL_INDEX_FILENAME = 'label_index.json'
 SPLITS_FILENAME = 'splits.json'
 
 
 #%% Main function
```

### Comparing `megadetector-5.0.8/classification/crop_detections.py` & `megadetector-5.0.9/classification/crop_detections.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,62 +1,47 @@
-########
-#
-# crop_detections.py
-#
-# Given a detections JSON file from MegaDetector, crops the bounding boxes above
-# a certain confidence threshold.
-# 
-# This script takes as input a detections JSON file, usually the output of
-# detection/run_tf_detector_batch.py or the output of the Batch API in the
-# "Batch processing API output format".
-# 
-# See https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing.
-# 
-# The script can crop images that are either available locally or that need to be
-# downloaded from an Azure Blob Storage container.
-# 
-# We assume that no image contains over 100 bounding boxes, and we always save
-# crops as RGB .jpg files for consistency. For each image, each bounding box is
-# cropped and saved to a file with a suffix "___cropXX_mdvY.Y.jpg" added to the
-# filename as the original image. "XX" ranges from "00" to "99" and "Y.Y"
-# ndicates the MegaDetector version. Based on the given confidence threshold, we
-# may skip saving certain bounding box crops, but we still increment the bounding
-# box number for skipped boxes.
-# 
-# Example cropped image path (with MegaDetector bbox):
-#
-#   "path/to/image.jpg___crop00_mdv4.1.jpg"
-# 
-# By default, the images are cropped exactly per the given bounding box
-# coordinates. However, if square crops are desired, pass the --square-crops
-# flag. This will always generate a square crop whose size is the larger of the
-# bounding box width or height. In the case that the square crop boundaries exceed
-# the original image size, the crop is padded with 0s.
-# 
-# This script outputs a log file to:
-#    
-#    <output_dir>/crop_detections_log_{timestamp}.json
-#
-# ...which contains images that failed to download and crop properly.
-# 
-########
+"""
 
-#%% Example usage
+crop_detections.py
+
+Given a detections JSON file from MegaDetector, crops the bounding boxes above
+a certain confidence threshold.
+
+This script takes as input a detections JSON file, usually the output of
+detection/run_tf_detector_batch.py or the output of the Batch API in the
+"Batch processing API output format".
+
+See https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing.
+
+The script can crop images that are either available locally or that need to be
+downloaded from an Azure Blob Storage container.
+
+We assume that no image contains over 100 bounding boxes, and we always save
+crops as RGB .jpg files for consistency. For each image, each bounding box is
+cropped and saved to a file with a suffix "___cropXX_mdvY.Y.jpg" added to the
+filename as the original image. "XX" ranges from "00" to "99" and "Y.Y"
+ndicates the MegaDetector version. Based on the given confidence threshold, we
+may skip saving certain bounding box crops, but we still increment the bounding
+box number for skipped boxes.
+
+Example cropped image path (with MegaDetector bbox):
+
+  "path/to/image.jpg___crop00_mdv4.1.jpg"
+
+By default, the images are cropped exactly per the given bounding box
+coordinates. However, if square crops are desired, pass the --square-crops
+flag. This will always generate a square crop whose size is the larger of the
+bounding box width or height. In the case that the square crop boundaries exceed
+the original image size, the crop is padded with 0s.
+
+This script outputs a log file to:
+   
+   <output_dir>/crop_detections_log_{timestamp}.json
+
+...which contains images that failed to download and crop properly.
 
-"""
-python crop_detections.py \
-    detections.json \
-    /path/to/crops \
-    --images-dir /path/to/images \
-    --container-url "https://account.blob.core.windows.net/container?sastoken" \
-    --detector-version "4.1" \
-    --threshold 0.8 \
-    --save-full-images --square-crops \
-    --threads 50 \
-    --logdir "."
 """
 
 #%% Imports
 
 from __future__ import annotations
 
 import argparse
@@ -69,14 +54,30 @@
 from typing import Any, BinaryIO, Optional
 
 from azure.storage.blob import ContainerClient
 from PIL import Image, ImageOps
 from tqdm import tqdm
 
 
+#%% Example usage
+
+"""
+python crop_detections.py \
+    detections.json \
+    /path/to/crops \
+    --images-dir /path/to/images \
+    --container-url "https://account.blob.core.windows.net/container?sastoken" \
+    --detector-version "4.1" \
+    --threshold 0.8 \
+    --save-full-images --square-crops \
+    --threads 50 \
+    --logdir "."
+"""
+
+
 #%% Main function
 
 def main(detections_json_path: str,
          cropped_images_dir: str,
          images_dir: Optional[str],
          container_url: Optional[str],
          detector_version: Optional[str],
```

### Comparing `megadetector-5.0.8/classification/csv_to_json.py` & `megadetector-5.0.9/classification/csv_to_json.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,109 +1,105 @@
-########
-#
-# csv_to_json.py
-#
-# Converts CSV to JSON format for label specification.
-# 
-# There are 3 possible values for the 'type' column in the CSV:
-#
-# - "row": this selects a specific rowfrom the master taxonomy CSV
-#     content syntax: <dataset_name>|<dataset_label>
-#
-# - "datasettaxon": this selects all animals in a taxon from a particular dataset
-#     content syntax: <dataset_name>|<taxon_level>|<taxon_name>
-#
-# - <taxon_level>: this selects all animals in a taxon across all datasets
-#     content syntax: <taxon_name>
-# 
-# Example CSV input:
-#
-"""   
-    # comment lines starting with '#' are allowed
-    output_label,type,content
-
-    cervid,row,idfg|deer
-    cervid,row,idfg|elk
-    cervid,row,idfg|prong
-    cervid,row,idfg_swwlf_2019|elk
-    cervid,row,idfg_swwlf_2019|muledeer
-    cervid,row,idfg_swwlf_2019|whitetaileddeer
-    cervid,max_count,50000
-
-    cervid,family,cervidae
-    cervid,datasettaxon,idfg|family|cervidae
-    cervid,datasettaxon,idfg_swwlf_2019|family|cervidae
-
-    bird,row,idfg_swwlf_2019|bird
-    bird,class,aves
-    bird,max_count,50000
-    bird,prioritize,"[['idfg_swwlf_2019'], ['idfg']]"
-
-    !bird,row,idfg_swwlf_2019|turkey
-    !bird,genus,meleagris
 """
-#
-# Example JSON output:
-#
-"""    
-    {
-        "cervid": {
-            "dataset_labels": {
-                "idfg": ["deer", "elk", "prong"],
-                "idfg_swwlf_2019": ["elk", "muledeer", "whitetaileddeer"]
-            },
-            "taxa": [
-                {
-                    "level": "family",
-                    "name": "cervidae"
-                },
-                {
-                    "level": "family",
-                    "name": "cervidae"
-                    "datasets": ["idfg"]
-                },
-                {
-                    "level": "family",
-                    "name": "cervidae"
-                    "datasets": ["idfg_swwlf_2019"]
-                }
-            ],
-            "max_count": 50000
-        },
-        "bird": {
-            "dataset_labels": {
-                "idfg_swwlf_2019": ["bird"]
-            },
-            "taxa": [
-                {
-                    "level": "class",
-                    "name": "aves"
-                }
-            ],
-            "exclude": {
-                "dataset_labels": {
-                    "idfg_swwlf_2019": ["turkey"]
-                },
-                "taxa": [
-                    {
-                        "level": "genus",
-                        "name": "meleagris"
-                    }
-                ]
-            },
-            "max_count": "50000",
-            "prioritize": [
-                ["idfg_swwlf_2019"],
-                ["idfg"]
-            ],
-        }
-    }
+
+csv_to_json.py
+
+Converts CSV to JSON format for label specification.
+
+There are 3 possible values for the 'type' column in the CSV:
+
+- "row": this selects a specific rowfrom the master taxonomy CSV
+    content syntax: <dataset_name>|<dataset_label>
+
+- "datasettaxon": this selects all animals in a taxon from a particular dataset
+    content syntax: <dataset_name>|<taxon_level>|<taxon_name>
+
+- <taxon_level>: this selects all animals in a taxon across all datasets
+    content syntax: <taxon_name>
+
+Example CSV input:
+
+"
+  # comment lines starting with '#' are allowed
+  output_label,type,content
+  cervid,row,idfg|deer
+  cervid,row,idfg|elk
+  cervid,row,idfg|prong
+  cervid,row,idfg_swwlf_2019|elk
+  cervid,row,idfg_swwlf_2019|muledeer
+  cervid,row,idfg_swwlf_2019|whitetaileddeer
+  cervid,max_count,50000
+  cervid,family,cervidae
+  cervid,datasettaxon,idfg|family|cervidae
+  cervid,datasettaxon,idfg_swwlf_2019|family|cervidae
+  bird,row,idfg_swwlf_2019|bird
+  bird,class,aves
+  bird,max_count,50000
+  bird,prioritize,"[['idfg_swwlf_2019'], ['idfg']]"
+  !bird,row,idfg_swwlf_2019|turkey
+  !bird,genus,meleagris
+"
+
+Example JSON output:
+
+"    
+  {
+      "cervid": {
+          "dataset_labels": {
+              "idfg": ["deer", "elk", "prong"],
+              "idfg_swwlf_2019": ["elk", "muledeer", "whitetaileddeer"]
+          },
+          "taxa": [
+              {
+                  "level": "family",
+                  "name": "cervidae"
+              },
+              {
+                  "level": "family",
+                  "name": "cervidae"
+                  "datasets": ["idfg"]
+              },
+              {
+                  "level": "family",
+                  "name": "cervidae"
+                  "datasets": ["idfg_swwlf_2019"]
+              }
+          ],
+          "max_count": 50000
+      },
+      "bird": {
+          "dataset_labels": {
+              "idfg_swwlf_2019": ["bird"]
+          },
+          "taxa": [
+              {
+                  "level": "class",
+                  "name": "aves"
+              }
+          ],
+          "exclude": {
+              "dataset_labels": {
+                  "idfg_swwlf_2019": ["turkey"]
+              },
+              "taxa": [
+                  {
+                      "level": "genus",
+                      "name": "meleagris"
+                  }
+              ]
+          },
+          "max_count": "50000",
+          "prioritize": [
+              ["idfg_swwlf_2019"],
+              ["idfg"]
+          ],
+      }
+  }
+"
+
 """
-#
-########
 
 #%% Imports
 
 from __future__ import annotations
 
 import argparse
 from collections import defaultdict
@@ -121,14 +117,15 @@
     for label in js:
         js[label] = order_spec_dict(js[label])
     with open(args.output_json_path, 'w') as f:
         json.dump(js, f, indent=args.json_indent)
 
 
 #%% Support functions
+
 def parse_csv_row(obj: dict[str, Any], rowtype: str, content: str) -> None:
     """
     Parses a row in the CSV.
     """
     
     if rowtype == 'row':
         if 'dataset_labels' not in obj:
```

### Comparing `megadetector-5.0.8/classification/detect_and_crop.py` & `megadetector-5.0.9/classification/detect_and_crop.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,115 +1,101 @@
-########
-#
-# detect_and_crop.py
-#
-# Run MegaDetector on images via Batch API, then save crops of the detected
-# bounding boxes.
-# 
-# The input to this script is a "queried images" JSON file, whose keys are paths
-# to images and values are dicts containing information relevant for training
-# a classifier, including labels and (optionally) ground-truth bounding boxes.
-# The image paths are in the format `<dataset-name>/<blob-name>` where we assume
-# that the dataset name does not contain '/'.
-# 
-# {
-#     "caltech/cct_images/59f79901-23d2-11e8-a6a3-ec086b02610b.jpg": {
-#         "dataset": "caltech",
-#         "location": 13,
-#         "class": "mountain_lion",  # class from dataset
-#         "bbox": [{"category": "animal",
-#                   "bbox": [0, 0.347, 0.237, 0.257]}],   # ground-truth bbox
-#         "label": ["monutain_lion"]  # labels to use in classifier
-#     },
-#     "caltech/cct_images/59f5fe2b-23d2-11e8-a6a3-ec086b02610b.jpg": {
-#         "dataset": "caltech",
-#         "location": 13,
-#         "class": "mountain_lion",  # class from dataset
-#         "label": ["monutain_lion"]  # labels to use in classifier
-#     },
-#     ...
-# }
-# 
-# We assume that no image contains over 100 bounding boxes, and we always save
-# crops as RGB .jpg files for consistency. For each image, each bounding box is
-# cropped and saved to a file with a suffix "___cropXX.jpg" (ground truth bbox) or
-# "___cropXX_mdvY.Y.jpg" (detected bbox) added to the filename of the original
-# image. "XX" ranges from "00" to "99" and "Y.Y" indicates the MegaDetector
-# version. If an image has ground truth bounding boxes, we assume that they are
-# exhaustive--i.e., there are no other objects of interest, so we don't need to
-# run MegaDetector on the image. If an image does not have ground truth bounding
-# boxes, we run MegaDetector on the image and label the detected boxes in order
-# from 00 up to 99. Based on the given confidence threshold, we may skip saving
-# certain bounding box crops, but we still increment the bounding box number for
-# skipped boxes.
-# 
-# Example cropped image path (with ground truth bbox from MegaDB)
-#
-#     "path/to/crops/image.jpg___crop00.jpg"
-#
-# Example cropped image path (with MegaDetector bbox)
-#
-#     "path/to/crops/image.jpg___crop00_mdv4.1.jpg"
-# 
-# By default, the images are cropped exactly per the given bounding box
-# coordinates. However, if square crops are desired, pass the --square-crops
-# flag. This will always generate a square crop whose size is the larger of the
-# bounding box width or height. In the case that the square crop boundaries exceed
-# the original image size, the crop is padded with 0s.
-# 
-# This script currently only supports running MegaDetector via the Batch Detection
-# API. See the classification README for instructions on running MegaDetector
-# locally. If running the Batch Detection API, set the following environment
-# variables for the Azure Blob Storage container in which we save the intermediate
-# task lists:
-# 
-#     BATCH_DETECTION_API_URL                  # API URL
-#     CLASSIFICATION_BLOB_STORAGE_ACCOUNT      # storage account name
-#     CLASSIFICATION_BLOB_CONTAINER            # container name
-#     CLASSIFICATION_BLOB_CONTAINER_WRITE_SAS  # SAS token, without leading '?'
-#     DETECTION_API_CALLER                     # allow-listed API caller
-# 
-# This script allows specifying a directory where MegaDetector outputs are cached
-# via the --detector-output-cache-dir argument. This directory must be
-# organized as:
-#
-#   <cache-dir>/<MegaDetector-version>/<dataset-name>.json
-# 
-#     Example: If the `cameratrapssc/classifier-training` Azure blob storage
-#     container is mounted to the local machine via blobfuse, it may be used as
-#     a MegaDetector output cache directory by passing
-#         "cameratrapssc/classifier-training/mdcache/"
-#     as the value for --detector-output-cache-dir.
-# 
-# This script outputs either 1 or 3 files, depending on whether the Batch Detection API
-# is run:
-# 
-# - <output_dir>/detect_and_crop_log_{timestamp}.json
-#     log of images missing detections and images that failed to properly
-#     download and crop
-# - <output_dir>/batchapi_tasklists/{task_id}.json
-#     (if --run-detector) task lists uploaded to the Batch Detection API
-# - <output_dir>/batchapi_response/{task_id}.json
-#     (if --run-detector) task status responses for completed tasks
-# 
-########
+"""
 
-#%% Example usage
+detect_and_crop.py
 
-"""
-    python detect_and_crop.py \
-        base_logdir/queried_images.json \
-        base_logdir \
-        --detector-output-cache-dir /path/to/classifier-training/mdcache \
-        --detector-version 4.1 \
-        --run-detector --resume-file base_logdir/resume.json \
-        --cropped-images-dir /path/to/crops --square-crops --threshold 0.9 \
-        --save-full-images --images-dir /path/to/images --threads 50
-"""
+Run MegaDetector on images via Batch API, then save crops of the detected
+bounding boxes.
+
+The input to this script is a "queried images" JSON file, whose keys are paths
+to images and values are dicts containing information relevant for training
+a classifier, including labels and (optionally) ground-truth bounding boxes.
+The image paths are in the format `<dataset-name>/<blob-name>` where we assume
+that the dataset name does not contain '/'.
+
+{
+    "caltech/cct_images/59f79901-23d2-11e8-a6a3-ec086b02610b.jpg": {
+        "dataset": "caltech",
+        "location": 13,
+        "class": "mountain_lion",  # class from dataset
+        "bbox": [{"category": "animal",
+                  "bbox": [0, 0.347, 0.237, 0.257]}],   # ground-truth bbox
+        "label": ["monutain_lion"]  # labels to use in classifier
+    },
+    "caltech/cct_images/59f5fe2b-23d2-11e8-a6a3-ec086b02610b.jpg": {
+        "dataset": "caltech",
+        "location": 13,
+        "class": "mountain_lion",  # class from dataset
+        "label": ["monutain_lion"]  # labels to use in classifier
+    },
+    ...
+}
+
+We assume that no image contains over 100 bounding boxes, and we always save
+crops as RGB .jpg files for consistency. For each image, each bounding box is
+cropped and saved to a file with a suffix "___cropXX.jpg" (ground truth bbox) or
+"___cropXX_mdvY.Y.jpg" (detected bbox) added to the filename of the original
+image. "XX" ranges from "00" to "99" and "Y.Y" indicates the MegaDetector
+version. If an image has ground truth bounding boxes, we assume that they are
+exhaustive--i.e., there are no other objects of interest, so we don't need to
+run MegaDetector on the image. If an image does not have ground truth bounding
+boxes, we run MegaDetector on the image and label the detected boxes in order
+from 00 up to 99. Based on the given confidence threshold, we may skip saving
+certain bounding box crops, but we still increment the bounding box number for
+skipped boxes.
+
+Example cropped image path (with ground truth bbox from MegaDB)
+
+    "path/to/crops/image.jpg___crop00.jpg"
+
+Example cropped image path (with MegaDetector bbox)
+
+    "path/to/crops/image.jpg___crop00_mdv4.1.jpg"
+
+By default, the images are cropped exactly per the given bounding box
+coordinates. However, if square crops are desired, pass the --square-crops
+flag. This will always generate a square crop whose size is the larger of the
+bounding box width or height. In the case that the square crop boundaries exceed
+the original image size, the crop is padded with 0s.
+
+This script currently only supports running MegaDetector via the Batch Detection
+API. See the classification README for instructions on running MegaDetector
+locally. If running the Batch Detection API, set the following environment
+variables for the Azure Blob Storage container in which we save the intermediate
+task lists:
+
+    BATCH_DETECTION_API_URL                  # API URL
+    CLASSIFICATION_BLOB_STORAGE_ACCOUNT      # storage account name
+    CLASSIFICATION_BLOB_CONTAINER            # container name
+    CLASSIFICATION_BLOB_CONTAINER_WRITE_SAS  # SAS token, without leading '?'
+    DETECTION_API_CALLER                     # allow-listed API caller
+
+This script allows specifying a directory where MegaDetector outputs are cached
+via the --detector-output-cache-dir argument. This directory must be
+organized as:
+
+  <cache-dir>/<MegaDetector-version>/<dataset-name>.json
+
+    Example: If the `cameratrapssc/classifier-training` Azure blob storage
+    container is mounted to the local machine via blobfuse, it may be used as
+    a MegaDetector output cache directory by passing
+        "cameratrapssc/classifier-training/mdcache/"
+    as the value for --detector-output-cache-dir.
+
+This script outputs either 1 or 3 files, depending on whether the Batch Detection API
+is run:
+
+- <output_dir>/detect_and_crop_log_{timestamp}.json
+    log of images missing detections and images that failed to properly
+    download and crop
+- <output_dir>/batchapi_tasklists/{task_id}.json
+    (if --run-detector) task lists uploaded to the Batch Detection API
+- <output_dir>/batchapi_response/{task_id}.json
+    (if --run-detector) task status responses for completed tasks
 
+"""
 
 #%% Imports
 
 from __future__ import annotations
 
 import argparse
 from collections.abc import Collection, Iterable, Mapping, Sequence
@@ -130,14 +116,28 @@
 from classification.cache_batchapi_outputs import cache_detections
 from classification.crop_detections import load_and_crop
 from data_management.megadb import megadb_utils
 from md_utils import path_utils
 from md_utils import sas_blob_utils
 
 
+#%% Example usage
+
+"""
+    python detect_and_crop.py \
+        base_logdir/queried_images.json \
+        base_logdir \
+        --detector-output-cache-dir /path/to/classifier-training/mdcache \
+        --detector-version 4.1 \
+        --run-detector --resume-file base_logdir/resume.json \
+        --cropped-images-dir /path/to/crops --square-crops --threshold 0.9 \
+        --save-full-images --images-dir /path/to/images --threads 50
+"""
+
+
 #%% Main function
 
 def main(queried_images_json_path: str,
          output_dir: str,
          detector_version: str,
          detector_output_cache_base_dir: str,
          run_detector: bool,
```

### Comparing `megadetector-5.0.8/classification/efficientnet/model.py` & `megadetector-5.0.9/classification/efficientnet/model.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/classification/efficientnet/utils.py` & `megadetector-5.0.9/classification/efficientnet/utils.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/classification/evaluate_model.py` & `megadetector-5.0.9/classification/evaluate_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,51 +1,40 @@
-########
-# 
-# evaluate_model.py
-#
-# Evaluate a species classifier.
-# 
-# Currently the implementation of multi-label multi-class classification is
-# non-functional.
-# 
-# Outputs the following files:
-# 
-# 1) outputs_{split}.csv, one file per split, contains columns:
-#     - 'path': str, path to cropped image
-#     - 'label': str
-#     - 'weight': float
-#     - [label names]: float, confidence in each label
-# 
-# 2) overall_metrics.csv, contains columns:
-#     - 'split': str
-#     - 'loss': float, mean per-example loss over entire epoch
-#     - 'acc_top{k}': float, accuracy@k over the entire epoch
-#     - 'loss_weighted' and 'acc_weighted_top{k}': float, weighted versions
-# 
-# 3) confusion_matrices.npz
-#     - keys ['train', 'val', 'test']
-#     - values are np.ndarray, confusion matrices
-# 
-# 4) label_stats.csv, per-label statistics, columns
-#     - 'split': str
-#     - 'label': str
-#     - 'precision': float
-#     - 'recall': float
-#
-########
+"""
 
-#%% Example usage
+evaluate_model.py
+
+Evaluate a species classifier.
+
+Currently the implementation of multi-label multi-class classification is
+non-functional.
+
+Outputs the following files:
+
+1) outputs_{split}.csv, one file per split, contains columns:
+    - 'path': str, path to cropped image
+    - 'label': str
+    - 'weight': float
+    - [label names]: float, confidence in each label
+
+2) overall_metrics.csv, contains columns:
+    - 'split': str
+    - 'loss': float, mean per-example loss over entire epoch
+    - 'acc_top{k}': float, accuracy@k over the entire epoch
+    - 'loss_weighted' and 'acc_weighted_top{k}': float, weighted versions
+
+3) confusion_matrices.npz
+    - keys ['train', 'val', 'test']
+    - values are np.ndarray, confusion matrices
+
+4) label_stats.csv, per-label statistics, columns
+    - 'split': str
+    - 'label': str
+    - 'precision': float
+    - 'recall': float
 
-"""
-    python evaluate_model.py \
-        $BASE_LOGDIR/$LOGDIR/params.json \
-        $BASE_LOGDIR/$LOGDIR/ckpt_XX.pt \
-        --output-dir $BASE_LOGDIR/$LOGDIR \
-        --splits train val test \
-        --batch-size 256
 """
 
 #%% Imports and constants
 
 from __future__ import annotations
 
 import argparse
@@ -60,14 +49,26 @@
 import sklearn.metrics
 import torch
 import torchvision
 import tqdm
 
 from classification import efficientnet, train_classifier
 
+
+#%% Example usage
+
+"""
+    python evaluate_model.py \
+        $BASE_LOGDIR/$LOGDIR/params.json \
+        $BASE_LOGDIR/$LOGDIR/ckpt_XX.pt \
+        --output-dir $BASE_LOGDIR/$LOGDIR \
+        --splits train val test \
+        --batch-size 256
+"""
+
 SPLITS = ['train', 'val', 'test']
 
 
 #%% Support functions
 
 def check_override(params: Mapping[str, Any], key: str,
                    override: Optional[Any]) -> Any:
```

### Comparing `megadetector-5.0.8/classification/identify_mislabeled_candidates.py` & `megadetector-5.0.9/classification/identify_mislabeled_candidates.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,56 +1,49 @@
-########
-#
-# identify_mislabeled_candidates.py
-#
-# Identify images that may have been mislabeled.
-# 
-# A "mislabeled candidate" is defined as an image meeting both criteria:
-#     
-# * according to the ground-truth label, the model made an incorrect prediction
-#
-# * the model's prediction confidence exceeds its confidence for the ground-truth
-#   label by at least <margin>
-# 
-# This script outputs for each dataset a text file containing the filenames of
-# mislabeled candidates, one per line. The text files are saved to:
-#    
-#     <logdir>/mislabeled_candidates_{split}_{dataset}.txt
-# 
-# To this list of files can then be passed to AzCopy to be downloaded:
-#
-"""  
-azcopy cp "http://<url_of_container>?<sas_token>" "/save/files/here" \
-         --list-of-files "/path/to/mislabeled_candidates_{split}_{dataset}.txt"
-"""
-# 
-# To save the filename as <dataset_name>/<blob_name> (instead of just <blob_name>
-# by default), pass the --include-dataset-in-filename flag. Then, the images can
-# be downloaded with:
-#
-"""
-    python data_management/megadb/download_images.py txt \
-        "/path/to/mislabeled_candidates_{split}_{dataset}.txt" \
-        /save/files/here \
-        --threads 50
 """
-#
-# Assumes the following directory layout:
-#     <base_logdir>/
-#         label_index.json
-#         <logdir>/
-#             outputs_{split}.csv.gz
-# 
-########
 
-#%% Example usage
+identify_mislabeled_candidates.py
+
+Identify images that may have been mislabeled.
+
+A "mislabeled candidate" is defined as an image meeting both criteria:
+    
+* according to the ground-truth label, the model made an incorrect prediction
+
+* the model's prediction confidence exceeds its confidence for the ground-truth
+  label by at least <margin>
+
+This script outputs for each dataset a text file containing the filenames of
+mislabeled candidates, one per line. The text files are saved to:
+   
+    <logdir>/mislabeled_candidates_{split}_{dataset}.txt
+
+To this list of files can then be passed to AzCopy to be downloaded:
+
+""  
+azcopy cp "http://<url_of_container>?<sas_token>" "/save/files/here" \
+       --list-of-files "/path/to/mislabeled_candidates_{split}_{dataset}.txt"
+""
+
+To save the filename as <dataset_name>/<blob_name> (instead of just <blob_name>
+by default), pass the --include-dataset-in-filename flag. Then, the images can
+be downloaded with:
+
+""
+  python data_management/megadb/download_images.py txt \
+      "/path/to/mislabeled_candidates_{split}_{dataset}.txt" \
+      /save/files/here \
+      --threads 50
+""
+
+Assumes the following directory layout:
+    <base_logdir>/
+        label_index.json
+        <logdir>/
+            outputs_{split}.csv.gz
 
-"""
-    python identify_mislabeled_candidates.py <base_logdir>/<logdir> \
-        --margin 0.5 --splits val test
 """
 
 #%% Imports
 
 from __future__ import annotations
 
 import argparse
@@ -59,14 +52,22 @@
 import json
 import os
 
 import pandas as pd
 from tqdm import tqdm
 
 
+#%% Example usage
+
+"""
+    python identify_mislabeled_candidates.py <base_logdir>/<logdir> \
+        --margin 0.5 --splits val test
+"""
+
+
 #%% Main function
 
 def main(logdir: str, splits: Iterable[str], margin: float,
          include_dataset_in_filename: bool) -> None:
     
     # load files
     logdir = os.path.normpath(logdir)  # removes any trailing slash
```

### Comparing `megadetector-5.0.8/classification/json_to_azcopy_list.py` & `megadetector-5.0.9/classification/json_to_azcopy_list.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-########
-#
-# json_to_azcopy_list.py
-#
-# Given a queried_images.json file output from json_validator.py, generates
-# one text file <dataset>_images.txt for every dataset included.
-# 
-# See: https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer
-#
-########
+"""
+
+json_to_azcopy_list.py
+
+Given a queried_images.json file output from json_validator.py, generates
+one text file <dataset>_images.txt for every dataset included.
+
+See: https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer
+
+"""
 
 #%% Imports and constants 
 
 import json
 import os
 
 from tqdm import tqdm
```

### Comparing `megadetector-5.0.8/classification/json_validator.py` & `megadetector-5.0.9/classification/json_validator.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,80 +1,72 @@
-########
-#
-# json_validator.py
-#
-# Validates a classification label specification JSON file and optionally
-# queries MegaDB to find matching image files.
-# 
-# See README.md for an example of a classification label specification JSON file.
-# 
-# The validation step takes the classification label specification JSON file and
-# finds the dataset labels that belong to each classification label. It checks
-# that the following conditions hold:
-#    
-# 1) Each classification label specification matches at least 1 dataset label.
-#
-# 2) If the classification label includes a taxonomical specification, then the
-#    taxa is actually a part of our master taxonomy.
-#    
-# 3) If the 'prioritize' key is found for a given label, then the label must
-#    also have a 'max_count' key.
-#
-# 4) If --allow-multilabel=False, then no dataset label is included in more than
-#    one classification label.
-#
-# If --output-dir <output_dir> is given, then we query MegaDB for images
-# that match the dataset labels identified during the validation step. We filter
-# out images that have unaccepted file extensions and images that don't actually
-# exist in Azure Blob Storage. In total, we output the following files:
-# 
-# <output_dir>/
-#
-# - included_dataset_labels.txt
-#   lists the original dataset classes included for each classification label
-#
-# - image_counts_by_label_presample.json
-#    number of images for each classification label after filtering bad
-#    images, but before sampling
-#
-# - image_counts_by_label_sampled.json
-#   number of images for each classification label in queried_images.json
-#
-# - json_validator_log_{timestamp}.json
-#   log of excluded images / labels
-#
-# - queried_images.json
-#  main output file, ex:
-#     
-#     {
-#         "caltech/cct_images/59f5fe2b-23d2-11e8-a6a3-ec086b02610b.jpg": {
-#             "dataset": "caltech",
-#             "location": 13,
-#             "class": "mountain_lion",  // class from dataset
-#             "label": ["monutain_lion"]  // labels to use in classifier
-#         },
-#         "caltech/cct_images/59f79901-23d2-11e8-a6a3-ec086b02610b.jpg": {
-#             "dataset": "caltech",
-#             "location": 13,
-#             "class": "mountain_lion",  // class from dataset
-#             "bbox": [{"category": "animal",
-#                     "bbox": [0, 0.347, 0.237, 0.257]}],
-#             "label": ["monutain_lion"]  // labels to use in classifier
-#         },
-#         ...
-#     }
-#
-########
+"""
 
-#%% Example usage
+json_validator.py
+
+Validates a classification label specification JSON file and optionally
+queries MegaDB to find matching image files.
+
+See README.md for an example of a classification label specification JSON file.
+
+The validation step takes the classification label specification JSON file and
+finds the dataset labels that belong to each classification label. It checks
+that the following conditions hold:
+   
+1) Each classification label specification matches at least 1 dataset label.
+
+2) If the classification label includes a taxonomical specification, then the
+   taxa is actually a part of our master taxonomy.
+   
+3) If the 'prioritize' key is found for a given label, then the label must
+   also have a 'max_count' key.
+
+4) If --allow-multilabel=False, then no dataset label is included in more than
+   one classification label.
+
+If --output-dir <output_dir> is given, then we query MegaDB for images
+that match the dataset labels identified during the validation step. We filter
+out images that have unaccepted file extensions and images that don't actually
+exist in Azure Blob Storage. In total, we output the following files:
+
+<output_dir>/
+
+- included_dataset_labels.txt
+  lists the original dataset classes included for each classification label
+
+- image_counts_by_label_presample.json
+   number of images for each classification label after filtering bad
+   images, but before sampling
+
+- image_counts_by_label_sampled.json
+  number of images for each classification label in queried_images.json
+
+- json_validator_log_{timestamp}.json
+  log of excluded images / labels
+
+- queried_images.json
+ main output file, ex:
+    
+    {
+        "caltech/cct_images/59f5fe2b-23d2-11e8-a6a3-ec086b02610b.jpg": {
+            "dataset": "caltech",
+            "location": 13,
+            "class": "mountain_lion",  // class from dataset
+            "label": ["monutain_lion"]  // labels to use in classifier
+        },
+        "caltech/cct_images/59f79901-23d2-11e8-a6a3-ec086b02610b.jpg": {
+            "dataset": "caltech",
+            "location": 13,
+            "class": "mountain_lion",  // class from dataset
+            "bbox": [{"category": "animal",
+                    "bbox": [0, 0.347, 0.237, 0.257]}],
+            "label": ["monutain_lion"]  // labels to use in classifier
+        },
+        ...
+    }
 
-"""
-    python json_validator.py label_spec.json \
-        $HOME/camera-traps-private/camera_trap_taxonomy_mapping.csv \
-        --output-dir run --json-indent 2
 """
 
 from __future__ import annotations
 
 import argparse
 from collections import defaultdict
 from collections.abc import Container, Iterable, Mapping, MutableMapping
@@ -92,14 +84,23 @@
 from tqdm import tqdm
 
 from data_management.megadb import megadb_utils
 from taxonomy_mapping.taxonomy_graph import (
     build_taxonomy_graph, dag_to_tree, TaxonNode)
 
 
+#%% Example usage
+
+"""
+    python json_validator.py label_spec.json \
+        $HOME/camera-traps-private/camera_trap_taxonomy_mapping.csv \
+        --output-dir run --json-indent 2
+"""
+
+
 #%% Main function
 
 def main(label_spec_json_path: str,
          taxonomy_csv_path: str,
          allow_multilabel: bool = False,
          single_parent_taxonomy: bool = False,
          check_blob_exists: bool | str = False,
```

### Comparing `megadetector-5.0.8/classification/map_classification_categories.py` & `megadetector-5.0.9/classification/map_classification_categories.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,52 +1,43 @@
-########
-#
-# map_classification_categories.py
-#
-# Maps a classifier's output categories to desired target categories.
-# 
-# In this file, we use the following terminology:
-# * "category": a category output by the classifier
-# * "target": name of a desired group, comprising >= 1 classifier categories
-# 
-# Takes as input 2 label specification JSON files:
-#
-# 1) desired label specification JSON file
-#    this should not have a target named "other"
-#    
-# 2) label specification JSON file of trained classifier
-#
-# The mapping is accomplished as follows:
-#    
-# 1. For each category in the classifier label spec, find all taxon nodes that
-#     belong to that category.
-#
-# 2. Given a target in the desired label spec, find all taxon nodes that belong
-#     to that target. If there is any classifier category whose nodes are a
-#     subset of the target nodes, then map the classifier category to that target.
-#     Any partial intersection between a target's nodes and a category's nodes
-#     is considered an error.
-#
-# 3. If there are any classifier categories that have not yet been assigned a
-#     target, group them into the "other" target.
-# 
-# This script outputs a JSON file that maps each target to a list of classifier
-# categories.
-# 
-# The taxonomy mapping parts of this script are very similar to json_validator.py.
-#
-########
+"""
 
-#%% Example usage
+map_classification_categories.py
+
+Maps a classifier's output categories to desired target categories.
+
+In this file, we use the following terminology:
+* "category": a category output by the classifier
+* "target": name of a desired group, comprising >= 1 classifier categories
+
+Takes as input 2 label specification JSON files:
+
+1) desired label specification JSON file
+   this should not have a target named "other"
+   
+2) label specification JSON file of trained classifier
+
+The mapping is accomplished as follows:
+   
+1. For each category in the classifier label spec, find all taxon nodes that
+    belong to that category.
+
+2. Given a target in the desired label spec, find all taxon nodes that belong
+    to that target. If there is any classifier category whose nodes are a
+    subset of the target nodes, then map the classifier category to that target.
+    Any partial intersection between a target's nodes and a category's nodes
+    is considered an error.
+
+3. If there are any classifier categories that have not yet been assigned a
+    target, group them into the "other" target.
+
+This script outputs a JSON file that maps each target to a list of classifier
+categories.
+
+The taxonomy mapping parts of this script are very similar to json_validator.py.
 
-"""
-    python map_classification_categories.py \
-        desired_label_spec.json \
-        /path/to/classifier/label_spec.json \
-        $HOME/camera-traps-private/camera_trap_taxonomy_mapping.csv
 """
 
 #%% Imports
 
 from __future__ import annotations
 
 import argparse
@@ -61,14 +52,24 @@
 import pandas as pd
 from tqdm import tqdm
 
 from taxonomy_mapping.taxonomy_graph import (
     build_taxonomy_graph, dag_to_tree, TaxonNode)
 
 
+#%% Example usage
+
+"""
+    python map_classification_categories.py \
+        desired_label_spec.json \
+        /path/to/classifier/label_spec.json \
+        $HOME/camera-traps-private/camera_trap_taxonomy_mapping.csv
+"""
+
+
 #%% Main function
 
 def main(desired_label_spec_json_path: str,
          classifier_label_spec_json_path: str,
          taxonomy_csv_path: str,
          output_json_path: str,
          classifier_label_index_path: Optional[str]) -> None:
```

### Comparing `megadetector-5.0.8/classification/merge_classification_detection_output.py` & `megadetector-5.0.9/classification/merge_classification_detection_output.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,78 +1,63 @@
-########
-#
-# merge_classification_detection_output.py
-#
-# Merges classification results with Batch Detection API outputs.
-#
-# This script takes 2 main files as input:
-#
-# 1) Either a "dataset CSV" (output of create_classification_dataset.py) or a
-#     "classification results CSV" (output of evaluate_model.py). The CSV is
-#     expected to have columns listed below. The 'label' and [label names] columns
-#     are optional, but at least one of them must be provided.
-#     * 'path': str, path to cropped image
-#         * if passing in a detections JSON, must match
-#             <img_file>___cropXX_mdvY.Y.jpg
-#         * if passing in a queried images JSON, must match
-#             <dataset>/<img_file>___cropXX_mdvY.Y.jpg or
-#             <dataset>/<img_file>___cropXX.jpg
-#     * 'label': str, label assigned to this crop
-#     * [label names]: float, confidence in each label
-# 
-# 2) Either a "detections JSON" (output of MegaDetector) or a "queried images
-#     JSON" (output of json_validatory.py).
-# 
-# If the CSV contains [label names] columns (e.g., output of evaluate_model.py),
-# then each crop's "classifications" output will have one value per category.
-# Categories are sorted decreasing by confidence.
-#     "classifications": [
-#         ["3", 0.901],
-#         ["1", 0.071],
-#         ["4", 0.025],
-#         ["2", 0.003],
-#    ]
-#
-# If the CSV only contains the 'label' column (e.g., output of
-# create_classification_dataset.py), then each crop's "classifications" output
-# will have only one value, with a confidence of 1.0. The label's classification
-# category ID is always greater than 1,000,000, to distinguish it from a predicted
-# category ID.
-#     "classifications": [
-#         ["1000004", 1.0]
-#     ]
-# 
-# If the CSV contains both [label names] and 'label' columns, then both the
-# predicted categories and label category will be included. By default, the
-# label-category is included last; if the --label-first flag is given, then the
-# label category is placed first in the results.
-#     "classifications": [
-#         ["1000004", 1.0],  # label put first if --label-first flag is given
-#         ["3", 0.901],  # all other results are sorted by confidence
-#         ["1", 0.071],
-#         ["4", 0.025],
-#         ["2", 0.003]
-#     ]
-#
-########
+"""
 
-#%% Example usage
+merge_classification_detection_output.py
 
-"""
-    python merge_classification_detection_output.py \
-        BASE_LOGDIR/LOGDIR/outputs_test.csv.gz \
-        BASE_LOGDIR/label_index.json \
-        BASE_LOGDIR/queried_images.json \
-        --classifier-name "efficientnet-b3-idfg-moredata" \
-        --detector-output-cache-dir $HOME/classifier-training/mdcache \
-        --detector-version "4.1" \
-        --output-json BASE_LOGDIR/LOGDIR/classifier_results.json \
-        --datasets idfg idfg_swwlf_2019
-"""
+Merges classification results with Batch Detection API outputs.
+
+This script takes 2 main files as input:
 
+1) Either a "dataset CSV" (output of create_classification_dataset.py) or a
+    "classification results CSV" (output of evaluate_model.py). The CSV is
+    expected to have columns listed below. The 'label' and [label names] columns
+    are optional, but at least one of them must be provided.
+    * 'path': str, path to cropped image
+        * if passing in a detections JSON, must match
+            <img_file>___cropXX_mdvY.Y.jpg
+        * if passing in a queried images JSON, must match
+            <dataset>/<img_file>___cropXX_mdvY.Y.jpg or
+            <dataset>/<img_file>___cropXX.jpg
+    * 'label': str, label assigned to this crop
+    * [label names]: float, confidence in each label
+
+2) Either a "detections JSON" (output of MegaDetector) or a "queried images
+    JSON" (output of json_validatory.py).
+
+If the CSV contains [label names] columns (e.g., output of evaluate_model.py),
+then each crop's "classifications" output will have one value per category.
+Categories are sorted decreasing by confidence.
+    "classifications": [
+        ["3", 0.901],
+        ["1", 0.071],
+        ["4", 0.025],
+        ["2", 0.003],
+   ]
+
+If the CSV only contains the 'label' column (e.g., output of
+create_classification_dataset.py), then each crop's "classifications" output
+will have only one value, with a confidence of 1.0. The label's classification
+category ID is always greater than 1,000,000, to distinguish it from a predicted
+category ID.
+    "classifications": [
+        ["1000004", 1.0]
+    ]
+
+If the CSV contains both [label names] and 'label' columns, then both the
+predicted categories and label category will be included. By default, the
+label-category is included last; if the --label-first flag is given, then the
+label category is placed first in the results.
+    "classifications": [
+        ["1000004", 1.0],  # label put first if --label-first flag is given
+        ["3", 0.901],  # all other results are sorted by confidence
+        ["1", 0.071],
+        ["4", 0.025],
+        ["2", 0.003]
+    ]
+
+"""
 
 #%% Imports
 
 from __future__ import annotations
 
 import argparse
 import datetime
@@ -84,14 +69,29 @@
 
 import pandas as pd
 from tqdm import tqdm
 
 from md_utils.ct_utils import truncate_float
 
 
+#%% Example usage
+
+"""
+    python merge_classification_detection_output.py \
+        BASE_LOGDIR/LOGDIR/outputs_test.csv.gz \
+        BASE_LOGDIR/label_index.json \
+        BASE_LOGDIR/queried_images.json \
+        --classifier-name "efficientnet-b3-idfg-moredata" \
+        --detector-output-cache-dir $HOME/classifier-training/mdcache \
+        --detector-version "4.1" \
+        --output-json BASE_LOGDIR/LOGDIR/classifier_results.json \
+        --datasets idfg idfg_swwlf_2019
+"""
+
+
 #%% Support functions
 
 def row_to_classification_list(row: Mapping[str, Any],
                                label_names: Sequence[str],
                                contains_preds: bool,
                                label_pos: str | None,
                                threshold: float,
```

### Comparing `megadetector-5.0.8/classification/prepare_classification_script.py` & `megadetector-5.0.9/classification/prepare_classification_script.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,191 +1,194 @@
-########
-#
-# prepare_classification_script.py
-#
-# Notebook-y script used to prepare a series of shell commands to run a classifier
-# (other than MegaClassifier) on a MegaDetector result set.
-#
-# Differs from prepare_classification_script_mc.py only in the final class mapping step.
-#
-########
+"""
+
+prepare_classification_script.py
+
+Notebook-y script used to prepare a series of shell commands to run a classifier
+(other than MegaClassifier) on a MegaDetector result set.
+
+Differs from prepare_classification_script_mc.py only in the final class mapping step.
+
+"""
 
 #%% Job options
 
 import os
 
-organization_name = 'idfg'
-job_name = 'idfg-2022-01-27-EOE2021S_Group6'
-input_filename = 'idfg-2022-01-27-EOE2021S_Group6_detections.filtered_rde_0.60_0.85_30_0.20.json'
-image_base = '/datadrive/idfg/EOE2021S_Group6'
-crop_path = os.path.join(os.path.expanduser('~/crops'),job_name + '_crops')
-device_id = 1
-
-working_dir_base = os.path.join(os.path.expanduser('~/postprocessing'),
-                                                   organization_name,
-                                                   job_name)
-
-output_base = os.path.join(working_dir_base,'combined_api_outputs')
-
-assert os.path.isdir(working_dir_base)
-assert os.path.isdir(output_base)
-
-output_file = os.path.join(working_dir_base,'run_idfgclassifier_' + job_name +  '.sh')
-
-input_files = [
-    os.path.join(
-        os.path.expanduser('~/postprocessing'),
-                           organization_name,
-                           job_name,
-                           'combined_api_outputs',
-                           input_filename
-        )
-    ]
+def main():
+    organization_name = 'idfg'
+    job_name = 'idfg-2022-01-27-EOE2021S_Group6'
+    input_filename = 'idfg-2022-01-27-EOE2021S_Group6_detections.filtered_rde_0.60_0.85_30_0.20.json'
+    image_base = '/datadrive/idfg/EOE2021S_Group6'
+    crop_path = os.path.join(os.path.expanduser('~/crops'),job_name + '_crops')
+    device_id = 1
+
+    working_dir_base = os.path.join(os.path.expanduser('~/postprocessing'),
+                                                    organization_name,
+                                                    job_name)
+
+    output_base = os.path.join(working_dir_base,'combined_api_outputs')
+
+    assert os.path.isdir(working_dir_base)
+    assert os.path.isdir(output_base)
+
+    output_file = os.path.join(working_dir_base,'run_idfgclassifier_' + job_name +  '.sh')
+
+    input_files = [
+        os.path.join(
+            os.path.expanduser('~/postprocessing'),
+                            organization_name,
+                            job_name,
+                            'combined_api_outputs',
+                            input_filename
+            )
+        ]
 
-for fn in input_files:
-    assert os.path.isfile(fn)
-    
+    for fn in input_files:
+        assert os.path.isfile(fn)
+        
 
-#%% Constants
+    #%% Constants
 
-include_cropping = False
+    include_cropping = False
 
-classifier_base = os.path.expanduser('~/models/camera_traps/idfg_classifier/idfg_classifier_20200905_042558')
-assert os.path.isdir(classifier_base)
+    classifier_base = os.path.expanduser('~/models/camera_traps/idfg_classifier/idfg_classifier_20200905_042558')
+    assert os.path.isdir(classifier_base)
 
-checkpoint_path = os.path.join(classifier_base,'idfg_classifier_ckpt_14_compiled.pt')
-assert os.path.isfile(checkpoint_path)
+    checkpoint_path = os.path.join(classifier_base,'idfg_classifier_ckpt_14_compiled.pt')
+    assert os.path.isfile(checkpoint_path)
 
-classifier_categories_path = os.path.join(classifier_base,'label_index.json')
-assert os.path.isfile(classifier_categories_path)
+    classifier_categories_path = os.path.join(classifier_base,'label_index.json')
+    assert os.path.isfile(classifier_categories_path)
 
-classifier_output_suffix = '_idfg_classifier_output.csv.gz'
-final_output_suffix = '_idfgclassifier.json'
+    classifier_output_suffix = '_idfg_classifier_output.csv.gz'
+    final_output_suffix = '_idfgclassifier.json'
 
-threshold_str = '0.65'
-n_threads_str = '50'
-image_size_str = '300'
-batch_size_str = '64'
-num_workers_str = '8'
-logdir = working_dir_base
+    threshold_str = '0.65'
+    n_threads_str = '50'
+    image_size_str = '300'
+    batch_size_str = '64'
+    num_workers_str = '8'
+    logdir = working_dir_base
 
-classification_threshold_str = '0.05'
+    classification_threshold_str = '0.05'
 
-# This is just passed along to the metadata in the output file, it has no impact
-# on how the classification scripts run.
-typical_classification_threshold_str = '0.75'
+    # This is just passed along to the metadata in the output file, it has no impact
+    # on how the classification scripts run.
+    typical_classification_threshold_str = '0.75'
 
-classifier_name = 'idfg4'
-        
+    classifier_name = 'idfg4'
+            
 
-#%% Set up environment
+    #%% Set up environment
 
-commands = []
-# commands.append('cd MegaDetector/classification\n')
-# commands.append('conda activate cameratraps-classifier\n')
+    commands = []
+    # commands.append('cd MegaDetector/classification\n')
+    # commands.append('conda activate cameratraps-classifier\n')
 
 
-#%% Crop images
+    #%% Crop images
+
+    if include_cropping:
+        
+        commands.append('\n### Cropping ###\n')
+        
+        # fn = input_files[0]
+        for fn in input_files:
+        
+            input_file_path = fn
+            crop_cmd = ''
+            
+            crop_comment = '\n# Cropping {}\n'.format(fn)
+            crop_cmd += crop_comment
+            
+            crop_cmd += "python crop_detections.py \\\n" + \
+                input_file_path + ' \\\n' + \
+                crop_path + ' \\\n' + \
+                '--images-dir "' + image_base + '"' + ' \\\n' + \
+                '--threshold "' + threshold_str + '"' + ' \\\n' + \
+                '--square-crops ' + ' \\\n' + \
+                '--threads "' + n_threads_str + '"' + ' \\\n' + \
+                '--logdir "' + logdir + '"' + ' \\\n' + \
+                '\n'
+            crop_cmd = '{}'.format(crop_cmd)
+            commands.append(crop_cmd)
+
+
+    #%% Run classifier
+
+    commands.append('\n### Classifying ###\n')
 
-if include_cropping:
-    
-    commands.append('\n### Cropping ###\n')
-    
     # fn = input_files[0]
     for fn in input_files:
-    
+
         input_file_path = fn
-        crop_cmd = ''
+        classifier_output_path = crop_path + classifier_output_suffix
         
-        crop_comment = '\n# Cropping {}\n'.format(fn)
-        crop_cmd += crop_comment
+        classify_cmd = ''
         
-        crop_cmd += "python crop_detections.py \\\n" + \
-        	 input_file_path + ' \\\n' + \
-             crop_path + ' \\\n' + \
-             '--images-dir "' + image_base + '"' + ' \\\n' + \
-             '--threshold "' + threshold_str + '"' + ' \\\n' + \
-             '--square-crops ' + ' \\\n' + \
-             '--threads "' + n_threads_str + '"' + ' \\\n' + \
-             '--logdir "' + logdir + '"' + ' \\\n' + \
-             '\n'
-        crop_cmd = '{}'.format(crop_cmd)
-        commands.append(crop_cmd)
-
+        classify_comment = '\n# Classifying {}\n'.format(fn)
+        classify_cmd += classify_comment
+        
+        classify_cmd += "python run_classifier.py \\\n" + \
+            checkpoint_path + ' \\\n' + \
+            crop_path + ' \\\n' + \
+            classifier_output_path + ' \\\n' + \
+            '--detections-json "' + input_file_path + '"' + ' \\\n' + \
+            '--classifier-categories "' + classifier_categories_path + '"' + ' \\\n' + \
+            '--image-size "' + image_size_str + '"' + ' \\\n' + \
+            '--batch-size "' + batch_size_str + '"' + ' \\\n' + \
+            '--num-workers "' + num_workers_str + '"' + ' \\\n'
+        
+        if device_id is not None:
+            classify_cmd += '--device {}'.format(device_id)
+            
+        classify_cmd += '\n\n'    
+        classify_cmd = '{}'.format(classify_cmd)
+        commands.append(classify_cmd)
+            
 
-#%% Run classifier
+    #%% Merge classification and detection outputs
 
-commands.append('\n### Classifying ###\n')
+    commands.append('\n### Merging ###\n')
 
-# fn = input_files[0]
-for fn in input_files:
+    # fn = input_files[0]
+    for fn in input_files:
 
-    input_file_path = fn
-    classifier_output_path = crop_path + classifier_output_suffix
-    
-    classify_cmd = ''
-    
-    classify_comment = '\n# Classifying {}\n'.format(fn)
-    classify_cmd += classify_comment
-    
-    classify_cmd += "python run_classifier.py \\\n" + \
-    	 checkpoint_path + ' \\\n' + \
-         crop_path + ' \\\n' + \
-         classifier_output_path + ' \\\n' + \
-         '--detections-json "' + input_file_path + '"' + ' \\\n' + \
-         '--classifier-categories "' + classifier_categories_path + '"' + ' \\\n' + \
-         '--image-size "' + image_size_str + '"' + ' \\\n' + \
-         '--batch-size "' + batch_size_str + '"' + ' \\\n' + \
-         '--num-workers "' + num_workers_str + '"' + ' \\\n'
-    
-    if device_id is not None:
-        classify_cmd += '--device {}'.format(device_id)
+        input_file_path = fn
+        classifier_output_path = crop_path + classifier_output_suffix
+        final_output_path = os.path.join(output_base,
+                                        os.path.basename(classifier_output_path)).\
+                                        replace(classifier_output_suffix,
+                                        final_output_suffix)
+        final_output_path = final_output_path.replace('_detections','')
+        final_output_path = final_output_path.replace('_crops','')
         
-    classify_cmd += '\n\n'    
-    classify_cmd = '{}'.format(classify_cmd)
-    commands.append(classify_cmd)
-		
-
-#%% Merge classification and detection outputs
-
-commands.append('\n### Merging ###\n')
-
-# fn = input_files[0]
-for fn in input_files:
-
-    input_file_path = fn
-    classifier_output_path = crop_path + classifier_output_suffix
-    final_output_path = os.path.join(output_base,
-                                     os.path.basename(classifier_output_path)).\
-                                     replace(classifier_output_suffix,
-                                     final_output_suffix)
-    final_output_path = final_output_path.replace('_detections','')
-    final_output_path = final_output_path.replace('_crops','')
-    
-    merge_cmd = ''
-    
-    merge_comment = '\n# Merging {}\n'.format(fn)
-    merge_cmd += merge_comment
-    
-    merge_cmd += "python merge_classification_detection_output.py \\\n" + \
-    	 classifier_output_path + ' \\\n' + \
-         classifier_categories_path + ' \\\n' + \
-         '--output-json "' + final_output_path + '"' + ' \\\n' + \
-         '--detection-json "' + input_file_path + '"' + ' \\\n' + \
-         '--classifier-name "' + classifier_name + '"' + ' \\\n' + \
-         '--threshold "' + classification_threshold_str + '"' + ' \\\n' + \
-         '--typical-confidence-threshold "' + typical_classification_threshold_str + '"' + ' \\\n' + \
-         '\n'
-    merge_cmd = '{}'.format(merge_cmd)
-    commands.append(merge_cmd)
-
-
-#%% Write everything out
-
-with open(output_file,'w') as f:
-    for s in commands:
-        f.write('{}'.format(s))
-
-import stat
-st = os.stat(output_file)
-os.chmod(output_file, st.st_mode | stat.S_IEXEC)
+        merge_cmd = ''
         
+        merge_comment = '\n# Merging {}\n'.format(fn)
+        merge_cmd += merge_comment
+        
+        merge_cmd += "python merge_classification_detection_output.py \\\n" + \
+            classifier_output_path + ' \\\n' + \
+            classifier_categories_path + ' \\\n' + \
+            '--output-json "' + final_output_path + '"' + ' \\\n' + \
+            '--detection-json "' + input_file_path + '"' + ' \\\n' + \
+            '--classifier-name "' + classifier_name + '"' + ' \\\n' + \
+            '--threshold "' + classification_threshold_str + '"' + ' \\\n' + \
+            '--typical-confidence-threshold "' + typical_classification_threshold_str + '"' + ' \\\n' + \
+            '\n'
+        merge_cmd = '{}'.format(merge_cmd)
+        commands.append(merge_cmd)
+
+
+    #%% Write everything out
+
+    with open(output_file,'w') as f:
+        for s in commands:
+            f.write('{}'.format(s))
+
+    import stat
+    st = os.stat(output_file)
+    os.chmod(output_file, st.st_mode | stat.S_IEXEC)
+    
+if __name__ == '__main__':
+    main()
```

### Comparing `megadetector-5.0.8/classification/prepare_classification_script_mc.py` & `megadetector-5.0.9/classification/prepare_classification_script_mc.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,228 +1,228 @@
-########
-#
-# prepare_classification_script_mc.py
-#
-# Notebook-y script used to prepare a series of shell commands to run MegaClassifier
-# on a MegaDetector result set.
-#
-# Differs from prepare_classification_script.py only in the final class mapping step.
-#
-########
-
-#%% Job options
-
-import os
-
-organization_name = 'idfg'
-job_name = 'idfg-2022-01-27-EOE2021S_Group6'
-input_filename = 'idfg-2022-01-27-EOE2021S_Group6_detections.filtered_rde_0.60_0.85_30_0.20.json'
-image_base = '/datadrive/idfg/EOE2021S_Group6'
-crop_path = os.path.join(os.path.expanduser('~/crops'),job_name + '_crops')
-device_id = 0
-
-working_dir_base = os.path.join(os.path.expanduser('~/postprocessing'),
-                                                   organization_name,
-                                                   job_name)
-
-output_base = os.path.join(working_dir_base,'combined_api_outputs')
-
-assert os.path.isdir(working_dir_base)
-assert os.path.isdir(output_base)
-
-output_file = os.path.join(working_dir_base,'run_megaclassifier_' + job_name +  '.sh')
-
-input_files = [
-    os.path.join(
-        os.path.expanduser('~/postprocessing'),
-                           organization_name,
-                           job_name,
-                           'combined_api_outputs',
-                           input_filename
-        )
-    ]
-
-for fn in input_files:
-    assert os.path.isfile(fn)
-    
-
-#%% Constants
-
-classifier_base = os.path.expanduser('~/models/camera_traps/megaclassifier/v0.1/')
-assert os.path.isdir(classifier_base)
-
-checkpoint_path = os.path.join(classifier_base,'v0.1_efficientnet-b3_compiled.pt')
-assert os.path.isfile(checkpoint_path)
-
-classifier_categories_path = os.path.join(classifier_base,'v0.1_index_to_name.json')
-assert os.path.isfile(classifier_categories_path)
-
-target_mapping_path = os.path.join(classifier_base,'idfg_to_megaclassifier_labels.json')
-assert os.path.isfile(target_mapping_path)
-
-classifier_output_suffix = '_megaclassifier_output.csv.gz'
-final_output_suffix = '_megaclassifier.json'
-
-threshold_str = '0.65'
-n_threads_str = '50'
-image_size_str = '300'
-batch_size_str = '64'
-num_workers_str = '8'
-logdir = working_dir_base
-
-classification_threshold_str = '0.05'
-
-# This is just passed along to the metadata in the output file, it has no impact
-# on how the classification scripts run.
-typical_classification_threshold_str = '0.75'
-
-classifier_name = 'megaclassifier_v0.1_efficientnet-b3'
-        
-
-#%% Set up environment
-
-commands = []
-# commands.append('cd MegaDetector/classification\n')
-# commands.append('conda activate cameratraps-classifier\n')
-
-
-#%% Crop images
-
-commands.append('\n### Cropping ###\n')
-
-# fn = input_files[0]
-for fn in input_files:
-
-    input_file_path = fn
-    crop_cmd = ''
-    
-    crop_comment = '\n# Cropping {}\n'.format(fn)
-    crop_cmd += crop_comment
-    
-    crop_cmd += "python crop_detections.py \\\n" + \
-    	 input_file_path + ' \\\n' + \
-         crop_path + ' \\\n' + \
-         '--images-dir "' + image_base + '"' + ' \\\n' + \
-         '--threshold "' + threshold_str + '"' + ' \\\n' + \
-         '--square-crops ' + ' \\\n' + \
-         '--threads "' + n_threads_str + '"' + ' \\\n' + \
-         '--logdir "' + logdir + '"' + ' \\\n' + \
-         '\n'
-    crop_cmd = '{}'.format(crop_cmd)
-    commands.append(crop_cmd)
-
-
-#%% Run classifier
-
-commands.append('\n### Classifying ###\n')
-
-# fn = input_files[0]
-for fn in input_files:
-
-    input_file_path = fn
-    classifier_output_path = crop_path + classifier_output_suffix
-    
-    classify_cmd = ''
-    
-    classify_comment = '\n# Classifying {}\n'.format(fn)
-    classify_cmd += classify_comment
-    
-    classify_cmd += "python run_classifier.py \\\n" + \
-    	 checkpoint_path + ' \\\n' + \
-         crop_path + ' \\\n' + \
-         classifier_output_path + ' \\\n' + \
-         '--detections-json "' + input_file_path + '"' + ' \\\n' + \
-         '--classifier-categories "' + classifier_categories_path + '"' + ' \\\n' + \
-         '--image-size "' + image_size_str + '"' + ' \\\n' + \
-         '--batch-size "' + batch_size_str + '"' + ' \\\n' + \
-         '--num-workers "' + num_workers_str + '"' + ' \\\n'
-    
-    if device_id is not None:
-        classify_cmd += '--device {}'.format(device_id)
-        
-    classify_cmd += '\n\n'        
-    classify_cmd = '{}'.format(classify_cmd)
-    commands.append(classify_cmd)
-		
-
-#%% Remap classifier outputs
-
-commands.append('\n### Remapping ###\n')
-
-# fn = input_files[0]
-for fn in input_files:
-
-    input_file_path = fn
-    classifier_output_path = crop_path + classifier_output_suffix
-    classifier_output_path_remapped = \
-        classifier_output_path.replace(".csv.gz","_remapped.csv.gz")
-    assert not (classifier_output_path == classifier_output_path_remapped)
-    
-    output_label_index = classifier_output_path_remapped.replace(
-        "_remapped.csv.gz","_label_index_remapped.json")
-                                       
-    remap_cmd = ''
-    
-    remap_comment = '\n# Remapping {}\n'.format(fn)
-    remap_cmd += remap_comment
-    
-    remap_cmd += "python aggregate_classifier_probs.py \\\n" + \
-        classifier_output_path + ' \\\n' + \
-        '--target-mapping "' + target_mapping_path + '"' + ' \\\n' + \
-        '--output-csv "' + classifier_output_path_remapped + '"' + ' \\\n' + \
-        '--output-label-index "' + output_label_index + '"' + ' \\\n' + \
-        '\n'
-     
-    remap_cmd = '{}'.format(remap_cmd)
-    commands.append(remap_cmd)
-    
-
-#%% Merge classification and detection outputs
-
-commands.append('\n### Merging ###\n')
-
-# fn = input_files[0]
-for fn in input_files:
-
-    input_file_path = fn
-    classifier_output_path = crop_path + classifier_output_suffix
-    
-    classifier_output_path_remapped = \
-        classifier_output_path.replace(".csv.gz","_remapped.csv.gz")
-    
-    output_label_index = classifier_output_path_remapped.replace(
-        "_remapped.csv.gz","_label_index_remapped.json")
-    
-    final_output_path = os.path.join(output_base,
-                                     os.path.basename(classifier_output_path)).\
-        replace(classifier_output_suffix,
-        final_output_suffix)
-    final_output_path = final_output_path.replace('_detections','')
-    final_output_path = final_output_path.replace('_crops','')
-    
-    merge_cmd = ''
-    
-    merge_comment = '\n# Merging {}\n'.format(fn)
-    merge_cmd += merge_comment
-    
-    merge_cmd += "python merge_classification_detection_output.py \\\n" + \
-    	 classifier_output_path_remapped + ' \\\n' + \
-         output_label_index + ' \\\n' + \
-         '--output-json "' + final_output_path + '"' + ' \\\n' + \
-         '--detection-json "' + input_file_path + '"' + ' \\\n' + \
-         '--classifier-name "' + classifier_name + '"' + ' \\\n' + \
-         '--threshold "' + classification_threshold_str + '"' + ' \\\n' + \
-         '--typical-confidence-threshold "' + typical_classification_threshold_str + '"' + ' \\\n' + \
-         '\n'
-    merge_cmd = '{}'.format(merge_cmd)
-    commands.append(merge_cmd)
-
-
-#%% Write everything out
-
-with open(output_file,'w') as f:
-    for s in commands:
-        f.write('{}'.format(s))
-
-import stat
-st = os.stat(output_file)
-os.chmod(output_file, st.st_mode | stat.S_IEXEC)
+"""
+
+prepare_classification_script_mc.py
+
+Notebook-y script used to prepare a series of shell commands to run MegaClassifier
+on a MegaDetector result set.
+
+Differs from prepare_classification_script.py only in the final class mapping step.
+
+"""
+
+#%% Job options
+
+import os
+
+organization_name = 'idfg'
+job_name = 'idfg-2022-01-27-EOE2021S_Group6'
+input_filename = 'idfg-2022-01-27-EOE2021S_Group6_detections.filtered_rde_0.60_0.85_30_0.20.json'
+image_base = '/datadrive/idfg/EOE2021S_Group6'
+crop_path = os.path.join(os.path.expanduser('~/crops'),job_name + '_crops')
+device_id = 0
+
+working_dir_base = os.path.join(os.path.expanduser('~/postprocessing'),
+                                                   organization_name,
+                                                   job_name)
+
+output_base = os.path.join(working_dir_base,'combined_api_outputs')
+
+assert os.path.isdir(working_dir_base)
+assert os.path.isdir(output_base)
+
+output_file = os.path.join(working_dir_base,'run_megaclassifier_' + job_name +  '.sh')
+
+input_files = [
+    os.path.join(
+        os.path.expanduser('~/postprocessing'),
+                           organization_name,
+                           job_name,
+                           'combined_api_outputs',
+                           input_filename
+        )
+    ]
+
+for fn in input_files:
+    assert os.path.isfile(fn)
+    
+
+#%% Constants
+
+classifier_base = os.path.expanduser('~/models/camera_traps/megaclassifier/v0.1/')
+assert os.path.isdir(classifier_base)
+
+checkpoint_path = os.path.join(classifier_base,'v0.1_efficientnet-b3_compiled.pt')
+assert os.path.isfile(checkpoint_path)
+
+classifier_categories_path = os.path.join(classifier_base,'v0.1_index_to_name.json')
+assert os.path.isfile(classifier_categories_path)
+
+target_mapping_path = os.path.join(classifier_base,'idfg_to_megaclassifier_labels.json')
+assert os.path.isfile(target_mapping_path)
+
+classifier_output_suffix = '_megaclassifier_output.csv.gz'
+final_output_suffix = '_megaclassifier.json'
+
+threshold_str = '0.65'
+n_threads_str = '50'
+image_size_str = '300'
+batch_size_str = '64'
+num_workers_str = '8'
+logdir = working_dir_base
+
+classification_threshold_str = '0.05'
+
+# This is just passed along to the metadata in the output file, it has no impact
+# on how the classification scripts run.
+typical_classification_threshold_str = '0.75'
+
+classifier_name = 'megaclassifier_v0.1_efficientnet-b3'
+        
+
+#%% Set up environment
+
+commands = []
+# commands.append('cd MegaDetector/classification\n')
+# commands.append('conda activate cameratraps-classifier\n')
+
+
+#%% Crop images
+
+commands.append('\n### Cropping ###\n')
+
+# fn = input_files[0]
+for fn in input_files:
+
+    input_file_path = fn
+    crop_cmd = ''
+    
+    crop_comment = '\n# Cropping {}\n'.format(fn)
+    crop_cmd += crop_comment
+    
+    crop_cmd += "python crop_detections.py \\\n" + \
+    	 input_file_path + ' \\\n' + \
+         crop_path + ' \\\n' + \
+         '--images-dir "' + image_base + '"' + ' \\\n' + \
+         '--threshold "' + threshold_str + '"' + ' \\\n' + \
+         '--square-crops ' + ' \\\n' + \
+         '--threads "' + n_threads_str + '"' + ' \\\n' + \
+         '--logdir "' + logdir + '"' + ' \\\n' + \
+         '\n'
+    crop_cmd = '{}'.format(crop_cmd)
+    commands.append(crop_cmd)
+
+
+#%% Run classifier
+
+commands.append('\n### Classifying ###\n')
+
+# fn = input_files[0]
+for fn in input_files:
+
+    input_file_path = fn
+    classifier_output_path = crop_path + classifier_output_suffix
+    
+    classify_cmd = ''
+    
+    classify_comment = '\n# Classifying {}\n'.format(fn)
+    classify_cmd += classify_comment
+    
+    classify_cmd += "python run_classifier.py \\\n" + \
+    	 checkpoint_path + ' \\\n' + \
+         crop_path + ' \\\n' + \
+         classifier_output_path + ' \\\n' + \
+         '--detections-json "' + input_file_path + '"' + ' \\\n' + \
+         '--classifier-categories "' + classifier_categories_path + '"' + ' \\\n' + \
+         '--image-size "' + image_size_str + '"' + ' \\\n' + \
+         '--batch-size "' + batch_size_str + '"' + ' \\\n' + \
+         '--num-workers "' + num_workers_str + '"' + ' \\\n'
+    
+    if device_id is not None:
+        classify_cmd += '--device {}'.format(device_id)
+        
+    classify_cmd += '\n\n'        
+    classify_cmd = '{}'.format(classify_cmd)
+    commands.append(classify_cmd)
+		
+
+#%% Remap classifier outputs
+
+commands.append('\n### Remapping ###\n')
+
+# fn = input_files[0]
+for fn in input_files:
+
+    input_file_path = fn
+    classifier_output_path = crop_path + classifier_output_suffix
+    classifier_output_path_remapped = \
+        classifier_output_path.replace(".csv.gz","_remapped.csv.gz")
+    assert not (classifier_output_path == classifier_output_path_remapped)
+    
+    output_label_index = classifier_output_path_remapped.replace(
+        "_remapped.csv.gz","_label_index_remapped.json")
+                                       
+    remap_cmd = ''
+    
+    remap_comment = '\n# Remapping {}\n'.format(fn)
+    remap_cmd += remap_comment
+    
+    remap_cmd += "python aggregate_classifier_probs.py \\\n" + \
+        classifier_output_path + ' \\\n' + \
+        '--target-mapping "' + target_mapping_path + '"' + ' \\\n' + \
+        '--output-csv "' + classifier_output_path_remapped + '"' + ' \\\n' + \
+        '--output-label-index "' + output_label_index + '"' + ' \\\n' + \
+        '\n'
+     
+    remap_cmd = '{}'.format(remap_cmd)
+    commands.append(remap_cmd)
+    
+
+#%% Merge classification and detection outputs
+
+commands.append('\n### Merging ###\n')
+
+# fn = input_files[0]
+for fn in input_files:
+
+    input_file_path = fn
+    classifier_output_path = crop_path + classifier_output_suffix
+    
+    classifier_output_path_remapped = \
+        classifier_output_path.replace(".csv.gz","_remapped.csv.gz")
+    
+    output_label_index = classifier_output_path_remapped.replace(
+        "_remapped.csv.gz","_label_index_remapped.json")
+    
+    final_output_path = os.path.join(output_base,
+                                     os.path.basename(classifier_output_path)).\
+        replace(classifier_output_suffix,
+        final_output_suffix)
+    final_output_path = final_output_path.replace('_detections','')
+    final_output_path = final_output_path.replace('_crops','')
+    
+    merge_cmd = ''
+    
+    merge_comment = '\n# Merging {}\n'.format(fn)
+    merge_cmd += merge_comment
+    
+    merge_cmd += "python merge_classification_detection_output.py \\\n" + \
+    	 classifier_output_path_remapped + ' \\\n' + \
+         output_label_index + ' \\\n' + \
+         '--output-json "' + final_output_path + '"' + ' \\\n' + \
+         '--detection-json "' + input_file_path + '"' + ' \\\n' + \
+         '--classifier-name "' + classifier_name + '"' + ' \\\n' + \
+         '--threshold "' + classification_threshold_str + '"' + ' \\\n' + \
+         '--typical-confidence-threshold "' + typical_classification_threshold_str + '"' + ' \\\n' + \
+         '\n'
+    merge_cmd = '{}'.format(merge_cmd)
+    commands.append(merge_cmd)
+
+
+#%% Write everything out
+
+with open(output_file,'w') as f:
+    for s in commands:
+        f.write('{}'.format(s))
+
+import stat
+st = os.stat(output_file)
+os.chmod(output_file, st.st_mode | stat.S_IEXEC)
```

### Comparing `megadetector-5.0.8/classification/run_classifier.py` & `megadetector-5.0.9/classification/run_classifier.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,35 +1,25 @@
-########
-#
-# run_classifier.py
-#
-# Run a species classifier.
-#
-# This script is the classifier counterpart to detection/run_tf_detector_batch.py.
-# This script takes as input:
-# 1) a detections JSON file, usually the output of run_tf_detector_batch.py or the
-#     output of the Batch API in the "Batch processing API output format"
-# 2) a path to a directory containing crops of bounding boxes from the detections
-#     JSON file
-# 3) a path to a PyTorch TorchScript compiled model file
-# 4) (if the model is EfficientNet) an image size
-#
-# By default, this script overwrites the detections JSON file, adding in
-# classification results. To output a new JSON file, use the --output argument.
-#
-########
+"""
 
-#%% Example usage
+run_classifier.py
+
+Run a species classifier.
+
+This script is the classifier counterpart to detection/run_tf_detector_batch.py.
+This script takes as input:
+1) a detections JSON file, usually the output of run_tf_detector_batch.py or the
+    output of the Batch API in the "Batch processing API output format"
+2) a path to a directory containing crops of bounding boxes from the detections
+    JSON file
+3) a path to a PyTorch TorchScript compiled model file
+4) (if the model is EfficientNet) an image size
+
+By default, this script overwrites the detections JSON file, adding in
+classification results. To output a new JSON file, use the --output argument.
 
-"""
-    python run_classifier.py \
-        detections.json \
-        /path/to/crops \
-        /path/to/model.pt \
-        --image-size 224
 """
 
 #%% Imports
 
 from __future__ import annotations
 
 import argparse
@@ -45,14 +35,25 @@
 import torch.utils
 import torchvision as tv
 from torchvision.datasets.folder import default_loader
 
 from classification import train_classifier
 
 
+#%% Example usage
+
+"""
+    python run_classifier.py \
+        detections.json \
+        /path/to/crops \
+        /path/to/model.pt \
+        --image-size 224
+"""
+
+
 #%% Classes
 
 class SimpleDataset(torch.utils.data.Dataset):
     """
     Very simple dataset.
     """
```

### Comparing `megadetector-5.0.8/classification/save_mislabeled.py` & `megadetector-5.0.9/classification/save_mislabeled.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-########
-#
-# save_mislabeled.py
-#
-# Update the list of known mislabeled images in MegaDB.
-#
-# List of known mislabeled images is stored in Azure Blob Storage.
-# * storage account: cameratrapsc
-# * container: classifier-training
-# * blob: megadb_mislabeled/{dataset}.csv, one file per dataset
-# 
-# Each file megadb_mislabeled/{dataset}.csv has two columns:
-#    
-# * 'file': str, blob name
-#
-# * 'correct_class': optional str, correct dataset class
-#
-#   if empty, indicates that the existing class in MegaDB is inaccurate, but
-#   the correct class is unknown.
-#
-# This script assumes that the classifier-training container is mounted locally.
-# 
-# Takes as input a CSV file (output from Timelapse) with the following columns:
-#
-# * 'File': str, <blob_basename>
-# * 'RelativePath': str, <dataset>\<blob_dirname>
-# * 'mislabeled': str, values in ['true', 'false']
-# * 'correct_class': either empty or str
-#
-########
+"""
+
+save_mislabeled.py
+
+Update the list of known mislabeled images in MegaDB.
+
+List of known mislabeled images is stored in Azure Blob Storage.
+* storage account: cameratrapsc
+* container: classifier-training
+* blob: megadb_mislabeled/{dataset}.csv, one file per dataset
+
+Each file megadb_mislabeled/{dataset}.csv has two columns:
+   
+* 'file': str, blob name
+
+* 'correct_class': optional str, correct dataset class
+
+  if empty, indicates that the existing class in MegaDB is inaccurate, but
+  the correct class is unknown.
+
+This script assumes that the classifier-training container is mounted locally.
+
+Takes as input a CSV file (output from Timelapse) with the following columns:
+
+* 'File': str, <blob_basename>
+* 'RelativePath': str, <dataset>\<blob_dirname>
+* 'mislabeled': str, values in ['true', 'false']
+* 'correct_class': either empty or str
+
+"""
 
 #%% Imports
 
 import argparse
 import os
 import pathlib
```

### Comparing `megadetector-5.0.8/classification/train_classifier.py` & `megadetector-5.0.9/classification/train_classifier.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,29 +1,19 @@
-########
-#
-# train_classifier.py
-#
-# Train a EfficientNet or ResNet classifier.
-# 
-# Currently the implementation of multi-label multi-class classification is
-# non-functional.
-#
-# During training, start tensorboard from within the classification/ directory:
-#   tensorboard --logdir run --bind_all --samples_per_plugin scalars=0,images=0
-# 
-########
+"""
 
-#%% Example usage
+train_classifier.py
+
+Train a EfficientNet or ResNet classifier.
+
+Currently the implementation of multi-label multi-class classification is
+non-functional.
+
+During training, start tensorboard from within the classification/ directory:
+  tensorboard --logdir run --bind_all --samples_per_plugin scalars=0,images=0
 
-"""    
-    python train_classifier.py run_idfg /ssd/crops_sq \
-        -m "efficientnet-b0" --pretrained --finetune --label-weighted \
-        --epochs 50 --batch-size 512 --lr 1e-4 \
-        --num-workers 12 --seed 123 \
-        --logdir run_idfg
 """
 
 #%% Imports and constants
 
 from __future__ import annotations
 
 import argparse
@@ -46,14 +36,24 @@
 from classification import efficientnet, evaluate_model
 from classification.train_utils import (
     HeapItem, recall_from_confusion_matrix, add_to_heap, fig_to_img,
     imgs_with_confidences, load_dataset_csv, prefix_all_keys)
 from md_visualization import plot_utils
 
 
+#%% Example usage
+
+"""    
+    python train_classifier.py run_idfg /ssd/crops_sq \
+        -m "efficientnet-b0" --pretrained --finetune --label-weighted \
+        --epochs 50 --batch-size 512 --lr 1e-4 \
+        --num-workers 12 --seed 123 \
+        --logdir run_idfg
+"""
+
 # mean/std values from https://pytorch.org/docs/stable/torchvision/models.html
 MEANS = np.asarray([0.485, 0.456, 0.406])
 STDS = np.asarray([0.229, 0.224, 0.225])
 
 VALID_MODELS = sorted(
     set(efficientnet.VALID_MODELS) |
     {'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50'})
```

### Comparing `megadetector-5.0.8/classification/train_classifier_tf.py` & `megadetector-5.0.9/classification/train_classifier_tf.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,31 +1,20 @@
-########
-# 
-# train_classifier_tf.py
-#
-# Train an EfficientNet classifier.
-#
-# Currently the implementation of multi-label multi-class classification is
-# non-functional.
-#
-# During training, start tensorboard from within the classification/ directory:
-#    tensorboard --logdir run --bind_all --samples_per_plugin scalars=0,images=0
-#
-########
+"""
 
-#%% Example usage
+train_classifier_tf.py
 
-"""    
-    python train_classifier_tf.py run_idfg /ssd/crops_sq \
-        -m "efficientnet-b0" --pretrained --finetune --label-weighted \
-        --epochs 50 --batch-size 512 --lr 1e-4 \
-        --seed 123 \
-        --logdir run_idfg
-"""
+Train an EfficientNet classifier.
 
+Currently the implementation of multi-label multi-class classification is
+non-functional.
+
+During training, start tensorboard from within the classification/ directory:
+   tensorboard --logdir run --bind_all --samples_per_plugin scalars=0,images=0
+
+"""
 
 #%% Imports and constants
 
 from __future__ import annotations
 
 import argparse
 from collections import defaultdict
@@ -43,15 +32,14 @@
 import tqdm
 
 from classification.train_utils import (
     HeapItem, recall_from_confusion_matrix, add_to_heap, fig_to_img,
     imgs_with_confidences, load_dataset_csv, prefix_all_keys)
 from md_visualization import plot_utils
 
-
 AUTOTUNE = tf.data.experimental.AUTOTUNE
 
 # match pytorch EfficientNet model names
 EFFICIENTNET_MODELS: Mapping[str, Mapping[str, Any]] = {
     'efficientnet-b0': dict(cls='EfficientNetB0', img_size=224, dropout=0.2),
     'efficientnet-b1': dict(cls='EfficientNetB1', img_size=240, dropout=0.2),
     'efficientnet-b2': dict(cls='EfficientNetB2', img_size=260, dropout=0.3),
@@ -59,14 +47,25 @@
     'efficientnet-b4': dict(cls='EfficientNetB4', img_size=380, dropout=0.4),
     'efficientnet-b5': dict(cls='EfficientNetB5', img_size=456, dropout=0.4),
     'efficientnet-b6': dict(cls='EfficientNetB6', img_size=528, dropout=0.5),
     'efficientnet-b7': dict(cls='EfficientNetB7', img_size=600, dropout=0.5)
 }
 
 
+#%% Example usage
+
+"""    
+    python train_classifier_tf.py run_idfg /ssd/crops_sq \
+        -m "efficientnet-b0" --pretrained --finetune --label-weighted \
+        --epochs 50 --batch-size 512 --lr 1e-4 \
+        --seed 123 \
+        --logdir run_idfg
+"""
+
+
 #%% Support functions
 
 def create_dataset(
         img_files: Sequence[str],
         labels: Sequence[Any],
         sample_weights: Optional[Sequence[float]] = None,
         img_base_dir: str = '',
```

### Comparing `megadetector-5.0.8/classification/train_utils.py` & `megadetector-5.0.9/classification/train_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-########
-# 
-# train_utils.py
-#
-# Utility functions useful for training a classifier.
-#
-# This script should NOT depend on any other file within this repo. It should
-# especially be agnostic to PyTorch vs. TensorFlow.
-#
-########
+"""
+
+train_utils.py
+
+Utility functions useful for training a classifier.
+
+This script should NOT depend on any other file within this repo. It should
+especially be agnostic to PyTorch vs. TensorFlow.
+
+"""
 
 #%% Imports
 
 from __future__ import annotations
 
 from collections.abc import Mapping, Sequence
 import dataclasses
```

### Comparing `megadetector-5.0.8/data_management/cct_json_utils.py` & `megadetector-5.0.9/data_management/cct_json_utils.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,89 +1,111 @@
-########
-#
-# cct_json_utils.py
-#
-# Utilities for working with COCO Camera Traps .json databases
-#
-# https://github.com/agentmorris/MegaDetector/blob/master/data_management/README.md#coco-cameratraps-format
-#
-########
+"""
+
+cct_json_utils.py
+
+Utilities for working with COCO Camera Traps .json databases
+
+https://github.com/agentmorris/MegaDetector/blob/master/data_management/README.md#coco-cameratraps-format
+
+"""
 
 #%% Constants and imports
 
 import json
 import os
 
 from tqdm import tqdm
 from collections import defaultdict, OrderedDict
-from typing import Any, Dict, Iterable, List, Mapping, Optional, Union
-
-JSONObject = Mapping[str, Any]
 
 
 #%% Classes
 
 class CameraTrapJsonUtils:
     """
     Miscellaneous utility functions for working with COCO Camera Traps databases
     """
     
     @staticmethod
     def annotations_to_string(annotations, cat_id_to_name):
         """
         Given a list of annotations and a mapping from class IDs to names, produces
-        a concatenated class list, always sorting alphabetically.
+        a comma-delimited string containing a list of class names, sorted alphabetically.
+        
+        Args:
+            annotations (list): a list of annotation dicts
+            cat_id_to_name (dict): a dict mapping category IDs to category names
+            
+        Returns:
+            str: a comma-delimited list of class names
         """
         
-        class_names = CameraTrapJsonUtils.annotationsToClassnames(annotations, cat_id_to_name)
+        class_names = CameraTrapJsonUtils.annotations_to_class_names(annotations, cat_id_to_name)
         return ','.join(class_names)
 
 
     @staticmethod
-    def annotations_to_classnames(annotations, cat_id_to_name):
+    def annotations_to_class_names(annotations, cat_id_to_name):
         """
         Given a list of annotations and a mapping from class IDs to names, produces
-        a list of class names, always sorting alphabetically.
+        a list of class names, sorted alphabetically.
+        
+        Args:
+            annotations (list): a list of annotation dicts
+            cat_id_to_name (dict): a dict mapping category IDs to category names
+            
+        Returns:
+            list: a list of class names present in [annotations]
         """
         
         # Collect all names
         class_names = [cat_id_to_name[ann['category_id']] for ann in annotations]
         # Make names unique and sort
         class_names = sorted(set(class_names))
         return class_names
 
 
     @staticmethod
-    def order_db_keys(db: JSONObject) -> OrderedDict:
+    def order_db_keys(db):
         """
         Given a dict representing a JSON database in the COCO Camera Trap
-        format, return an OrderedDict with keys in the order of 'info',
+        format, returns an OrderedDict with keys in the order of 'info',
         'categories', 'annotations' and 'images'. When this OrderedDict is
         serialized with json.dump(), the order of the keys are preserved.
 
         Args:
-            db: dict representing a JSON database in the COCO Camera Trap format
+            db (dict): a JSON database in the COCO Camera Trap format
 
         Returns:
-            the same db but as an OrderedDict with keys ordered for readability
+            dict: the same content as [db] but as an OrderedDict with keys ordered for 
+                readability
         """
         
         ordered = OrderedDict([
             ('info', db['info']),
             ('categories', db['categories']),
             ('annotations', db['annotations']),
             ('images', db['images'])])
         return ordered
 
 
     @staticmethod
-    def annotations_groupby_image_field(db_indexed, image_field='seq_id'):
+    def group_annotations_by_image_field(db_indexed, image_field='seq_id'):
         """
         Given an instance of IndexedJsonDb, group annotation entries by a field in the
-        image entry.
+        image entry.  Typically used to find all the annotations associated with a sequence.
+        
+        Args:
+            db_indexed (IndexedJsonDb): an initialized IndexedJsonDb, typically loaded from a 
+                COCO Camera Traps .json file
+            image_field (str, optional): a field by which to group annotations (defaults
+                to 'seq_id')
+        
+        Returns:
+            dict: a dict mapping objects (typically strings, in fact typically sequence IDs) to
+                lists of annotations
         """
         
         image_id_to_image_field = {}
         for image_id, image_entry in db_indexed.image_id_to_image.items():
             image_id_to_image_field[image_id] = image_entry[image_field]
 
         res = defaultdict(list)
@@ -91,35 +113,32 @@
             for annotation_entry in annotations:
                 field_value = image_id_to_image_field[annotation_entry['image_id']]
                 res[field_value].append(annotation_entry)
         return res
 
 
     @staticmethod
-    def get_entries_from_locations(db: JSONObject, locations: Iterable[str]
-                                   ) -> Dict[str, Any]:
+    def get_entries_for_locations(db, locations):
         """
-        Given a dict representing a JSON database in the COCO Camera Trap format, return a dict
+        Given a dict representing a JSON database in the COCO Camera Trap format, returns a dict
         with the 'images' and 'annotations' fields in the CCT format, each is an array that only
-        includes entries in the original `db` that are in the `locations` set.
+        includes entries in the original [db] that are in the [locations] set.
+        
         Args:
-            db: a dict representing a JSON database in the COCO Camera Trap format
-            locations: a set or list of locations to include; each item is a string
+            db (dict): a dict representing a JSON database in the COCO Camera Trap format
+            locations (set): a set or list of locations to include; each item is a string
 
         Returns:
-            a dict with the 'images' and 'annotations' fields in the CCT format
+            dict: a dict with the 'images' and 'annotations' fields in the CCT format
         """
         
         locations = set(locations)
         print('Original DB has {} image and {} annotation entries.'.format(
             len(db['images']), len(db['annotations'])))
-        new_db: Dict[str, Any] = {
-            'images': [],
-            'annotations': []
-        }
+        new_db = { 'images': [], 'annotations': [] }
         new_images = set()
         for i in db['images']:
             # cast location to string as the entries in locations are strings
             if str(i['location']) in locations:
                 new_db['images'].append(i)
                 new_images.add(i['id'])
         for a in db['annotations']:
@@ -135,21 +154,34 @@
     """
     Wrapper for a COCO Camera Traps database.
 
     Handles boilerplate dictionary creation that we do almost every time we load
     a .json database.
     """
 
-    def __init__(self, json_filename: Union[str, JSONObject],
-                 b_normalize_paths: bool = False,
-                 filename_replacements: Optional[Mapping[str, str]] = None,
-                 b_convert_classes_to_lower: bool = True,
-                 b_force_forward_slashes: bool = True):
+    def __init__(self, 
+                 json_filename,
+                 b_normalize_paths=False,
+                 filename_replacements=None,
+                 b_convert_classes_to_lower=True,
+                 b_force_forward_slashes=True):
         """
-        json_filename can also be an existing json db
+        Constructor for IndexedJsonDb that loads from a .json file or CCT-formatted dict.
+        
+        Args:
+            json_filename (str): filename to load, or an already-loaded dict
+            b_normalize_paths (bool, optional): whether to invoke os.path.normpath on 
+                all filenames.  Not relevant if b_force_forward_slashes is True.
+            filename_replacements (dict, optional): a set of string --> string mappings
+                that will trigger replacements in all filenames, typically used to remove
+                leading folders
+            b_convert_classes_to_lower (bool, optional): whether to convert all class
+                names to lowercase
+            b_force_forward_slashes (bool, optional): whether to convert backslashes to
+                forward slashes in all path names            
         """
         
         if isinstance(json_filename, str):
             with open(json_filename) as f:
                 self.db = json.load(f)
         else:
             self.db = json_filename
@@ -193,45 +225,56 @@
             im['file_name']: im['id'] for im in self.db['images']}
 
         # Image ID --> image object
         self.image_id_to_image = {im['id']: im for im in self.db['images']}
 
         # Image ID --> annotations
         # Each image can potentially multiple annotations, hence using lists
-        self.image_id_to_annotations: Dict[str, List[Dict[str, Any]]]
+        self.image_id_to_annotations = {}
         self.image_id_to_annotations = defaultdict(list)
         for ann in self.db['annotations']:
             self.image_id_to_annotations[ann['image_id']].append(ann)
 
     # ...__init__
 
 
-    def get_annotations_for_image(self, image: JSONObject
-                                  ) -> Optional[List[Dict[str, Any]]]:
+    def get_annotations_for_image(self, image):
         """
-        Returns: list of annotations associated with an image,
-            None if the db has not been loaded,
-            [] if no annotations are available
+        Finds all the annnotations associated with the image dict [image].
+        
+        Args:
+            image (dict): an image dict loaded from a CCT .json file.  Only the 'id' field
+                is used.
+            
+        Returns:
+            list: list of annotations associated with this image.  Returns None if the db 
+                has not been loaded, or [] if no annotations are available for this image.
         """
         
         if self.db is None:
             return None
 
         if image['id'] not in self.image_id_to_annotations:
             return []
 
         image_annotations = self.image_id_to_annotations[image['id']]
         return image_annotations
 
 
-    def get_classes_for_image(self, image: JSONObject) -> Optional[List[str]]:
+    def get_classes_for_image(self, image):
         """
-        Returns a list of class names associated with [image]
+        Returns a list of class names associated with [image].
 
-        Returns None is the db has not been loaded, [] if no annotations are available
+        Args:
+            image (dict): an image dict loaded from a CCT .json file.  Only the 'id' field
+                is used.
+            
+        Returns:
+            list: list of class names associated with this image.  Returns None if the db 
+                has not been loaded, or [] if no annotations are available for this image.
         """
         
         if self.db is None:
             return None
 
         if image['id'] not in self.image_id_to_annotations:
             return []
@@ -243,35 +286,35 @@
         class_ids = sorted(set(class_ids))
         class_names = [self.cat_id_to_name[x] for x in class_ids]
 
         return class_names
 
 # ...class IndexedJsonDb
 
-
-#%% Functions
-
 class SequenceOptions:
+    """
+    Options parameterizing the grouping of images into sequences by time.
+    """
     
+    #: Images separated by <= this duration will be grouped into the same sequence.
     episode_interval_seconds = 60.0
 
     
+#%% Functions
+
 def create_sequences(image_info,options=None):
     """
-    Synthesize episodes/sequences/bursts for the images in [image_info].  [image_info]
-    should be a list of dicts in CCT format, i.e. with fields 'file_name','datetime','location'.
-    
-    'filename' should be a string.
-    
-    'datetime' should be a Python datetime object
-    
-    'location' should be a string.
+    Synthesizes episodes/sequences/bursts for the images in [image_info].
     
     Modifies [image_info], populating the 'seq_id', 'seq_num_frames', and 'frame_num' fields
     for each image.
+    
+    Args:
+        image_info (dict): a list of dicts in CCT format, i.e. with fields 'file_name' (str),
+            'datetime' (datetime), and 'location' (str).    
     """
     
     if options is None:
         options = SequenceOptions()
         
     # Find all unique locations
     locations = set()
```

### Comparing `megadetector-5.0.8/data_management/cct_to_md.py` & `megadetector-5.0.9/data_management/cct_to_md.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,158 +1,176 @@
-########
-#
-# cct_to_md.py
-#
-# "Converts" a COCO Camera Traps file to a MD results file.  Currently ignores
-# non-bounding-box annotations, and gives all annotations a confidence of 1.0.
-#
-# The only reason to do this is if you are going to add information to an existing
-# CCT-formatted dataset, and want to do that in Timelapse.
-#
-# Currently assumes that width and height are present in the input data, does not
-# read them from images.
-#
-########
-
-#%% Constants and imports
-
-import os
-import json
-
-from collections import defaultdict
-from tqdm import tqdm
-
-
-#%% Functions
-
-def cct_to_md(input_filename,output_filename=None):
-    
-    ## Validate input
-    
-    assert os.path.isfile(input_filename)
-    
-    if (output_filename is None):
-        
-        tokens = os.path.splitext(input_filename)
-        assert len(tokens) == 2
-        output_filename = tokens[0] + '_md-format' + tokens[1]
-    
-        
-    ## Read input
-    
-    with open(input_filename,'r') as f:
-        d = json.load(f)
-        
-    for s in ['annotations','images','categories']:
-        assert s in d.keys(), 'Cannot find category {} in input file, is this a CCT file?'.format(s)
-        
-    
-    ## Prepare metadata
-    
-    image_id_to_annotations = defaultdict(list)
-    
-    # ann = d['annotations'][0]
-    for ann in tqdm(d['annotations']):
-        image_id_to_annotations[ann['image_id']].append(ann)
-    
-    category_id_to_name = {}
-    for cat in d['categories']:
-        category_id_to_name[str(cat['id'])] = cat['name']
-        
-    results = {}
-    
-    info = {}
-    info['format_version'] = "1.3"
-    info['detector'] = 'cct_to_md'
-    results['info'] = info
-    results['detection_categories'] = category_id_to_name
-        
-    
-    ## Process images
-    
-    images_out = []
-    
-    # im = d['images'][0]
-    for im in tqdm(d['images']):
-        
-        im_out = {}
-        im_out['file'] = im['file_name']
-        im_out['location'] = im['location']
-        im_out['id'] = im['id']
-        
-        image_h = im['height']
-        image_w = im['width']
-        
-        detections = []
-        
-        annotations_this_image = image_id_to_annotations[im['id']]
-        
-        # This field is no longer included in MD output files by default
-        # max_detection_conf = 0
-        
-        for ann in annotations_this_image:
-            
-               if 'bbox' in ann:
-                   
-                   det = {}
-                   det['category'] = str(ann['category_id'])
-                   det['conf'] = 1.0
-                   # max_detection_conf = 1.0
-                   
-                   # MegaDetector: [x,y,width,height] (normalized, origin upper-left)
-                   # CCT: [x,y,width,height] (absolute, origin upper-left)
-                   bbox_in = ann['bbox']
-                   bbox_out = [bbox_in[0]/image_w,bbox_in[1]/image_h,
-                               bbox_in[2]/image_w,bbox_in[3]/image_h]
-                   det['bbox'] = bbox_out
-                   detections.append(det)
-                   
-              # ...if there's a bounding box
-              
-        # ...for each annotation
-        
-        im_out['detections'] = detections
-        
-        # This field is no longer included in MD output files by default
-        # im_out['max_detection_conf'] = max_detection_conf
-    
-        images_out.append(im_out)
-        
-    # ...for each image
-    
-    
-    ## Write output
-    
-    results['images'] = images_out
-    
-    with open(output_filename,'w') as f:
-        json.dump(results, f, indent=1)
-        
-    return output_filename
-
-# ...cct_to_md()    
-    
-
-#%% Interactive driver
-
-if False:
-    
-    pass
-
-    #%%
-
-    input_filename = r"G:\temp\noaa_estuary_fish.json"
-    output_filename = None
-    output_filename = cct_to_md(input_filename,output_filename)
-    
-    #%%
-    
-    from md_visualization import visualize_detector_output
-    
-    visualize_detector_output.visualize_detector_output(
-                              detector_output_path=output_filename,
-                              out_dir=r'g:\temp\fish_output',
-                              images_dir=r'G:\temp\noaa_estuary_fish-images\JPEGImages',
-                              output_image_width=-1,
-                              sample=100,
-                              render_detections_only=True)
-
+"""
+
+cct_to_md.py
+
+"Converts" a COCO Camera Traps file to a MD results file.  Currently ignores
+non-bounding-box annotations, and gives all annotations a confidence of 1.0.
+
+The only reason to do this is if you are going to add information to an existing
+CCT-formatted dataset, and you want to do that in Timelapse.
+
+Currently assumes that width and height are present in the input data, does not
+read them from images.
+
+"""
+
+#%% Constants and imports
+
+import os
+import json
+
+from collections import defaultdict
+from tqdm import tqdm
+
+
+#%% Functions
+
+def cct_to_md(input_filename,output_filename=None):
+    """
+    "Converts" a COCO Camera Traps file to a MD results file.  Currently ignores
+    non-bounding-box annotations, and gives all annotations a confidence of 1.0.
+
+    The only reason to do this is if you are going to add information to an existing
+    CCT-formatted dataset, and you want to do that in Timelapse.
+
+    Currently assumes that width and height are present in the input data, does not
+    read them from images.
+    
+    Args:
+        input_filename (str): the COCO Camera Traps .json file to read
+        output_filename (str, optional): the .json file to write in MD results format
+        
+    Returns:
+        dict: MD-formatted results, identical to the content of [output_filename] if
+        [output_filename] is not None
+    """
+    
+    ## Validate input
+    
+    assert os.path.isfile(input_filename)
+    
+    if (output_filename is None):
+        
+        tokens = os.path.splitext(input_filename)
+        assert len(tokens) == 2
+        output_filename = tokens[0] + '_md-format' + tokens[1]
+    
+        
+    ## Read input
+    
+    with open(input_filename,'r') as f:
+        d = json.load(f)
+        
+    for s in ['annotations','images','categories']:
+        assert s in d.keys(), 'Cannot find category {} in input file, is this a CCT file?'.format(s)
+        
+    
+    ## Prepare metadata
+    
+    image_id_to_annotations = defaultdict(list)
+    
+    # ann = d['annotations'][0]
+    for ann in tqdm(d['annotations']):
+        image_id_to_annotations[ann['image_id']].append(ann)
+    
+    category_id_to_name = {}
+    for cat in d['categories']:
+        category_id_to_name[str(cat['id'])] = cat['name']
+        
+    results = {}
+    
+    info = {}
+    info['format_version'] = "1.3"
+    info['detector'] = 'cct_to_md'
+    results['info'] = info
+    results['detection_categories'] = category_id_to_name
+        
+    
+    ## Process images
+    
+    images_out = []
+    
+    # im = d['images'][0]
+    for im in tqdm(d['images']):
+        
+        im_out = {}
+        im_out['file'] = im['file_name']
+        im_out['location'] = im['location']
+        im_out['id'] = im['id']
+        
+        image_h = im['height']
+        image_w = im['width']
+        
+        detections = []
+        
+        annotations_this_image = image_id_to_annotations[im['id']]
+        
+        # This field is no longer included in MD output files by default
+        # max_detection_conf = 0
+        
+        for ann in annotations_this_image:
+            
+               if 'bbox' in ann:
+                   
+                   det = {}
+                   det['category'] = str(ann['category_id'])
+                   det['conf'] = 1.0
+                   # max_detection_conf = 1.0
+                   
+                   # MegaDetector: [x,y,width,height] (normalized, origin upper-left)
+                   # CCT: [x,y,width,height] (absolute, origin upper-left)
+                   bbox_in = ann['bbox']
+                   bbox_out = [bbox_in[0]/image_w,bbox_in[1]/image_h,
+                               bbox_in[2]/image_w,bbox_in[3]/image_h]
+                   det['bbox'] = bbox_out
+                   detections.append(det)
+                   
+              # ...if there's a bounding box
+              
+        # ...for each annotation
+        
+        im_out['detections'] = detections
+        
+        # This field is no longer included in MD output files by default
+        # im_out['max_detection_conf'] = max_detection_conf
+    
+        images_out.append(im_out)
+        
+    # ...for each image
+    
+    
+    ## Write output
+    
+    results['images'] = images_out
+    
+    with open(output_filename,'w') as f:
+        json.dump(results, f, indent=1)
+        
+    return output_filename
+
+# ...cct_to_md()    
+    
+
+#%% Interactive driver
+
+if False:
+    
+    pass
+
+    #%%
+
+    input_filename = r"G:\temp\noaa_estuary_fish.json"
+    output_filename = None
+    output_filename = cct_to_md(input_filename,output_filename)
+    
+    #%%
+    
+    from md_visualization import visualize_detector_output
+    
+    visualize_detector_output.visualize_detector_output(
+                              detector_output_path=output_filename,
+                              out_dir=r'g:\temp\fish_output',
+                              images_dir=r'g:\temp\noaa_estuary_fish-images\JPEGImages',
+                              output_image_width=-1,
+                              sample=100,
+                              render_detections_only=True)
+
```

### Comparing `megadetector-5.0.8/data_management/cct_to_wi.py` & `megadetector-5.0.9/data_management/cct_to_wi.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,261 +1,289 @@
-########
-#
-# cct_to_wi.py
-#
-# Converts COCO Camera Traps .json files to the Wildlife Insights
-# batch upload format
-#
-# Also see:
-#
-# https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration
-#
-# https://data.naturalsciences.org/wildlife-insights/taxonomy/search
-#
-########
+"""
+
+cct_to_wi.py
+
+Converts COCO Camera Traps .json files to the Wildlife Insights
+batch upload format.
+
+**This is very much just a demo script; all the relevant constants are hard-coded 
+at the top of main().**
+
+But given that caveat, it works.  You need to set up all the paths in the "paths" cell
+at the top of main().
+
+Also see:
+
+* https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration
+* https://data.naturalsciences.org/wildlife-insights/taxonomy/search
+
+"""
 
 #%% Imports
 
 import os
 import json
 import pandas as pd
 from collections import defaultdict 
 
 
-#%% Paths
+#%% Main wrapper
 
-# A COCO camera traps file with information about this dataset
-input_file = r'c:\temp\camera_trap_images_no_people\bellevue_camera_traps.2020-12-26.json'
-assert os.path.isfile(input_file)
-
-# A .json dictionary mapping common names in this dataset to dictionaries with the 
-# WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species
-taxonomy_file = r'c:\temp\camera_trap_images_no_people\belleve_camera_traps_to_wi.json'
-assert os.path.isfile(taxonomy_file)
-
-templates_dir = r'c:\temp\wi_batch_upload_templates'
-assert os.path.isdir(templates_dir)
+def main():
+    """
+    Converts COCO Camera Traps .json files to the Wildlife Insights
+    batch upload format; to use this, you need to modify all the paths in the "Paths"    
+    cell.
+    """
+    
+    #%% Paths
 
-output_base = r'c:\temp\wi_output'
-os.makedirs(output_base,exist_ok = True)
+    # A COCO camera traps file with information about this dataset
+    input_file = r'c:\temp\camera_trap_images_no_people\bellevue_camera_traps.2020-12-26.json'
+    
+    # A .json dictionary mapping common names in this dataset to dictionaries with the 
+    # WI taxonomy fields: common_name, wi_taxon_id, class, order, family, genus, species
+    taxonomy_file = r'c:\temp\camera_trap_images_no_people\bellevue_camera_traps_to_wi.json'
 
+    # The folder where the .csv template files live
+    templates_dir = r'c:\temp\wi_batch_upload_templates'
+    
+    # The folder to which you want to write WI-formatted .csv files
+    output_base = r'c:\temp\wi_output'
+    
+    
+    #%% Path validation
     
-#%% Constants
+    assert os.path.isfile(input_file)
+    assert os.path.isfile(taxonomy_file)
+    assert os.path.isdir(templates_dir)
+    os.makedirs(output_base,exist_ok = True)
 
-projects_file_name = 'Template Wildlife Insights Batch Upload - Projectv1.0.csv'
-deployments_file_name = 'Template Wildlife Insights Batch Upload - Deploymentv1.0.csv'
-images_file_name = 'Template Wildlife Insights Batch Upload - Imagev1.0.csv'
-cameras_file_name = 'Template Wildlife Insights Batch Upload - Camerav1.0.csv'
-
-assert all([os.path.isfile(os.path.join(templates_dir,fn)) for fn in \
-            [projects_file_name,deployments_file_name,images_file_name,cameras_file_name]])
-
-
-#%% Project information
-
-project_info = {}
-project_info['project_name'] = 'Bellevue Camera Traps'
-project_info['project_id'] = 'bct_001'
-project_info['project_short_name'] = 'BCT'
-project_info['project_objectives'] = 'none'
-project_info['project_species'] = 'Multiple'
-project_info['project_species_individual'] = ''
-project_info['project_sensor_layout'] = 'Convenience'
-project_info['project_sensor_layout_targeted_type'] = ''
-project_info['project_bait_use'] = 'No'
-project_info['project_bait_type'] = 'None'
-project_info['project_stratification'] = 'No'
-project_info['project_stratification_type'] = ''
-project_info['project_sensor_method'] = 'Sensor Detection'
-project_info['project_individual_animals'] = 'No'
-project_info['project_admin'] = 'Dan Morris'
-project_info['project_admin_email'] = 'cameratraps@lila.science'
-project_info['country_code'] = 'USA'
-project_info['embargo'] = str(0)
-project_info['initiative_id'] = ''
-project_info['metadata_license'] = 'CC0'
-project_info['image_license'] = 'CC0'
-
-project_info['project_blank_images'] = 'No'
-project_info['project_sensor_cluster'] = 'No'
-
-camera_info = {}
-camera_info['project_id'] = project_info['project_id'] 
-camera_info['camera_id'] = '0000'
-camera_info['make'] = ''
-camera_info['model'] = ''
-camera_info['serial_number'] = ''
-camera_info['year_purchased'] = ''
-
-deployment_info = {}
-
-deployment_info['project_id'] = project_info['project_id'] 
-deployment_info['deployment_id'] = 'test_deployment'
-deployment_info['subproject_name'] = 'test_subproject'
-deployment_info['subproject_design'] = ''
-deployment_info['placename'] = 'yard'
-deployment_info['longitude'] = '47.6101'
-deployment_info['latitude'] = '-122.2015'
-deployment_info['start_date'] = '2016-01-01 00:00:00'
-deployment_info['end_date'] = '2026-01-01 00:00:00'
-deployment_info['event_name'] = ''
-deployment_info['event_description'] = ''
-deployment_info['event_type'] = ''
-deployment_info['bait_type'] = ''
-deployment_info['bait_description'] = ''
-deployment_info['feature_type'] = 'None'
-deployment_info['feature_type_methodology'] = ''
-deployment_info['camera_id'] = camera_info['camera_id']
-deployment_info['quiet_period'] = str(60)
-deployment_info['camera_functioning'] = 'Camera Functioning'
-deployment_info['sensor_height'] = 'Chest height'
-deployment_info['height_other'] = ''
-deployment_info['sensor_orientation'] = 'Parallel'
-deployment_info['orientation_other'] = ''
-deployment_info['recorded_by'] = 'Dan Morris'
+        
+    #%% Constants
 
-image_info = {}
-image_info['identified_by'] = 'Dan Morris'
+    projects_file_name = 'Template Wildlife Insights Batch Upload - Projectv1.0.csv'
+    deployments_file_name = 'Template Wildlife Insights Batch Upload - Deploymentv1.0.csv'
+    images_file_name = 'Template Wildlife Insights Batch Upload - Imagev1.0.csv'
+    cameras_file_name = 'Template Wildlife Insights Batch Upload - Camerav1.0.csv'
+
+    assert all([os.path.isfile(os.path.join(templates_dir,fn)) for fn in \
+                [projects_file_name,deployments_file_name,images_file_name,cameras_file_name]])
+
+
+    #%% Project information
+
+    project_info = {}
+    project_info['project_name'] = 'Bellevue Camera Traps'
+    project_info['project_id'] = 'bct_001'
+    project_info['project_short_name'] = 'BCT'
+    project_info['project_objectives'] = 'none'
+    project_info['project_species'] = 'Multiple'
+    project_info['project_species_individual'] = ''
+    project_info['project_sensor_layout'] = 'Convenience'
+    project_info['project_sensor_layout_targeted_type'] = ''
+    project_info['project_bait_use'] = 'No'
+    project_info['project_bait_type'] = 'None'
+    project_info['project_stratification'] = 'No'
+    project_info['project_stratification_type'] = ''
+    project_info['project_sensor_method'] = 'Sensor Detection'
+    project_info['project_individual_animals'] = 'No'
+    project_info['project_admin'] = 'Dan Morris'
+    project_info['project_admin_email'] = 'cameratraps@lila.science'
+    project_info['country_code'] = 'USA'
+    project_info['embargo'] = str(0)
+    project_info['initiative_id'] = ''
+    project_info['metadata_license'] = 'CC0'
+    project_info['image_license'] = 'CC0'
+
+    project_info['project_blank_images'] = 'No'
+    project_info['project_sensor_cluster'] = 'No'
+
+    camera_info = {}
+    camera_info['project_id'] = project_info['project_id'] 
+    camera_info['camera_id'] = '0000'
+    camera_info['make'] = ''
+    camera_info['model'] = ''
+    camera_info['serial_number'] = ''
+    camera_info['year_purchased'] = ''
+
+    deployment_info = {}
+
+    deployment_info['project_id'] = project_info['project_id'] 
+    deployment_info['deployment_id'] = 'test_deployment'
+    deployment_info['subproject_name'] = 'test_subproject'
+    deployment_info['subproject_design'] = ''
+    deployment_info['placename'] = 'yard'
+    deployment_info['longitude'] = '47.6101'
+    deployment_info['latitude'] = '-122.2015'
+    deployment_info['start_date'] = '2016-01-01 00:00:00'
+    deployment_info['end_date'] = '2026-01-01 00:00:00'
+    deployment_info['event_name'] = ''
+    deployment_info['event_description'] = ''
+    deployment_info['event_type'] = ''
+    deployment_info['bait_type'] = ''
+    deployment_info['bait_description'] = ''
+    deployment_info['feature_type'] = 'None'
+    deployment_info['feature_type_methodology'] = ''
+    deployment_info['camera_id'] = camera_info['camera_id']
+    deployment_info['quiet_period'] = str(60)
+    deployment_info['camera_functioning'] = 'Camera Functioning'
+    deployment_info['sensor_height'] = 'Chest height'
+    deployment_info['height_other'] = ''
+    deployment_info['sensor_orientation'] = 'Parallel'
+    deployment_info['orientation_other'] = ''
+    deployment_info['recorded_by'] = 'Dan Morris'
 
+    image_info = {}
+    image_info['identified_by'] = 'Dan Morris'
 
-#%% Read templates
 
-def parse_fields(templates_dir,file_name):
-    
-    with open(os.path.join(templates_dir,file_name),'r') as f:
-        lines = f.readlines()
-        lines = [s.strip() for s in lines if len(s.strip().replace(',','')) > 0]
-        assert len(lines) == 1, 'Error processing template {}'.format(file_name)
-        fields = lines[0].split(',')
-        print('Parsed {} columns from {}'.format(len(fields),file_name))
-    return fields
-
-projects_fields = parse_fields(templates_dir,projects_file_name)
-deployments_fields = parse_fields(templates_dir,deployments_file_name)
-images_fields = parse_fields(templates_dir,images_file_name)
-cameras_fields = parse_fields(templates_dir,cameras_file_name)
+    #%% Read templates
 
+    def parse_fields(templates_dir,file_name):
+        
+        with open(os.path.join(templates_dir,file_name),'r') as f:
+            lines = f.readlines()
+            lines = [s.strip() for s in lines if len(s.strip().replace(',','')) > 0]
+            assert len(lines) == 1, 'Error processing template {}'.format(file_name)
+            fields = lines[0].split(',')
+            print('Parsed {} columns from {}'.format(len(fields),file_name))
+        return fields
+
+    projects_fields = parse_fields(templates_dir,projects_file_name)
+    deployments_fields = parse_fields(templates_dir,deployments_file_name)
+    images_fields = parse_fields(templates_dir,images_file_name)
+    cameras_fields = parse_fields(templates_dir,cameras_file_name)
 
-#%% Compare dictionary to template lists
 
-def compare_info_to_template(info,template_fields,name):
-    
-    for s in info.keys():
-        assert s in template_fields,'Field {} not specified in {}_fields'.format(s,name)
-    for s in template_fields:
-        assert s in info.keys(),'Field {} not specified in {}_info'.format(s,name)
+    #%% Compare dictionary to template lists
 
+    def compare_info_to_template(info,template_fields,name):
+        
+        for s in info.keys():
+            assert s in template_fields,'Field {} not specified in {}_fields'.format(s,name)
+        for s in template_fields:
+            assert s in info.keys(),'Field {} not specified in {}_info'.format(s,name)
 
-def write_table(file_name,info,template_fields):
-    
-    assert len(info) == len(template_fields)
-    
-    project_output_file = os.path.join(output_base,file_name)
-    with open(project_output_file,'w') as f:
-    
-        # Write the header
-        for i_field,s in enumerate(template_fields):
-            f.write(s)
-            if i_field != len(template_fields)-1:
-                f.write(',')
-        f.write('\n')
-        
-        # Write values
-        for i_field,s in enumerate(template_fields):
-            f.write(info[s])
-            if i_field != len(template_fields)-1:
-                f.write(',')
-        f.write('\n')
-    
 
-#%% Project file
+    def write_table(file_name,info,template_fields):
+        
+        assert len(info) == len(template_fields)
+        
+        project_output_file = os.path.join(output_base,file_name)
+        with open(project_output_file,'w') as f:
+        
+            # Write the header
+            for i_field,s in enumerate(template_fields):
+                f.write(s)
+                if i_field != len(template_fields)-1:
+                    f.write(',')
+            f.write('\n')
+            
+            # Write values
+            for i_field,s in enumerate(template_fields):
+                f.write(info[s])
+                if i_field != len(template_fields)-1:
+                    f.write(',')
+            f.write('\n')
+        
 
-compare_info_to_template(project_info,projects_fields,'project')
-write_table(projects_file_name,project_info,projects_fields)
+    #%% Project file
 
+    compare_info_to_template(project_info,projects_fields,'project')
+    write_table(projects_file_name,project_info,projects_fields)
 
-#%% Camera file
 
-compare_info_to_template(camera_info,cameras_fields,'camera')
-write_table(cameras_file_name,camera_info,cameras_fields)
+    #%% Camera file
 
+    compare_info_to_template(camera_info,cameras_fields,'camera')
+    write_table(cameras_file_name,camera_info,cameras_fields)
 
-#%% Deployment file
 
-compare_info_to_template(deployment_info,deployments_fields,'deployment')
-write_table(deployments_file_name,deployment_info,deployments_fields)
+    #%% Deployment file
 
+    compare_info_to_template(deployment_info,deployments_fields,'deployment')
+    write_table(deployments_file_name,deployment_info,deployments_fields)
 
-#%% Images file
 
-# Read .json file with image information
-with open(input_file,'r') as f:
-    input_data = json.load(f)
+    #%% Images file
 
-# Read taxonomy dictionary
-with open(taxonomy_file,'r') as f:
-    taxonomy_mapping = json.load(f)
-    
-url_base = taxonomy_mapping['url_base']
-taxonomy_mapping = taxonomy_mapping['taxonomy']
+    # Read .json file with image information
+    with open(input_file,'r') as f:
+        input_data = json.load(f)
 
-# Populate output information
-# df = pd.DataFrame(columns = images_fields)
+    # Read taxonomy dictionary
+    with open(taxonomy_file,'r') as f:
+        taxonomy_mapping = json.load(f)
+        
+    url_base = taxonomy_mapping['url_base']
+    taxonomy_mapping = taxonomy_mapping['taxonomy']
 
-category_id_to_name = {cat['id']:cat['name'] for cat in input_data['categories']}
+    # Populate output information
+    # df = pd.DataFrame(columns = images_fields)
 
-image_id_to_annotations = defaultdict(list)
+    category_id_to_name = {cat['id']:cat['name'] for cat in input_data['categories']}
 
-annotations = input_data['annotations']
-                         
-# annotation = annotations[0]
-for annotation in annotations:
-    image_id_to_annotations[annotation['image_id']].append(
-        category_id_to_name[annotation['category_id']])
+    image_id_to_annotations = defaultdict(list)
 
-rows = []
+    annotations = input_data['annotations']
+                            
+    # annotation = annotations[0]
+    for annotation in annotations:
+        image_id_to_annotations[annotation['image_id']].append(
+            category_id_to_name[annotation['category_id']])
 
-# im = input_data['images'][0]
-for im in input_data['images']:
+    rows = []
 
-    row = {}
-    
-    url = url_base + im['file_name'].replace('\\','/')
-    row['project_id'] = project_info['project_id']
-    row['deployment_id'] = deployment_info['deployment_id']
-    row['image_id'] = im['id']
-    row['location'] = url
-    row['identified_by'] = image_info['identified_by']
-    
-    category_names = image_id_to_annotations[im['id']]
-    assert len(category_names) == 1
-    category_name = category_names[0]
-    
-    taxon_info = taxonomy_mapping[category_name]
-    
-    assert len(taxon_info.keys()) == 7
-    
-    for s in taxon_info.keys():
-        row[s] = taxon_info[s]    
-    
-    # We don't have counts, but we can differentiate between zero and 1
-    if category_name == 'empty':
-        row['number_of_objects'] = 0
-    else:
-        row['number_of_objects'] = 1
-        
-    row['uncertainty'] = None
-    row['timestamp'] = im['datetime']; assert isinstance(im['datetime'],str)
-    row['highlighted'] = 0
-    row['age'] = None
-    row['sex'] = None
-    row['animal_recognizable'] = 'No'
-    row['individual_id'] = None
-    row['individual_animal_notes'] = None
-    row['markings'] = None
-    
-    assert len(row) == len(images_fields)
-    rows.append(row)
-    
-df = pd.DataFrame(rows)
+    # im = input_data['images'][0]
+    for im in input_data['images']:
+
+        row = {}
+        
+        url = url_base + im['file_name'].replace('\\','/')
+        row['project_id'] = project_info['project_id']
+        row['deployment_id'] = deployment_info['deployment_id']
+        row['image_id'] = im['id']
+        row['location'] = url
+        row['identified_by'] = image_info['identified_by']
+        
+        category_names = image_id_to_annotations[im['id']]
+        assert len(category_names) == 1
+        category_name = category_names[0]
+        
+        taxon_info = taxonomy_mapping[category_name]
+        
+        assert len(taxon_info.keys()) == 7
+        
+        for s in taxon_info.keys():
+            row[s] = taxon_info[s]    
+        
+        # We don't have counts, but we can differentiate between zero and 1
+        if category_name == 'empty':
+            row['number_of_objects'] = 0
+        else:
+            row['number_of_objects'] = 1
+            
+        row['uncertainty'] = None
+        row['timestamp'] = im['datetime']; assert isinstance(im['datetime'],str)
+        row['highlighted'] = 0
+        row['age'] = None
+        row['sex'] = None
+        row['animal_recognizable'] = 'No'
+        row['individual_id'] = None
+        row['individual_animal_notes'] = None
+        row['markings'] = None
+        
+        assert len(row) == len(images_fields)
+        rows.append(row)
+        
+    df = pd.DataFrame(rows)
+
+    df.to_csv(os.path.join(output_base,images_file_name),index=False)
+
+# ...main()
+
+
+#%% Command-line driver
 
-df.to_csv(os.path.join(output_base,images_file_name),index=False)
+if __name__ == '__main__':
+    main()
```

### Comparing `megadetector-5.0.8/data_management/coco_to_labelme.py` & `megadetector-5.0.9/data_management/coco_to_labelme.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,263 +1,272 @@
-########
-#
-# coco_to_labelme.py
-#
-# Converts a COCO dataset to labelme format (one .json per image file).
-#
-# If you want to convert YOLO data to labelme, use yolo_to_coco, then coco_to_labelme.
-#
-########
-
-#%% Imports and constants
-
-import os
-import json
-
-from tqdm import tqdm
-from collections import defaultdict
-
-from md_visualization.visualization_utils import open_image
-
-
-#%% Functions
-
-def get_labelme_dict_for_image_from_coco_record(im,annotations,categories,info=None):
-    """
-    For the given image struct in COCO format and associated list of annotations, reformat the detections 
-    into labelme format.  Returns a dict.  All annotations in this list should point to this image.
-    
-    "categories" is in the standard COCO format.
-    
-    'height' and 'width' are required in [im].    
-    """
-    
-    image_base_name = os.path.basename(im['file_name'])
-    
-    output_dict = {}
-    if info is not None:
-        output_dict['custom_info'] = info
-    output_dict['version'] = '5.3.0a0'
-    output_dict['flags'] = {}
-    output_dict['shapes'] = []
-    output_dict['imagePath'] = image_base_name
-    output_dict['imageHeight'] = im['height']
-    output_dict['imageWidth'] = im['width']
-    output_dict['imageData'] = None
-    
-    # Store COCO categories in case we want to reconstruct the original IDs later
-    output_dict['coco_categories'] = categories
-    
-    category_id_to_name = {c['id']:c['name'] for c in categories}
-    
-    if 'flags' in im:
-        output_dict['flags'] = im['flags']
-        
-    # ann = annotations[0]
-    for ann in annotations:
-        
-        if 'bbox' not in ann:
-            continue
-        
-        shape = {}
-        shape['label'] = category_id_to_name[ann['category_id']] 
-        shape['shape_type'] = 'rectangle'
-        shape['description'] = ''
-        shape['group_id'] = None
-        
-        # COCO boxes are [x_min, y_min, width_of_box, height_of_box] (absolute)
-        # 
-        # labelme boxes are [[x0,y0],[x1,y1]] (absolute)
-        x0 = ann['bbox'][0]
-        y0 = ann['bbox'][1]
-        x1 = ann['bbox'][0] + ann['bbox'][2]
-        y1 = ann['bbox'][1] + ann['bbox'][3]
-        
-        shape['points'] = [[x0,y0],[x1,y1]]
-        output_dict['shapes'].append(shape)
-    
-    # ...for each detection
-    
-    return output_dict
-
-# ...def get_labelme_dict_for_image()
-
-
-def coco_to_labelme(coco_data,image_base,overwrite=False,bypass_image_size_check=False,verbose=False):
-    """
-    For all the images in [coco_data] (a dict or a filename), write a .json file in 
-    labelme format alongside the corresponding relative path within image_base.    
-    """
-    
-    # Load COCO data if necessary
-    if isinstance(coco_data,str):
-        with open(coco_data,'r') as f:
-            coco_data = json.load(f)
-    assert isinstance(coco_data,dict)
-     
-    
-    ## Read image sizes if necessary
-    
-    if bypass_image_size_check:
-        
-        print('Bypassing size check')
-        
-    else:
-    
-        # TODO: parallelize this loop
-        
-        print('Reading/validating image sizes...')
-        
-        # im = coco_data['images'][0]
-        for im in tqdm(coco_data['images']):
-            
-            # Make sure this file exists
-            im_full_path = os.path.join(image_base,im['file_name'])
-            assert os.path.isfile(im_full_path), 'Image file {} does not exist'.format(im_full_path)
-            
-            # Load w/h information if necessary
-            if 'height' not in im or 'width' not in im:
-                
-                try:
-                    pil_im = open_image(im_full_path)
-                    im['width'] = pil_im.width
-                    im['height'] = pil_im.height
-                except Exception:
-                    print('Warning: cannot open image {}'.format(im_full_path))
-                    if 'failure' not in im:
-                        im['failure'] = 'Failure image access'
-    
-            # ...if we need to read w/h information
-            
-        # ...for each image
-    
-    # ...if we need to load image sizes
-    
-    
-    ## Generate labelme files
-    
-    print('Generating .json files...')
-    
-    image_id_to_annotations = defaultdict(list)
-    for ann in coco_data['annotations']:
-        image_id_to_annotations[ann['image_id']].append(ann)
-        
-    n_json_files_written = 0
-    n_json_files_error = 0
-    n_json_files_exist = 0
-    
-    # Write output
-    for im in tqdm(coco_data['images']):
-        
-        # Skip this image if it failed to load in whatever system generated this COCO file
-        skip_image = False
-        
-        # Errors are represented differently depending on the source
-        for error_string in ('failure','error'):
-            if (error_string in im) and (im[error_string] is not None):
-                if verbose:
-                    print('Warning: skipping labelme file generation for failed image {}'.format(
-                        im['file_name']))
-                skip_image = True
-                n_json_files_error += 1
-                break
-        if skip_image:
-            continue
-                    
-        im_full_path = os.path.join(image_base,im['file_name'])
-        json_path = os.path.splitext(im_full_path)[0] + '.json'
-        
-        if (not overwrite) and (os.path.isfile(json_path)):
-            if verbose:
-                print('Skipping existing file {}'.format(json_path))
-            n_json_files_exist += 1
-            continue
-    
-        annotations_this_image = image_id_to_annotations[im['id']]
-        output_dict = get_labelme_dict_for_image_from_coco_record(im,
-                                                                  annotations_this_image,
-                                                                  coco_data['categories'],
-                                                                  info=None)
-        
-        n_json_files_written += 1
-        with open(json_path,'w') as f:
-            json.dump(output_dict,f,indent=1)
-            
-    # ...for each image
-    
-    print('\nWrote {} .json files (skipped {} for errors, {} because they exist)'.format(
-        n_json_files_written,n_json_files_error,n_json_files_exist))
-    
-# ...def coco_to_labelme()
-
-
-#%% Interactive driver
-
-if False:
-    
-    pass
-
-    #%% Configure options
-    
-    coco_file = \
-        r'C:\\temp\\snapshot-exploration\\images\\training-images-good\\training-images-good_from_yolo.json'
-    image_folder = os.path.dirname(coco_file)    
-    overwrite = True    
-    
-    
-    #%% Programmatic execution
-    
-    coco_to_labelme(coco_data=coco_file,image_base=image_folder,overwrite=overwrite)
-
-    
-    #%% Command-line execution
-    
-    s = 'python coco_to_labelme.py "{}" "{}"'.format(coco_file,image_folder)
-    if overwrite:
-        s += ' --overwrite'
-        
-    print(s)
-    import clipboard; clipboard.copy(s)
-
-
-    #%% Opening labelme
-    
-    s = 'python labelme {}'.format(image_folder)
-    print(s)
-    import clipboard; clipboard.copy(s)
-    
-
-#%% Command-line driver
-
-import sys,argparse
-
-def main():
-
-    parser = argparse.ArgumentParser(
-        description='Convert a COCO database to labelme annotation format')
-    
-    parser.add_argument(
-        'coco_file',
-        type=str,
-        help='Path to COCO data file (.json)')
-    
-    parser.add_argument(
-        'image_base',
-        type=str,
-        help='Path to images (also the output folder)')
-    
-    parser.add_argument(
-        '--overwrite',
-        action='store_true',
-        help='Overwrite existing labelme .json files')
-
-    if len(sys.argv[1:]) == 0:
-        parser.print_help()
-        parser.exit()
-
-    args = parser.parse_args()
-
-    coco_to_labelme(coco_data=args.coco_file,image_base=args.image_base,overwrite=args.overwrite)    
-    
-    
-if __name__ == '__main__':
-    main()
+"""
+
+coco_to_labelme.py
+
+Converts a COCO dataset to labelme format (one .json per image file).
+
+If you want to convert YOLO-formatted data to labelme format, use yolo_to_coco, then 
+coco_to_labelme.
+
+"""
+
+#%% Imports and constants
+
+import os
+import json
+
+from tqdm import tqdm
+from collections import defaultdict
+
+from md_visualization.visualization_utils import open_image
+
+
+#%% Functions
+
+def get_labelme_dict_for_image_from_coco_record(im,annotations,categories,info=None):
+    """
+    For the given image struct in COCO format and associated list of annotations, reformats the 
+    detections into labelme format.  
+    
+    Args:
+        im (dict): image dict, as loaded from a COCO .json file; 'height' and 'width' are required
+        annotations (list): a list of annotations that refer to this image (this function errors if 
+            that's not the case)
+        categories (list): a list of category in dicts in COCO format ({'id':x,'name':'s'})
+        info (dict, optional): a dict to store in a non-standard "custom_info"  field in the output
+        
+    Returns:
+        dict: a dict in labelme format, suitable for writing to a labelme .json file
+    """
+    
+    image_base_name = os.path.basename(im['file_name'])
+    
+    output_dict = {}
+    if info is not None:
+        output_dict['custom_info'] = info
+    output_dict['version'] = '5.3.0a0'
+    output_dict['flags'] = {}
+    output_dict['shapes'] = []
+    output_dict['imagePath'] = image_base_name
+    output_dict['imageHeight'] = im['height']
+    output_dict['imageWidth'] = im['width']
+    output_dict['imageData'] = None
+    
+    # Store COCO categories in case we want to reconstruct the original IDs later
+    output_dict['coco_categories'] = categories
+    
+    category_id_to_name = {c['id']:c['name'] for c in categories}
+    
+    if 'flags' in im:
+        output_dict['flags'] = im['flags']
+        
+    # ann = annotations[0]
+    for ann in annotations:
+        
+        assert ann['image_id'] == im['id'], 'Annotation {} does not refer to image {}'.format(
+            ann['id'],im['id'])
+        
+        if 'bbox' not in ann:
+            continue
+        
+        shape = {}
+        shape['label'] = category_id_to_name[ann['category_id']] 
+        shape['shape_type'] = 'rectangle'
+        shape['description'] = ''
+        shape['group_id'] = None
+        
+        # COCO boxes are [x_min, y_min, width_of_box, height_of_box] (absolute)
+        # 
+        # labelme boxes are [[x0,y0],[x1,y1]] (absolute)
+        x0 = ann['bbox'][0]
+        y0 = ann['bbox'][1]
+        x1 = ann['bbox'][0] + ann['bbox'][2]
+        y1 = ann['bbox'][1] + ann['bbox'][3]
+        
+        shape['points'] = [[x0,y0],[x1,y1]]
+        output_dict['shapes'].append(shape)
+    
+    # ...for each detection
+    
+    return output_dict
+
+# ...def get_labelme_dict_for_image()
+
+
+def coco_to_labelme(coco_data,image_base,overwrite=False,bypass_image_size_check=False,verbose=False):
+    """
+    For all the images in [coco_data] (a dict or a filename), write a .json file in 
+    labelme format alongside the corresponding relative path within image_base.    
+    """
+    
+    # Load COCO data if necessary
+    if isinstance(coco_data,str):
+        with open(coco_data,'r') as f:
+            coco_data = json.load(f)
+    assert isinstance(coco_data,dict)
+     
+    
+    ## Read image sizes if necessary
+    
+    if bypass_image_size_check:
+        
+        print('Bypassing size check')
+        
+    else:
+    
+        # TODO: parallelize this loop
+        
+        print('Reading/validating image sizes...')
+        
+        # im = coco_data['images'][0]
+        for im in tqdm(coco_data['images']):
+            
+            # Make sure this file exists
+            im_full_path = os.path.join(image_base,im['file_name'])
+            assert os.path.isfile(im_full_path), 'Image file {} does not exist'.format(im_full_path)
+            
+            # Load w/h information if necessary
+            if 'height' not in im or 'width' not in im:
+                
+                try:
+                    pil_im = open_image(im_full_path)
+                    im['width'] = pil_im.width
+                    im['height'] = pil_im.height
+                except Exception:
+                    print('Warning: cannot open image {}'.format(im_full_path))
+                    if 'failure' not in im:
+                        im['failure'] = 'Failure image access'
+    
+            # ...if we need to read w/h information
+            
+        # ...for each image
+    
+    # ...if we need to load image sizes
+    
+    
+    ## Generate labelme files
+    
+    print('Generating .json files...')
+    
+    image_id_to_annotations = defaultdict(list)
+    for ann in coco_data['annotations']:
+        image_id_to_annotations[ann['image_id']].append(ann)
+        
+    n_json_files_written = 0
+    n_json_files_error = 0
+    n_json_files_exist = 0
+    
+    # Write output
+    for im in tqdm(coco_data['images']):
+        
+        # Skip this image if it failed to load in whatever system generated this COCO file
+        skip_image = False
+        
+        # Errors are represented differently depending on the source
+        for error_string in ('failure','error'):
+            if (error_string in im) and (im[error_string] is not None):
+                if verbose:
+                    print('Warning: skipping labelme file generation for failed image {}'.format(
+                        im['file_name']))
+                skip_image = True
+                n_json_files_error += 1
+                break
+        if skip_image:
+            continue
+                    
+        im_full_path = os.path.join(image_base,im['file_name'])
+        json_path = os.path.splitext(im_full_path)[0] + '.json'
+        
+        if (not overwrite) and (os.path.isfile(json_path)):
+            if verbose:
+                print('Skipping existing file {}'.format(json_path))
+            n_json_files_exist += 1
+            continue
+    
+        annotations_this_image = image_id_to_annotations[im['id']]
+        output_dict = get_labelme_dict_for_image_from_coco_record(im,
+                                                                  annotations_this_image,
+                                                                  coco_data['categories'],
+                                                                  info=None)
+        
+        n_json_files_written += 1
+        with open(json_path,'w') as f:
+            json.dump(output_dict,f,indent=1)
+            
+    # ...for each image
+    
+    print('\nWrote {} .json files (skipped {} for errors, {} because they exist)'.format(
+        n_json_files_written,n_json_files_error,n_json_files_exist))
+    
+# ...def coco_to_labelme()
+
+
+#%% Interactive driver
+
+if False:
+    
+    pass
+
+    #%% Configure options
+    
+    coco_file = \
+        r'C:\\temp\\snapshot-exploration\\images\\training-images-good\\training-images-good_from_yolo.json'
+    image_folder = os.path.dirname(coco_file)    
+    overwrite = True    
+    
+    
+    #%% Programmatic execution
+    
+    coco_to_labelme(coco_data=coco_file,image_base=image_folder,overwrite=overwrite)
+
+    
+    #%% Command-line execution
+    
+    s = 'python coco_to_labelme.py "{}" "{}"'.format(coco_file,image_folder)
+    if overwrite:
+        s += ' --overwrite'
+        
+    print(s)
+    import clipboard; clipboard.copy(s)
+
+
+    #%% Opening labelme
+    
+    s = 'python labelme {}'.format(image_folder)
+    print(s)
+    import clipboard; clipboard.copy(s)
+    
+
+#%% Command-line driver
+
+import sys,argparse
+
+def main():
+
+    parser = argparse.ArgumentParser(
+        description='Convert a COCO database to labelme annotation format')
+    
+    parser.add_argument(
+        'coco_file',
+        type=str,
+        help='Path to COCO data file (.json)')
+    
+    parser.add_argument(
+        'image_base',
+        type=str,
+        help='Path to images (also the output folder)')
+    
+    parser.add_argument(
+        '--overwrite',
+        action='store_true',
+        help='Overwrite existing labelme .json files')
+
+    if len(sys.argv[1:]) == 0:
+        parser.print_help()
+        parser.exit()
+
+    args = parser.parse_args()
+
+    coco_to_labelme(coco_data=args.coco_file,image_base=args.image_base,overwrite=args.overwrite)    
+
+if __name__ == '__main__':
+    main()
```

### Comparing `megadetector-5.0.8/data_management/coco_to_yolo.py` & `megadetector-5.0.9/data_management/coco_to_yolo.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-########
-#
-# coco_to_yolo.py
-#
-# Converts a COCO-formatted dataset to a YOLO-formatted dataset, flattening
-# the dataset (to a single folder) in the process.
-#
-# If the input and output folders are the same, writes .txt files to the input folder,
-# and neither moves nor modifies images.
-#
-# Currently ignores segmentation masks, and errors if an annotation has a 
-# segmentation polygon but no bbox.
-# 
-# Has only been tested on a handful of COCO Camera Traps data sets; if you
-# use it for more general COCO conversion, YMMV.
-#
-########
+"""
+
+coco_to_yolo.py
+
+Converts a COCO-formatted dataset to a YOLO-formatted dataset, flattening
+the dataset (to a single folder) in the process.
+
+If the input and output folders are the same, writes .txt files to the input folder,
+and neither moves nor modifies images.
+
+Currently ignores segmentation masks, and errors if an annotation has a 
+segmentation polygon but no bbox.
+
+Has only been tested on a handful of COCO Camera Traps data sets; if you
+use it for more general COCO conversion, YMMV.
+
+"""
 
 #%% Imports and constants
 
 import json
 import os
 import shutil
 
@@ -33,24 +33,24 @@
 def write_yolo_dataset_file(yolo_dataset_file,
                             dataset_base_dir,
                             class_list,
                             train_folder_relative=None,
                             val_folder_relative=None,
                             test_folder_relative=None):
     """
-    Write a YOLOv5 dataset.yaml file to the absolute path yolo_dataset_file (should
+    Write a YOLOv5 dataset.yaml file to the absolute path [yolo_dataset_file] (should
     have a .yaml extension, though it's only a warning if it doesn't).  
-
-    [dataset_base_dir] should be the absolute path of the dataset root.
     
-    yolo_dataset_file does not have to be within dataset_base_dir.
-
-    [class_list] can be an ordered list of class names (the first item will be class 0, 
-    etc.), or the name of a text file containing an ordered list of class names (one per 
-    line, starting from class zero).
+    Args:
+        yolo_dataset_file (str): the file, typically ending in .yaml or .yml, to write.  
+            Does not have to be within dataset_base_dir.
+        dataset_base_dir (str): the absolute base path of the YOLO dataset
+        class_list (list or str): an ordered list of class names (the first item will be class 0, 
+            etc.), or the name of a text file containing an ordered list of class names (one per 
+            line, starting from class zero).
     """
     
     # Read class names
     if isinstance(class_list,str):
         with open(class_list,'r') as f:
             class_lines = f.readlines()
         class_lines = [s.strip() for s in class_lines]    
@@ -78,64 +78,85 @@
         f.write('names:\n')
         for i_class,class_name in enumerate(class_list):
             f.write('  {}: {}\n'.format(i_class,class_name))
 
 # ...def write_yolo_dataset_file(...)
 
             
-def coco_to_yolo(input_image_folder,output_folder,input_file,
+def coco_to_yolo(input_image_folder,
+                 output_folder,
+                 input_file,
                  source_format='coco',
                  overwrite_images=False,
                  create_image_and_label_folders=False,
                  class_file_name='classes.txt',
                  allow_empty_annotations=False,
                  clip_boxes=False,
                  image_id_to_output_image_json_file=None,
                  images_to_exclude=None,
                  path_replacement_char='#',
                  category_names_to_exclude=None,
                  category_names_to_include=None,
                  write_output=True,
                  flatten_paths=True):
     """
-    Convert a COCO-formatted dataset to a YOLO-formatted dataset, optionally flattening the 
+    Converts a COCO-formatted dataset to a YOLO-formatted dataset, optionally flattening the 
     dataset to a single folder in the process.
     
     If the input and output folders are the same, writes .txt files to the input folder,
     and neither moves nor modifies images.
     
     Currently ignores segmentation masks, and errors if an annotation has a 
     segmentation polygon but no bbox.
     
-    source_format can be 'coco' (default) or 'coco_camera_traps'.  The only difference
-    is that when source_format is 'coco_camera_traps', we treat an image with a non-bbox
-    annotation with a category id of 0 as a special case, i.e. that's how an empty image
-    is indicated.  The original COCO standard is a little ambiguous on this issue.  If
-    source_format is 'coco', we either treat images as empty or error, depending on the value
-    of allow_empty_annotations.  allow_empty_annotations has no effect if source_format is
-    'coco_camera_traps'.
-    
-    If create_image_and_label_folders is false, a/b/c/image001.jpg will become a#b#c#image001.jpg, 
-    and the corresponding text file will be a#b#c#image001.txt.  
-    
-    If create_image_and_label_folders is true, a/b/c/image001.jpg will become 
-    images/a#b#c#image001.jpg, and the corresponding text file will be 
-    labels/a#b#c#image001.txt.  Some tools still use this variant of the YOLO standard.
-    
-    If clip_boxes is True, bounding boxes coordinates will be clipped to [0,1].
-    
-    image_id_to_output_image_json_file is an optional *output* file, to which we will write
-    a mapping from image IDs to output file names.
-    
-    images_to_exclude is a list of image files (relative paths in the input folder) that we 
-    should ignore.
-    
-    write_output determines whether we actually copy images and write annotations;
-    setting this to False basically puts this function in "test mode".  The class list
-    file is written regardless of the value of write_output.
+    Args:
+        input_image_folder (str): the folder where images live; filenames in the COCO .json
+            file [input_file] should be relative to this folder
+        output_folder (str): the base folder for the YOLO dataset
+        input_file (str): a .json file in COCO format; can be the same as [input_image_folder], in which case
+            images are left alone.
+        source_format (str, optional): can be 'coco' (default) or 'coco_camera_traps'.  The only difference
+            is that when source_format is 'coco_camera_traps', we treat an image with a non-bbox
+            annotation with a category id of 0 as a special case, i.e. that's how an empty image
+            is indicated.  The original COCO standard is a little ambiguous on this issue.  If
+            source_format is 'coco', we either treat images as empty or error, depending on the value
+            of [allow_empty_annotations].  [allow_empty_annotations] has no effect if source_format is
+            'coco_camera_traps'.
+        create_image_and_label_folder (bool, optional): whether to create separate folders called 'images' and
+            'labels' in the YOLO output folder.  If create_image_and_label_folders is False, 
+            a/b/c/image001.jpg will become a#b#c#image001.jpg, and the corresponding text file will 
+            be a#b#c#image001.txt.  If create_image_and_label_folders is True, a/b/c/image001.jpg will become 
+            images/a#b#c#image001.jpg, and the corresponding text file will be 
+            labels/a#b#c#image001.txt.    
+        clip_boxes (bool, optional): whether to clip bounding box coordinates to the range [0,1] before
+            converting to YOLO xywh format
+        image_id_to_output_image_json_file (str, optional): an optional *output* file, to which we will write
+            a mapping from image IDs to output file names
+        images_to_exclude (list, optional): a list of image files (relative paths in the input folder) that we 
+            should ignore
+        path_replacement_char (str, optional): only relevant if [flatten_paths] is True; this is used to replace
+            path separators, e.g. if [path_replacement_char] is '#' and [flatten_paths] is True, a/b/c/d.jpg
+            becomes a#b#c#d.jpg
+        category_names_to_exclude (str, optional): category names that should not be represented in the
+            YOLO output; only impacts annotations, does not prevent copying images.  There's almost no reason
+            you would want to specify this and [category_names_to_include]. 
+        category_names_to_include (str, optional): allow-list of category names that should be represented in the
+            YOLO output; only impacts annotations, does not prevent copying images.  There's almost no reason
+            you would want to specify this and [category_names_to_exclude]. 
+        write_output (bool, optional): determines whether we actually copy images and write annotations;
+            setting this to False mostly puts this function in "dry run" "mode.  The class list
+            file is written regardless of the value of write_output.
+    
+    Returns:
+        dict: information about the coco --> yolo mapping, containing at least the fields:
+        
+        - class_list_filename: the filename to which we wrote the flat list of class names required 
+          by the YOLO format.
+        - source_image_to_dest_image: a dict mapping source images to destination images
+        - coco_id_to_yolo_id: a dict mapping COCO category IDs to YOLO category IDs        
     """
         
     ## Validate input
     
     if category_names_to_include is not None and category_names_to_exclude is not None:
         raise ValueError('category_names_to_include and category_names_to_exclude are mutually exclusive')
         
@@ -496,20 +517,20 @@
 
 
 def create_yolo_symlinks(source_folder,images_folder,labels_folder,
                          class_list_file=None,
                          class_list_output_name='object.data',
                          force_lowercase_image_extension=False):
     """
-    Given a YOLO-formatted folder of images and .txt files, create a folder
+    Given a YOLO-formatted folder of images and .txt files, creates a folder
     of symlinks to all the images, and a folder of symlinks to all the labels. 
-    Used to support preview/editing tools (like BoundingBoxEditor) that assume
-    images and labels are in separate folders.
+    Used to support preview/editing tools that assume images and labels are in separate 
+    folders.
     
-    images_folder and labels_folder are absolute paths.
+    :meta private:
     """    
     
     assert source_folder != images_folder and source_folder != labels_folder
     
     os.makedirs(images_folder,exist_ok=True)
     os.makedirs(labels_folder,exist_ok=True)
     
@@ -615,15 +636,15 @@
         'input_file',
         type=str,
         help='Path to COCO dataset file (.json)')
     
     parser.add_argument(
         '--create_bounding_box_editor_symlinks',
         action='store_true',
-        help='Prepare symlinks so the whole folder is BoundingBoxEditor-friendly')        
+        help='Prepare symlinks so the whole folder appears to contain "images" and "labels" folderss')        
     
     if len(sys.argv[1:]) == 0:
         parser.print_help()
         parser.exit()
 
     args = parser.parse_args()
```

### Comparing `megadetector-5.0.8/data_management/databases/add_width_and_height_to_db.py` & `megadetector-5.0.9/data_management/databases/add_width_and_height_to_db.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,29 +1,33 @@
-########
-#
-# add_width_and_height_to_db.py
-#
-# Grabs width and height from actual image files for a .json database that is missing w/h.
-#
-########
+"""
+
+add_width_and_height_to_db.py
+
+Grabs width and height from actual image files for a .json database that is missing w/h.
+
+TODO: this is a one-off script waiting to be cleaned up for more general use.
+
+"""
 
 #%% Imports and constants
 
 import json
 from PIL import Image
 
 datafile = '/datadrive/snapshotserengeti/databases/snapshotserengeti.json'
 image_base = '/datadrive/snapshotserengeti/images/'
 
+def main():
 
-#%% Execution
+    with open(datafile,'r') as f:
+        data = json.load(f)
 
-with open(datafile,'r') as f:
-    data = json.load(f)
+    for im in data['images']:
+        if 'height' not in im:
+            im_w, im_h = Image.open(image_base+im['file_name']).size
+            im['height'] = im_h
+            im['width'] = im_w
 
-for im in data['images']:
-    if 'height' not in im:
-        im_w, im_h = Image.open(image_base+im['file_name']).size
-        im['height'] = im_h
-        im['width'] = im_w
+    json.dump(data, open(datafile,'w'))
 
-json.dump(data, open(datafile,'w'))
+if __name__ == '__main__':
+    main()
```

### Comparing `megadetector-5.0.8/data_management/databases/combine_coco_camera_traps_files.py` & `megadetector-5.0.9/data_management/databases/combine_coco_camera_traps_files.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,48 +1,49 @@
-########
-#
-# combine_coco_camera_traps_files.py
-# 
-# Merges two or more .json files in COCO Camera Traps format, optionally
-# writing the results to another .json file.
-#
-# - Concatenates image lists, erroring if images are not unique.
-# - Errors on unrecognized fields.
-# - Checks compatibility in info structs, within reason.
-# 
-# combine_coco_camera_traps_files input1.json input2.json ... inputN.json output.json
-#
-########
+"""
+
+combine_coco_camera_traps_files.py
+
+Merges two or more .json files in COCO Camera Traps format, optionally
+writing the results to another .json file.
+
+- Concatenates image lists, erroring if images are not unique.
+- Errors on unrecognized fields.
+- Checks compatibility in info structs, within reason.
+
+*Example command-line invocation*
+    
+combine_coco_camera_traps_files input1.json input2.json ... inputN.json output.json
+
+"""
 
 #%% Constants and imports
 
 import argparse
 import json
 import sys
 
-from typing import Any, Dict, Iterable, Mapping, List, Optional
-
 
 #%% Merge functions
 
-def combine_cct_files(input_files: List[str],
-                             output_file: Optional[str] = None,
-                             require_uniqueness: Optional[bool] = True,
-                             filename_prefixes: Optional[dict] = None
-                             ) -> Dict[str, Any]:
+def combine_cct_files(input_files, output_file=None, require_uniqueness=True,
+                      filename_prefixes=None):                             
     """
-    Merges list of COCO Camera Traps files *input_files* into a single
-    dictionary, optionally writing the result to *output_file*.
+    Merges the list of COCO Camera Traps files [input_files] into a single
+    dictionary, optionally writing the result to [output_file].
 
     Args:
-        input_files: list of str, paths to JSON detection files
-        output_file: optional str, path to write merged JSON
-        require_uniqueness: bool, whether to require that the images in
+        input_files (list): paths to CCT .json files
+        output_file (str, optional): path to write merged .json file
+        require_uniqueness (bool): whether to require that the images in
             each input_dict be unique
+    
+    Returns:
+        dict: the merged COCO-formatted .json dict
     """
+    
     input_dicts = []
     print('Loading input files')
     for fn in input_files:
         with open(fn, 'r', encoding='utf-8') as f:
             d = json.load(f)
             if filename_prefixes is not None:
                 assert fn in filename_prefixes
@@ -57,27 +58,26 @@
     if output_file is not None:
         with open(output_file, 'w') as f:
             json.dump(merged_dict, f, indent=1)
 
     return merged_dict
 
 
-def combine_cct_dictionaries(input_dicts: Iterable[Mapping[str, Any]],
-                                    require_uniqueness: Optional[bool] = True                                    
-                                    ) -> Dict[str, Any]:
+def combine_cct_dictionaries(input_dicts, require_uniqueness=True):                                    
     """
-    Merges the list of COCO Camera Traps dictionaries *input_dicts*.  See header
+    Merges the list of COCO Camera Traps dictionaries [input_dicts].  See module header
     comment for details on merge rules.
 
     Args:
         input_dicts: list of CCT dicts
         require_uniqueness: bool, whether to require that the images in
             each input_dict be unique
 
-    Returns: dict, represents the merged JSON
+    Returns: 
+        dict: the merged COCO-formatted .json dict
     """
     
     filename_to_image = {}
     all_annotations = []
     info = None
     
     category_name_to_id = {}
@@ -173,20 +173,24 @@
     all_categories = [{'id':category_name_to_id[cat_name],'name':cat_name} for\
                       cat_name in category_name_to_id.keys()]
     
     merged_dict = {'info': info,
                    'categories': all_categories,
                    'images': sorted_images,
                    'annotations': all_annotations}
+    
     return merged_dict
 
+# ...combine_cct_dictionaries(...)
+
 
 #%% Command-line driver
 
 def main():
+    
     parser = argparse.ArgumentParser()
     parser.add_argument(
         'input_paths', nargs='+',
         help='List of input .json files')
     parser.add_argument(
         'output_path',
         help='Output .json file')
```

### Comparing `megadetector-5.0.8/data_management/databases/integrity_check_json_db.py` & `megadetector-5.0.9/data_management/databases/integrity_check_json_db.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-########
-#
-# integrity_check_json_db.py
-#
-# Does some integrity-checking and computes basic statistics on a db, specifically:
-#
-# * Verifies that required fields are present and have the right types
-# * Verifies that annotations refer to valid images
-# * Verifies that annotations refer to valid categories
-# * Verifies that image, category, and annotation IDs are unique 
-# * Optionally checks file existence
-# * Finds un-annotated images
-# * Finds unused categories
-# * Prints a list of categories sorted by count
-#
-########
+"""
+
+integrity_check_json_db.py
+
+Does some integrity-checking and computes basic statistics on a COCO Camera Traps .json file, specifically:
+
+* Verifies that required fields are present and have the right types
+* Verifies that annotations refer to valid images
+* Verifies that annotations refer to valid categories
+* Verifies that image, category, and annotation IDs are unique 
+* Optionally checks file existence
+* Finds un-annotated images
+* Finds unused categories
+* Prints a list of categories sorted by count
+
+"""
 
 #%% Constants and environment
 
 import argparse
 import json
 import os
 import sys
@@ -29,32 +29,64 @@
 from md_visualization.visualization_utils import open_image
 from md_utils import ct_utils
 
 
 #%% Classes and environment
 
 class IntegrityCheckOptions:
+    """
+    Options for integrity_check_json_db()
+    """
     
+    #: Image path; the filenames in the .json file should be relative to this folder
     baseDir = ''
+    
+    #: Should we validate the image sizes?
     bCheckImageSizes = False
+    
+    #: Should we check that all the images in the .json file exist on disk?
     bCheckImageExistence = False
+    
+    #: Should we search [baseDir] for images that are not used in the .json file?
     bFindUnusedImages = False
+    
+    #: Should we require that all images in the .json file have a 'location' field?
     bRequireLocation = True
+    
+    #: For debugging, limit the number of images we'll process
     iMaxNumImages = -1
+    
+    #: Number of threads to use for parallelization, set to <= 1 to disable parallelization
     nThreads = 10
+    
+    #: Enable additional debug output
     verbose = True
     
     
 # This is used in a medium-hacky way to share modified options across threads
 defaultOptions = IntegrityCheckOptions()
 
 
 #%% Functions
 
-def check_image_existence_and_size(image,options=None):
+def _check_image_existence_and_size(image,options=None):
+    """
+    Validate the image represented in the CCT image dict [image], which should have fields:
+    
+    * file_name
+    * width
+    * height
+
+    Args:
+        image (dict): image to validate
+        options (IntegrityCheckOptions): parameters impacting validation
+    
+    Returns:
+        bool: whether this image passes validation
+    """
 
     if options is None:        
         options = defaultOptions
     
     assert options.bCheckImageExistence
     
     filePath = os.path.join(options.baseDir,image['file_name'])
@@ -76,17 +108,25 @@
             return False
         
     return True
 
   
 def integrity_check_json_db(jsonFile, options=None):
     """
-    jsonFile can be a filename or an already-loaded json database
+    Does some integrity-checking and computes basic statistics on a COCO Camera Traps .json file; see
+    module header comment for a list of the validation steps.
     
-    return sortedCategories, data, errorInfo
+    Args:
+        jsonFile (str): filename to validate, or an already-loaded dict
+    
+    Returns:
+        tuple: tuple containing:
+            - sortedCategories (dict): list of categories used in [jsonFile], sorted by frequency
+            - data (dict): the data loaded from [jsonFile]
+            - errorInfo (dict): specific validation errors
     """
     
     if options is None:       
         options = IntegrityCheckOptions()
     
     if options.bCheckImageSizes:        
         options.bCheckImageExistence = True
@@ -260,19 +300,19 @@
         
         if options.nThreads is not None and options.nThreads > 1:
             pool = ThreadPool(options.nThreads)
             # results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)
             defaultOptions.baseDir = options.baseDir
             defaultOptions.bCheckImageSizes = options.bCheckImageSizes
             defaultOptions.bCheckImageExistence = options.bCheckImageExistence
-            results = tqdm(pool.imap(check_image_existence_and_size, images), total=len(images))
+            results = tqdm(pool.imap(_check_image_existence_and_size, images), total=len(images))
         else:
             results = []
             for im in tqdm(images):
-                results.append(check_image_existence_and_size(im,options))
+                results.append(_check_image_existence_and_size(im,options))
                 
         for iImage,r in enumerate(results):
             if not r:            
                 validationErrors.append(os.path.join(options.baseDir,images[iImage]['file_name']))
                             
     # ...for each image
     
@@ -403,17 +443,15 @@
         
     args = parser.parse_args()
     args.bRequireLocation = (not args.bAllowNoLocation)
     options = IntegrityCheckOptions()
     ct_utils.args_to_object(args, options)
     integrity_check_json_db(args.jsonFile,options)
 
-
-if __name__ == '__main__':
-    
+if __name__ == '__main__':    
     main()
 
 
 #%% Interactive driver(s)
 
 if False:
```

### Comparing `megadetector-5.0.8/data_management/databases/remove_corrupted_images_from_db.py` & `megadetector-5.0.9/data_management/get_image_sizes.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,191 +1,188 @@
-########
-#
-# remove_corrupted_images_from_db.py
-#
-# Given a coco-camera-traps .json file, checks all images for corruption
-# and generates a new .json file that only contains the non-corrupted images.
-#
-########
+"""
 
-#%% Imports and constants
+get_image_sizes.py
+
+Given a json-formatted list of image filenames, retrieves the width and height of 
+every image, optionally writing the results to a new .json file.
+
+"""
+
+#%% Constants and imports
 
 import argparse
-import gc
 import json
-import sys
 import os
-import time
+from PIL import Image
+import sys
 
 from multiprocessing.pool import ThreadPool
+from multiprocessing.pool import Pool
+from functools import partial
+from tqdm import tqdm
 
-import humanfriendly
-import numpy as np
-import tensorflow as tf
+from md_utils.path_utils import find_images
 
-N_THREADS = 16
-DEBUG_MAX_IMAGES = -1
-IMAGE_PRINT_FREQUENCY = 10
+image_base = ''
+default_n_threads = 1
+use_threads = False
 
 
-#%% Function definitions
+#%% Processing functions
 
-def check_images(images, image_file_root):    
+def _get_image_size(image_path,image_prefix=None):
+    """
+    Support function to get the size of a single image.  Returns a (path,w,h) tuple.
+    w and h will be -1 if the image fails to load.
     """
-    Checks all the images in [images] for corruption using TF.
-    
-    [images] is a list of image dictionaries, as they would appear in COCO
-    files.
-    
-    Returns a dictionary mapping image IDs to booleans. 
-    """    
     
-    # I sometimes pass in a list of images, sometimes a dict with a single
-    # element mapping a job ID to the list of images
-    if isinstance(images,dict):
-        assert(len(images) == 1)
-        jobID = list(images.keys())[0]
-        images = images[jobID]
+    if image_prefix is not None:
+        full_path = os.path.join(image_prefix,image_path)
     else:
-        jobID = 0
-        
-    keep_im = {im['id']:True for im in images}
-            
-    count = 0
-    nImages = len(images)
+        full_path = image_path
     
-    # We're about to start a lot of TF sessions, and we don't want gobs 
-    # of debugging information printing out for every session.
-    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
-    config=tf.ConfigProto(log_device_placement=False)
-    
-    # At some point we were creating a single session and looping over images
-    # within that session, but the only way I found to reliably not run out
-    # of GPU memory was to create a session per image and gc.collect() after
-    # each session.
-    for iImage,im in enumerate(images):
-
-        with tf.Session(config=config) as sess:
-
-            if ((DEBUG_MAX_IMAGES > 0) and (iImage >= DEBUG_MAX_IMAGES)):
-                print('Breaking after {} images'.format(DEBUG_MAX_IMAGES))
-                break
-            
-            if (count % IMAGE_PRINT_FREQUENCY == 0):
-                print('Job {}: processed {} of {} images'.format(jobID,count,nImages))
-                
-            count += 1
-            image_file = os.path.join(image_file_root,im['file_name'])
-            assert(os.path.isfile(image_file))
-            
-            try:
-                image_data = tf.gfile.FastGFile(image_file,'rb').read()
-                image = tf.image.decode_jpeg(image_data)
-                sess.run(image)
-            except:
-                keep_im[im['id']] = False
-
-        gc.collect()
-        
-    return keep_im
-
-
-def remove_corrupted_images_from_database(data, image_file_root):
+    # Is this image on disk?
+    if not os.path.isfile(full_path):
+        print('Could not find image {}'.format(full_path))
+        return (image_path,-1,-1)
+
+    try:        
+        pil_im = Image.open(full_path)
+        w = pil_im.width            
+        h = pil_im.height
+        return (image_path,w,h)
+    except Exception as e:    
+        print('Error reading image {}: {}'.format(full_path,str(e)))
+        return (image_path,-1,-1)
+    
+    
+def get_image_sizes(filenames,image_prefix=None,output_file=None,
+                    n_workers=default_n_threads,use_threads=True,
+                    recursive=True):
     """
-    Given the COCO database [data], checks all images for corruption using
-    TF, and returns a subset of [data] containing only non-corrupted images.
-    """
-    
-    # Map Image IDs to boolean (should I keep this image?)
-    images = data['images']
-    
-    if (N_THREADS == 1):
+    Gets the width and height of all images in [filenames], which can be:
         
-        keep_im = check_images(images,image_file_root)
+    * A .json-formatted file containing list of strings
+    * A folder
+    * A list of files
+
+    ...returning a list of (path,w,h) tuples, and optionally writing the results to [output_file].
+    
+    Args:
+        filenames (str or list): the image filenames for which we should retrieve sizes, 
+            can be the name of a .json-formatted file containing list of strings, a folder 
+            in which we should enumerate images, or a list of files.
+        image_prefix (str, optional): optional prefix to add to images to get to full paths;
+            useful when [filenames] contains relative files, in which case [image_prefix] is the 
+            base folder for the source images.
+        output_file (str, optional): a .json file to write the imgae sizes
+        n_workers (int, optional): number of parallel workers to use, set to <=1 to
+            disable parallelization
+        use_threads (bool, optional): whether to use threads (True) or processes (False)
+            for parallelization; not relevant if [n_workers] <= 1
+        recursive (bool, optional): only relevant if [filenames] is actually a folder,
+            determines whether image enumeration within that folder will be recursive
+        
+    Returns:
+        list: list of (path,w,h) tuples
+    """        
+    
+    if output_file is not None:
+        assert os.path.isdir(os.path.dirname(output_file)), \
+            'Illegal output file {}, parent folder does not exist'.format(output_file)
+        
+    if isinstance(filenames,str) and os.path.isfile(filenames):
+        with open(filenames,'r') as f:        
+            filenames = json.load(f)
+        filenames = [s.strip() for s in filenames]
+    elif isinstance(filenames,str) and os.path.isdir(filenames):
+        filenames = find_images(filenames,recursive=recursive,
+                                return_relative_paths=False,convert_slashes=True)
+    else:
+        assert isinstance(filenames,list)        
+    
+    if n_workers <= 1:
         
+        all_results = []
+        for i_file,fn in tqdm(enumerate(filenames),total=len(filenames)):
+            all_results.append(_get_image_size(fn,image_prefix=image_prefix))
+    
     else:
         
-        start = time.time()
-        imageChunks = np.array_split(images,N_THREADS)
-        # Convert to lists, append job numbers to the image lists
-        for iChunk in range(0,len(imageChunks)):
-            imageChunks[iChunk] = list(imageChunks[iChunk])
-            imageChunks[iChunk] = {iChunk:imageChunks[iChunk]}
-        pool = ThreadPool(N_THREADS)
-        # results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)
-        results = pool.map(lambda x: check_images(x,image_file_root), imageChunks)
-        processingTime = time.time() - start
-        
-        # Merge results
-        keep_im = {}
-        for d in results:
-            keep_im.update(d)
-        bValid = keep_im.values()
+        print('Creating a pool with {} workers'.format(n_workers))
+        if use_threads:
+            pool = ThreadPool(n_workers)        
+        else:
+            pool = Pool(n_workers)
+        # all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))
+        all_results = list(tqdm(pool.imap(
+            partial(_get_image_size,image_prefix=image_prefix), filenames), total=len(filenames)))
+    
+    if output_file is not None:
+        with open(output_file,'w') as f:
+            json.dump(all_results,f,indent=1)
             
-        print("Checked image corruption in {}, found {} invalid images (of {})".format(
-                humanfriendly.format_timespan(processingTime),
-                len(bValid)-sum(bValid),len(bValid)))
-        
-    data['images'] = [im for im in data['images'] if keep_im[im['id']]]
-    data['annotations'] = [ann for ann in data['annotations'] if keep_im[ann['image_id']]]
+    return all_results
+
     
-    return data
+#%% Interactive driver
 
+if False:
 
-#%% Interactive driver
+    pass    
 
-if False:    
+    #%%
+    
+    # List images in a test folder
+    base_dir = r'c:\temp\test_images'
+    image_list_file = os.path.join(base_dir,'images.json')
+    relative_image_list_file = os.path.join(base_dir,'images_relative.json')
+    image_size_file = os.path.join(base_dir,'image_sizes.json')
+    from md_utils import path_utils
+    image_names = path_utils.find_images(base_dir,recursive=True)
+    
+    with open(image_list_file,'w') as f:
+        json.dump(image_names,f,indent=1)
+        
+    relative_image_names = []
+    for s in image_names:
+        relative_image_names.append(os.path.relpath(s,base_dir))
+    
+    with open(relative_image_list_file,'w') as f:
+        json.dump(relative_image_names,f,indent=1)
+    
     
     #%%
     
-    # base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation'
-    base_dir = r'/data/ss_corruption_check'
-    input_file = os.path.join(base_dir,'imerit_batch7_renamed.json')
-    output_file = os.path.join(base_dir,'imerit_batch7_renamed_uncorrupted.json')
-    image_file_root = os.path.join(base_dir,'imerit_batch7_images_renamed')
-    assert(os.path.isfile(input_file))
-    assert(os.path.isdir(image_file_root))
-
-    # Load annotations
-    with open(input_file,'r') as f:
-            data = json.load(f)    
-            
-    # Check for corruption
-    data_uncorrupted = remove_corrupted_images_from_database(data,image_file_root)
+    get_image_sizes(relative_image_list_file,image_size_file,image_prefix=base_dir,n_threads=4)
     
-    # Write out only the uncorrupted data
-    json.dump(data_uncorrupted, open(output_file,'w'))
     
+#%% Command-line driver
     
-#%% Command-line driver 
-
 def main():
     
-    parser = argparse.ArgumentParser(description = 'Remove images from a .json file that can''t be opened in TF')
-
-    parser.add_argument('--input_file', dest='input_file',
-                         help='Path to .json database that includes corrupted jpegs',
-                         type=str, required=True)
-    parser.add_argument('--image_file_root', dest='image_file_root',
-                         help='Path to image files',
-                         type=str, required=True)
-    parser.add_argument('--output_file', dest='output_file',
-                         help='Path to store uncorrupted .json database',
-                         type=str, required=True)
-
-    if len(sys.argv[1:]) == 0:
+    parser = argparse.ArgumentParser()
+    parser.add_argument('filenames',type=str,
+                        help='Folder from which we should fetch image sizes, or .json file with a list of filenames')
+    parser.add_argument('output_file',type=str,
+                        help='Output file (.json) to which we should write image size information')
+    parser.add_argument('--image_prefix', type=str, default=None,
+                        help='Prefix to append to image filenames, only relevant if [filenames] points to a list of ' + \
+                             'relative paths')        
+    parser.add_argument('--n_threads', type=int, default=default_n_threads,
+                        help='Number of concurrent workers, set to <=1 to disable parallelization (default {})'.format(
+                            default_n_threads))
+                        
+    if len(sys.argv[1:])==0:
         parser.print_help()
         parser.exit()
-
+        
     args = parser.parse_args()
     
-    print('Reading input file')
-    with open(args.input_file,'r') as f:
-        data = json.load(f)
-    print('Removing corrupted images from database')
-    uncorrupted_data = remove_corrupted_images_from_database(data, args.image_file_root)
-
-    json.dump(uncorrupted_data, open(args.output_file,'w'))
-
-
-if __name__ == '__main__':    
+    _ = get_image_sizes(filenames=args.filenames,
+                        output_file=args.output_file,
+                        image_prefix=args.image_prefix,
+                        n_workers=args.n_threads)
+        
+if __name__ == '__main__':
+    
     main()
```

### Comparing `megadetector-5.0.8/data_management/databases/subset_json_db.py` & `megadetector-5.0.9/data_management/databases/subset_json_db.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-########
-#
-# subset_json_db.py
-#
-# Select a subset of images (and associated annotations) from a .json file
-# in COCO Camera Traps format.
-#
-# To subset the .json files in the MegaDetector output format, see 
-# subset_json_detector_output.py
-#
-########
+"""
+
+subset_json_db.py
+
+Select a subset of images (and associated annotations) from a .json file in COCO 
+Camera Traps format based on a string query.
+
+To subset .json files in the MegaDetector output format, see
+subset_json_detector_output.py.
+
+"""
     
 #%% Constants and imports
 
 import sys
 import json
 import argparse
 
@@ -22,14 +22,24 @@
 #%% Functions
 
 def subset_json_db(input_json, query, output_json=None, ignore_case=False):
     """
     Given a json file (or dictionary already loaded from a json file), produce a new 
     database containing only the images whose filenames contain the string 'query', 
     optionally writing that DB output to a new json file.
+    
+    Args:
+        input_json (str): COCO Camera Traps .json file to load, or an already-loaded dict
+        query (str): string to query for, only include images in the output whose filenames 
+            contain this string.
+        output_json (str, optional): file to write the resulting .json file to
+        ignore_case (bool, optional): whether to perform a case-insensitive search for [query]
+        
+    Returns:
+        dict: possibly-modified CCT dictionary
     """
     
     if ignore_case:
         query = query.lower()
         
     # Load the input file if necessary
     if isinstance(input_json,str):
@@ -72,16 +82,16 @@
 
 #%% Interactive driver
 
 if False:
     
     #%%
     
-    input_json = r"E:\Statewide_wolf_container\idfg_20190409.json"
-    output_json = r"E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"
+    input_json = r"e:\Statewide_wolf_container\idfg_20190409.json"
+    output_json = r"e:\Statewide_wolf_container\idfg_20190409_clearcreek.json"
     query = 'clearcreek'
     ignore_case = True
     db = subset_json_db(input_json, query, output_json, ignore_case)
     
 
 #%% Command-line driver
 
@@ -97,10 +107,9 @@
         parser.print_help()
         parser.exit()
 
     args = parser.parse_args()    
     
     subset_json_db(args.input_json,args.query,args.output_json,args.ignore_case)
 
-if __name__ == '__main__':
-    
+if __name__ == '__main__':    
     main()
```

### Comparing `megadetector-5.0.8/data_management/generate_crops_from_cct.py` & `megadetector-5.0.9/data_management/generate_crops_from_cct.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,28 +1,45 @@
-########
-#
-# generate_crops_from_cct.py
-#
-# Given a .json file in COCO Camera Traps format, create a cropped image for
-# each bounding box.
-#
-########
+"""
+
+generate_crops_from_cct.py
+
+Given a .json file in COCO Camera Traps format, creates a cropped image for
+each bounding box.
+
+"""
 
 #%% Imports and constants
 
 import os
 import json
 
 from tqdm import tqdm
 from PIL import Image
 
 
 #%% Functions
 
 def generate_crops_from_cct(cct_file,image_dir,output_dir,padding=0,flat_output=True):
+    """
+    Given a .json file in COCO Camera Traps format, creates a cropped image for
+    each bounding box.
+    
+    Args:
+        cct_file (str): the COCO .json file from which we should load data
+        image_dir (str): the folder where the images live; filenames in the .json
+            file should be relative to this folder
+        output_dir (str): the folder where we should write cropped images
+        padding (float, optional): number of pixels we should expand each box before
+            cropping
+        flat_output (bool, optional): if False, folder structure will be preserved
+            in the output, e.g. the image a/b/c/d.jpg will result in image files 
+            in the output folder called, e.g., a/b/c/d_crop_000_id_12345.jpg.  If
+            [flat_output] is True, the corresponding output image will be 
+            a_b_c_d_crop_000_id_12345.jpg.            
+    """
     
     ## Read and validate input
     
     assert os.path.isfile(cct_file)
     assert os.path.isdir(image_dir)
     os.makedirs(output_dir,exist_ok=True)
 
@@ -119,49 +136,14 @@
     
     cct_file = os.path.expanduser('~/data/noaa/noaa_estuary_fish.json')
     image_dir = os.path.expanduser('~/data/noaa/JPEGImages')
     padding = 50
     flat_output = True
     output_dir = '/home/user/tmp/noaa-fish-crops'
     
-    #%%
-    
     generate_crops_from_cct(cct_file,image_dir,output_dir,padding,flat_output=True)
     files = os.listdir(output_dir)
     
-    #%%
-    
-    import random
-    fn = os.path.join(output_dir,random.choice(files))
-    
-    from md_utils.path_utils import open_file
-    open_file(fn)
-
-
-#%% Scrap
-
-if False:
     
-    pass
+#%% Command-line driver
 
-    #%%
-    
-    from md_visualization.visualize_db import DbVizOptions,visualize_db
-    
-    db_path = cct_file
-    output_dir = os.path.expanduser('~/tmp/noaa-fish-preview')
-    image_base_dir = image_dir
-    
-    options = DbVizOptions()
-    options.num_to_visualize = None
-    
-    options.parallelize_rendering_n_cores = 5
-    options.parallelize_rendering = True    
-    
-    options.viz_size = (-1, -1)
-    options.trim_to_images_with_bboxes = True
-    
-    options.box_thickness = 4
-    options.box_expansion = 25
-    
-    htmlOutputFile,db = visualize_db(db_path,output_dir,image_base_dir,options)
-    
+# TODO
```

### Comparing `megadetector-5.0.8/data_management/importers/add_nacti_sizes.py` & `megadetector-5.0.9/data_management/importers/add_nacti_sizes.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# add_nacti_sizes.py
-#
-# NACTI bounding box metadata was posted before we inclduded width and height as semi-standard
-# fields; pull size information from the main metadata file and add to the bbox file.
-#
-########
+"""
+
+ add_nacti_sizes.py
+
+ NACTI bounding box metadata was posted before we inclduded width and height as semi-standard
+ fields; pull size information from the main metadata file and add to the bbox file.
+
+"""
 
 #%% Constants and environment
 
 import json
 from tqdm import tqdm
 
 input_file = 'G:/temp/nacti_metadata.json'
```

### Comparing `megadetector-5.0.8/data_management/importers/animl_results_to_md_results.py` & `megadetector-5.0.9/data_management/importers/animl_results_to_md_results.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,158 +1,158 @@
-########
-#
-# animl_results_to_md_results.py
-#
-# Convert a .csv file produced by the Animl package:
-#
-# https://github.com/conservationtechlab/animl-py
-#
-# ...to a MD results file suitable for import into Timelapse.
-#
-# Columns are expected to be:
-#
-# file
-# category (MD category identifies: 1==animal, 2==person, 3==vehicle)
-# detection_conf
-# bbox1,bbox2,bbox3,bbox4
-# class
-# classification_conf
-#
-########
-
-#%% Imports and constants
-
-import pandas as pd
-import json
-
-# It's a little icky to hard-code this here rather than importing from elsewhere
-# in the MD repo, but it seemed silly to take a dependency on lots of MD code
-# just for this, so, hard-coding.
-detection_category_id_to_name = {'1':'animal','2':'person','3':'vehicle'}
-
-
-#%% Main function
-
-def animl_results_to_md_results(input_file,output_file=None):
-    """
-    Converts the .csv file [input_file] to the MD-formatted .json file [output_file].
-    
-    If [output_file] is None, '.json' will be appended to the input file.
-    """
-    
-    if output_file is None:
-        output_file = input_file + '.json'
-
-    df = pd.read_csv(input_file)
-    
-    expected_columns = ('file','category','detection_conf',
-                        'bbox1','bbox2','bbox3','bbox4','class','classification_conf')
-    
-    for s in expected_columns:
-        assert s in df.columns,\
-            'Expected column {} not found'.format(s)
-            
-    classification_category_name_to_id = {}    
-    filename_to_results = {}
-    
-    # i_row = 0; row = df.iloc[i_row]
-    for i_row,row in df.iterrows():
-            
-        # Is this the first detection we've seen for this file?            
-        if row['file'] in filename_to_results:
-            im = filename_to_results[row['file']]
-        else:
-            im = {}
-            im['detections'] = []
-            im['file'] = row['file']
-            filename_to_results[im['file']] = im
-            
-        assert isinstance(row['category'],int),'Invalid category identifier in row {}'.format(im['file'])
-        detection_category_id = str(row['category'])
-        assert detection_category_id in detection_category_id_to_name,\
-            'Unrecognized detection category ID {}'.format(detection_category_id)
-            
-        detection = {}
-        detection['category'] = detection_category_id
-        detection['conf'] = row['detection_conf']
-        bbox = [row['bbox1'],row['bbox2'],row['bbox3'],row['bbox4']]
-        detection['bbox'] = bbox
-        classification_category_name = row['class']
-        
-        # Have we seen this classification category before?
-        if classification_category_name in classification_category_name_to_id:
-            classification_category_id = \
-                classification_category_name_to_id[classification_category_name]
-        else:
-            classification_category_id = str(len(classification_category_name_to_id))
-            classification_category_name_to_id[classification_category_name] = \
-                classification_category_id
-        
-        classifications = [[classification_category_id,row['classification_conf']]]
-        detection['classifications'] = classifications
-        
-        im['detections'].append(detection)
-        
-    # ...for each row        
-        
-    info = {}
-    info['format_version'] = '1.3'
-    info['detector'] = 'Animl'
-    info['classifier'] = 'Animl'
-    
-    results = {}
-    results['info'] = info
-    results['detection_categories'] = detection_category_id_to_name
-    results['classification_categories'] = \
-        {v: k for k, v in classification_category_name_to_id.items()}
-    results['images'] = list(filename_to_results.values())
-    
-    with open(output_file,'w') as f:
-        json.dump(results,f,indent=1)
-        
-# ...animl_results_to_md_results(...)
-        
-
-#%% Interactive driver
-
-if False:
-
-    pass
-
-    #%%
-    
-    input_file = r"G:\temp\animl-runs\animl-runs\Coati_v2\manifest.csv"
-    output_file = None
-    animl_results_to_md_results(input_file,output_file)
-    
-
-#%% Command-line driver
-
-import sys,argparse
-
-def main():
-
-    parser = argparse.ArgumentParser(
-        description='Convert an Animl-formatted .csv results file to MD-formatted .json results file')
-    
-    parser.add_argument(
-        'input_file',
-        type=str,
-        help='input .csv file')
-    
-    parser.add_argument(
-        '--output_file',
-        type=str,
-        default=None,
-        help='output .json file (defaults to input file appended with ".json")')
-    
-    if len(sys.argv[1:]) == 0:
-        parser.print_help()
-        parser.exit()
-
-    args = parser.parse_args()
-
-    animl_results_to_md_results(args.input_file,args.output_file)
-    
-if __name__ == '__main__':
-    main()
-    
+"""
+
+ animl_results_to_md_results.py
+
+ Convert a .csv file produced by the Animl package:
+
+ https://github.com/conservationtechlab/animl-py
+
+ ...to a MD results file suitable for import into Timelapse.
+
+ Columns are expected to be:
+
+ file
+ category (MD category identifies: 1==animal, 2==person, 3==vehicle)
+ detection_conf
+ bbox1,bbox2,bbox3,bbox4
+ class
+ classification_conf
+
+"""
+
+#%% Imports and constants
+
+import pandas as pd
+import json
+
+# It's a little icky to hard-code this here rather than importing from elsewhere
+# in the MD repo, but it seemed silly to take a dependency on lots of MD code
+# just for this, so, hard-coding.
+detection_category_id_to_name = {'1':'animal','2':'person','3':'vehicle'}
+
+
+#%% Main function
+
+def animl_results_to_md_results(input_file,output_file=None):
+    """
+    Converts the .csv file [input_file] to the MD-formatted .json file [output_file].
+    
+    If [output_file] is None, '.json' will be appended to the input file.
+    """
+    
+    if output_file is None:
+        output_file = input_file + '.json'
+
+    df = pd.read_csv(input_file)
+    
+    expected_columns = ('file','category','detection_conf',
+                        'bbox1','bbox2','bbox3','bbox4','class','classification_conf')
+    
+    for s in expected_columns:
+        assert s in df.columns,\
+            'Expected column {} not found'.format(s)
+            
+    classification_category_name_to_id = {}    
+    filename_to_results = {}
+    
+    # i_row = 0; row = df.iloc[i_row]
+    for i_row,row in df.iterrows():
+            
+        # Is this the first detection we've seen for this file?            
+        if row['file'] in filename_to_results:
+            im = filename_to_results[row['file']]
+        else:
+            im = {}
+            im['detections'] = []
+            im['file'] = row['file']
+            filename_to_results[im['file']] = im
+            
+        assert isinstance(row['category'],int),'Invalid category identifier in row {}'.format(im['file'])
+        detection_category_id = str(row['category'])
+        assert detection_category_id in detection_category_id_to_name,\
+            'Unrecognized detection category ID {}'.format(detection_category_id)
+            
+        detection = {}
+        detection['category'] = detection_category_id
+        detection['conf'] = row['detection_conf']
+        bbox = [row['bbox1'],row['bbox2'],row['bbox3'],row['bbox4']]
+        detection['bbox'] = bbox
+        classification_category_name = row['class']
+        
+        # Have we seen this classification category before?
+        if classification_category_name in classification_category_name_to_id:
+            classification_category_id = \
+                classification_category_name_to_id[classification_category_name]
+        else:
+            classification_category_id = str(len(classification_category_name_to_id))
+            classification_category_name_to_id[classification_category_name] = \
+                classification_category_id
+        
+        classifications = [[classification_category_id,row['classification_conf']]]
+        detection['classifications'] = classifications
+        
+        im['detections'].append(detection)
+        
+    # ...for each row        
+        
+    info = {}
+    info['format_version'] = '1.3'
+    info['detector'] = 'Animl'
+    info['classifier'] = 'Animl'
+    
+    results = {}
+    results['info'] = info
+    results['detection_categories'] = detection_category_id_to_name
+    results['classification_categories'] = \
+        {v: k for k, v in classification_category_name_to_id.items()}
+    results['images'] = list(filename_to_results.values())
+    
+    with open(output_file,'w') as f:
+        json.dump(results,f,indent=1)
+        
+# ...animl_results_to_md_results(...)
+        
+
+#%% Interactive driver
+
+if False:
+
+    pass
+
+    #%%
+    
+    input_file = r"G:\temp\animl-runs\animl-runs\Coati_v2\manifest.csv"
+    output_file = None
+    animl_results_to_md_results(input_file,output_file)
+    
+
+#%% Command-line driver
+
+import sys,argparse
+
+def main():
+
+    parser = argparse.ArgumentParser(
+        description='Convert an Animl-formatted .csv results file to MD-formatted .json results file')
+    
+    parser.add_argument(
+        'input_file',
+        type=str,
+        help='input .csv file')
+    
+    parser.add_argument(
+        '--output_file',
+        type=str,
+        default=None,
+        help='output .json file (defaults to input file appended with ".json")')
+    
+    if len(sys.argv[1:]) == 0:
+        parser.print_help()
+        parser.exit()
+
+    args = parser.parse_args()
+
+    animl_results_to_md_results(args.input_file,args.output_file)
+    
+if __name__ == '__main__':
+    main()
+
```

### Comparing `megadetector-5.0.8/data_management/importers/auckland_doc_test_to_json.py` & `megadetector-5.0.9/data_management/importers/auckland_doc_test_to_json.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-########
-#
-# auckland_doc_test_to_json.py
-#
-# Convert Auckland DOC data set to COCO camera traps format.  This was
-# for a testing data set where a .csv file was provided with class
-# information.
-#
-########
+"""
+
+ auckland_doc_test_to_json.py
+
+ Convert Auckland DOC data set to COCO camera traps format.  This was
+ for a testing data set where a .csv file was provided with class
+ information.
+
+"""
 
 #%% Constants and imports
 
 import json
 import os
 import uuid
 import pandas as pd
```

### Comparing `megadetector-5.0.8/data_management/importers/auckland_doc_to_json.py` & `megadetector-5.0.9/data_management/importers/auckland_doc_to_json.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# auckland_doc_to_json.py
-#
-# Convert Auckland DOC data set to COCO camera traps format.  This was
-# for a training data set where class names were encoded in path names.
-#
-########
+"""
+
+ auckland_doc_to_json.py
+
+ Convert Auckland DOC data set to COCO camera traps format.  This was
+ for a training data set where class names were encoded in path names.
+
+"""
 
 #%% Constants and imports
 
 import json
 import os
 import uuid
 import datetime
```

### Comparing `megadetector-5.0.8/data_management/importers/awc_to_json.py` & `megadetector-5.0.9/data_management/importers/awc_to_json.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-########
-#
-# awc_to_json.py
-#
-# Convert a particular .csv file from Australian Wildlife Conservancy to CCT format.
-#
-########
+"""
+
+ awc_to_json.py
+
+ Convert a particular .csv file from Australian Wildlife Conservancy to CCT format.
+
+"""
 
 #%% Constants and environment
 
 import pandas as pd
 import uuid
 import json
 import time
```

### Comparing `megadetector-5.0.8/data_management/importers/bellevue_to_json.py` & `megadetector-5.0.9/data_management/importers/bellevue_to_json.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-########
-#
-# bellevue_to_json.py
-#
-# "Bellevue Camera Traps" is the rather unremarkable camera trap data set
-# used by one of the repo's maintainers for testing.  It's organized as:
-#
-# approximate_date/[loose_camera_specifier/]/species    
-#
-# E.g.:
-#    
-# "2018.03.30\coyote\DSCF0091.JPG"
-# "2018.07.18\oldcam\empty\DSCF0001.JPG"
-#
-########
+"""
+
+ bellevue_to_json.py
+
+ "Bellevue Camera Traps" is the rather unremarkable camera trap data set
+ used by one of the repo's maintainers for testing.  It's organized as:
+
+ approximate_date/[loose_camera_specifier/]/species    
+
+ E.g.:
+    
+ "2018.03.30\coyote\DSCF0091.JPG"
+ "2018.07.18\oldcam\empty\DSCF0001.JPG"
+
+"""
 
 #%% Constants and imports
 
 import json
 import os
 import uuid
 import datetime
```

### Comparing `megadetector-5.0.8/data_management/importers/cacophony-thermal-importer.py` & `megadetector-5.0.9/data_management/importers/cacophony-thermal-importer.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-########
-#
-# cacophony-thermal-importer.py
-#
-# Create data and metadata for LILA from the Cacophony thermal dataset.  Takes a folder
-# of HDF files, and produces .json metadata, along with compressed/normalized videos for
-# each HDF file.
-#
-# Source format notes for this dataset:
-#    
-# https://docs.google.com/document/d/12sw5JtwdMf9MiXuNCBcvhvZ04Jwa1TH2Lf6LnJmF8Bk/edit
-#
-########
+"""
+
+ cacophony-thermal-importer.py
+
+ Create data and metadata for LILA from the Cacophony thermal dataset.  Takes a folder
+ of HDF files, and produces .json metadata, along with compressed/normalized videos for
+ each HDF file.
+
+ Source format notes for this dataset:
+    
+ https://docs.google.com/document/d/12sw5JtwdMf9MiXuNCBcvhvZ04Jwa1TH2Lf6LnJmF8Bk/edit
+
+"""
 
 #%% Imports and constants
 
 import os
 import h5py
 import numpy as np
 import json
```

### Comparing `megadetector-5.0.8/data_management/importers/carrizo_shrubfree_2018.py` & `megadetector-5.0.9/data_management/importers/carrizo_shrubfree_2018.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# carrizo_shrubfree_2018.py
-#
-# Convert the .csv file provided for the Carrizo Mojave data set to a 
-# COCO-camera-traps .json file
-#
-########
+"""
+
+ carrizo_shrubfree_2018.py
+
+ Convert the .csv file provided for the Carrizo Mojave data set to a 
+ COCO-camera-traps .json file
+
+"""
 
 #%% Constants and environment
 
 import pandas as pd
 import os
 import json
 import uuid
```

### Comparing `megadetector-5.0.8/data_management/importers/carrizo_trail_cam_2017.py` & `megadetector-5.0.9/data_management/importers/carrizo_trail_cam_2017.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# carrizo_trail_cam_2017.py
-#
-# Convert the .csv files provided for the "Trail Cam Carrizo" 2017 data set to 
-# a COCO-camera-traps .json file.
-#
-########
+"""
+
+ carrizo_trail_cam_2017.py
+
+ Convert the .csv files provided for the "Trail Cam Carrizo" 2017 data set to 
+ a COCO-camera-traps .json file.
+
+"""
 
 #%% Constants and environment
 
 import pandas as pd
 import os
 import json
 import uuid
```

### Comparing `megadetector-5.0.8/data_management/importers/cct_field_adjustments.py` & `megadetector-5.0.9/data_management/importers/cct_field_adjustments.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-########
-#
-# cct_field_adjustments.py
-#
-# CCT metadata was posted with int locations instead of strings.
-#
-# This script fixes those issues and rev's the version number.
-#
-########
+"""
+
+ cct_field_adjustments.py
+
+ CCT metadata was posted with int locations instead of strings.
+
+ This script fixes those issues and rev's the version number.
+
+"""
 
 #%% Constants and environment
 
 from data_management.databases import integrity_check_json_db
 import json
 import os
```

### Comparing `megadetector-5.0.8/data_management/importers/channel_islands_to_cct.py` & `megadetector-5.0.9/data_management/importers/channel_islands_to_cct.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-########
-#
-# channel_islands_to_cct.py
-#
-# Convert the Channel Islands data set to a COCO-camera-traps .json file
-#
-# Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,
-# because every Python package we tried failed to pull the "Maker Notes" field properly.
-#
-########
+"""
+
+ channel_islands_to_cct.py
+
+ Convert the Channel Islands data set to a COCO-camera-traps .json file
+
+ Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,
+ because every Python package we tried failed to pull the "Maker Notes" field properly.
+
+"""
 
 #%% Imports, constants, paths
 
 ## Imports ##
 
 import os
 import json
```

### Comparing `megadetector-5.0.8/data_management/importers/eMammal/copy_and_unzip_emammal.py` & `megadetector-5.0.9/data_management/importers/eMammal/copy_and_unzip_emammal.py`

 * *Files 0% similar despite different names*

```diff
@@ -151,14 +151,15 @@
     with open(os.path.join(log_folder, 'download_unzip_results_{}.json'.format(cur_time)), 'w') as f:
         json.dump(results, f)
 
 
 #%% Command-line driver
         
 if __name__ == '__main__':
+    
     if origin == 'cloud':
         container = 'wpz'
         desired_blob_prefix = 'emammal/0Robert Long/'
 
     print('Start timing...')
     start_time = datetime.now()
```

### Comparing `megadetector-5.0.8/data_management/importers/eMammal/eMammal_helpers.py` & `megadetector-5.0.9/data_management/importers/eMammal/eMammal_helpers.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/data_management/importers/eMammal/make_eMammal_json.py` & `megadetector-5.0.9/data_management/importers/eMammal/make_eMammal_json.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/data_management/importers/ena24_to_json.py` & `megadetector-5.0.9/data_management/importers/ena24_to_json.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-########
-#
-# ena24_to_json_2017.py
-#
-# Convert the ENA24 data set to a COCO-camera-traps .json file
-#
-########
+"""
+
+ ena24_to_json_2017.py
+
+ Convert the ENA24 data set to a COCO-camera-traps .json file
+
+"""
 
 #%% Constants and environment
 
 import os
 import json
 import uuid
 import time
```

### Comparing `megadetector-5.0.8/data_management/importers/filenames_to_json.py` & `megadetector-5.0.9/data_management/importers/filenames_to_json.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# filenames_to_json.py
-#
-# Take a directory of images in which species labels are encoded by folder
-# names, and produces a COCO-style .json file 
-# 
-########
+"""
+
+ filenames_to_json.py
+
+ Take a directory of images in which species labels are encoded by folder
+ names, and produces a COCO-style .json file 
+ 
+"""
 
 #%% Constants and imports
 
 import json
 import io
 import os
 import uuid
```

### Comparing `megadetector-5.0.8/data_management/importers/helena_to_cct.py` & `megadetector-5.0.9/data_management/importers/helena_to_cct.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-########
-#
-# helena_to_cct.py
-#
-# Convert the Helena Detections data set to a COCO-camera-traps .json file
-#
-########
+"""
+
+ helena_to_cct.py
+
+ Convert the Helena Detections data set to a COCO-camera-traps .json file
+
+"""
 
 #%% Constants and environment
 
 import os
 import json
 import uuid
 import time
```

### Comparing `megadetector-5.0.8/data_management/importers/idaho-camera-traps.py` & `megadetector-5.0.9/data_management/importers/idaho-camera-traps.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-########
-#
-# idaho-camera-traps.py
-#
-# Prepare the Idaho Camera Traps dataset for release on LILA.
-#
-########
+"""
+
+ idaho-camera-traps.py
+
+ Prepare the Idaho Camera Traps dataset for release on LILA.
+
+"""
 
 #%% Imports and constants
 
 import json
 import os
 import numpy as np
 import dateutil
```

### Comparing `megadetector-5.0.8/data_management/importers/idfg_iwildcam_lila_prep.py` & `megadetector-5.0.9/data_management/importers/idfg_iwildcam_lila_prep.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-########
-#
-# idfg_iwildcam_lila_prep.py
-#
-# Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG 
-# test set, in preparation for release on LILA.
-#
-# This version works with the public iWildCam release images.
-#
-########
+"""
+
+ idfg_iwildcam_lila_prep.py
+
+ Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG 
+ test set, in preparation for release on LILA.
+
+ This version works with the public iWildCam release images.
+
+"""
 
 #%% ############ Take one, from iWildCam .json files ############ 
 
 #%% Imports and constants
 
 import uuid
 import json
```

### Comparing `megadetector-5.0.8/data_management/importers/jb_csv_to_json.py` & `megadetector-5.0.9/data_management/importers/jb_csv_to_json.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-########
-#
-# jb_csv_to_json.py
-#
-# Convert a particular .csv file to CCT format.  Images were not available at
-# the time I wrote this script, so this is much shorter than other scripts 
-# in this folder.
-#
-########
+"""
+
+ jb_csv_to_json.py
+
+ Convert a particular .csv file to CCT format.  Images were not available at
+ the time I wrote this script, so this is much shorter than other scripts 
+ in this folder.
+
+"""
 
 #%% Constants and environment
 
 import pandas as pd
 import uuid
 import json
```

### Comparing `megadetector-5.0.8/data_management/importers/mcgill_to_json.py` & `megadetector-5.0.9/data_management/importers/mcgill_to_json.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# mcgill_to_json.py
-#
-# Convert the .csv file provided for the McGill test data set to a 
-# COCO-camera-traps .json file
-#
-########
+"""
+
+ mcgill_to_json.py
+
+ Convert the .csv file provided for the McGill test data set to a 
+ COCO-camera-traps .json file
+
+"""
 
 #%% Constants and environment
 
 import pandas as pd
 import os
 import glob
 import json
```

### Comparing `megadetector-5.0.8/data_management/importers/missouri_to_json.py` & `megadetector-5.0.9/data_management/importers/missouri_to_json.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,25 +1,25 @@
-########
-#
-# missouri_to_json.py
-#
-# Create .json files from the original source files for the Missouri Camera Traps
-# data set.  Metadata was provided here in two formats:
-#
-# 1) In one subset of the data, folder names indicated species names.  In Set 1,
-#    there are no empty sequences.  Set 1 has a metadata file to indicate image-level
-#    bounding boxes.
-#
-# 2) A subset of the data (overlapping with (1)) was annotated with bounding
-#    boxes, specified in a whitespace-delimited text file.  In set 2, there are
-#    some sequences omitted from the metadata file, which implied emptiness.
-# 
-# In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1.
-#
-########
+"""
+
+ missouri_to_json.py
+
+ Create .json files from the original source files for the Missouri Camera Traps
+ data set.  Metadata was provided here in two formats:
+
+ 1) In one subset of the data, folder names indicated species names.  In Set 1,
+    there are no empty sequences.  Set 1 has a metadata file to indicate image-level
+    bounding boxes.
+
+ 2) A subset of the data (overlapping with (1)) was annotated with bounding
+    boxes, specified in a whitespace-delimited text file.  In set 2, there are
+    some sequences omitted from the metadata file, which implied emptiness.
+ 
+ In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1.
+
+"""
 
 #%% Constants and imports
 
 import json
 import os
 import uuid
 import time
```

### Comparing `megadetector-5.0.8/data_management/importers/nacti_fieldname_adjustments.py` & `megadetector-5.0.9/data_management/importers/nacti_fieldname_adjustments.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-########
-#
-# nacti_fieldname_adjustments.py
-#
-# NACTI metadata was posted with "filename" in images instead of "file_name", and
-# used string (rather than int) category IDs (in categories, but not in annotations).
-#
-# This script fixes those issues and rev's the version number.
-#
-########
+"""
+
+ nacti_fieldname_adjustments.py
+
+ NACTI metadata was posted with "filename" in images instead of "file_name", and
+ used string (rather than int) category IDs (in categories, but not in annotations).
+
+ This script fixes those issues and rev's the version number.
+
+"""
 
 #%% Constants and environment
 
 import json
 import os
 
 inputJsonFile = r'/datadrive1/nacti_metadata_orig.json'
```

### Comparing `megadetector-5.0.8/data_management/importers/noaa_seals_2019.py` & `megadetector-5.0.9/data_management/importers/noaa_seals_2019.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-########
-#
-# noaa_seals_2019.py
-#
-# Prepare the NOAA Arctic Seals 2019 metadata for LILA.
-#
-########
+"""
+
+ noaa_seals_2019.py
+
+ Prepare the NOAA Arctic Seals 2019 metadata for LILA.
+
+"""
 
 #%% Imports and constants
 
 import os
 import pandas as pd
 from tqdm import tqdm
```

### Comparing `megadetector-5.0.8/data_management/importers/pc_to_json.py` & `megadetector-5.0.9/data_management/importers/pc_to_json.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-########
-#
-# pc_to_json.py
-#
-# Convert a particular collection of .csv files from Parks Canada to CCT format.
-#
-########
+"""
+
+ pc_to_json.py
+
+ Convert a particular collection of .csv files from Parks Canada to CCT format.
+
+"""
 
 #%% Constants and environment
 
 import pandas as pd
 import uuid
 import json
 import time
```

### Comparing `megadetector-5.0.8/data_management/importers/plot_wni_giraffes.py` & `megadetector-5.0.9/data_management/importers/plot_wni_giraffes.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-########
-#
-# plot_wni_giraffes.py
-#
-# Plot keypoints on a random sample of images from the wni-giraffes data set.
-#
-########
+"""
+
+ plot_wni_giraffes.py
+
+ Plot keypoints on a random sample of images from the wni-giraffes data set.
+
+"""
 
 #%% Constants and imports
 
 import os
 import json
 import random
```

### Comparing `megadetector-5.0.8/data_management/importers/prepare_zsl_imerit.py` & `megadetector-5.0.9/data_management/importers/prepare_zsl_imerit.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-########
-# 
-# prepare_zsl_imerit.py
-#
-# Prepare ZSL Borneo data for annotation (convert input data to iMerit-friendly format).
-#
-########
+"""
+ 
+ prepare_zsl_imerit.py
+
+ Prepare ZSL Borneo data for annotation (convert input data to iMerit-friendly format).
+
+"""
 
 #%% Imports and constants
 
 import json
 import os
 
 from tqdm import tqdm
```

### Comparing `megadetector-5.0.8/data_management/importers/rspb_to_json.py` & `megadetector-5.0.9/data_management/importers/rspb_to_json.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# rspb_to_json.py
-#
-# Convert the .csv file provided for the RSPB data set to a 
-# COCO-camera-traps .json file
-#
-########
+"""
+
+ rspb_to_json.py
+
+ Convert the .csv file provided for the RSPB data set to a 
+ COCO-camera-traps .json file
+
+"""
 
 #%% Constants and environment
 
 import pandas as pd
 import os
 import glob
 import json
```

### Comparing `megadetector-5.0.8/data_management/importers/save_the_elephants_survey_A.py` & `megadetector-5.0.9/data_management/importers/save_the_elephants_survey_A.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# save_the_elephants_survey_A.py
-#
-# Convert the .csv file provided for the Save the Elephants Survey A data set to a 
-# COCO-camera-traps .json file
-#
-########
+"""
+
+ save_the_elephants_survey_A.py
+
+ Convert the .csv file provided for the Save the Elephants Survey A data set to a 
+ COCO-camera-traps .json file
+
+"""
 
 #%% Constants and environment
 
 import pandas as pd
 import os
 import json
 import uuid
```

### Comparing `megadetector-5.0.8/data_management/importers/save_the_elephants_survey_B.py` & `megadetector-5.0.9/data_management/importers/save_the_elephants_survey_B.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# save_the_elephants_survey_B.py
-#
-# Convert the .csv file provided for the Save the Elephants Survey B data set to a 
-# COCO-camera-traps .json file
-#
-########
+"""
+
+ save_the_elephants_survey_B.py
+
+ Convert the .csv file provided for the Save the Elephants Survey B data set to a 
+ COCO-camera-traps .json file
+
+"""
 
 #%% Constants and environment
 
 from md_visualization import visualize_db
 from data_management.databases import integrity_check_json_db
 import pandas as pd
 import os
@@ -201,15 +201,15 @@
             images.append(im)
     
         species = row['Species']
         
         if (isinstance(species,float) or \
             (isinstance(species,str) and (len(species) == 0))):
             category_name = 'empty'
-        elif species.startswith('?')
+        elif species.startswith('?'):
             category_name = 'unknown'
         else:
             category_name = species
         
         # Special cases based on the 'photo type' field
         if 'vehicle' in photo_type:
             category_name = 'vehicle'
```

### Comparing `megadetector-5.0.8/data_management/importers/snapshot_safari_importer.py` & `megadetector-5.0.9/data_management/importers/snapshot_safari_importer.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-########
-#
-# Import a Snapshot Safari project (one project, one season)
-#
-# Before running this script:
-#
-# * Mount the blob container where the images live, or copy the 
-#   images to local storage
-#
-# What this script does:
-#
-# * Creates a .json file
-# * Creates zip archives of the season without humans.
-# * Copies animals and humans to separate folders 
-#
-# After running this script:
-#
-# * Create or update LILA page
-# * Push zipfile and unzipped images to LILA
-# * Push unzipped humans to wildlifeblobssc
-# * Delete images from UMN upload storage
-#
-# Snapshot Serengeti is handled specially, because we're dealing with bounding
-# boxes too.  See snapshot_serengeti_lila.py.
-#
-########
+"""
+
+ Import a Snapshot Safari project (one project, one season)
+
+ Before running this script:
+
+ * Mount the blob container where the images live, or copy the 
+   images to local storage
+
+ What this script does:
+
+ * Creates a .json file
+ * Creates zip archives of the season without humans.
+ * Copies animals and humans to separate folders 
+
+ After running this script:
+
+ * Create or update LILA page
+ * Push zipfile and unzipped images to LILA
+ * Push unzipped humans to wildlifeblobssc
+ * Delete images from UMN upload storage
+
+ Snapshot Serengeti is handled specially, because we're dealing with bounding
+ boxes too.  See snapshot_serengeti_lila.py.
+
+"""
 
 #%% Imports
 
 import pandas as pd
 import json
 import os
 import uuid
```

### Comparing `megadetector-5.0.8/data_management/importers/snapshot_safari_importer_reprise.py` & `megadetector-5.0.9/data_management/importers/snapshot_safari_importer_reprise.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,665 +1,665 @@
-########
-#
-# snapshot_safari_importer_reprise.py
-#
-# This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that
-# we didn't do the last time we imported Snapshot data (like updating the big taxonomy)
-# file, and we skip a bunch of things now that we used to do (like generating massive
-# zipfiles).  So, new year, new importer.
-#
-########
-
-#%% Constants and imports
-
-import os
-import glob
-import json
-import shutil
-import random
-
-import pandas as pd
-
-from tqdm import tqdm
-from collections import defaultdict
-
-from md_utils import path_utils
-
-input_base = '/media/user/Elements'
-output_base = os.path.expanduser('~/data/snapshot-safari-metadata')
-file_list_cache_file = os.path.join(output_base,'file_list.json')
-
-assert os.path.isdir(input_base)
-os.makedirs(output_base,exist_ok=True)
-
-# We're going to copy all the .csv files to a faster location
-annotation_cache_dir = os.path.join(output_base,'csv_files')
-os.makedirs(annotation_cache_dir,exist_ok=True)
-
-
-#%% List files
-
-# Do a one-time enumeration of the entire drive; this will take a long time,
-# but will save a lot of hassle later.
-
-if os.path.isfile(file_list_cache_file):
-    print('Loading file list from {}'.format(file_list_cache_file))
-    with open(file_list_cache_file,'r') as f:
-        all_files = json.load(f)    
-else:
-    all_files = glob.glob(os.path.join(input_base,'**','*.*'),recursive=True)
-    all_files = [fn for fn in all_files if '$RECYCLE.BIN' not in fn]
-    all_files = [fn for fn in all_files if 'System Volume Information' not in fn]
-    print('Enumerated {} files'.format(len(all_files)))
-    with open(file_list_cache_file,'w') as f:
-        json.dump(all_files,f,indent=1)
-    print('Wrote file list to {}'.format(file_list_cache_file))
-
-
-#%% Create derived lists
-
-# Takes about 60 seconds
-
-all_files_relative = [os.path.relpath(fn,input_base) for fn in all_files]
-all_files_relative = [fn.replace('\\','/') for fn in all_files_relative]
-all_files_relative_set = set(all_files_relative)
-
-# CSV files are one of:
-#
-# _report_lila.csv (species/count/etc. for each capture)
-# _report_lila_image_inventory.csv (maps captures to images)
-# _report_lila_overview.csv (distribution of species)
-csv_files = [fn for fn in all_files_relative if fn.endswith('.csv')]
-
-all_image_files = path_utils.find_image_strings(all_files_relative)
-
-print('Found a total of {} files, {} of which are images'.format(
-    len(all_files_relative),len(all_image_files)))
-
-
-#%% Copy all csv files to the annotation cache folder
-
-# fn = csv_files[0]
-for fn in csv_files:
-    target_file = os.path.join(annotation_cache_dir,os.path.basename(fn))
-    source_file = os.path.join(input_base,fn)
-    shutil.copyfile(source_file,target_file)    
-
-def read_cached_csv_file(fn):
-    """
-    Later cells will ask to read a .csv file from the original hard drive;
-    read from the annotation cache instead.
-    """
-    
-    cached_csv_file = os.path.join(annotation_cache_dir,os.path.basename(fn))
-    df = pd.read_csv(cached_csv_file)
-    return df
-
-
-#%% List project folders
-
-# There are two formats for project folder names:
-#
-# APN
-# Snapshot Cameo/DEB
-project_code_to_project_folder = {}
-
-folders = os.listdir(input_base)
-folders = [fn for fn in folders if (not fn.startswith('$') and \
-                                    not 'System Volume' in fn)]
-
-for fn in folders:
-    if len(fn) == 3:
-        assert fn not in project_code_to_project_folder
-        project_code_to_project_folder[fn] = fn
-    else:
-        assert 'Snapshot' in fn
-        subfolders = os.listdir('/'.join([input_base,fn]))
-        for subfn in subfolders:
-            assert len(subfn) == 3
-            assert subfn not in project_code_to_project_folder
-            project_code_to_project_folder[subfn] = '/'.join([fn,subfn])
-
-project_folder_to_project_code = {v: k for k, v in project_code_to_project_folder.items()}
-project_codes = sorted(list(project_code_to_project_folder.keys()))
-project_folders = sorted(list(project_code_to_project_folder.values()))
-
-def file_to_project_folder(fn):
-    """
-    For a given filename relative to the drive root, return the corresponding
-    project folder (also relative to the drive root).
-    """
-    
-    tokens = fn.split('/')
-    if len(tokens[0]) == 3:
-        project_folder = tokens[0]
-    else:
-        assert 'Snapshot' in tokens[0]
-        project_folder = '/'.join(tokens[0:2])
-    assert project_folder in project_folders
-    return project_folder
-
-
-def file_to_project_code(fn):
-    """
-    For a given filename relative to the drive root, return the corresponding
-    three-letter project code (e.g. "CDB").
-    """    
-    
-    return project_folder_to_project_code[file_to_project_folder(fn)]
-
-assert file_to_project_folder(
-    'APN/APN_S2/DW/DW_R5/APN_S2_DW_R5_IMAG0003.JPG') == 'APN'
-assert file_to_project_folder(
-    'Snapshot South Africa/BLO/BLO_S1/B05/B05_R1/BLO_S1_B05_R1_IMAG0003.JPG') == \
-    'Snapshot South Africa/BLO'
-assert file_to_project_code(
-    'Snapshot South Africa/BLO/BLO_S1/B05/B05_R1/BLO_S1_B05_R1_IMAG0003.JPG') == \
-    'BLO'
-    
-    
-#%% Map report and inventory files to codes
-
-# Maps a three-letter project code to a list of per-season _report_lila.csv files
-#
-# E.g.:
-#
-# 'DHP': ['Snapshot South Africa/DHP/LILA_Reports/DHP_S1_report_lila.csv',
-# 'Snapshot South Africa/DHP/LILA_Reports/DHP_S2_report_lila.csv',
-# 'Snapshot South Africa/DHP/LILA_Reports/DHP_S3_report_lila.csv']
-#
-project_code_to_report_files = defaultdict(list)
-
-# fn = csv_files[0]
-for fn in csv_files:
-    if 'report_lila.csv' not in fn:
-        continue
-    project_code = project_folder_to_project_code[file_to_project_folder(fn)]
-    project_code_to_report_files[project_code].append(fn)
-
-project_codes_with_no_reports = set()
-
-for project_code in project_code_to_project_folder.keys():
-    if project_code not in project_code_to_report_files:
-        project_codes_with_no_reports.add(project_code)
-        print('Warning: no report files available for {}'.format(project_code))
-
-
-#%% Make sure that every report has a corresponding inventory file
-
-all_report_files = [item for sublist in project_code_to_report_files.values() \
-                    for item in sublist]
-
-for fn in all_report_files:
-    inventory_file = fn.replace('.csv','_image_inventory.csv')
-    assert inventory_file in csv_files
-    
-    
-#%% Count species based on overview and report files
-
-# The overview and report files should produce the same counts; we'll verify this
-# in the next cell.
-
-species_to_count_overview = defaultdict(int)
-species_to_count_report = defaultdict(int)
-
-for report_file in all_report_files:
-        
-    overview_file = report_file.replace('.csv','_overview.csv')    
-    
-    df = read_cached_csv_file(overview_file)
-    
-    for i_row,row in df.iterrows():
-        
-        if row['question'] == 'question__species':
-            
-            assert isinstance(row['answer'],str)
-            assert isinstance(row['count'],int)
-            species = row['answer']
-            
-            if len(species) < 3:
-                assert species == '0' or species == '1'
-                
-            species_to_count_overview[species] += row['count']
-    
-    # ...for each capture in the overview file
-    
-    df = read_cached_csv_file(report_file)
-    
-    for i_row,row in df.iterrows():
-        
-        species = row['question__species']
-        assert isinstance(species,str)
-        
-        # Ignore results from the blank/non-blank workflow
-        if len(species) < 3:
-            assert species == '0' or species == '1'                
-        species_to_count_report[species] += 1
-                    
-    # ...for each capture in the report file
-    
-# ...for each report file
-    
-
-#%% Print counts
-
-species_to_count_overview_sorted = \
-    {k: v for k, v in sorted(species_to_count_overview.items(), 
-                             key=lambda item: item[1], reverse=True)}
-species_to_count_report_sorted = \
-    {k: v for k, v in sorted(species_to_count_report.items(), 
-                             key=lambda item: item[1], reverse=True)}
-
-string_count = 0
-non_blank_count = 0
-
-for species in species_to_count_overview_sorted.keys():        
-    
-    # The overview and report files should produce the same counts
-    assert species_to_count_overview_sorted[species] == \
-        species_to_count_report[species]    
-    count = species_to_count_overview_sorted[species]
-    if species not in ('0','1'):
-        string_count += count
-        if species != 'blank':
-            non_blank_count += count
-            
-    print('{}{}'.format(species.ljust(25),count))
-
-n_images = len(all_files)
-n_sequences = sum(species_to_count_overview_sorted.values())
-
-print('\n{} total images\n{} total sequences'.format(n_images,n_sequences))
-
-print('\nString count: {}'.format(string_count))
-print('Non-blank count: {}'.format(non_blank_count))
-
-
-#%% Make sure that capture IDs in the reports/inventory files match
-
-# ...and confirm that (almost) all the images in the inventory tables are 
-# present on disk.
-
-all_relative_paths_in_inventory = set()
-files_missing_on_disk = []
-
-for report_file in all_report_files:
-        
-    project_base = file_to_project_folder(report_file)                
-    inventory_file = report_file.replace('.csv','_image_inventory.csv')    
-    
-    inventory_df = read_cached_csv_file(inventory_file)
-    report_df = read_cached_csv_file(report_file)
-    
-    capture_ids_in_report = set()
-    for i_row,row in report_df.iterrows():
-        capture_ids_in_report.add(row['capture_id'])
-    
-    capture_ids_in_inventory = set()
-    for i_row,row in inventory_df.iterrows():
-        
-        capture_ids_in_inventory.add(row['capture_id'])
-        image_path_relative = project_base + '/' + row['image_path_rel']
-        
-        # assert image_path_relative in all_files_relative_set
-        if image_path_relative not in all_files_relative_set:
-            
-            # Make sure this isn't just a case issue
-            assert image_path_relative.replace('.JPG','.jpg') \
-                not in all_files_relative_set
-            assert image_path_relative.replace('.jpg','.JPG') \
-                not in all_files_relative_set
-            files_missing_on_disk.append(image_path_relative)
-            
-        assert image_path_relative not in all_relative_paths_in_inventory
-        all_relative_paths_in_inventory.add(image_path_relative)
-                
-    # Make sure the set of capture IDs appearing in this report is
-    # the same as the set of capture IDs appearing in the corresponding
-    # inventory file.
-    assert capture_ids_in_report == capture_ids_in_inventory
-
-# ...for each report file
-
-print('\n{} missing files (of {})'.format(
-    len(files_missing_on_disk),len(all_relative_paths_in_inventory)))
-
-    
-#%% For all the files we have on disk, see which are and aren't in the inventory files
-
-# There aren't any capital-P .PNG files, but if I don't include .PNG
-# in this list, I'll look at this in a year and wonder whether I forgot
-# to include it.
-image_extensions = set(['.JPG','.jpg','.PNG','.png'])
-
-images_not_in_inventory = []
-n_images_in_inventoried_projects = 0
-
-# fn = all_files_relative[0]
-for fn in tqdm(all_files_relative):
-
-    if os.path.splitext(fn)[1] not in image_extensions:
-        continue
-    project_code = file_to_project_code(fn)
-    if project_code in project_codes_with_no_reports:
-        # print('Skipping project {}'.format(project_code))
-        continue
-    n_images_in_inventoried_projects += 1
-    if fn not in all_relative_paths_in_inventory:
-        images_not_in_inventory.append(fn)
-
-print('\n{} images on disk are not in inventory (of {} in eligible projects)'.format(
-    len(images_not_in_inventory),n_images_in_inventoried_projects))
-
-
-#%% Map captures to images, and vice-versa
-
-capture_id_to_images = defaultdict(list)
-image_to_capture_id = {}
-
-# report_file = all_report_files[0]
-for report_file in tqdm(all_report_files):
-        
-    inventory_file = report_file.replace('.csv','_image_inventory.csv')    
-    inventory_df = read_cached_csv_file(inventory_file)
-    
-    project_folder = file_to_project_folder(inventory_file)
-    
-    # row = inventory_df.iloc[0]
-    for i_row,row in inventory_df.iterrows():
-    
-        capture_id = row['capture_id']
-        image_file_relative = os.path.join(project_folder,row['image_path_rel'])
-        capture_id_to_images[capture_id].append(image_file_relative)
-        assert image_file_relative not in image_to_capture_id
-        image_to_capture_id[image_file_relative] = capture_id
-        
-    # ...for each row (one image per row)
-        
-# ...for each report file
-    
-
-#%% Map captures to species (just species for now, we'll go back and get other metadata later)
-
-capture_id_to_species = defaultdict(list)
-
-for project_code in tqdm(project_codes):
-    
-    report_files = project_code_to_report_files[project_code]
-    
-    for report_file in report_files:
-        
-        report_df = read_cached_csv_file(report_file)
-
-        for i_row,row in report_df.iterrows():
-        
-            capture_id = row['capture_id']
-            species = row['question__species']
-            capture_id_to_species[capture_id].append(species)
-
-        # ...for each row
-        
-    # ...for each report file in this project
-    
-# ...for each project
-
-
-#%% Take a look at the annotations "0" and "1"
-
-captures_0 = []
-captures_1 = []
-captures_1_alone = []
-captures_1_with_species = []
-
-for capture_id in tqdm(capture_id_to_species):
-    
-    species_this_capture_id = capture_id_to_species[capture_id]
-    
-    # Multiple rows may be present for a capture, but they should be unique
-    assert len(species_this_capture_id) == len(set(species_this_capture_id))
-    
-    if '0' in species_this_capture_id:
-        captures_0.append(capture_id)
-        # '0' should always appear alone
-        assert len(species_this_capture_id) == 1
-
-    if '1' in species_this_capture_id:
-        captures_1.append(capture_id)
-        assert '0' not in species_this_capture_id
-        # '1' should never appear alone
-        # assert len(species_this_capture_id) > 1
-        if len(species_this_capture_id) == 1:
-            captures_1_alone.append(capture_id)
-        else:
-            captures_1_with_species.append(capture_id)
-
-# ...for each capture ID
-
-print('')
-print('Number of captures with "0" as the species (always appears alone): {}'.format(len(captures_0)))
-print('Number of captures with "1" as the species: {}'.format(len(captures_1)))
-print('Number of captures with "1" as the species, with no other species: {}'.format(
-    len(captures_1_alone)))
-print('Number of captures with "1" as the species, with other species: {}'.format(
-    len(captures_1_with_species)))
-
-
-#%% Sample some of those captures with mysterious "0" and "1" annotations
-
-random.seed(0)
-n_to_sample = 500
-captures_0_samples = random.sample(captures_0,n_to_sample)
-captures_1_samples = random.sample(captures_1,n_to_sample)
-
-capture_0_sample_output_folder = os.path.join(output_base,'capture_0_samples')
-capture_1_sample_output_folder = os.path.join(output_base,'capture_1_samples')
-os.makedirs(capture_0_sample_output_folder,exist_ok=True)
-os.makedirs(capture_1_sample_output_folder,exist_ok=True)
-
-def copy_sampled_captures(sampled_captures,sample_capture_output_folder):
-
-    for capture_id in tqdm(sampled_captures):    
-        images_this_capture = capture_id_to_images[capture_id]
-        for fn in images_this_capture:            
-            # assert fn in all_files_relative_set
-            if fn not in all_files_relative_set:
-                print('Warning: missing file {}'.format(fn))
-                continue
-            source_image = os.path.join(input_base,fn)
-            target_image = os.path.join(sample_capture_output_folder,os.path.basename(fn))
-            shutil.copyfile(source_image,target_image)
-        # ....for each image
-    # ...for each capture
-        
-copy_sampled_captures(captures_0_samples,capture_0_sample_output_folder)    
-copy_sampled_captures(captures_1_samples,capture_1_sample_output_folder)
-
-
-#%% Find images that MD thinks contain people
-
-md_results_folder = os.path.expanduser(
-    '~/postprocessing/snapshot-safari/snapshot-safari-2023-04-21-v5a.0.0/json_subsets')
-md_results_files = os.listdir(md_results_folder)
-
-md_human_detection_threshold = 0.2
-md_vehicle_detection_threshold = 0.2
-
-# We'll make sure this is actually correct for all the files we load
-md_human_category = '2'
-md_vehicle_category = '3'
-
-md_human_images = set()
-md_vehicle_images = set()
-
-# project_code = project_codes[0]
-for project_code in project_codes:
-    
-    print('Finding human images for {}'.format(project_code))
-    
-    project_folder = project_code_to_project_folder[project_code]
-    
-    md_results_file = [fn for fn in md_results_files if project_code in fn]
-    assert len(md_results_file) == 1
-    md_results_file = os.path.join(md_results_folder,md_results_file[0])
-    
-    with open(md_results_file,'r') as f:
-        md_results = json.load(f)
-    assert md_results['detection_categories'][md_human_category] == 'person'
-    assert md_results['detection_categories'][md_vehicle_category] == 'vehicle'
-    
-    # im = md_results['images'][0]
-    for im in tqdm(md_results['images']):
-        
-        if 'detections' not in im:
-            continue
-        
-        # MD results files are each relative to their own projects, we want
-        # filenames to be relative to the base of the drive
-        fn = os.path.join(project_folder,im['file'])
-        for det in im['detections']:
-            if det['category'] == md_human_category and \
-                det['conf'] >= md_human_detection_threshold:
-                    md_human_images.add(fn)
-            if det['category'] == md_vehicle_category and \
-                det['conf'] >= md_vehicle_detection_threshold:
-                    md_vehicle_images.add(fn)
-                    
-        # ...for each detection
-        
-    # ...for each image
-                    
-# ...for each project
-
-print('MD found {} human images, {} vehicle images'.format(
-    len(md_human_images),len(md_vehicle_images)))
-
-md_human_or_vehicle_images = \
-    set(md_human_images).union(set(md_vehicle_images))
-    
-# next(iter(md_human_or_vehicle_images))
-
-
-#%% Find images where the ground truth says humans or vehicles are present
-
-human_species_id = 'human'
-vehicle_species_id = 'humanvehicle'
-
-gt_human_capture_ids = set()
-gt_vehicle_capture_ids = set()
-
-for capture_id in capture_id_to_species:
-    
-    species_this_capture_id = capture_id_to_species[capture_id]
-    
-    for species in species_this_capture_id:
-        if species == human_species_id:
-            gt_human_capture_ids.add(capture_id)
-        elif species == vehicle_species_id:
-            gt_vehicle_capture_ids.add(capture_id)
-    
-# ...for each capture ID
-
-gt_human_images = []
-gt_vehicle_images = []
-
-for capture_id in gt_human_capture_ids:
-    images_this_capture_id = capture_id_to_images[capture_id]
-    gt_human_images.extend(images_this_capture_id)
-for capture_id in gt_vehicle_capture_ids:
-    images_this_capture_id = capture_id_to_images[capture_id]
-    gt_vehicle_images.extend(images_this_capture_id)    
-    
-print('Ground truth includes {} human images ({} captures), {} vehicle images ({} captures)'.format(
-    len(gt_human_images),len(gt_human_capture_ids),
-    len(gt_vehicle_images),len(gt_vehicle_capture_ids)))
-
-ground_truth_human_or_vehicle_images = \
-    set(gt_human_images).union(set(gt_vehicle_images))
-    
-# next(iter(ground_truth_human_or_vehicle_images))
-
-
-#%% Find mismatches
-
-gt_missing_human_images = []
-gt_missing_vehicle_images = []
-
-for fn in md_human_images:
-    if fn not in ground_truth_human_or_vehicle_images:
-        gt_missing_human_images.append(fn)
-
-for fn in md_vehicle_images:
-    if fn not in ground_truth_human_or_vehicle_images:
-        gt_missing_vehicle_images.append(fn)
-        
-print('Of {} images where MD found a human, {} are not in the ground truth'.format(
-    len(md_human_images),len(gt_missing_human_images)))
-
-print('Of {} images where MD found a vehicle, {} are not in the ground truth'.format(
-    len(md_vehicle_images),len(gt_missing_vehicle_images)))
-
-
-#%% Sample mismatches
-
-random.seed(0)
-n_to_sample = 1000
-sampled_human_mismatches = random.sample(gt_missing_human_images,n_to_sample)
-sampled_vehicle_mismatches = random.sample(gt_missing_vehicle_images,n_to_sample)
-
-human_mismatch_output_folder = os.path.join(output_base,'mismatches_human')
-vehicle_mismatch_output_folder = os.path.join(output_base,'mismatches_vehicle')
-os.makedirs(human_mismatch_output_folder,exist_ok=True)
-os.makedirs(vehicle_mismatch_output_folder,exist_ok=True)
-
-def copy_sampled_images(sampled_images,sampled_images_output_folder):
-
-    for fn in tqdm(sampled_images): 
-        if fn not in all_files_relative_set:
-            print('Warning: missing file {}'.format(fn))
-            continue
-        source_image = os.path.join(input_base,fn)
-        target_image = os.path.join(sampled_images_output_folder,os.path.basename(fn))
-        shutil.copyfile(source_image,target_image)
-    
-copy_sampled_images(sampled_human_mismatches,human_mismatch_output_folder)
-copy_sampled_images(sampled_vehicle_mismatches,vehicle_mismatch_output_folder)
-
-
-#%% See what's up with some of the mismatches
-
-filename_base_to_filename = {}
-
-from md_utils.path_utils import is_image_file
-
-# fn = all_files_relative[0]
-for fn in tqdm(all_files_relative):
-
-    if not is_image_file(fn):
-        continue
-    if 'Indiv_Recognition' in fn:
-        continue
-    bn = os.path.basename(fn)
-    assert bn not in filename_base_to_filename
-    filename_base_to_filename[bn] = fn
-
-
-if False:
-
-    bn = 'TSW_S2_KA02_R3_IMAG0002.JPG'
-    fn = filename_base_to_filename[bn]
-    capture_id = image_to_capture_id[fn]
-    species = capture_id_to_species[capture_id]
-    
-    
-#%% Look at the distribution of labels for the mismatched images    
-
-gt_missing_images = set(gt_missing_human_images).union(set(gt_missing_vehicle_images))
-
-missing_image_species_to_count = defaultdict(int)
-
-for fn in gt_missing_images:
-    if fn not in image_to_capture_id:
-        continue
-    capture_id = image_to_capture_id[fn]
-    species = capture_id_to_species[capture_id]
-    for s in species:
-        missing_image_species_to_count[s] += 1
+"""
+
+ snapshot_safari_importer_reprise.py
+
+ This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that
+ we didn't do the last time we imported Snapshot data (like updating the big taxonomy)
+ file, and we skip a bunch of things now that we used to do (like generating massive
+ zipfiles).  So, new year, new importer.
+
+"""
+
+#%% Constants and imports
+
+import os
+import glob
+import json
+import shutil
+import random
+
+import pandas as pd
+
+from tqdm import tqdm
+from collections import defaultdict
+
+from md_utils import path_utils
+
+input_base = '/media/user/Elements'
+output_base = os.path.expanduser('~/data/snapshot-safari-metadata')
+file_list_cache_file = os.path.join(output_base,'file_list.json')
+
+assert os.path.isdir(input_base)
+os.makedirs(output_base,exist_ok=True)
+
+# We're going to copy all the .csv files to a faster location
+annotation_cache_dir = os.path.join(output_base,'csv_files')
+os.makedirs(annotation_cache_dir,exist_ok=True)
+
+
+#%% List files
+
+# Do a one-time enumeration of the entire drive; this will take a long time,
+# but will save a lot of hassle later.
+
+if os.path.isfile(file_list_cache_file):
+    print('Loading file list from {}'.format(file_list_cache_file))
+    with open(file_list_cache_file,'r') as f:
+        all_files = json.load(f)    
+else:
+    all_files = glob.glob(os.path.join(input_base,'**','*.*'),recursive=True)
+    all_files = [fn for fn in all_files if '$RECYCLE.BIN' not in fn]
+    all_files = [fn for fn in all_files if 'System Volume Information' not in fn]
+    print('Enumerated {} files'.format(len(all_files)))
+    with open(file_list_cache_file,'w') as f:
+        json.dump(all_files,f,indent=1)
+    print('Wrote file list to {}'.format(file_list_cache_file))
+
+
+#%% Create derived lists
+
+# Takes about 60 seconds
+
+all_files_relative = [os.path.relpath(fn,input_base) for fn in all_files]
+all_files_relative = [fn.replace('\\','/') for fn in all_files_relative]
+all_files_relative_set = set(all_files_relative)
+
+# CSV files are one of:
+#
+# _report_lila.csv (species/count/etc. for each capture)
+# _report_lila_image_inventory.csv (maps captures to images)
+# _report_lila_overview.csv (distribution of species)
+csv_files = [fn for fn in all_files_relative if fn.endswith('.csv')]
+
+all_image_files = path_utils.find_image_strings(all_files_relative)
+
+print('Found a total of {} files, {} of which are images'.format(
+    len(all_files_relative),len(all_image_files)))
+
+
+#%% Copy all csv files to the annotation cache folder
+
+# fn = csv_files[0]
+for fn in csv_files:
+    target_file = os.path.join(annotation_cache_dir,os.path.basename(fn))
+    source_file = os.path.join(input_base,fn)
+    shutil.copyfile(source_file,target_file)    
+
+def read_cached_csv_file(fn):
+    """
+    Later cells will ask to read a .csv file from the original hard drive;
+    read from the annotation cache instead.
+    """
+    
+    cached_csv_file = os.path.join(annotation_cache_dir,os.path.basename(fn))
+    df = pd.read_csv(cached_csv_file)
+    return df
+
+
+#%% List project folders
+
+# There are two formats for project folder names:
+#
+# APN
+# Snapshot Cameo/DEB
+project_code_to_project_folder = {}
+
+folders = os.listdir(input_base)
+folders = [fn for fn in folders if (not fn.startswith('$') and \
+                                    not 'System Volume' in fn)]
+
+for fn in folders:
+    if len(fn) == 3:
+        assert fn not in project_code_to_project_folder
+        project_code_to_project_folder[fn] = fn
+    else:
+        assert 'Snapshot' in fn
+        subfolders = os.listdir('/'.join([input_base,fn]))
+        for subfn in subfolders:
+            assert len(subfn) == 3
+            assert subfn not in project_code_to_project_folder
+            project_code_to_project_folder[subfn] = '/'.join([fn,subfn])
+
+project_folder_to_project_code = {v: k for k, v in project_code_to_project_folder.items()}
+project_codes = sorted(list(project_code_to_project_folder.keys()))
+project_folders = sorted(list(project_code_to_project_folder.values()))
+
+def file_to_project_folder(fn):
+    """
+    For a given filename relative to the drive root, return the corresponding
+    project folder (also relative to the drive root).
+    """
+    
+    tokens = fn.split('/')
+    if len(tokens[0]) == 3:
+        project_folder = tokens[0]
+    else:
+        assert 'Snapshot' in tokens[0]
+        project_folder = '/'.join(tokens[0:2])
+    assert project_folder in project_folders
+    return project_folder
+
+
+def file_to_project_code(fn):
+    """
+    For a given filename relative to the drive root, return the corresponding
+    three-letter project code (e.g. "CDB").
+    """    
+    
+    return project_folder_to_project_code[file_to_project_folder(fn)]
+
+assert file_to_project_folder(
+    'APN/APN_S2/DW/DW_R5/APN_S2_DW_R5_IMAG0003.JPG') == 'APN'
+assert file_to_project_folder(
+    'Snapshot South Africa/BLO/BLO_S1/B05/B05_R1/BLO_S1_B05_R1_IMAG0003.JPG') == \
+    'Snapshot South Africa/BLO'
+assert file_to_project_code(
+    'Snapshot South Africa/BLO/BLO_S1/B05/B05_R1/BLO_S1_B05_R1_IMAG0003.JPG') == \
+    'BLO'
+    
+    
+#%% Map report and inventory files to codes
+
+# Maps a three-letter project code to a list of per-season _report_lila.csv files
+#
+# E.g.:
+#
+# 'DHP': ['Snapshot South Africa/DHP/LILA_Reports/DHP_S1_report_lila.csv',
+# 'Snapshot South Africa/DHP/LILA_Reports/DHP_S2_report_lila.csv',
+# 'Snapshot South Africa/DHP/LILA_Reports/DHP_S3_report_lila.csv']
+#
+project_code_to_report_files = defaultdict(list)
+
+# fn = csv_files[0]
+for fn in csv_files:
+    if 'report_lila.csv' not in fn:
+        continue
+    project_code = project_folder_to_project_code[file_to_project_folder(fn)]
+    project_code_to_report_files[project_code].append(fn)
+
+project_codes_with_no_reports = set()
+
+for project_code in project_code_to_project_folder.keys():
+    if project_code not in project_code_to_report_files:
+        project_codes_with_no_reports.add(project_code)
+        print('Warning: no report files available for {}'.format(project_code))
+
+
+#%% Make sure that every report has a corresponding inventory file
+
+all_report_files = [item for sublist in project_code_to_report_files.values() \
+                    for item in sublist]
+
+for fn in all_report_files:
+    inventory_file = fn.replace('.csv','_image_inventory.csv')
+    assert inventory_file in csv_files
+    
+    
+#%% Count species based on overview and report files
+
+# The overview and report files should produce the same counts; we'll verify this
+# in the next cell.
+
+species_to_count_overview = defaultdict(int)
+species_to_count_report = defaultdict(int)
+
+for report_file in all_report_files:
+        
+    overview_file = report_file.replace('.csv','_overview.csv')    
+    
+    df = read_cached_csv_file(overview_file)
+    
+    for i_row,row in df.iterrows():
+        
+        if row['question'] == 'question__species':
+            
+            assert isinstance(row['answer'],str)
+            assert isinstance(row['count'],int)
+            species = row['answer']
+            
+            if len(species) < 3:
+                assert species == '0' or species == '1'
+                
+            species_to_count_overview[species] += row['count']
+    
+    # ...for each capture in the overview file
+    
+    df = read_cached_csv_file(report_file)
+    
+    for i_row,row in df.iterrows():
+        
+        species = row['question__species']
+        assert isinstance(species,str)
+        
+        # Ignore results from the blank/non-blank workflow
+        if len(species) < 3:
+            assert species == '0' or species == '1'                
+        species_to_count_report[species] += 1
+                    
+    # ...for each capture in the report file
+    
+# ...for each report file
+    
+
+#%% Print counts
+
+species_to_count_overview_sorted = \
+    {k: v for k, v in sorted(species_to_count_overview.items(), 
+                             key=lambda item: item[1], reverse=True)}
+species_to_count_report_sorted = \
+    {k: v for k, v in sorted(species_to_count_report.items(), 
+                             key=lambda item: item[1], reverse=True)}
+
+string_count = 0
+non_blank_count = 0
+
+for species in species_to_count_overview_sorted.keys():        
+    
+    # The overview and report files should produce the same counts
+    assert species_to_count_overview_sorted[species] == \
+        species_to_count_report[species]    
+    count = species_to_count_overview_sorted[species]
+    if species not in ('0','1'):
+        string_count += count
+        if species != 'blank':
+            non_blank_count += count
+            
+    print('{}{}'.format(species.ljust(25),count))
+
+n_images = len(all_files)
+n_sequences = sum(species_to_count_overview_sorted.values())
+
+print('\n{} total images\n{} total sequences'.format(n_images,n_sequences))
+
+print('\nString count: {}'.format(string_count))
+print('Non-blank count: {}'.format(non_blank_count))
+
+
+#%% Make sure that capture IDs in the reports/inventory files match
+
+# ...and confirm that (almost) all the images in the inventory tables are 
+# present on disk.
+
+all_relative_paths_in_inventory = set()
+files_missing_on_disk = []
+
+for report_file in all_report_files:
+        
+    project_base = file_to_project_folder(report_file)                
+    inventory_file = report_file.replace('.csv','_image_inventory.csv')    
+    
+    inventory_df = read_cached_csv_file(inventory_file)
+    report_df = read_cached_csv_file(report_file)
+    
+    capture_ids_in_report = set()
+    for i_row,row in report_df.iterrows():
+        capture_ids_in_report.add(row['capture_id'])
+    
+    capture_ids_in_inventory = set()
+    for i_row,row in inventory_df.iterrows():
+        
+        capture_ids_in_inventory.add(row['capture_id'])
+        image_path_relative = project_base + '/' + row['image_path_rel']
+        
+        # assert image_path_relative in all_files_relative_set
+        if image_path_relative not in all_files_relative_set:
+            
+            # Make sure this isn't just a case issue
+            assert image_path_relative.replace('.JPG','.jpg') \
+                not in all_files_relative_set
+            assert image_path_relative.replace('.jpg','.JPG') \
+                not in all_files_relative_set
+            files_missing_on_disk.append(image_path_relative)
+            
+        assert image_path_relative not in all_relative_paths_in_inventory
+        all_relative_paths_in_inventory.add(image_path_relative)
+                
+    # Make sure the set of capture IDs appearing in this report is
+    # the same as the set of capture IDs appearing in the corresponding
+    # inventory file.
+    assert capture_ids_in_report == capture_ids_in_inventory
+
+# ...for each report file
+
+print('\n{} missing files (of {})'.format(
+    len(files_missing_on_disk),len(all_relative_paths_in_inventory)))
+
+    
+#%% For all the files we have on disk, see which are and aren't in the inventory files
+
+# There aren't any capital-P .PNG files, but if I don't include .PNG
+# in this list, I'll look at this in a year and wonder whether I forgot
+# to include it.
+image_extensions = set(['.JPG','.jpg','.PNG','.png'])
+
+images_not_in_inventory = []
+n_images_in_inventoried_projects = 0
+
+# fn = all_files_relative[0]
+for fn in tqdm(all_files_relative):
+
+    if os.path.splitext(fn)[1] not in image_extensions:
+        continue
+    project_code = file_to_project_code(fn)
+    if project_code in project_codes_with_no_reports:
+        # print('Skipping project {}'.format(project_code))
+        continue
+    n_images_in_inventoried_projects += 1
+    if fn not in all_relative_paths_in_inventory:
+        images_not_in_inventory.append(fn)
+
+print('\n{} images on disk are not in inventory (of {} in eligible projects)'.format(
+    len(images_not_in_inventory),n_images_in_inventoried_projects))
+
+
+#%% Map captures to images, and vice-versa
+
+capture_id_to_images = defaultdict(list)
+image_to_capture_id = {}
+
+# report_file = all_report_files[0]
+for report_file in tqdm(all_report_files):
+        
+    inventory_file = report_file.replace('.csv','_image_inventory.csv')    
+    inventory_df = read_cached_csv_file(inventory_file)
+    
+    project_folder = file_to_project_folder(inventory_file)
+    
+    # row = inventory_df.iloc[0]
+    for i_row,row in inventory_df.iterrows():
+    
+        capture_id = row['capture_id']
+        image_file_relative = os.path.join(project_folder,row['image_path_rel'])
+        capture_id_to_images[capture_id].append(image_file_relative)
+        assert image_file_relative not in image_to_capture_id
+        image_to_capture_id[image_file_relative] = capture_id
+        
+    # ...for each row (one image per row)
+        
+# ...for each report file
+    
+
+#%% Map captures to species (just species for now, we'll go back and get other metadata later)
+
+capture_id_to_species = defaultdict(list)
+
+for project_code in tqdm(project_codes):
+    
+    report_files = project_code_to_report_files[project_code]
+    
+    for report_file in report_files:
+        
+        report_df = read_cached_csv_file(report_file)
+
+        for i_row,row in report_df.iterrows():
+        
+            capture_id = row['capture_id']
+            species = row['question__species']
+            capture_id_to_species[capture_id].append(species)
+
+        # ...for each row
+        
+    # ...for each report file in this project
+    
+# ...for each project
+
+
+#%% Take a look at the annotations "0" and "1"
+
+captures_0 = []
+captures_1 = []
+captures_1_alone = []
+captures_1_with_species = []
+
+for capture_id in tqdm(capture_id_to_species):
+    
+    species_this_capture_id = capture_id_to_species[capture_id]
+    
+    # Multiple rows may be present for a capture, but they should be unique
+    assert len(species_this_capture_id) == len(set(species_this_capture_id))
+    
+    if '0' in species_this_capture_id:
+        captures_0.append(capture_id)
+        # '0' should always appear alone
+        assert len(species_this_capture_id) == 1
+
+    if '1' in species_this_capture_id:
+        captures_1.append(capture_id)
+        assert '0' not in species_this_capture_id
+        # '1' should never appear alone
+        # assert len(species_this_capture_id) > 1
+        if len(species_this_capture_id) == 1:
+            captures_1_alone.append(capture_id)
+        else:
+            captures_1_with_species.append(capture_id)
+
+# ...for each capture ID
+
+print('')
+print('Number of captures with "0" as the species (always appears alone): {}'.format(len(captures_0)))
+print('Number of captures with "1" as the species: {}'.format(len(captures_1)))
+print('Number of captures with "1" as the species, with no other species: {}'.format(
+    len(captures_1_alone)))
+print('Number of captures with "1" as the species, with other species: {}'.format(
+    len(captures_1_with_species)))
+
+
+#%% Sample some of those captures with mysterious "0" and "1" annotations
+
+random.seed(0)
+n_to_sample = 500
+captures_0_samples = random.sample(captures_0,n_to_sample)
+captures_1_samples = random.sample(captures_1,n_to_sample)
+
+capture_0_sample_output_folder = os.path.join(output_base,'capture_0_samples')
+capture_1_sample_output_folder = os.path.join(output_base,'capture_1_samples')
+os.makedirs(capture_0_sample_output_folder,exist_ok=True)
+os.makedirs(capture_1_sample_output_folder,exist_ok=True)
+
+def copy_sampled_captures(sampled_captures,sample_capture_output_folder):
+
+    for capture_id in tqdm(sampled_captures):    
+        images_this_capture = capture_id_to_images[capture_id]
+        for fn in images_this_capture:            
+            # assert fn in all_files_relative_set
+            if fn not in all_files_relative_set:
+                print('Warning: missing file {}'.format(fn))
+                continue
+            source_image = os.path.join(input_base,fn)
+            target_image = os.path.join(sample_capture_output_folder,os.path.basename(fn))
+            shutil.copyfile(source_image,target_image)
+        # ....for each image
+    # ...for each capture
+        
+copy_sampled_captures(captures_0_samples,capture_0_sample_output_folder)    
+copy_sampled_captures(captures_1_samples,capture_1_sample_output_folder)
+
+
+#%% Find images that MD thinks contain people
+
+md_results_folder = os.path.expanduser(
+    '~/postprocessing/snapshot-safari/snapshot-safari-2023-04-21-v5a.0.0/json_subsets')
+md_results_files = os.listdir(md_results_folder)
+
+md_human_detection_threshold = 0.2
+md_vehicle_detection_threshold = 0.2
+
+# We'll make sure this is actually correct for all the files we load
+md_human_category = '2'
+md_vehicle_category = '3'
+
+md_human_images = set()
+md_vehicle_images = set()
+
+# project_code = project_codes[0]
+for project_code in project_codes:
+    
+    print('Finding human images for {}'.format(project_code))
+    
+    project_folder = project_code_to_project_folder[project_code]
+    
+    md_results_file = [fn for fn in md_results_files if project_code in fn]
+    assert len(md_results_file) == 1
+    md_results_file = os.path.join(md_results_folder,md_results_file[0])
+    
+    with open(md_results_file,'r') as f:
+        md_results = json.load(f)
+    assert md_results['detection_categories'][md_human_category] == 'person'
+    assert md_results['detection_categories'][md_vehicle_category] == 'vehicle'
+    
+    # im = md_results['images'][0]
+    for im in tqdm(md_results['images']):
+        
+        if 'detections' not in im:
+            continue
+        
+        # MD results files are each relative to their own projects, we want
+        # filenames to be relative to the base of the drive
+        fn = os.path.join(project_folder,im['file'])
+        for det in im['detections']:
+            if det['category'] == md_human_category and \
+                det['conf'] >= md_human_detection_threshold:
+                    md_human_images.add(fn)
+            if det['category'] == md_vehicle_category and \
+                det['conf'] >= md_vehicle_detection_threshold:
+                    md_vehicle_images.add(fn)
+                    
+        # ...for each detection
+        
+    # ...for each image
+                    
+# ...for each project
+
+print('MD found {} human images, {} vehicle images'.format(
+    len(md_human_images),len(md_vehicle_images)))
+
+md_human_or_vehicle_images = \
+    set(md_human_images).union(set(md_vehicle_images))
+    
+# next(iter(md_human_or_vehicle_images))
+
+
+#%% Find images where the ground truth says humans or vehicles are present
+
+human_species_id = 'human'
+vehicle_species_id = 'humanvehicle'
+
+gt_human_capture_ids = set()
+gt_vehicle_capture_ids = set()
+
+for capture_id in capture_id_to_species:
+    
+    species_this_capture_id = capture_id_to_species[capture_id]
+    
+    for species in species_this_capture_id:
+        if species == human_species_id:
+            gt_human_capture_ids.add(capture_id)
+        elif species == vehicle_species_id:
+            gt_vehicle_capture_ids.add(capture_id)
+    
+# ...for each capture ID
+
+gt_human_images = []
+gt_vehicle_images = []
+
+for capture_id in gt_human_capture_ids:
+    images_this_capture_id = capture_id_to_images[capture_id]
+    gt_human_images.extend(images_this_capture_id)
+for capture_id in gt_vehicle_capture_ids:
+    images_this_capture_id = capture_id_to_images[capture_id]
+    gt_vehicle_images.extend(images_this_capture_id)    
+    
+print('Ground truth includes {} human images ({} captures), {} vehicle images ({} captures)'.format(
+    len(gt_human_images),len(gt_human_capture_ids),
+    len(gt_vehicle_images),len(gt_vehicle_capture_ids)))
+
+ground_truth_human_or_vehicle_images = \
+    set(gt_human_images).union(set(gt_vehicle_images))
+    
+# next(iter(ground_truth_human_or_vehicle_images))
+
+
+#%% Find mismatches
+
+gt_missing_human_images = []
+gt_missing_vehicle_images = []
+
+for fn in md_human_images:
+    if fn not in ground_truth_human_or_vehicle_images:
+        gt_missing_human_images.append(fn)
+
+for fn in md_vehicle_images:
+    if fn not in ground_truth_human_or_vehicle_images:
+        gt_missing_vehicle_images.append(fn)
+        
+print('Of {} images where MD found a human, {} are not in the ground truth'.format(
+    len(md_human_images),len(gt_missing_human_images)))
+
+print('Of {} images where MD found a vehicle, {} are not in the ground truth'.format(
+    len(md_vehicle_images),len(gt_missing_vehicle_images)))
+
+
+#%% Sample mismatches
+
+random.seed(0)
+n_to_sample = 1000
+sampled_human_mismatches = random.sample(gt_missing_human_images,n_to_sample)
+sampled_vehicle_mismatches = random.sample(gt_missing_vehicle_images,n_to_sample)
+
+human_mismatch_output_folder = os.path.join(output_base,'mismatches_human')
+vehicle_mismatch_output_folder = os.path.join(output_base,'mismatches_vehicle')
+os.makedirs(human_mismatch_output_folder,exist_ok=True)
+os.makedirs(vehicle_mismatch_output_folder,exist_ok=True)
+
+def copy_sampled_images(sampled_images,sampled_images_output_folder):
+
+    for fn in tqdm(sampled_images): 
+        if fn not in all_files_relative_set:
+            print('Warning: missing file {}'.format(fn))
+            continue
+        source_image = os.path.join(input_base,fn)
+        target_image = os.path.join(sampled_images_output_folder,os.path.basename(fn))
+        shutil.copyfile(source_image,target_image)
+    
+copy_sampled_images(sampled_human_mismatches,human_mismatch_output_folder)
+copy_sampled_images(sampled_vehicle_mismatches,vehicle_mismatch_output_folder)
+
+
+#%% See what's up with some of the mismatches
+
+filename_base_to_filename = {}
+
+from md_utils.path_utils import is_image_file
+
+# fn = all_files_relative[0]
+for fn in tqdm(all_files_relative):
+
+    if not is_image_file(fn):
+        continue
+    if 'Indiv_Recognition' in fn:
+        continue
+    bn = os.path.basename(fn)
+    assert bn not in filename_base_to_filename
+    filename_base_to_filename[bn] = fn
+
+
+if False:
+
+    bn = 'TSW_S2_KA02_R3_IMAG0002.JPG'
+    fn = filename_base_to_filename[bn]
+    capture_id = image_to_capture_id[fn]
+    species = capture_id_to_species[capture_id]
+    
+    
+#%% Look at the distribution of labels for the mismatched images    
+
+gt_missing_images = set(gt_missing_human_images).union(set(gt_missing_vehicle_images))
+
+missing_image_species_to_count = defaultdict(int)
+
+for fn in gt_missing_images:
+    if fn not in image_to_capture_id:
+        continue
+    capture_id = image_to_capture_id[fn]
+    species = capture_id_to_species[capture_id]
+    for s in species:
+        missing_image_species_to_count[s] += 1
```

### Comparing `megadetector-5.0.8/data_management/importers/snapshot_serengeti_lila.py` & `megadetector-5.0.9/data_management/importers/snapshot_serengeti_lila.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-########
-#
-# snapshot_serengeti_lila.py
-#
-# Create zipfiles of Snapshot Serengeti S1-S11.
-#
-# Create a metadata file for S1-S10, plus separate metadata files
-# for S1-S11.  At the time this code was written, S11 was under embargo.
-#
-# Create zip archives of each season without humans.
-#
-# Create a human zip archive.
-#
-########
+"""
+
+ snapshot_serengeti_lila.py
+
+ Create zipfiles of Snapshot Serengeti S1-S11.
+
+ Create a metadata file for S1-S10, plus separate metadata files
+ for S1-S11.  At the time this code was written, S11 was under embargo.
+
+ Create zip archives of each season without humans.
+
+ Create a human zip archive.
+
+"""
 
 #%% Constants and imports
 
 import pandas as pd
 import json
 import os
 import uuid
```

### Comparing `megadetector-5.0.8/data_management/importers/snapshotserengeti/make_full_SS_json.py` & `megadetector-5.0.9/data_management/importers/snapshotserengeti/make_full_SS_json.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/data_management/importers/snapshotserengeti/make_per_season_SS_json.py` & `megadetector-5.0.9/data_management/importers/snapshotserengeti/make_per_season_SS_json.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/data_management/importers/sulross_get_exif.py` & `megadetector-5.0.9/data_management/importers/sulross_get_exif.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# sulross_get_exif.py
-#
-# For the Sul Ross dataset, species informationw was stored in XMP metadata; pull 
-# all that metadata out to .json.
-#
-########
+"""
+
+ sulross_get_exif.py
+
+ For the Sul Ross dataset, species informationw was stored in XMP metadata; pull 
+ all that metadata out to .json.
+
+"""
 
 import os
 import json
 from tqdm import tqdm
 
 import exiftool
 
@@ -57,10 +57,9 @@
                     json.dump(image_id_to_metadata, f, indent=1)
 
     print('Length of meta data read: ', len(image_id_to_metadata))
     with open('/home/beaver/cameratraps/data/sulross/20190522_metadata.json', 'w') as f:
         json.dump(image_id_to_metadata, f, indent=1)
     print('Results saved. Done!')
 
-
 if __name__ == '__main__':
     get_metadata()
```

### Comparing `megadetector-5.0.8/data_management/importers/timelapse_csv_set_to_json.py` & `megadetector-5.0.9/data_management/importers/timelapse_csv_set_to_json.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-########
-#
-# timelapse_csv_set_to_json.py
-#
-# Given a directory full of reasonably-consistent Timelapse-exported
-# .csvs, assemble a CCT .json.
-#
-# Assumes that you have a list of all files in the directory tree, including 
-# image and .csv files.
-#
-########
+"""
+
+ timelapse_csv_set_to_json.py
+
+ Given a directory full of reasonably-consistent Timelapse-exported
+ .csvs, assemble a CCT .json.
+
+ Assumes that you have a list of all files in the directory tree, including 
+ image and .csv files.
+
+"""
 
 #%% Constants and imports
 
 import uuid
 import json
 import time
 import re
```

### Comparing `megadetector-5.0.8/data_management/importers/ubc_to_json.py` & `megadetector-5.0.9/data_management/importers/ubc_to_json.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-########
-#
-# ubc_to_json.py
-#
-# Convert the .csv file provided for the UBC data set to a 
-# COCO-camera-traps .json file
-#
-# Images were provided in eight folders, each of which contained a .csv
-# file with annotations.  Those annotations came in two slightly different 
-# formats, the two formats corresponding to folders starting with "SC_" and 
-# otherwise.
-#
-########
+"""
+
+ ubc_to_json.py
+
+ Convert the .csv file provided for the UBC data set to a 
+ COCO-camera-traps .json file
+
+ Images were provided in eight folders, each of which contained a .csv
+ file with annotations.  Those annotations came in two slightly different 
+ formats, the two formats corresponding to folders starting with "SC_" and 
+ otherwise.
+
+"""
 
 #%% Constants and environment
 
 import pandas as pd
 import os
 import json
 import uuid
```

### Comparing `megadetector-5.0.8/data_management/importers/umn_to_json.py` & `megadetector-5.0.9/data_management/importers/umn_to_json.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-########
-#
-# umn_to_json.py
-#
-# Prepare images and metadata for the Orinoqua Camera Traps dataset.
-#
-########
+"""
+
+ umn_to_json.py
+
+ Prepare images and metadata for the Orinoqua Camera Traps dataset.
+
+"""
 
 #%% Imports and constants
 
 import os
 import json
 import pandas as pd
 import shutil
```

### Comparing `megadetector-5.0.8/data_management/importers/wellington_to_json.py` & `megadetector-5.0.9/data_management/importers/wellington_to_json.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# wellington_to_json.py
-#
-# Convert the .csv file provided for the Wellington data set to a 
-# COCO-camera-traps .json file
-#
-########
+"""
+
+ wellington_to_json.py
+
+ Convert the .csv file provided for the Wellington data set to a 
+ COCO-camera-traps .json file
+
+"""
 
 #%% Constants and environment
 
 import pandas as pd
 import os
 import glob
 import json
```

### Comparing `megadetector-5.0.8/data_management/importers/wi_to_json.py` & `megadetector-5.0.9/data_management/importers/wi_to_json.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-########
-#
-# wi_to_json
-#
-# Prepares CCT-formatted metadata based on a Wildlife Insights data export.
-# 
-# Mostly assumes you have the images also, for validation/QA.
-#
-########
+"""
+
+ wi_to_json
+
+ Prepares CCT-formatted metadata based on a Wildlife Insights data export.
+ 
+ Mostly assumes you have the images also, for validation/QA.
+
+"""
 
 #%% Imports and constants
 
 import os
 import json
 import pandas as pd
 import shutil
```

### Comparing `megadetector-5.0.8/data_management/importers/zamba_results_to_md_results.py` & `megadetector-5.0.9/data_management/importers/zamba_results_to_md_results.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,181 +1,181 @@
-########
-#
-# zamba_results_to_md_results.py
-#
-# Convert a labels.csv file produced by Zamba Cloud to a MD results file suitable
-# for import into Timelapse.
-#
-# Columns are expected to be:
-#
-# video_uuid (not used)
-# original_filename (assumed to be a relative path name)
-# top_k_label,top_k_probability, for k = 1..N
-# [category name 1],[category name 2],...
-# corrected_label
-#
-# Because the MD results file fundamentally stores detections, what we'll
-# actually do is created bogus detections that fill the entire image.  Detection
-# coordinates are not currently used in Timelapse video video anyway.
-#
-# There is no special handling of empty/blank categories; because these results are
-# based on a classifier, rather than a detector (where "blank" would be the absence of
-# all other categories), "blank" can be queried in Timelapse just like any other class.
-#
-########
-
-#%% Imports and constants
-
-import pandas as pd
-import json
-
-
-#%% Main function
-
-def zamba_results_to_md_results(input_file,output_file=None):
-    """
-    Converts the .csv file [input_file] to the MD-formatted .json file [output_file].
-    
-    If [output_file] is None, '.json' will be appended to the input file.
-    """
-    
-    if output_file is None:
-        output_file = input_file + '.json'
-
-    df = pd.read_csv(input_file)
-    
-    expected_columns = ('video_uuid','corrected_label','original_filename')
-    for s in expected_columns:
-        assert s in df.columns,\
-            'Expected column {} not found, are you sure this is a Zamba results .csv file?'.format(
-                s)
-            
-    # How many results are included per file?
-    assert 'top_1_probability' in df.columns and 'top_1_label' in df.columns
-    top_k = 2
-    while(True):
-        p_string = 'top_' + str(top_k) + '_probability'
-        label_string = 'top_' + str(top_k) + '_label'
-        
-        if p_string in df.columns:
-            assert label_string in df.columns,\
-                'Oops, {} is a column but {} is not'.format(
-                    p_string,label_string)
-            top_k += 1
-            continue
-        else:
-            assert label_string not in df.columns,\
-                'Oops, {} is a column but {} is not'.format(
-                    label_string,p_string)
-            top_k -= 1
-            break
-            
-    print('Found {} probability column pairs'.format(top_k))
-    
-    # Category names start after the fixed columns and the probability columns
-    category_names = []
-    column_names = list(df.columns)
-    first_category_name_index = 0
-    while('top_' in column_names[first_category_name_index] or \
-          column_names[first_category_name_index] in expected_columns):
-        first_category_name_index += 1
-        
-    i_column = first_category_name_index
-    while( (i_column < len(column_names)) and (column_names[i_column] != 'corrected_label') ):
-        category_names.append(column_names[i_column])
-        i_column += 1
-        
-    print('Found {} categories:\n'.format(len(category_names)))
-    
-    for s in category_names:
-        print(s)
-    
-    info = {}
-    info['format_version'] = '1.3'
-    info['detector'] = 'Zamba Cloud'
-    info['classifier'] = 'Zamba Cloud'    
-    
-    detection_category_id_to_name = {}
-    for category_id,category_name in enumerate(category_names):
-        detection_category_id_to_name[str(category_id)] = category_name
-    detection_category_name_to_id = {v: k for k, v in detection_category_id_to_name.items()}
-    
-    images = []
-    
-    # i_row = 0; row = df.iloc[i_row]
-    for i_row,row in df.iterrows():
-        
-        im = {}
-        images.append(im)
-        im['file'] = row['original_filename']
-        
-        detections = []
-        
-        # k = 1
-        for k in range(1,top_k+1):
-            label = row['top_{}_label'.format(k)]
-            confidence = row['top_{}_probability'.format(k)]
-            det = {}
-            det['category'] = detection_category_name_to_id[label]
-            det['conf'] = confidence
-            det['bbox'] = [0,0,1.0,1.0]
-            detections.append(det)
-            
-        im['detections'] = detections
-        
-    # ...for each row
-    
-    results = {}
-    results['info'] = info
-    results['detection_categories'] = detection_category_id_to_name
-    results['images'] = images
-    
-    with open(output_file,'w') as f:
-        json.dump(results,f,indent=1)
-        
-# ...zamba_results_to_md_results(...)
-        
-
-#%% Interactive driver
-
-if False:
-
-    pass
-
-    #%%
-    
-    input_file = r"G:\temp\labels-job-b95a4b76-e332-4e17-ab40-03469392d36a-2023-11-04_16-28-50.060130.csv"
-    output_file = None
-    zamba_results_to_md_results(input_file,output_file)
-    
-
-#%% Command-line driver
-
-import sys,argparse
-
-def main():
-
-    parser = argparse.ArgumentParser(
-        description='Convert a Zamba-formatted .csv results file to a MD-formatted .json results file')
-    
-    parser.add_argument(
-        'input_file',
-        type=str,
-        help='input .csv file')
-    
-    parser.add_argument(
-        '--output_file',
-        type=str,
-        default=None,
-        help='output .json file (defaults to input file appended with ".json")')
-    
-    if len(sys.argv[1:]) == 0:
-        parser.print_help()
-        parser.exit()
-
-    args = parser.parse_args()
-
-    zamba_results_to_md_results(args.input_file,args.output_file)
-    
-if __name__ == '__main__':
-    main()
-    
+"""
+
+ zamba_results_to_md_results.py
+
+ Convert a labels.csv file produced by Zamba Cloud to a MD results file suitable
+ for import into Timelapse.
+
+ Columns are expected to be:
+
+ video_uuid (not used)
+ original_filename (assumed to be a relative path name)
+ top_k_label,top_k_probability, for k = 1..N
+ [category name 1],[category name 2],...
+ corrected_label
+
+ Because the MD results file fundamentally stores detections, what we'll
+ actually do is created bogus detections that fill the entire image.  Detection
+ coordinates are not currently used in Timelapse video video anyway.
+
+ There is no special handling of empty/blank categories; because these results are
+ based on a classifier, rather than a detector (where "blank" would be the absence of
+ all other categories), "blank" can be queried in Timelapse just like any other class.
+
+"""
+
+#%% Imports and constants
+
+import pandas as pd
+import json
+
+
+#%% Main function
+
+def zamba_results_to_md_results(input_file,output_file=None):
+    """
+    Converts the .csv file [input_file] to the MD-formatted .json file [output_file].
+    
+    If [output_file] is None, '.json' will be appended to the input file.
+    """
+    
+    if output_file is None:
+        output_file = input_file + '.json'
+
+    df = pd.read_csv(input_file)
+    
+    expected_columns = ('video_uuid','corrected_label','original_filename')
+    for s in expected_columns:
+        assert s in df.columns,\
+            'Expected column {} not found, are you sure this is a Zamba results .csv file?'.format(
+                s)
+            
+    # How many results are included per file?
+    assert 'top_1_probability' in df.columns and 'top_1_label' in df.columns
+    top_k = 2
+    while(True):
+        p_string = 'top_' + str(top_k) + '_probability'
+        label_string = 'top_' + str(top_k) + '_label'
+        
+        if p_string in df.columns:
+            assert label_string in df.columns,\
+                'Oops, {} is a column but {} is not'.format(
+                    p_string,label_string)
+            top_k += 1
+            continue
+        else:
+            assert label_string not in df.columns,\
+                'Oops, {} is a column but {} is not'.format(
+                    label_string,p_string)
+            top_k -= 1
+            break
+            
+    print('Found {} probability column pairs'.format(top_k))
+    
+    # Category names start after the fixed columns and the probability columns
+    category_names = []
+    column_names = list(df.columns)
+    first_category_name_index = 0
+    while('top_' in column_names[first_category_name_index] or \
+          column_names[first_category_name_index] in expected_columns):
+        first_category_name_index += 1
+        
+    i_column = first_category_name_index
+    while( (i_column < len(column_names)) and (column_names[i_column] != 'corrected_label') ):
+        category_names.append(column_names[i_column])
+        i_column += 1
+        
+    print('Found {} categories:\n'.format(len(category_names)))
+    
+    for s in category_names:
+        print(s)
+    
+    info = {}
+    info['format_version'] = '1.3'
+    info['detector'] = 'Zamba Cloud'
+    info['classifier'] = 'Zamba Cloud'    
+    
+    detection_category_id_to_name = {}
+    for category_id,category_name in enumerate(category_names):
+        detection_category_id_to_name[str(category_id)] = category_name
+    detection_category_name_to_id = {v: k for k, v in detection_category_id_to_name.items()}
+    
+    images = []
+    
+    # i_row = 0; row = df.iloc[i_row]
+    for i_row,row in df.iterrows():
+        
+        im = {}
+        images.append(im)
+        im['file'] = row['original_filename']
+        
+        detections = []
+        
+        # k = 1
+        for k in range(1,top_k+1):
+            label = row['top_{}_label'.format(k)]
+            confidence = row['top_{}_probability'.format(k)]
+            det = {}
+            det['category'] = detection_category_name_to_id[label]
+            det['conf'] = confidence
+            det['bbox'] = [0,0,1.0,1.0]
+            detections.append(det)
+            
+        im['detections'] = detections
+        
+    # ...for each row
+    
+    results = {}
+    results['info'] = info
+    results['detection_categories'] = detection_category_id_to_name
+    results['images'] = images
+    
+    with open(output_file,'w') as f:
+        json.dump(results,f,indent=1)
+        
+# ...zamba_results_to_md_results(...)
+        
+
+#%% Interactive driver
+
+if False:
+
+    pass
+
+    #%%
+    
+    input_file = r"G:\temp\labels-job-b95a4b76-e332-4e17-ab40-03469392d36a-2023-11-04_16-28-50.060130.csv"
+    output_file = None
+    zamba_results_to_md_results(input_file,output_file)
+    
+
+#%% Command-line driver
+
+import sys,argparse
+
+def main():
+
+    parser = argparse.ArgumentParser(
+        description='Convert a Zamba-formatted .csv results file to a MD-formatted .json results file')
+    
+    parser.add_argument(
+        'input_file',
+        type=str,
+        help='input .csv file')
+    
+    parser.add_argument(
+        '--output_file',
+        type=str,
+        default=None,
+        help='output .json file (defaults to input file appended with ".json")')
+    
+    if len(sys.argv[1:]) == 0:
+        parser.print_help()
+        parser.exit()
+
+    args = parser.parse_args()
+
+    zamba_results_to_md_results(args.input_file,args.output_file)
+    
+if __name__ == '__main__':
+    main()
+
```

### Comparing `megadetector-5.0.8/data_management/labelme_to_coco.py` & `megadetector-5.0.9/data_management/labelme_to_coco.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-########
-#
-# labelme_to_coco.py
-#
-# Converts a folder of labelme-formatted .json files to COCO format.
-#
-########
+"""
+
+labelme_to_coco.py
+
+Converts a folder of labelme-formatted .json files to COCO.
+
+"""
 
 #%% Constants and imports
 
 import json
 import os
 import uuid
 
@@ -19,18 +19,18 @@
 from functools import partial
 
 from tqdm import tqdm
 
 
 #%% Support functions
 
-def add_category(category_name,category_name_to_id,candidate_category_id=0):
+def _add_category(category_name,category_name_to_id,candidate_category_id=0):
     """
-    Add the category [category_name] to the dict [category_name_to_id], by default
-    using the next available integer index.
+    Adds the category [category_name] to the dict [category_name_to_id], by default
+    using the next available integer index.    
     """
     
     if category_name in category_name_to_id:
         return category_name_to_id[category_name]
     while candidate_category_id in category_name_to_id.values():
         candidate_category_id += 1
     category_name_to_id[category_name] = candidate_category_id
@@ -117,15 +117,15 @@
             im['flags'] = labelme_data['flags']
 
     annotations_this_image = []
     
     if len(shapes) == 0:
         
         if allow_new_categories:
-            category_id = add_category('empty',category_name_to_id)
+            category_id = _add_category('empty',category_name_to_id)
         else:
             assert 'empty' in category_name_to_id
             category_id = category_name_to_id['empty']
             
         ann = {}
         ann['id'] = str(uuid.uuid1())
         ann['image_id'] = im['id']
@@ -144,15 +144,15 @@
             
             if use_folders_as_labels:
                 category_name = os.path.basename(os.path.dirname(image_fn_abs))
             else:
                 category_name = shape['label']                
             
             if allow_new_categories:
-                category_id = add_category(category_name,category_name_to_id)
+                category_id = _add_category(category_name,category_name_to_id)
             else:
                 assert category_name in category_name_to_id
                 category_id = category_name_to_id[category_name]
             
             points = shape['points']
             if len(points) != 2:
                 print('Warning: illegal rectangle with {} points for {}'.format(
@@ -198,15 +198,15 @@
                     use_folders_as_labels=False,
                     recursive=True,
                     no_json_handling='skip',
                     validate_image_sizes=True,
                     max_workers=1, 
                     use_threads=True):
     """
-    Find all images in [input_folder] that have corresponding .json files, and convert
+    Finds all images in [input_folder] that have corresponding .json files, and converts
     to a COCO .json file.
     
     Currently only supports bounding box annotations and image-level flags (i.e., does not
     support point or general polygon annotations).
     
     Labelme's image-level flags don't quite fit the COCO annotations format, so they are attached
     to image objects, rather than annotation objects.
@@ -220,19 +220,46 @@
         
     images/train/lion/image0001.json
     
     ...all boxes in image0001.json will be given the label "lion", regardless of the labels in the 
     file.  Empty images in the "lion" folder will still be given the label "empty" (or 
     [empty_category_name]).
     
-    no_json_handling can be:
+    Args:
+        input_folder (str): input folder to search for images and Labelme .json files
+        output_file (str, optional): output file to which we should write COCO-formatted data; if None
+            this function just returns the COCO-formatted dict
+        category_id_to_category_name (dict, optional): dict mapping category IDs to category names;
+            really used to map Labelme category names to COCO category IDs.  IDs will be auto-generated
+            if this is None.
+        empty_category_id (int, optional): category ID to use for the not-very-COCO-like "empty" category;
+            also see the no_json_handling parameter.
+        info_struct (dict, optional): dict to stash in the "info" field of the resulting COCO dict
+        relative_paths_to_include (list, optional): allowlist of relative paths to include in the COCO
+            dict; there's no reason to specify this along with relative_paths_to_exclude.
+        relative_paths_to_exclude (list, optional): blocklist of relative paths to exclude from the COCO
+            dict; there's no reason to specify this along with relative_paths_to_include.
+        use_folders_as_labels (bool, optional): if this is True, class names will be pulled from folder names,
+            useful if you have images like a/b/cat/image001.jpg, a/b/dog/image002.jpg, etc.
+        recursive (bool, optional): whether to recurse into [input_folder]
+        no_json_handling (str, optional): how to deal with image files that have no corresponding .json files, 
+            can be:
+                
+                - 'skip': ignore image files with no corresponding .json files
+                - 'empty': treat image files with no corresponding .json files as empty
+                - 'error': throw an error when an image file has no corresponding .json file
+        validate_image_sizes (bool, optional): whether to load images to verify that the sizes specified
+            in the labelme files are correct
+        max_workers (int, optional): number of workers to use for parallelization, set to <=1 to disable
+            parallelization
+        use_threads (bool, optional): whether to use threads (True) or processes (False) for parallelization,
+            not relevant if max_workers <= 1
         
-    * 'skip': ignore image files with no corresponding .json files
-    * 'empty': treat image files with no corresponding .json files as empty
-    * 'error': throw an error when an image file has no corresponding .json file    
+    Returns:
+        dict: a COCO-formatted dictionary, identical to what's written to [output_file] if [output_file] is not None.
     """
     
     if max_workers > 1:
         assert category_id_to_category_name is not None, \
             'When parallelizing labelme --> COCO conversion, you must supply a category mapping'
             
     if category_id_to_category_name is None:
@@ -284,15 +311,15 @@
     if empty_category_id is not None:
         try:
             empty_category_id = int(empty_category_id)
         except ValueError:
             raise ValueError('Category IDs must be ints or string-formatted ints')
         
     if empty_category_id is None:
-        empty_category_id = add_category(empty_category_name,category_name_to_id)
+        empty_category_id = _add_category(empty_category_name,category_name_to_id)
             
     if max_workers <= 1:
         
         image_results = []
         for image_fn_relative in tqdm(image_filenames_relative):
             
             result = _process_labelme_file(image_fn_relative,input_folder,use_folders_as_labels,
@@ -362,20 +389,34 @@
 
 
 def find_empty_labelme_files(input_folder,recursive=True):
     """
     Returns a list of all image files in in [input_folder] associated with .json files that have 
     no boxes in them.  Also returns a list of images with no associated .json files.  Specifically,
     returns a dict:
-        
-    {
-       'images_with_empty_json_files':[list],
-       'images_with_no_json_files':[list],
-       'images_with_non_empty_json_files':[list]
-    }    
+    
+    .. code-block: none
+    
+        {
+            'images_with_empty_json_files':[list],
+            'images_with_no_json_files':[list],
+            'images_with_non_empty_json_files':[list]
+        }
+    
+    Args:
+        input_folder (str): the folder to search for empty (i.e., box-less) Labelme .json files
+        recursive (bool, optional): whether to recurse into [input_folder]
+    
+    Returns:
+        dict: a dict with fields:
+            - images_with_empty_json_files: a list of all image files in [input_folder] associated with 
+              .json files that have no boxes in them
+            - images_with_no_json_files: a list of images in [input_folder] with no associated .json files
+            - images_with_non_empty_json_files: a list of images in [input_folder] associated with .json
+              files that have at least one box        
     """
     image_filenames_relative = path_utils.find_images(input_folder,recursive=True,
                                                       return_relative_paths=True)
     
     images_with_empty_json_files = []
     images_with_no_json_files = []
     images_with_non_empty_json_files = []
```

### Comparing `megadetector-5.0.8/data_management/labelme_to_yolo.py` & `megadetector-5.0.9/data_management/labelme_to_yolo.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-########
-#
-# labelme_to_yolo.py
-#
-# Create YOLO .txt files in a folder containing labelme .json files.
-#
-########
+"""
+
+labelme_to_yolo.py
+
+Create YOLO .txt files in a folder containing labelme .json files.
+
+"""
 
 #%% Imports
 
 import os
 import json
 
 from multiprocessing.pool import Pool, ThreadPool
@@ -73,15 +73,15 @@
             'Category {} not in category mapping'.format(shape['label'])
         assert len(shape['points']) == 2, 'Illegal rectangle'
         category_id = category_name_to_category_id[shape['label']]
         
         p0 = shape['points'][0]
         p1 = shape['points'][1]
         
-        # LabelMe: [[x0,y0],[x1,y1]] (arbitrarily sorted) (absolute coordinates)
+        # Labelme: [[x0,y0],[x1,y1]] (arbitrarily sorted) (absolute coordinates)
         #
         # YOLO: [class, x_center, y_center, width, height] (normalized coordinates)
         minx_abs = min(p0[0],p1[0])
         maxx_abs = max(p0[0],p1[0])
         miny_abs = min(p0[1],p1[1])
         maxy_abs = max(p0[1],p1[1])
```

### Comparing `megadetector-5.0.8/data_management/lila/add_locations_to_island_camera_traps.py` & `megadetector-5.0.9/data_management/lila/add_locations_to_island_camera_traps.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-########
-#
-# add_locations_to_island_camera_traps.py
-#
-# The Island Conservation Camera Traps dataset had unique camera identifiers embedded
-# in filenames, but not in the proper metadata fields.  This script copies that information
-# to metadata.
-#
-########
+"""
+
+add_locations_to_island_camera_traps.py
+
+The Island Conservation Camera Traps dataset had unique camera identifiers embedded
+in filenames, but not in the proper metadata fields.  This script copies that information
+to metadata.
+
+"""
 
 #%% Imports and constants
 
 import os
 import json
 from tqdm import tqdm
```

### Comparing `megadetector-5.0.8/data_management/lila/create_lila_blank_set.py` & `megadetector-5.0.9/data_management/lila/create_lila_blank_set.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-########
-#
-# create_lila_blank_set.py
-#
-# Create a folder of blank images sampled from LILA.  We'll aim for diversity, so less-common
-# locations will be oversampled relative to more common locations.  We'll also run MegaDetector
-# (with manual review) to remove some incorrectly-labeled, not-actually-empty images from our 
-# blank set.
-#
-# We'll store location information for each image in a .json file, so we can split locations
-# into train/val in downstream tasks.
-#
-########
+"""
+
+create_lila_blank_set.py
+
+Create a folder of blank images sampled from LILA.  We'll aim for diversity, so less-common
+locations will be oversampled relative to more common locations.  We'll also run MegaDetector
+(with manual review) to remove some incorrectly-labeled, not-actually-empty images from our 
+blank set.
+
+We'll store location information for each image in a .json file, so we can split locations
+into train/val in downstream tasks.
+
+"""
 
 #%% Constants and imports
 
 import os
 import random
 import math
 import json
```

### Comparing `megadetector-5.0.8/data_management/lila/create_lila_test_set.py` & `megadetector-5.0.9/data_management/lila/create_lila_test_set.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# create_lila_test_set.py
-#
-# Create a test set of camera trap images, containing N empty and N non-empty 
-# images from each LILA data set.
-#
-########
+"""
+
+create_lila_test_set.py
+
+Create a test set of camera trap images, containing N empty and N non-empty 
+images from each LILA data set.
+
+"""
 
 #%% Constants and imports
 
 import json
 import os
 import random
```

### Comparing `megadetector-5.0.8/data_management/lila/create_links_to_md_results_files.py` & `megadetector-5.0.9/data_management/lila/create_links_to_md_results_files.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,106 +1,106 @@
-########
-#
-# create_links_to_md_results_files.py
-#
-# One-off script to populate the columns in the camera trap data .csv file that point to MD results.
-#
-########
-
-#%% Imports and constants
-
-import os
-
-import pandas as pd
-
-input_csv_file = r'g:\temp\lila_camera_trap_datasets_no_md_results.csv'
-output_csv_file = r'g:\temp\lila_camera_trap_datasets.csv'
-
-md_results_local_folder = r'g:\temp\lila-md-results'
-md_base_url = 'https://lila.science/public/lila-md-results/'
-assert md_base_url.endswith('/')
-
-# No RDE files for datasets with no location information        
-datasets_without_location_info = ('ena24','missouri-camera-traps')
-
-md_results_column_names = ['mdv4_results_raw','mdv5a_results_raw','mdv5b_results_raw','md_results_with_rde']
-
-validate_urls = False
-
-
-#%% Read input data
-
-df = pd.read_csv(input_csv_file)
-for s in md_results_column_names:
-    df[s] = ''
-    
-    
-#%% Find matching files locally, and create URLs
-
-local_files = os.listdir(md_results_local_folder)
-local_files = [fn for fn in local_files if fn.endswith('.zip')]
-
-# i_row = 0; row = df.iloc[i_row]
-for i_row,row in df.iterrows():
-    
-    if not isinstance(row['name'],str):
-        continue
-    
-    dataset_shortname = row['short_name']
-    matching_files = [fn for fn in local_files if dataset_shortname in fn]
-    
-    # No RDE files for datasets with no location information        
-    if dataset_shortname in datasets_without_location_info:
-        assert len(matching_files) == 2
-        mdv5a_files = [fn for fn in matching_files if 'mdv5a' in fn]
-        mdv5b_files = [fn for fn in matching_files if 'mdv5b' in fn]
-        assert len(mdv5a_files) == 1 and len(mdv5b_files) == 1
-        df.loc[i_row,'mdv5a_results_raw'] = md_base_url + mdv5a_files[0]
-        df.loc[i_row,'mdv5b_results_raw'] = md_base_url + mdv5b_files[0]
-    else:
-        # Exclude single-season files for snapshot-serengeti    
-        if dataset_shortname == 'snapshot-serengeti':
-            matching_files = [fn for fn in matching_files if '_S' not in fn]
-            assert len(matching_files) == 2        
-            assert all(['mdv4' in fn for fn in matching_files])
-            rde_files = [fn for fn in matching_files if 'rde' in fn]
-            raw_files = [fn for fn in matching_files if 'rde' not in fn]
-            assert len(rde_files) == 1 and len(raw_files) == 1
-            df.loc[i_row,'mdv4_results_raw'] = md_base_url + raw_files[0]
-            df.loc[i_row,'md_results_with_rde'] = md_base_url + rde_files[0]
-        else:
-            assert len(matching_files) == 3
-            mdv5a_files = [fn for fn in matching_files if 'mdv5a' in fn and 'rde' not in fn]
-            mdv5b_files = [fn for fn in matching_files if 'mdv5b' in fn and 'rde' not in fn]
-            rde_files = [fn for fn in matching_files if 'rde' in fn]
-            assert len(mdv5a_files) == 1 and len(mdv5b_files) == 1 and len(rde_files) == 1
-            df.loc[i_row,'mdv5a_results_raw'] = md_base_url + mdv5a_files[0]
-            df.loc[i_row,'mdv5b_results_raw'] = md_base_url + mdv5b_files[0]
-            df.loc[i_row,'md_results_with_rde'] = md_base_url + rde_files[0]
-            
-    print('Found {} matching files for {}'.format(len(matching_files),dataset_shortname))
-
-# ...for each row    
-
-
-#%% Validate URLs
-
-if validate_urls:
-    
-    from md_utils.url_utils import test_urls
-    
-    urls = set()
-    
-    for i_row,row in df.iterrows():
-        for column_name in md_results_column_names:
-            if len(row[column_name]) > 0:
-                assert row[column_name] not in urls        
-                urls.add(row[column_name])
-                
-    test_urls(urls,error_on_failure=True)            
-    
-    print('Validated {} URLs'.format(len(urls)))
-
-
-#%% Write new .csv file
-
-df.to_csv(output_csv_file,header=True,index=False)
+"""
+
+create_links_to_md_results_files.py
+
+One-off script to populate the columns in the camera trap data .csv file that point to MD results.
+
+"""
+
+#%% Imports and constants
+
+import os
+
+import pandas as pd
+
+input_csv_file = r'g:\temp\lila_camera_trap_datasets_no_md_results.csv'
+output_csv_file = r'g:\temp\lila_camera_trap_datasets.csv'
+
+md_results_local_folder = r'g:\temp\lila-md-results'
+md_base_url = 'https://lila.science/public/lila-md-results/'
+assert md_base_url.endswith('/')
+
+# No RDE files for datasets with no location information        
+datasets_without_location_info = ('ena24','missouri-camera-traps')
+
+md_results_column_names = ['mdv4_results_raw','mdv5a_results_raw','mdv5b_results_raw','md_results_with_rde']
+
+validate_urls = False
+
+
+#%% Read input data
+
+df = pd.read_csv(input_csv_file)
+for s in md_results_column_names:
+    df[s] = ''
+    
+    
+#%% Find matching files locally, and create URLs
+
+local_files = os.listdir(md_results_local_folder)
+local_files = [fn for fn in local_files if fn.endswith('.zip')]
+
+# i_row = 0; row = df.iloc[i_row]
+for i_row,row in df.iterrows():
+    
+    if not isinstance(row['name'],str):
+        continue
+    
+    dataset_shortname = row['short_name']
+    matching_files = [fn for fn in local_files if dataset_shortname in fn]
+    
+    # No RDE files for datasets with no location information        
+    if dataset_shortname in datasets_without_location_info:
+        assert len(matching_files) == 2
+        mdv5a_files = [fn for fn in matching_files if 'mdv5a' in fn]
+        mdv5b_files = [fn for fn in matching_files if 'mdv5b' in fn]
+        assert len(mdv5a_files) == 1 and len(mdv5b_files) == 1
+        df.loc[i_row,'mdv5a_results_raw'] = md_base_url + mdv5a_files[0]
+        df.loc[i_row,'mdv5b_results_raw'] = md_base_url + mdv5b_files[0]
+    else:
+        # Exclude single-season files for snapshot-serengeti    
+        if dataset_shortname == 'snapshot-serengeti':
+            matching_files = [fn for fn in matching_files if '_S' not in fn]
+            assert len(matching_files) == 2        
+            assert all(['mdv4' in fn for fn in matching_files])
+            rde_files = [fn for fn in matching_files if 'rde' in fn]
+            raw_files = [fn for fn in matching_files if 'rde' not in fn]
+            assert len(rde_files) == 1 and len(raw_files) == 1
+            df.loc[i_row,'mdv4_results_raw'] = md_base_url + raw_files[0]
+            df.loc[i_row,'md_results_with_rde'] = md_base_url + rde_files[0]
+        else:
+            assert len(matching_files) == 3
+            mdv5a_files = [fn for fn in matching_files if 'mdv5a' in fn and 'rde' not in fn]
+            mdv5b_files = [fn for fn in matching_files if 'mdv5b' in fn and 'rde' not in fn]
+            rde_files = [fn for fn in matching_files if 'rde' in fn]
+            assert len(mdv5a_files) == 1 and len(mdv5b_files) == 1 and len(rde_files) == 1
+            df.loc[i_row,'mdv5a_results_raw'] = md_base_url + mdv5a_files[0]
+            df.loc[i_row,'mdv5b_results_raw'] = md_base_url + mdv5b_files[0]
+            df.loc[i_row,'md_results_with_rde'] = md_base_url + rde_files[0]
+            
+    print('Found {} matching files for {}'.format(len(matching_files),dataset_shortname))
+
+# ...for each row    
+
+
+#%% Validate URLs
+
+if validate_urls:
+    
+    from md_utils.url_utils import test_urls
+    
+    urls = set()
+    
+    for i_row,row in df.iterrows():
+        for column_name in md_results_column_names:
+            if len(row[column_name]) > 0:
+                assert row[column_name] not in urls        
+                urls.add(row[column_name])
+                
+    test_urls(urls,error_on_failure=True)            
+    
+    print('Validated {} URLs'.format(len(urls)))
+
+
+#%% Write new .csv file
+
+df.to_csv(output_csv_file,header=True,index=False)
```

### Comparing `megadetector-5.0.8/data_management/lila/generate_lila_per_image_labels.py` & `megadetector-5.0.9/data_management/lila/generate_lila_per_image_labels.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,46 +1,43 @@
-########
-#
-# generate_lila_per_image_labels.py
-# 
-# Generate a .csv file with one row per annotation, containing full URLs to every
-# camera trap image on LILA, with taxonomically expanded labels.
-#
-# Typically there will be one row per image, though images with multiple annotations
-# will have multiple rows.
-#
-# Some images may not physically exist, particularly images that are labeled as "human".
-# This script does not validate image URLs.
-#
-# Does not include bounding box annotations.
-#
-########
+"""
+
+generate_lila_per_image_labels.py
+
+Generate a .csv file with one row per annotation, containing full URLs to every
+camera trap image on LILA, with taxonomically expanded labels.
+
+Typically there will be one row per image, though images with multiple annotations
+will have multiple rows.
+
+Some images may not physically exist, particularly images that are labeled as "human".
+This script does not validate image URLs.
+
+Does not include bounding box annotations.
+
+"""
 
 #%% Constants and imports
 
 import os
 import json
 import pandas as pd
 import numpy as np
 import dateparser
 import csv
-import urllib
-import urllib.request
 
 from collections import defaultdict
 from tqdm import tqdm
 
 from data_management.lila.lila_common import read_lila_metadata, \
     read_metadata_file_for_dataset, \
     read_lila_taxonomy_mapping
 
 from md_utils import write_html_image_list
 from md_utils.path_utils import zip_file
 from md_utils.path_utils import open_file
-from md_utils.url_utils import download_url
 
 # We'll write images, metadata downloads, and temporary files here
 lila_local_base = os.path.expanduser('~/lila')
 preview_folder = os.path.join(lila_local_base,'csv_preview')
 
 os.makedirs(lila_local_base,exist_ok=True)
 
@@ -103,20 +100,23 @@
     ds_label_to_taxonomy[ds_label] = row.to_dict()
     
 
 #%% Process annotations for each dataset
 
 # Takes several hours
 
-header = ['dataset_name','url','image_id','sequence_id','location_id','frame_num','original_label',\
-          'scientific_name','common_name','datetime','annotation_level']
+# The order of these headers needs to match the order in which fields are added later in this cell;
+# don't mess with this order.
+header = ['dataset_name','url_gcp','url_aws','url_azure',
+          'image_id','sequence_id','location_id','frame_num',
+          'original_label','scientific_name','common_name','datetime','annotation_level']
 
 taxonomy_levels_to_include = \
     ['kingdom','phylum','subphylum','superclass','class','subclass','infraclass','superorder','order',
-     'suborder','infraorder','superfamily','family','subfamily','tribe','genus','species','subspecies',\
+     'suborder','infraorder','superfamily','family','subfamily','tribe','genus','species','subspecies',
      'variety']
     
 header.extend(taxonomy_levels_to_include)
 
 missing_annotations = set()
 
 def clearnan(v):
@@ -175,18 +175,25 @@
         for i_image,im in enumerate(images):
             
             if (debug_max_images_per_dataset is not None) and (debug_max_images_per_dataset > 0) \
                 and (i_image >= debug_max_images_per_dataset):
                 break
             
             file_name = im['file_name'].replace('\\','/')
-            base_url = metadata_table[ds_name]['image_base_url']
-            assert not base_url.endswith('/')
-            url = base_url + '/' + file_name
-            
+            base_url_gcp = metadata_table[ds_name]['image_base_url_gcp']
+            base_url_aws = metadata_table[ds_name]['image_base_url_aws']
+            base_url_azure = metadata_table[ds_name]['image_base_url_azure']
+            assert not base_url_gcp.endswith('/')
+            assert not base_url_aws.endswith('/')
+            assert not base_url_azure.endswith('/')
+            
+            url_gcp = base_url_gcp + '/' + file_name
+            url_aws = base_url_aws + '/' + file_name
+            url_azure = base_url_azure + '/' + file_name
+                        
             for k in im.keys():
                 if ('date' in k or 'time' in k) and (k not in ['datetime','date_captured']):
                     raise ValueError('Unrecognized datetime field')
                     
             # This field name was only used for Caltech Camera Traps
             if 'date_captured' in im:
                 assert ds_name == 'Caltech Camera Traps'
@@ -293,15 +300,17 @@
                     ['dataset_name','url','image_id','sequence_id','location_id',
                      'frame_num','original_label','scientific_name','common_name',
                      'datetime','annotation_level']
                 """
                 
                 row = []
                 row.append(ds_name)
-                row.append(url)
+                row.append(url_gcp)
+                row.append(url_aws)
+                row.append(url_azure)
                 row.append(image_id)
                 row.append(sequence_id)
                 row.append(location_id)
                 row.append(frame_num)
                 row.append(taxonomy_labels['query'])
                 row.append(clearnan(taxonomy_labels['scientific_name']))
                 row.append(clearnan(taxonomy_labels['common_name']))
@@ -361,15 +370,16 @@
 # Collect a list of locations within each dataset; we'll use this
 # in the next cell to look for datasets that only have a single location
 dataset_name_to_locations = defaultdict(set)
 
 def check_row(row):
     
     assert row['dataset_name'] in metadata_table.keys()
-    assert row['url'].startswith('https://')
+    for url_column in ['url_gcp','url_aws','url_azure']:
+        assert row[url_column].startswith('https://') or row[url_column].startswith('http://')
     assert ' : ' in row['image_id']
     assert 'seq' not in row['location_id'].lower()
     assert row['annotation_level'] in valid_annotation_levels
 
     # frame_num should either be NaN or an integer
     if isinstance(row['frame_num'],float):
         assert np.isnan(row['frame_num'])
@@ -442,36 +452,39 @@
     images_to_download.extend(non_empty_rows.to_dict('records'))
     
  # ...for each dataset
 
 print('Selected {} total images'.format(len(images_to_download)))
 
 
-#%% Download images
+#%% Download images (prep)
 
 # Expect a few errors for images with human or vehicle labels (or things like "ignore" that *could* be humans)
 
-# TODO: trivially parallelizable
-#
+preferred_cloud = 'aws'
+
+url_to_target_file = {}
+
 # i_image = 10; image = images_to_download[i_image]
 for i_image,image in tqdm(enumerate(images_to_download),total=len(images_to_download)):
     
-    url = image['url']
+    url = image['url_' + preferred_cloud]
     ext = os.path.splitext(url)[1]
-    image_file = os.path.join(preview_folder,'image_{}'.format(str(i_image).zfill(4)) + ext)
-    relative_file = os.path.relpath(image_file,preview_folder)
-    try:
-        download_url(url,image_file,verbose=False)
-        image['relative_file'] = relative_file
-    except urllib.error.HTTPError:
-        print('Image {} does not exist ({}:{})'.format(
-            i_image,image['dataset_name'],image['original_label']))
-        image['relative_file'] = None
+    fn_relative = 'image_{}'.format(str(i_image).zfill(4)) + ext
+    fn_abs = os.path.join(preview_folder,fn_relative)
+    image['relative_file'] = fn_relative
+    image['url'] = url
+    url_to_target_file[url] = fn_abs
+    
 
-# ...for each image we need to download
+#%% Download images (execution)
+
+from md_utils.url_utils import parallel_download_urls
+download_results = parallel_download_urls(url_to_target_file,verbose=False,overwrite=True,
+                                          n_workers=20,pool_type='thread')
 
 
 #%% Write preview HTML
 
 html_filename = os.path.join(preview_folder,'index.html')
 
 html_images = []
@@ -495,8 +508,8 @@
 open_file(html_filename)
 
 
 #%% Zip output file
 
 zipped_output_file = zip_file(output_file,verbose=True)
 
-print('Zipped {} to {}'.format(output_file,zipped_output_file))
+print('Zipped {} to {}'.format(output_file,zipped_output_file))
```

### Comparing `megadetector-5.0.8/data_management/lila/get_lila_annotation_counts.py` & `megadetector-5.0.9/data_management/lila/get_lila_annotation_counts.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,29 +1,32 @@
-########
-#
-# get_lila_annotation_counts.py
-#
-# Generates a .json-formatted dictionary mapping each LILA dataset to all categories
-# that exist for that dataset, with counts for the number of occurrences of each category 
-# (the number of *annotations* for each category, not the number of *images*).
-#
-# Also loads the taxonomy mapping file, to include scientific names for each category.
-#
-# get_lila_image_counts.py counts the number of *images* for each category in each dataset.
-#
-########
+"""
+
+get_lila_annotation_counts.py
+
+Generates a .json-formatted dictionary mapping each LILA dataset to all categories
+that exist for that dataset, with counts for the number of occurrences of each category 
+(the number of *annotations* for each category, not the number of *images*).
+
+Also loads the taxonomy mapping file, to include scientific names for each category.
+
+get_lila_image_counts.py counts the number of *images* for each category in each dataset.
+
+"""
 
 #%% Constants and imports
 
 import json
 import os
 
 from data_management.lila.lila_common import read_lila_metadata,\
     read_metadata_file_for_dataset, read_lila_taxonomy_mapping
 
+# cloud provider to use for downloading images; options are 'gcp', 'azure', or 'aws'
+preferred_cloud = 'gcp'
+
 # array to fill for output
 category_list = []
 
 # We'll write images, metadata downloads, and temporary files here
 lila_local_base = os.path.expanduser('~/lila')
 
 output_dir = os.path.join(lila_local_base,'lila_categories_list')
@@ -92,17 +95,17 @@
     
     taxonomy_mapping_available = (ds_name in datasets_with_taxonomy_mapping)
     
     if not taxonomy_mapping_available:
         print('Warning: taxonomy mapping not available for {}'.format(ds_name))
         
     print('Finding categories in {}'.format(ds_name))
-    
+
     json_filename = metadata_table[ds_name]['json_filename']
-    base_url = metadata_table[ds_name]['image_base_url']
+    base_url = metadata_table[ds_name]['image_base_url_' + preferred_cloud]
     assert not base_url.endswith('/')
     
     # Open the metadata file    
     with open(json_filename, 'r') as f:
         data = json.load(f)
     
     # Collect list of categories and mappings to category name
```

### Comparing `megadetector-5.0.8/data_management/lila/get_lila_image_counts.py` & `megadetector-5.0.9/data_management/lila/get_lila_image_counts.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-########
-#
-# get_lila_image_counts.py
-#
-# Count the number of images and bounding boxes with each label in one or more LILA datasets.
-#
-# This script doesn't write these counts out anywhere other than the console, it's just intended
-# as a template for doing operations like this on LILA data.  get_lila_annotation_counts.py writes 
-# information out to a .json file, but it counts *annotations*, not *images*, for each category.
-#
-########
+"""
+
+get_lila_image_counts.py
+
+Count the number of images and bounding boxes with each label in one or more LILA datasets.
+
+This script doesn't write these counts out anywhere other than the console, it's just intended
+as a template for doing operations like this on LILA data.  get_lila_annotation_counts.py writes 
+information out to a .json file, but it counts *annotations*, not *images*, for each category.
+
+"""
 
 #%% Constants and imports
 
 import json
 import os
 
 from collections import defaultdict
```

### Comparing `megadetector-5.0.8/data_management/lila/lila_common.py` & `megadetector-5.0.9/data_management/lila/lila_common.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,27 +1,27 @@
-########
-#
-# lila_common.py
-#
-# Common constants and functions related to LILA data management/retrieval.
-#
-########
+"""
+
+lila_common.py
+
+Common constants and functions related to LILA data management/retrieval.
+
+"""
 
 #%% Imports and constants
 
 import os
 import json
 import zipfile
 import pandas as pd
-import numpy as np
 
 from urllib.parse import urlparse
 
 from md_utils.url_utils import download_url
 from md_utils.path_utils import unzip_file
+from md_utils.ct_utils import is_empty
 
 # LILA camera trap primary metadata file
 lila_metadata_url = 'http://lila.science/wp-content/uploads/2023/06/lila_camera_trap_datasets.csv'
 lila_taxonomy_mapping_url = 'https://lila.science/public/lila-taxonomy-mapping_release.csv'
 lila_all_images_url = 'https://lila.science/public/lila_image_urls_and_labels.csv.zip'
 
 wildlife_insights_page_size = 30000
@@ -29,28 +29,40 @@
     wildlife_insights_page_size)
 wildlife_insights_taxonomy_local_json_filename = 'wi_taxonomy.json'
 wildlife_insights_taxonomy_local_csv_filename = \
     wildlife_insights_taxonomy_local_json_filename.replace('.json','.csv')
 
 # Filenames are consistent across clouds relative to these URLs
 lila_base_urls = {
-    'azure':'https://lilablobssc.blob.core.windows.net/',
+    'azure':'https://lilawildlife.blob.core.windows.net/lila-wildlife/',
     'gcp':'https://storage.googleapis.com/public-datasets-lila/',
     'aws':'http://us-west-2.opendata.source.coop.s3.amazonaws.com/agentmorris/lila-wildlife/'
 }
 
+lila_cloud_urls = {
+    'azure':'https://lilawildlife.blob.core.windows.net/lila-wildlife/',
+    'gcp':'gs://public-datasets-lila/',
+    'aws':'s3://us-west-2.opendata.source.coop/agentmorris/lila-wildlife/'
+}
+
+for url in lila_base_urls.values():
+    assert url.endswith('/')
 
 
 #%% Common functions
 
 def read_wildlife_insights_taxonomy_mapping(metadata_dir):
     """
     Reads the WI taxonomy mapping file, downloading the .json data (and writing to .csv) if necessary.
     
-    Returns a Pandas dataframe.
+    Args:
+        metadata_dir (str): folder to use for temporary LILA metadata files
+        
+    Returns:
+        pd.dataframe: A DataFrame with taxonomy information
     """
     
     wi_taxonomy_csv_path = os.path.join(metadata_dir,wildlife_insights_taxonomy_local_csv_filename)
     
     if os.path.exists(wi_taxonomy_csv_path):
         df = pd.read_csv(wi_taxonomy_csv_path)
     else:
@@ -81,44 +93,62 @@
     return df
 
     
 def read_lila_taxonomy_mapping(metadata_dir):
     """
     Reads the LILA taxonomy mapping file, downloading the .csv file if necessary.
     
-    Returns a Pandas dataframe, with one row per identification.
+    Args:
+        metadata_dir (str): folder to use for temporary LILA metadata files
+        
+    Returns:
+        pd.DataFrame: a DataFrame with one row per identification
     """
     
     p = urlparse(lila_taxonomy_mapping_url)
     taxonomy_filename = os.path.join(metadata_dir,os.path.basename(p.path))
     download_url(lila_taxonomy_mapping_url, taxonomy_filename)
     
     df = pd.read_csv(lila_taxonomy_mapping_url)
     
     return df
 
    
-def is_empty(v):
-    if v is None:
-        return True
-    if isinstance(v,str) and v == '':
-        return True
-    if isinstance(v,float) and np.isnan(v):
-        return True
-    return False
-
-
 def read_lila_metadata(metadata_dir):
     """
-    Reads LILA metadata (URLs to each dataset), downloading the txt file if necessary.
+    Reads LILA metadata (URLs to each dataset), downloading the .csv file if necessary.
     
-    Returns a dict mapping dataset names (e.g. "Caltech Camera Traps") to dicts
-    with keys corresponding to the headers in the .csv file, currently:
+    Args:
+        metadata_dir (str): folder to use for temporary LILA metadata files
         
-    name,image_base_url,metadata_url,bbox_url,continent,country,region
+    Returns:
+        dict: a dict mapping dataset names (e.g. "Caltech Camera Traps") to dicts
+        with keys corresponding to the headers in the .csv file, currently:
+        
+        - name
+        - short_name
+        - continent
+        - country
+        - region
+        - image_base_url_relative
+        - metadata_url_relative
+        - bbox_url_relative
+        - image_base_url_gcp
+        - metadata_url_gcp
+        - bbox_url_gcp
+        - image_base_url_aws
+        - metadata_url_aws
+        - bbox_url_aws
+        - image_base_url_azure
+        - metadata_url_azure
+        - box_url_azure
+        - mdv4_results_raw
+        - mdv5b_results_raw
+        - md_results_with_rde
+        - json_filename
     """
     
     # Put the master metadata file in the same folder where we're putting images
     p = urlparse(lila_metadata_url)
     metadata_filename = os.path.join(metadata_dir,os.path.basename(p.path))
     download_url(lila_metadata_url, metadata_filename)
     
@@ -144,14 +174,20 @@
     return metadata_table    
     
 
 def read_lila_all_images_file(metadata_dir):
     """
     Downloads if necessary - then unzips if necessary - the .csv file with label mappings for
     all LILA files, and opens the resulting .csv file as a Pandas DataFrame.
+    
+    Args:
+        metadata_dir (str): folder to use for temporary LILA metadata files
+        
+    Returns:
+        pd.DataFrame: a DataFrame containing one row per identification in a LILA camera trap image
     """
         
     p = urlparse(lila_all_images_url)
     lila_all_images_zip_filename = os.path.join(metadata_dir,os.path.basename(p.path))
     download_url(lila_all_images_url, lila_all_images_zip_filename)
     
     with zipfile.ZipFile(lila_all_images_zip_filename,'r') as z:
@@ -165,26 +201,45 @@
         print('{} already unzipped'.format(unzipped_csv_filename))    
     
     df = pd.read_csv(unzipped_csv_filename)
     
     return df
 
 
-def read_metadata_file_for_dataset(ds_name,metadata_dir,metadata_table=None,json_url=None):
+def read_metadata_file_for_dataset(ds_name,
+                                   metadata_dir,
+                                   metadata_table=None,
+                                   json_url=None,
+                                   preferred_cloud='gcp'):
     """
     Downloads if necessary - then unzips if necessary - the .json file for a specific dataset.
-    Returns the .json filename on the local disk.
+    
+    Args:
+        ds_name (str): the name of the dataset for which you want to retrieve metadata (e.g.
+            "Caltech Camera Traps")        
+        metadata_dir (str): folder to use for temporary LILA metadata files
+        metadata_table (dict, optional): an optional dictionary already loaded via
+            read_lila_metadata()
+        json_url (str, optional): the URL of the metadata file, if None will be retrieved
+            via read_lila_metadata()
+        preferred_cloud (str, optional): 'gcp' (default), 'azure', or 'aws'
+        
+    Returns:
+        str: the .json filename on the local disk
+    
     """
     
+    assert preferred_cloud in lila_base_urls.keys()
+    
     if json_url is None:
         
         if metadata_table is None:
             metadata_table = read_lila_metadata(metadata_dir)
             
-        json_url = metadata_table[ds_name]['metadata_url']
+        json_url = metadata_table[ds_name]['metadata_url_' + preferred_cloud]
     
     p = urlparse(json_url)
     json_filename = os.path.join(metadata_dir,os.path.basename(p.path))
     download_url(json_url, json_filename)
     
     # Unzip if necessary
     if json_filename.endswith('.zip'):
@@ -211,27 +266,35 @@
     #%% Verify that all base URLs exist
     
     # LILA camera trap primary metadata file
     urls = (lila_metadata_url,lila_taxonomy_mapping_url,lila_all_images_url,wildlife_insights_taxonomy_url)
     
     from md_utils import url_utils
     
-    status_codes = url_utils.test_urls(urls)
+    status_codes = url_utils.test_urls(urls,timeout=2.0)
+    assert all([code == 200 for code in status_codes])
     
     
     #%% Verify that the metadata URLs exist for individual datasets
     
     metadata_dir = os.path.expanduser('~/lila/metadata')
     
     dataset_metadata = read_lila_metadata(metadata_dir)
     
     urls_to_test = []
+    
     # ds_name = next(iter(dataset_metadata.keys()))
     for ds_name in dataset_metadata.keys():
         
         ds_info = dataset_metadata[ds_name]
-        urls_to_test.append(ds_info['metadata_url'])
-        if ds_info['bbox_url'] != None:
-            urls_to_test.append(ds_info['bbox_url'])
+        for cloud_name in lila_base_urls.keys():
+            urls_to_test.append(ds_info['metadata_url_' + cloud_name])
+            if ds_info['bbox_url_relative'] != None:
+                urls_to_test.append(ds_info['bbox_url_' + cloud_name])
             
-    status_codes = url_utils.test_urls(urls_to_test)    
+    status_codes = url_utils.test_urls(urls_to_test,
+                                       error_on_failure=True,
+                                       n_workers=10,
+                                       pool_type='process',
+                                       timeout=2.0)
+    assert all([code == 200 for code in status_codes])
```

### Comparing `megadetector-5.0.8/data_management/ocr_tools.py` & `megadetector-5.0.9/data_management/ocr_tools.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-########
-#
-# ocr_tools.py
-#
-# Use OCR (via the Tesseract package) to pull metadata (particularly times and
-# dates from camera trap images).
-#
-# The general approach is:
-#
-# * Crop a fixed percentage from the top and bottom of an image, slightly larger
-#   than the largest examples we've seen of how much space is used for metadata.
-#
-# * Define the background color as the median pixel value, and find rows that are
-#   mostly that color to refine the crop.
-#
-# * Crop to the refined crop, then run pytesseract to extract text.
-#
-# * Use regular expressions to find time and date.
-#
-# Prior to using this module:
-#
-# * Install Tesseract from https://tesseract-ocr.github.io/tessdoc/Installation.html
-#
-# * pip install pytesseract
-#    
-# Known limitations:
-#
-# * Semi-transparent overlays (which I've only seen on consumer cameras) usually fail.
-#    
-########
+"""
+
+ocr_tools.py
+
+Use OCR (via the Tesseract package) to pull metadata (particularly times and
+dates from camera trap images).
+
+The general approach is:
+
+* Crop a fixed percentage from the top and bottom of an image, slightly larger
+  than the largest examples we've seen of how much space is used for metadata.
+
+* Define the background color as the median pixel value, and find rows that are
+  mostly that color to refine the crop.
+
+* Crop to the refined crop, then run pytesseract to extract text.
+
+* Use regular expressions to find time and date.
+
+Prior to using this module:
+
+* Install Tesseract from https://tesseract-ocr.github.io/tessdoc/Installation.html
+
+* pip install pytesseract
+   
+Known limitations:
+
+* Semi-transparent overlays (which I've only seen on consumer cameras) usually fail.
+   
+"""
 
 #%% Notes to self
 
 """
 
 * To use the legacy engine (--oem 0), I had to download an updated eng.traineddata file from:
     
@@ -51,91 +51,102 @@
 from dateutil.parser import parse as dateparse
 
 import cv2
 from PIL import Image, ImageFilter
 from tqdm import tqdm
 
 from md_utils.path_utils import find_images
-from md_visualization import visualization_utils as vis_utils
-from md_utils import write_html_image_list        
 from md_utils.path_utils import open_file
+from md_utils import write_html_image_list        
+from md_utils.ct_utils import is_iterable
+from md_visualization import visualization_utils as vis_utils
 
 # pip install pytesseract
 #
 # Also install tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add
 # the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)
 import pytesseract
 
 
 #%% Extraction options
 
 class DatetimeExtractionOptions:
-            
+    """
+    Options used to parameterize datetime extraction in most functions in this module.
+    """
+    
     def __init__(self):
         
-        # Using a semi-arbitrary metric of how much it feels like we found the 
-        # text-containing region, discard regions that appear to be extraction failures
+        #: Using a semi-arbitrary metric of how much it feels like we found the 
+        #: text-containing region, discard regions that appear to be extraction failures
         self.p_crop_success_threshold = 0.5
         
-        # Pad each crop with a few pixels to make tesseract happy
+        #: Pad each crop with a few pixels to make tesseract happy
         self.crop_padding = 10        
         
-        # Discard short text, typically text from the top of the image
+        #: Discard short text, typically text from the top of the image
         self.min_text_length = 4
         
-        # When we're looking for pixels that match the background color, allow some 
-        # tolerance around the dominant color
+        #: When we're looking for pixels that match the background color, allow some 
+        #: tolerance around the dominant color
         self.background_tolerance = 2
             
-        # We need to see a consistent color in at least this fraction of pixels in our rough 
-        # crop to believe that we actually found a candidate metadata region.
+        #: We need to see a consistent color in at least this fraction of pixels in our rough 
+        #: crop to believe that we actually found a candidate metadata region.
         self.min_background_fraction = 0.3
         
-        # What fraction of the [top,bottom] of the image should we use for our rough crop?
+        #: What fraction of the [top,bottom] of the image should we use for our rough crop?
         self.image_crop_fraction = [0.045 , 0.045]
         # self.image_crop_fraction = [0.08 , 0.08]
         
-        # Within that rough crop, how much should we use for determining the background color?
+        #: Within that rough crop, how much should we use for determining the background color?
         self.background_crop_fraction_of_rough_crop = 0.5
         
-        # A row is considered a probable metadata row if it contains at least this fraction
-        # of the background color.  This is used only to find the top and bottom of the crop area, 
-        # so it's not that *every* row needs to hit this criteria, only the rows that are generally
-        # above and below the text.
+        #: A row is considered a probable metadata row if it contains at least this fraction
+        #: of the background color.  This is used only to find the top and bottom of the crop area, 
+        #: so it's not that *every* row needs to hit this criteria, only the rows that are generally
+        #: above and below the text.
         self.min_background_fraction_for_background_row = 0.5
         
-        # psm 6: "assume a single uniform block of text"
-        # psm 13: raw line
-        # oem: 0 == legacy, 1 == lstm
-        # tesseract_config_string = '--oem 0 --psm 6'
-        #
-        # Try these configuration strings in order until we find a valid datetime
+        #: psm 6: "assume a single uniform block of text"
+        #: psm 13: raw line
+        #: oem: 0 == legacy, 1 == lstm
+        #: tesseract_config_string = '--oem 0 --psm 6'
+        #:
+        #: Try these configuration strings in order until we find a valid datetime
         self.tesseract_config_strings = ['--oem 1 --psm 13','--oem 0 --psm 13',
                                          '--oem 1 --psm 6','--oem 0 --psm 6']
         
+        #: If this is False, and one set of options appears to succeed for an image, we'll
+        #: stop there.  If this is True, we always run all option sets on every image.
         self.force_all_ocr_options = False
         
+        #: Whether to apply PIL's ImageFilter.SHARPEN prior to OCR
         self.apply_sharpening_filter = True
         
-        # Tesseract should be on your system path, but you can also specify the
-        # path explicitly.
-        #
-        # os.environ['PATH'] += r';C:\Program Files\Tesseract-OCR'
-        # self.tesseract_cmd = 'r"C:\Program Files\Tesseract-OCR\tesseract.exe"'
+        #: Tesseract should be on your system path, but you can also specify the
+        #: path explicitly, e.g. you can do either of these:
+        #:
+        #: * os.environ['PATH'] += r';C:\Program Files\Tesseract-OCR'
+        #: * self.tesseract_cmd = 'r"C:\Program Files\Tesseract-OCR\tesseract.exe"'
         self.tesseract_cmd = 'tesseract.exe'
 
 
 #%% Support functions
 
 def make_rough_crops(image,options=None):
     """
-    Crops the top and bottom regions out of an image, returns a dict with fields
-    'top' and 'bottom', each pointing to a PIL image.
+    Crops the top and bottom regions out of an image.
     
-    [image] can be a PIL image or a file name.
+    Args:
+        image (Image or str): a PIL Image or file name
+        options (DatetimeExtractionOptions, optional): OCR parameters
+        
+    Returns:
+        dict: a dict with fields 'top' and 'bottom', each pointing to a new PIL Image        
     """
     
     if options is None:
         options = DatetimeExtractionOptions()
         
     if isinstance(image,str):
         image = vis_utils.open_image(image)
@@ -154,26 +165,30 @@
     return {'top':top_crop,'bottom':bottom_crop}
     
 # ...def make_rough_crops(...)
 
 
 def crop_to_solid_region(rough_crop,crop_location,options=None):
     """       
-    Given a rough crop from the top or bottom of an imaeg, find the background color
-    and crop to the metadata region.
-    
-    rough_crop should be PIL Image, crop_location should be 'top' or 'bottom'.
+    Given a rough crop from the top or bottom of an image, finds the background color
+    and crops to the metadata region.
     
     Within a region of an image (typically a crop from the top-ish or bottom-ish part of 
     an image), tightly crop to the solid portion (typically a region with a black background).
 
     The success metric is just a binary indicator right now: 1.0 if we found a region we believe
     contains a solid background, 0.0 otherwise.
     
-    Returns cropped_image,p_success,padded_image
+    Args:
+        rough_crop (Image): the PIL Image to crop
+        crop_location (str): 'top' or 'bottom'
+        options (DatetimeExtractionOptions, optional): OCR parameters
+        
+    Returns:
+        tuple: a tuple containing (a cropped_image (Image), p_success (float), padded_image (Image))
     """
     
     if options is None:
         options = DatetimeExtractionOptions()    
 
     crop_to_solid_region_result = {}
     crop_to_solid_region_result['crop_pil'] = None
@@ -279,16 +294,25 @@
     return crop_to_solid_region_result
     
 # ...crop_to_solid_region(...)    
 
 
 def find_text_in_crops(rough_crops,options=None,tesseract_config_string=None):
     """
-    Find all text in each Image in the dict [rough_crops]; those images should be pretty small 
+    Finds all text in each Image in the dict [rough_crops]; those images should be pretty small 
     regions by the time they get to this function, roughly the top or bottom 20% of an image.
+    
+    Args:
+        rough_crops (list): list of Image objects that have been cropped close to text
+        options (DatetimeExtractionOptions, optional): OCR parameters
+        tesseract_config_string (str, optional): optional CLI argument to pass to tesseract.exe
+        
+    Returns:
+        dict: a dict with keys "top" and "bottom", where each value is a dict with keys
+        'text' (text found, if any) and 'crop_to_solid_region_results' (metadata about the OCR pass)
     """
     
     if options is None:
         options = DatetimeExtractionOptions()
     
     if tesseract_config_string is None:
         tesseract_config_string = options.tesseract_config_strings[0]
@@ -334,15 +358,15 @@
     # ...for each cropped region
     
     return find_text_in_crops_results
     
 # ...def find_text_in_crops(...)
     
 
-def datetime_string_to_datetime(matched_string):
+def _datetime_string_to_datetime(matched_string):
     """
     Takes an OCR-matched datetime string, does a little cleanup, and parses a date
     from it.
     
     By the time a string gets to this function, it should be a proper date string, with
     no extraneous characters other than spaces around colons or hyphens.
     """
@@ -354,15 +378,15 @@
     try:
         extracted_datetime = dateparse(matched_string)
     except Exception:
         extracted_datetime = None
     return extracted_datetime
 
 
-def get_datetime_from_strings(strings,options=None):
+def _get_datetime_from_strings(strings,options=None):
     """
     Given a string or list of strings, search for exactly one datetime in those strings.
     using a series of regular expressions.
     
     Strings are currently just concatenated before searching for a datetime.
     """
     
@@ -377,80 +401,84 @@
     s = ''.join(e for e in s if e.isalnum() or e in ':-/' or e.isspace())
         
     ### AM/PM
     
     # 2013-10-02 11:40:50 AM
     m = re.search('(\d\d\d\d)\s?-\s?(\d\d)\s?-\s?(\d\d)\s+(\d+)\s?:?\s?(\d\d)\s?:\s?(\d\d)\s*([a|p]m)',s)
     if m is not None:        
-        return datetime_string_to_datetime(m.group(0))        
+        return _datetime_string_to_datetime(m.group(0))        
     
     # 04/01/2017 08:54:00AM
     m = re.search('(\d\d)\s?/\s?(\d\d)\s?/\s?(\d\d\d\d)\s+(\d+)\s?:\s?(\d\d)\s?:\s?(\d\d)\s*([a|p]m)',s)
     if m is not None:        
-        return datetime_string_to_datetime(m.group(0))    
+        return _datetime_string_to_datetime(m.group(0))    
     
     # 2017/04/01 08:54:00AM
     m = re.search('(\d\d\d\d)\s?/\s?(\d\d)\s?/\s?(\d\d)\s+(\d+)\s?:\s?(\d\d)\s?:\s?(\d\d)\s*([a|p]m)',s)
     if m is not None:        
-        return datetime_string_to_datetime(m.group(0))        
+        return _datetime_string_to_datetime(m.group(0))        
     
     # 04/01/2017 08:54AM
     m = re.search('(\d\d)\s?/\s?(\d\d)\s?/\s?(\d\d\d\d)\s+(\d+)\s?:\s?(\d\d)\s*([a|p]m)',s)
     if m is not None:        
-        return datetime_string_to_datetime(m.group(0))    
+        return _datetime_string_to_datetime(m.group(0))    
     
     # 2017/04/01 08:54AM
     m = re.search('(\d\d\d\d)\s?/\s?(\d\d)\s?/\s?(\d\d)\s+(\d+)\s?:\s?(\d\d)\s*([a|p]m)',s)
     if m is not None:        
-        return datetime_string_to_datetime(m.group(0))        
+        return _datetime_string_to_datetime(m.group(0))        
     
     ### No AM/PM
     
     # 2013-07-27 04:56:35
     m = re.search('(\d\d\d\d)\s?-\s?(\d\d)\s?-\s?(\d\d)\s*(\d\d)\s?:\s?(\d\d)\s?:\s?(\d\d)',s)
     if m is not None:        
-        return datetime_string_to_datetime(m.group(0))        
+        return _datetime_string_to_datetime(m.group(0))        
     
     # 07-27-2013 04:56:35
     m = re.search('(\d\d)\s?-\s?(\d\d)\s?-\s?(\d\d\d\d)\s*(\d\d)\s?:\s?(\d\d)\s?:\s?(\d\d)',s)
     if m is not None:        
-        return datetime_string_to_datetime(m.group(0))        
+        return _datetime_string_to_datetime(m.group(0))        
     
     # 2013/07/27 04:56:35
     m = re.search('(\d\d\d\d)\s?/\s?(\d\d)\s?/\s?(\d\d)\s*(\d\d)\s?:\s?(\d\d)\s?:\s?(\d\d)',s)
     if m is not None:        
-        return datetime_string_to_datetime(m.group(0))        
+        return _datetime_string_to_datetime(m.group(0))        
     
     # 07/27/2013 04:56:35
     m = re.search('(\d\d)\s?/\s?(\d\d)\s?/\s?(\d\d\d\d)\s*(\d\d)\s?:\s?(\d\d)\s?:\s?(\d\d)',s)
     if m is not None:        
-        return datetime_string_to_datetime(m.group(0))        
+        return _datetime_string_to_datetime(m.group(0))        
     
     return None
     
-# ...def get_datetime_from_strings(...)
+# ...def _get_datetime_from_strings(...)
 
 
 def get_datetime_from_image(image,include_crops=True,options=None):
     """
-    Find the datetime string (if present) in [image], which can be a PIL image or a 
-    filename.  Returns a dict:
-        
-    datetime: Python datetime object, or None
-    
-    text_results: length-2 list of strings
-    
-    all_extracted_datetimes: if we ran multiple option sets, this will contain the 
-    datetimes extracted for each option set
+    Tries to find the datetime string (if present) in an image.
     
-    ocr_results: detailed results from the OCR process, including crops as PIL images;
-    only included if include_crops is True.
-        
-    [options] can be None, a DatetimeExtractionOptions object, or a list of 
-    DatetimeExtractionOptions objects to try for each image.    
+    Args:
+        image (Image or str): the PIL Image object or image filename in which we should look for
+            datetime information.
+        include_crops (bool, optional): whether to include cropped images in the return dict (set
+            this to False if you're worried about size and you're processing a zillion images)
+        options (DatetimeExtractionOptions or list, optional): OCR parameters, either one 
+            DatetimeExtractionOptions object or a list of options to try
+    
+    Returns:
+        dict: a dict with fields:
+            
+            - datetime: Python datetime object, or None
+            - text_results: length-2 list of strings
+            - all_extracted_datetimes: if we ran multiple option sets, this will contain the 
+              datetimes extracted for each option set
+            - ocr_results: detailed results from the OCR process, including crops as PIL images;
+              only included if include_crops is True
     """
     
     if options is None:
         options = DatetimeExtractionOptions()
     
     if isinstance(image,str):
         image = vis_utils.open_image(image)
@@ -474,15 +502,15 @@
         all_ocr_results.append(ocr_results)
         
         text_results = [v['text'] for v in ocr_results.values()]
         assert len(text_results) == 2
         all_text_results.append(text_results)
             
         # Find datetime
-        extracted_datetime_this_option_set = get_datetime_from_strings(text_results,options)
+        extracted_datetime_this_option_set = _get_datetime_from_strings(text_results,options)
         assert isinstance(extracted_datetime_this_option_set,datetime.datetime) or \
             (extracted_datetime_this_option_set is None)
         
         all_extracted_datetimes[tesseract_config_string] = \
             extracted_datetime_this_option_set
             
         if extracted_datetime_this_option_set is not None:
@@ -508,26 +536,35 @@
         to_return['ocr_results'] = None
         
     return to_return
 
 # ...def get_datetime_from_image(...)
 
 
-def is_iterable(x):
-    try:
-        _ = iter(x)
-    except:
-       return False
-    return True
-
-
 def try_get_datetime_from_image(filename,include_crops=False,options=None):
     """
     Try/catch wrapper for get_datetime_from_image, optionally trying multiple option sets
     until we find a datetime.
+    
+    Args:
+        image (Image or str): the PIL Image object or image filename in which we should look for
+            datetime information.
+        include_crops (bool, optional): whether to include cropped images in the return dict (set
+            this to False if you're worried about size and you're processing a zillion images)
+        options (DatetimeExtractionOptions or list, optional): OCR parameters, either one 
+            DatetimeExtractionOptions object or a list of options to try
+    
+    Returns:
+        dict: A dict with fields:
+            - datetime: Python datetime object, or None
+            - text_results: length-2 list of strings
+            - all_extracted_datetimes: if we ran multiple option sets, this will contain the 
+              datetimes extracted for each option set
+            - ocr_results: detailed results from the OCR process, including crops as PIL images;
+              only included if include_crops is True
     """
     
     if options is None:
         options = DatetimeExtractionOptions()
 
     if not is_iterable(options):
         options = [options]
@@ -543,63 +580,72 @@
                 break
         except Exception as e:
             result['error'] = str(e)
     
     return result
 
 
-def get_datetimes_for_folder(folder_name,output_file=None,n_to_sample=-1,options=None):
+def get_datetimes_for_folder(folder_name,output_file=None,n_to_sample=-1,options=None,
+                             n_workers=16,use_threads=False):
     """
-    Retrieve metadata from every image in [folder_name], and 
-    write the results to the .json file [output_file].
-    
-    [options] can be None, a DatetimeExtractionOptions object, or a list of 
-    DatetimeExtractionOptions objects to try for each image.
+    The main entry point for this module.  Tries to retrieve metadata from pixels for every 
+    image in [folder_name], optionally the results to the .json file [output_file].
     
-    Returns a dict mapping filenames to datetime extraction results.  Optionally writes
-    results to the .json file [output_file].
+    Args:
+        folder_name (str): the folder of images to process recursively
+        output_file (str, optional): the .json file to which we should write results; if None,
+            just returns the results
+        n_to_sample (int, optional): for debugging only, used to limit the number of images
+            we process
+        options (DatetimeExtractionOptions or list, optional): OCR parameters, either one 
+            DatetimeExtractionOptions object or a list of options to try for each image
+        n_workers (int, optional): the number of parallel workers to use; set to <= 1 to disable
+            parallelization
+        use_threads (bool, optional): whether to use threads (True) or processes (False) for
+            parallelization; not relevant if n_workers <= 1
+            
+    Returns:
+        dict: a dict mapping filenames to datetime extraction results, see try_get_datetime_from_images
+        for the format of each value in the dict.
     """
     
     if options is None:
         options = DatetimeExtractionOptions()
     
     image_file_names = \
         find_images(folder_name,convert_slashes=True,
                     return_relative_paths=False,recursive=True)
     
     if n_to_sample > 0:
         import random
         random.seed(0)
         image_file_names = random.sample(image_file_names,n_to_sample)
-        
-    n_cores = 16
-    use_threads = False
-    
-    if n_cores <= 1:
+            
+    if n_workers <= 1:
         
         all_results = []
         for fn_abs in tqdm(image_file_names):
             all_results.append(try_get_datetime_from_image(fn_abs,options=options))
             
     else:    
         
         # Don't spawn more than one worker per image
-        if n_cores > len(image_file_names):
-            n_cores = len(image_file_names)
+        if n_workers > len(image_file_names):
+            n_workers = len(image_file_names)
             
         if use_threads:
             from multiprocessing.pool import ThreadPool
-            pool = ThreadPool(n_cores)
+            pool = ThreadPool(n_workers)
             worker_string = 'threads'        
         else:
             from multiprocessing.pool import Pool
-            pool = Pool(n_cores)
+            pool = Pool(n_workers)
             worker_string = 'processes'
             
-        print('Starting a pool of {} {}'.format(n_cores,worker_string))
+        print('Starting a pool of {} {}'.format(n_workers,worker_string))
         
         all_results = list(tqdm(pool.imap(
             partial(try_get_datetime_from_image,options=options),image_file_names),
             total=len(image_file_names)))
     
     filename_to_results = {}
     
@@ -617,15 +663,14 @@
 #%% Interactive driver
 
 if False:
     
     #%% Process images
     
     folder_name = r'g:\temp\island_conservation_camera_traps'
-    # folder_name = r'g:\camera_traps\camera_trap_images'
     output_file = r'g:\temp\ocr_results.json'
     from md_utils.path_utils import insert_before_extension
     output_file = insert_before_extension(output_file)
     n_to_sample = -1
     assert os.path.isdir(folder_name)
     options_a = DatetimeExtractionOptions()
     options_b = DatetimeExtractionOptions()
@@ -646,19 +691,14 @@
     filenames = sorted(list(filename_to_results.keys()))
     print('Loaded results for {} files'.format(len(filename_to_results)))
     
     
     #%% Scrap cell
     
     fn = 'g:/camera_traps/camera_trap_images/2018.07.02/newcam/people/DSCF0273.JPG'
-    # fn = r'g:\camera_traps\camera_trap_images\2022.01.29\cam0\coyote\DSCF0057.JPG'
-    # fn = 'g:/temp/island_conservation_camera_traps/chile/frances01/frances012013/chile_frances012013_02012013105658.jpg'
-    # fn = 'g:/temp/island_conservation_camera_traps/dominicanrepublic/camara06/cam0618junio2016/dominicanrepublic_cam0618junio2016_20160614_114115_img_0013.jpg'
-    # fn = os.path.join(folder_name,r'dominicanrepublic\camara22\cam228noviembre2015\dominicanrepublic_cam228noviembre2015_20151105_071226_img_0132.jpg')
-    # fn = 'g:/camera_traps/camera_trap_images/2021.06.06/camera01/empty/DSCF0873.JPG'
     include_crops = False
     options_a = DatetimeExtractionOptions()
     options_b = DatetimeExtractionOptions()
     options_b.image_crop_fraction = [0.08 , 0.08]
     image = vis_utils.open_image(fn) # noqa    
     result = try_get_datetime_from_image(fn,options=[options_a,options_b]) # noqa
     print(result)
@@ -823,7 +863,12 @@
         if len(matches_list) == 1:
             extracted_datetime = matches_list[0]
         else:
             extracted_datetime = None
         
     if extracted_datetime is not None:        
         assert extracted_datetime.year <= 2023 and extracted_datetime.year >= 1990
+
+
+#%% Command-line driver
+
+# TODO
```

### Comparing `megadetector-5.0.8/data_management/read_exif.py` & `megadetector-5.0.9/data_management/read_exif.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,100 +1,97 @@
-########
-#
-# read_exif.py
-#
-# Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images, 
-# and write them to  a .json or .csv file.  
-#
-# This module can use either PIL (which can only reliably read EXIF data) or exiftool (which
-# can read everything).  The latter approach expects that exiftool is available on the system
-# path.  No attempt is made to be consistent in format across the two approaches.
-#
-########
+"""
+
+read_exif.py
+
+Given a folder of images, reads relevant metadata (EXIF/IPTC/XMP) fields from all images, 
+and writes them to  a .json or .csv file.  
+
+This module can use either PIL (which can only reliably read EXIF data) or exiftool (which
+can read everything).  The latter approach expects that exiftool is available on the system
+path.  No attempt is made to be consistent in format across the two approaches.
+
+"""
 
 #%% Imports and constants
 
 import os
 import subprocess
 import json
 from datetime import datetime
 
 from multiprocessing.pool import ThreadPool as ThreadPool
 from multiprocessing.pool import Pool as Pool
 
 from tqdm import tqdm
 from PIL import Image, ExifTags
 
-from md_utils.path_utils import find_images
+from md_utils.path_utils import find_images, is_executable
 from md_utils.ct_utils import args_to_object
 
 debug_max_images = None
 
 
 #%% Options
 
 class ReadExifOptions:
+    """
+    Parameters controlling metadata extraction.
+    """
     
+    #: Enable additional debug console output
     verbose = False
     
-    # If this is True and an output file is specified for read_exif_from_folder,
-    # and we encounter a serialization issue, we'll return the results but won't
-    # error.    
+    #: If this is True and an output file is specified for read_exif_from_folder,
+    #: and we encounter a serialization issue, we'll return the results but won't
+    #: error.    
     allow_write_error = False
     
-    # Number of concurrent workers
+    #: Number of concurrent workers, set to <= 1 to disable parallelization
     n_workers = 1
     
-    # Should we use threads (vs. processes) for parallelization?
-    #
-    # Not relevant if n_workers is 1.
+    #: Should we use threads (vs. processes) for parallelization?
+    #:
+    #: Not relevant if n_workers is <= 1.
     use_threads = True
         
-    # "File" and "ExifTool" are tag types used by ExifTool to report data that 
-    # doesn't come from EXIF, rather from the file (e.g. file size).
+    #: "File" and "ExifTool" are tag types used by ExifTool to report data that 
+    #: doesn't come from EXIF, rather from the file (e.g. file size).
     tag_types_to_ignore = set(['File','ExifTool'])
     
-    # Include/exclude specific tags (mutually incompatible)
+    #: Include/exclude specific tags (tags_to_include and tags_to_exclude are mutually incompatible)
+    #:
+    #: A useful set of tags one might want to limit queries for:
+    #:
+    #: options.tags_to_include = ['DateTime','Model','Make','ExifImageWidth','ExifImageHeight','DateTime',
+    #: 'DateTimeOriginal','Orientation']    
     tags_to_include = None
-    tags_to_exclude = None
     
-    # A useful set of tags one might want to limit queries for
-    # options.tags_to_include = ['DateTime','Model','Make','ExifImageWidth','ExifImageHeight','DateTime','DateTimeOriginal','Orientation']
+    #: Include/exclude specific tags (tags_to_include and tags_to_exclude are mutually incompatible)
+    tags_to_exclude = None
     
+    #: The command line to invoke if using exiftool, can be an absolute path to exiftool.exe, or
+    #: can be just "exiftool", in which case it should be on your system path.
     exiftool_command_name = 'exiftool'
     
-    # How should we handle byte-formatted EXIF tags?
-    #
-    # 'convert_to_string': convert to a Python string
-    # 'delete': don't include at all
-    # 'raw': include as a byte string
+    #: How should we handle byte-formatted EXIF tags?
+    #:
+    #: 'convert_to_string': convert to a Python string
+    #: 'delete': don't include at all
+    #: 'raw': include as a byte string
     byte_handling = 'convert_to_string' # 'convert_to_string','delete','raw'
     
-    # Should we use exiftool or pil?
+    #: Should we use exiftool or PIL?
     processing_library = 'pil' # 'exiftool','pil'
-    
-    
+        
 
 #%% Functions
 
-def enumerate_files(input_folder,recursive=True):
-    """
-    Enumerates all image files in input_folder, returning relative paths
-    """
-    
-    image_files = find_images(input_folder,recursive=recursive)
-    image_files = [os.path.relpath(s,input_folder) for s in image_files]
-    image_files = [s.replace('\\','/') for s in image_files]
-    print('Enumerated {} files'.format(len(image_files)))
-    return image_files
-
-
-def get_exif_ifd(exif):
+def _get_exif_ifd(exif):
     """
-    Read EXIF data by finding the EXIF offset and reading tags directly
+    Read EXIF data from by finding the EXIF offset and reading tags directly
     
     https://github.com/python-pillow/Pillow/issues/5863
     """
     
     # Find the offset for all the EXIF information
     for key, value in ExifTags.TAGS.items():
         if value == "ExifOffset":
@@ -104,16 +101,24 @@
         ExifTags.TAGS.get(key, key): value
         for key, value in info.items()
     }
 
 
 def read_pil_exif(im,options=None):
     """
-    Read all the EXIF data we know how to read from [im] (path or PIL Image), whether it's 
-    in the PIL default EXIF data or not.  Returns a dict.
+    Read all the EXIF data we know how to read from an image, using PIL.  This is primarily
+    an internal function; the main entry point for single-image EXIF information is 
+    read_exif_tags_for_image().
+    
+    Args:
+        im (str or PIL.Image.Image): image (as a filename or an Image object) from which 
+            we should read EXIF data.
+    
+    Returns:
+        dict: a dictionary mapping EXIF tag names to their values
     """
     
     if options is None:
         options = ReadExifOptions()
         
     image_name = '[image]'
     if isinstance(im,str):
@@ -134,18 +139,18 @@
             'Invalid EXIF key {}'.format(str(k))
         if k in ExifTags.TAGS:
             exif_tags[ExifTags.TAGS[k]] = str(v)
         else:
             # print('Warning: unrecognized EXIF tag: {}'.format(k))
             exif_tags[k] = str(v)
     
-    exif_idf_tags = get_exif_ifd(exif_info)
+    exif_ifd_tags = _get_exif_ifd(exif_info)
     
-    for k in exif_idf_tags.keys():
-        v = exif_idf_tags[k]
+    for k in exif_ifd_tags.keys():
+        v = exif_ifd_tags[k]
         if k in exif_tags:
             if options.verbose:
                 print('Warning: redundant EXIF values for {} in {}:\n{}\n{}'.format(
                     k,image_name,exif_tags[k],v))
         else:
             exif_tags[k] = v
     
@@ -173,28 +178,35 @@
     return exif_tags
 
 # ...read_pil_exif()
 
 
 def format_datetime_as_exif_datetime_string(dt):
     """
-    Returns a Python datetime object rendered using the standard Exif datetime
-    string format
+    Returns a Python datetime object rendered using the standard EXIF datetime
+    string format ('%Y:%m:%d %H:%M:%S')
     """
     
     return datetime.strftime(dt, '%Y:%m:%d %H:%M:%S')
     
 
 def parse_exif_datetime_string(s,verbose=False):
     """"
     Exif datetimes are strings, but in a standard format:        
         
     %Y:%m:%d %H:%M:%S
     
-    Parse one of those strings into a Python datetime object.
+    Parses one of those strings into a Python datetime object.
+    
+    Args:
+        s (str): datetime string to parse, should be in standard EXIF datetime format
+        verbose (bool, optional): enable additional debug output
+    
+    Returns:
+        datetime: the datetime object created from [s]
     """
     
     dt = None
     try:
         dt = datetime.strptime(s, '%Y:%m:%d %H:%M:%S')
     except Exception:
         if verbose:
@@ -228,21 +240,21 @@
         return tags_to_return
 
 
 def read_exif_tags_for_image(file_path,options=None):
     """
     Get relevant fields from EXIF data for an image
     
-    Returns a dict with fields 'status' (str) and 'tags'
-    
-    The exact format of 'tags' depends on options.processing_library
-    
-    For exiftool, 'tags' is a list of lists, where each element is (type/tag/value)
-    
-    For pil, 'tags' is a dict (str:str)
+    Returns:
+        dict: a dict with fields 'status' (str) and 'tags'. The exact format of 'tags' depends on 
+        options (ReadExifOptions, optional): parameters controlling metadata extraction
+        options.processing_library:
+            
+            - For exiftool, 'tags' is a list of lists, where each element is (type/tag/value)
+            - For PIL, 'tags' is a dict (str:str)
     """
     
     if options is None:
         options = ReadExifOptions()
     
     result = {'status':'unknown','tags':[]}
     
@@ -340,15 +352,15 @@
             options.processing_library))
 
     # ...which processing library are we using?
     
 # ...read_exif_tags_for_image()
 
 
-def populate_exif_data(im, image_base, options=None):
+def _populate_exif_data(im, image_base, options=None):
     """
     Populate EXIF data into the 'exif_tags' field in the image object [im].
     
     im['file_name'] should be prepopulated, relative to image_base.
     
     Returns a modified version of [im], also modifies [im] in place.
     """
@@ -382,48 +394,51 @@
         print(s)
         im['error'] = s
         im['status'] = 'read failure'
         im['exif_tags'] = None
     
     return im
 
-# ...populate_exif_data()
+# ..._populate_exif_data()
 
 
-def create_image_objects(image_files,recursive=True):
+def _create_image_objects(image_files,recursive=True):
     """
     Create empty image objects for every image in [image_files], which can be a 
     list of relative paths (which will get stored without processing, so the base 
     path doesn't matter here), or a folder name.
     
     Returns a list of dicts with field 'file_name' (a relative path).
     
     "recursive" is ignored if "image_files" is a list.
     """
     
     # Enumerate *relative* paths
     if isinstance(image_files,str):    
         print('Enumerating image files in {}'.format(image_files))
         assert os.path.isdir(image_files), 'Invalid image folder {}'.format(image_files)
-        image_files = enumerate_files(image_files,recursive=recursive)
+        image_files = find_images(image_files,
+                                  recursive=recursive,
+                                  return_relative_paths=True,
+                                  convert_slashes=True)
         
     images = []
     for fn in image_files:
         im = {}
         im['file_name'] = fn
         images.append(im)
     
     if debug_max_images is not None:
         print('Trimming input list to {} images'.format(debug_max_images))
         images = images[0:debug_max_images]
     
     return images
 
 
-def populate_exif_for_images(image_base,images,options=None):
+def _populate_exif_for_images(image_base,images,options=None):
     """
     Main worker loop: read EXIF data for each image object in [images] and 
     populate the image objects.
     
     'images' should be a list of dicts with the field 'file_name' containing
     a relative path (relative to 'image_base').    
     """
@@ -431,33 +446,33 @@
     if options is None:
         options = ReadExifOptions()
 
     if options.n_workers == 1:
       
         results = []
         for im in tqdm(images):
-            results.append(populate_exif_data(im,image_base,options))
+            results.append(_populate_exif_data(im,image_base,options))
         
     else:
         
         from functools import partial
         if options.use_threads:
             print('Starting parallel thread pool with {} workers'.format(options.n_workers))
             pool = ThreadPool(options.n_workers)
         else:
             print('Starting parallel process pool with {} workers'.format(options.n_workers))
             pool = Pool(options.n_workers)
     
-        results = list(tqdm(pool.imap(partial(populate_exif_data,image_base=image_base,
+        results = list(tqdm(pool.imap(partial(_populate_exif_data,image_base=image_base,
                                         options=options),images),total=len(images)))
 
     return results
 
 
-def write_exif_results(results,output_file):
+def _write_exif_results(results,output_file):
     """
     Write EXIF information to [output_file].
     
     'results' is a list of dicts with fields 'exif_tags' and 'file_name'.
 
     Writes to .csv or .json depending on the extension of 'output_file'.         
     """
@@ -526,36 +541,32 @@
             output_file))
         
     # ...if we're writing to .json/.csv
     
     print('Wrote results to {}'.format(output_file))
 
 
-def is_executable(name):
-    
-    """Check whether `name` is on PATH and marked as executable."""
-    
-    # https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script
-
-    from shutil import which
-    return which(name) is not None
-
-
 def read_exif_from_folder(input_folder,output_file=None,options=None,filenames=None,recursive=True):
     """
-    Read EXIF data for all images in input_folder.
-    
-    If filenames is not None, it should be a list of relative filenames; only those files will 
-    be processed.
-    
-    input_folder can be None or '', in which case filenames should be a list of absolute paths.
-    
-    if output_file is not None, results will be written to the specified .json file.
+    Read EXIF data for a folder of images.
     
-    returns a dictionary mapping relative filenames to EXIF data.
+    Args:
+        input_folder (str): folder to process; if this is None, [filenames] should be a list of absolute
+            paths
+        output_file (str, optional): .json file to which we should write results; if this is None, results
+            are returned but not written to disk
+        options (ReadExifOptions, optional): parameters controlling metadata extraction
+        filenames (list, optional): allowlist of relative filenames (if [input_folder] is not None) or
+            a list of absolute filenames (if [input_folder] is None)
+        recursive (bool, optional): whether to recurse into [input_folder], not relevant if [input_folder]
+            is None.
+            
+    Returns:
+        dict: a dictionary mapping relative filenames to EXIF data, whose format depends on whether 
+        we're using PIL or exiftool.
     """
     
     if options is None:
         options = ReadExifOptions()
     
     # Validate options
     if options.tags_to_include is not None:
@@ -585,24 +596,24 @@
             print('Could not write to file {}'.format(output_file))
             raise
         
     if options.processing_library == 'exif':
         assert is_executable(options.exiftool_command_name), 'exiftool not available'
 
     if filenames is None:
-        images = create_image_objects(input_folder,recursive=recursive)
+        images = _create_image_objects(input_folder,recursive=recursive)
     else:
         assert isinstance(filenames,list)
-        images = create_image_objects(filenames)
+        images = _create_image_objects(filenames)
         
-    results = populate_exif_for_images(input_folder,images,options)
+    results = _populate_exif_for_images(input_folder,images,options)
     
     if output_file is not None:
         try:
-            write_exif_results(results,output_file)
+            _write_exif_results(results,output_file)
         except Exception as e:
             if not options.allow_write_error:
                 raise
             else:
                 print('Warning: error serializing EXIF data: {}'.format(str(e)))                
         
     return results
@@ -641,16 +652,18 @@
 def main():
 
     options = ReadExifOptions()
     
     parser = argparse.ArgumentParser(description=('Read EXIF information from all images in' + \
                                                   ' a folder, and write the results to .csv or .json'))
 
-    parser.add_argument('input_folder', type=str)
-    parser.add_argument('output_file', type=str)
+    parser.add_argument('input_folder', type=str, 
+                        help='Folder of images from which we should read EXIF information')
+    parser.add_argument('output_file', type=str,
+                        help='Output file (.json) to which we should write EXIF information')
     parser.add_argument('--n_workers', type=int, default=1,
                         help='Number of concurrent workers to use (defaults to 1)')
     parser.add_argument('--use_threads', action='store_true',
                         help='Use threads (instead of processes) for multitasking')
     parser.add_argument('--processing_library', type=str, default=options.processing_library,
                         help='Processing library (exif or pil)')
```

### Comparing `megadetector-5.0.8/data_management/remap_coco_categories.py` & `megadetector-5.0.9/data_management/remap_coco_categories.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,84 +1,84 @@
-########
-#
-# remap_coco_categories.py
-#
-# Given a COCO-formatted dataset, remap the categories to a new mapping.
-#
-########
-
-#%% Imports and constants
-
-import os
-import json
-
-from copy import deepcopy
-
-
-#%% Main function
-
-def remap_coco_categories(input_data,
-                          output_category_name_to_id,
-                          input_category_name_to_output_category_name,
-                          output_file=None):
-    """
-    Given a COCO-formatted dataset, remap the categories to a new categories mapping, optionally
-    writing the results to a new file.
-    
-    output_category_name_to_id is a dict mapping strings to ints.
-    
-    input_category_name_to_output_category_name is a dict mapping strings to strings.
-    
-    [input_data] can be a COCO-formatted dict or a filename.  If it's a dict, it will be copied,
-    not modified in place.
-    """
-    
-    if isinstance(input_data,str):
-        assert os.path.isfile(input_data), "Can't find file {}".format(input_data)
-        with open(input_data,'r') as f:
-            input_data = json.load(f)
-        assert isinstance(input_data,dict), 'Illegal COCO input data'
-    else:
-        assert isinstance(input_data,dict), 'Illegal COCO input data'
-        input_data = deepcopy(input_data)
-    
-    # It's safe to modify in-place now
-    output_data = input_data
-    
-    # Read input name --> ID mapping
-    input_category_name_to_input_category_id = {}
-    for c in input_data['categories']:
-        input_category_name_to_input_category_id[c['name']] = c['id']
-    
-    # Map input IDs --> output IDs
-    input_category_id_to_output_category_id = {}
-    for input_name in input_category_name_to_output_category_name.keys():
-        output_name = input_category_name_to_output_category_name[input_name]
-        assert output_name in output_category_name_to_id, \
-            'No output ID for {} --> {}'.format(input_name,output_name)
-        input_id = input_category_name_to_input_category_id[input_name]
-        output_id = output_category_name_to_id[output_name]
-        input_category_id_to_output_category_id[input_id] = output_id
-        
-    # Map annotations
-    for ann in output_data['annotations']:
-        assert ann['category_id'] in input_category_id_to_output_category_id, \
-            'Unrecognized category ID {}'.format(ann['category_id'])
-        ann['category_id'] = input_category_id_to_output_category_id[ann['category_id']]
-        
-    # Update the category list
-    output_categories = []
-    for output_name in output_category_name_to_id:
-        category = {'name':output_name,'id':output_category_name_to_id[output_name]}
-        output_categories.append(category)
-    output_data['categories'] = output_categories
-        
-    if output_file is not None:
-        with open(output_file,'w') as f:
-            json.dump(output_data,f,indent=1)
-            
-    return input_data
-
-
-#%% Command-line driver
-
+"""
+
+remap_coco_categories.py
+
+Given a COCO-formatted dataset, remap the categories to a new mapping.
+
+"""
+
+#%% Imports and constants
+
+import os
+import json
+
+from copy import deepcopy
+
+
+#%% Main function
+
+def remap_coco_categories(input_data,
+                          output_category_name_to_id,
+                          input_category_name_to_output_category_name,
+                          output_file=None):
+    """
+    Given a COCO-formatted dataset, remap the categories to a new categories mapping, optionally
+    writing the results to a new file.
+    
+    output_category_name_to_id is a dict mapping strings to ints.
+    
+    input_category_name_to_output_category_name is a dict mapping strings to strings.
+    
+    [input_data] can be a COCO-formatted dict or a filename.  If it's a dict, it will be copied,
+    not modified in place.
+    """
+    
+    if isinstance(input_data,str):
+        assert os.path.isfile(input_data), "Can't find file {}".format(input_data)
+        with open(input_data,'r') as f:
+            input_data = json.load(f)
+        assert isinstance(input_data,dict), 'Illegal COCO input data'
+    else:
+        assert isinstance(input_data,dict), 'Illegal COCO input data'
+        input_data = deepcopy(input_data)
+    
+    # It's safe to modify in-place now
+    output_data = input_data
+    
+    # Read input name --> ID mapping
+    input_category_name_to_input_category_id = {}
+    for c in input_data['categories']:
+        input_category_name_to_input_category_id[c['name']] = c['id']
+    
+    # Map input IDs --> output IDs
+    input_category_id_to_output_category_id = {}
+    for input_name in input_category_name_to_output_category_name.keys():
+        output_name = input_category_name_to_output_category_name[input_name]
+        assert output_name in output_category_name_to_id, \
+            'No output ID for {} --> {}'.format(input_name,output_name)
+        input_id = input_category_name_to_input_category_id[input_name]
+        output_id = output_category_name_to_id[output_name]
+        input_category_id_to_output_category_id[input_id] = output_id
+        
+    # Map annotations
+    for ann in output_data['annotations']:
+        assert ann['category_id'] in input_category_id_to_output_category_id, \
+            'Unrecognized category ID {}'.format(ann['category_id'])
+        ann['category_id'] = input_category_id_to_output_category_id[ann['category_id']]
+        
+    # Update the category list
+    output_categories = []
+    for output_name in output_category_name_to_id:
+        category = {'name':output_name,'id':output_category_name_to_id[output_name]}
+        output_categories.append(category)
+    output_data['categories'] = output_categories
+        
+    if output_file is not None:
+        with open(output_file,'w') as f:
+            json.dump(output_data,f,indent=1)
+            
+    return input_data
+
+
+#%% Command-line driver
+
 # TODO
```

### Comparing `megadetector-5.0.8/data_management/remove_exif.py` & `megadetector-5.0.9/data_management/remove_exif.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,70 +1,66 @@
-########
-#
-# remove_exif.py
-#
-# Removes all EXIF/IPTC/XMP metadata from a folder of images, without making 
-# backup copies, using pyexiv2.
-#
-########
+"""
+
+remove_exif.py
+
+Removes all EXIF/IPTC/XMP metadata from a folder of images, without making 
+backup copies, using pyexiv2.
+
+TODO: This is a one-off script waiting to be cleaned up for more general use.
+
+"""
+
+input_base = r'f:\images'
+
 
 #%% Imports and constants
 
 import os
 import glob
 
-input_base = r'f:\images'
-assert os.path.isdir(input_base)
+def main():
 
+    assert os.path.isdir(input_base)
 
-#%% List files
+    ##%% List files
 
-all_files = [f for f in glob.glob(input_base + "*/**", recursive=True)]
-image_files = [s for s in all_files if (s.lower().endswith('.jpg'))]
-    
-
-#%% Remove EXIF data (support)
-
-import pyexiv2
-
-# PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS
-def remove_exif(fn):
-    
-    try:
-        img = pyexiv2.Image(fn)
-        # data = img.read_exif(); print(data)
-        img.clear_exif()
-        img.clear_iptc()
-        img.clear_xmp()
-        img.close()        
-    except Exception as e:
-        print('EXIF error on {}: {}'.format(fn,str(e)))
-    
-
-#%% Debug
-
-if False:    
-    #%%
-    fn = image_files[-10001]
-    os.startfile(fn)    
-    #%%
-    remove_exif(fn)
-    os.startfile(fn)
-    
-    
-#%% Remove EXIF data (execution)
-
-from joblib import Parallel, delayed
-
-n_exif_threads = 50
-    
-if n_exif_threads == 1:
-    
-    # fn = image_files[0]
-    for fn in image_files:
-        remove_exif(fn)
+    all_files = [f for f in glob.glob(input_base + "*/**", recursive=True)]
+    image_files = [s for s in all_files if (s.lower().endswith('.jpg'))]
         
-else:
-    # joblib.Parallel defaults to a process-based backend, but let's be sure
-    # results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])
-    results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files)
 
+    ##%% Remove EXIF data (support)
+
+    import pyexiv2
+
+    # PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS
+    def remove_exif(fn):
+        
+        try:
+            img = pyexiv2.Image(fn)
+            # data = img.read_exif(); print(data)
+            img.clear_exif()
+            img.clear_iptc()
+            img.clear_xmp()
+            img.close()        
+        except Exception as e:
+            print('EXIF error on {}: {}'.format(fn,str(e)))
+        
+
+    ##%% Remove EXIF data (execution)
+
+    from joblib import Parallel, delayed
+
+    n_exif_threads = 50
+        
+    if n_exif_threads == 1:
+        
+        # fn = image_files[0]
+        for fn in image_files:
+            remove_exif(fn)
+            
+    else:
+        # joblib.Parallel defaults to a process-based backend, but let's be sure
+        # results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])
+        _ = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files)
+            
+if __name__ == '__main__':
+    main()
```

### Comparing `megadetector-5.0.8/data_management/resize_coco_dataset.py` & `megadetector-5.0.9/data_management/resize_coco_dataset.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# resize_coco_dataset.py
-#
-# Given a COCO-formatted dataset, resize all the images to a target size,
-# scaling bounding boxes accordingly.
-#
-########
+"""
+
+resize_coco_dataset.py
+
+Given a COCO-formatted dataset, resizes all the images to a target size,
+scaling bounding boxes accordingly.
+
+"""
 
 #%% Imports and constants
 
 import os
 import json
 import shutil
 
@@ -24,30 +24,37 @@
 #%% Functions
 
 def resize_coco_dataset(input_folder,input_filename,
                         output_folder,output_filename,
                         target_size=(-1,-1),
                         correct_size_image_handling='copy'):
     """
-    Given a COCO-formatted dataset (images in input_folder, data in input_filename), resize 
-    all the images to a target size (in output_folder) and scale bounding boxes accordingly
-    (in output_filename).
-    
-    target_size should be a tuple/list of ints, length 2.  If either dimension is -1, aspect ratio
-    will be preserved.  If both dimensions are -1, this means "keep the original size".  If 
-    both dimensions are -1 and correct_size_image_handling is copy, this function is basically 
-    a no-op.
+    Given a COCO-formatted dataset (images in input_folder, data in input_filename), resizes
+    all the images to a target size (in output_folder) and scales bounding boxes accordingly.
     
-    correct_size_image_handling can be 'copy' (in which case the original image is just copied 
-    to the output folder) or 'rewrite' (in which case the image is opened via PIL and re-written,
-    attempting to preserve the same quality).  The only reason to do this is the case where 
-    you're superstitious about biases coming from images in a training set being written
-    by different image encoders.
+    Args:
+        input_folder (str): the folder where images live; filenames in [input_filename] should 
+            be relative to [input_folder]
+        input_filename (str): the (input) COCO-formatted .json file containing annotations
+        output_folder (str): the folder to which we should write resized images; can be the
+            same as [input_folder], in which case images are over-written
+        output_filename (str): the COCO-formatted .json file we should generate that refers to
+            the resized images
+        target_size (list or tuple of ints): this should be tuple/list of ints, with length 2 (w,h).
+            If either dimension is -1, aspect ratio will be preserved.  If both dimensions are -1, this means 
+            "keep the original size".  If  both dimensions are -1 and correct_size_image_handling is copy, this 
+            function is basically a no-op.    
+        correct_size_image_handling (str): can be 'copy' (in which case the original image is just copied 
+            to the output folder) or 'rewrite' (in which case the image is opened via PIL and re-written,
+            attempting to preserve the same quality).  The only reason to do use 'rewrite' 'is the case where 
+            you're superstitious about biases coming from images in a training set being written by different 
+            image encoders.
    
-    Returns the COCO database with resized images.
+    Returns:
+        dict: the COCO database with resized images, identical to the content of [output_filename]
     """
     
     # Read input data
     with open(input_filename,'r') as f:
         d = json.load(f)
     
     # Map image IDs to annotations
```

### Comparing `megadetector-5.0.8/data_management/wi_download_csv_to_coco.py` & `megadetector-5.0.9/data_management/wi_download_csv_to_coco.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,239 +1,246 @@
-########
-#
-# wi_download_csv_to_coco.py
-#
-# Convert a .csv file from a Wildlife Insights project export to a COCO camera traps .json file.
-#
-# Currently assumes that common names are unique identifiers, which is convenient but unreliable.
-#
-########
-
-#%% Imports and constants
-
-import os
-import json
-import pandas as pd
-import numpy as np
-
-from tqdm import tqdm
-from collections import defaultdict
-
-from md_visualization import visualization_utils as vis_utils
-
-wi_extra_annotation_columns = \
-    ('is_blank','identified_by','wi_taxon_id','class','order','family','genus','species','uncertainty',
-             'number_of_objects','age','sex','animal_recognizable','individual_id','individual_animal_notes',
-             'behavior','highlighted','markings')
-
-wi_extra_image_columns = ('project_id','deployment_id')
-
-def make_location_id(project_id,deployment_id):    
-    return 'project_' + str(project_id) + '_deployment_' + deployment_id
-        
-def isnan(v):
-    try:
-        return np.isnan(v)
-    except Exception:
-        return False
-
-default_category_remappings = {
-    'Homo Species':'Human',
-    'Human-Camera Trapper':'Human',
-    'No CV Result':'Unknown'
-}
-
-
-#%% 
-
-def wi_download_csv_to_coco(csv_file_in,
-                            coco_file_out=None,
-                            image_folder=None,
-                            validate_images=False,
-                            gs_prefix=None,
-                            verbose=True,
-                            category_remappings=default_category_remappings):
-    """
-    Convert a .csv file from a Wildlife Insights project export to a COCO 
-    camera traps .json file.
-    
-    If [coco_file_out] is None, uses [csv_file_in].json
-    
-    gs_prefix is a string to remove from GS URLs to convert to path names... for example, if
-    your gs:// URLs look like:
-        
-    gs://11234134_xyz/deployment/55554/dfadfasdfs.jpg
-    
-    ...and you specify gs_prefix='11234134_xyz/deployment/', the filenames in
-    the .json file will look like:
-        
-    55554/dfadfasdfs.jpg
-    
-    exclude_re discards matching images; typically use to omit thumbnail images.
-    """
-    
-    #%% Create COCO dictionaries
-    
-    category_name_to_id = {}
-    category_name_to_id['empty'] = 0
-    
-    df = pd.read_csv(csv_file_in)
-    
-    print('Read {} rows from {}'.format(len(df),csv_file_in))
-    
-    image_id_to_image = {}
-    image_id_to_annotations = defaultdict(list)
-    
-    # i_row = 0; row = df.iloc[i_row]
-    for i_row,row in df.iterrows():
-        
-        image_id = row['image_id']
-        
-        if image_id not in image_id_to_image:
-            
-            im = {}
-            image_id_to_image[image_id] = im
-            
-            im['id'] = image_id
-            
-            gs_url = row['location']
-            assert gs_url.startswith('gs://')
-            
-            file_name = gs_url.replace('gs://','')
-            if gs_prefix is not None:
-                file_name = file_name.replace(gs_prefix,'')
-                
-            location_id = make_location_id(row['project_id'],row['deployment_id'])
-            im['file_name'] = file_name
-            im['location'] = location_id
-            im['datetime'] = row['timestamp']
-            
-            im['wi_image_info'] = {}
-            for s in wi_extra_image_columns:
-                im['wi_image_info'][s] = str(row[s])
-            
-        else:
-            
-            im = image_id_to_image[image_id]
-            assert im['datetime'] == row['timestamp']
-            location_id = make_location_id(row['project_id'],row['deployment_id'])
-            assert im['location'] == location_id
-            
-        category_name = row['common_name']
-        if category_remappings is not None and category_name in category_remappings:
-            category_name = category_remappings[category_name]
-            
-        if category_name == 'Blank':
-            category_name = 'empty'
-            assert row['is_blank'] == 1
-        else:
-            assert row['is_blank'] == 0
-        assert isinstance(category_name,str)
-        if category_name in category_name_to_id:
-            category_id = category_name_to_id[category_name]
-        else:
-            category_id = len(category_name_to_id)
-            category_name_to_id[category_name] = category_id
-        
-        ann = {}
-        ann['image_id'] = image_id
-        annotations_this_image = image_id_to_annotations[image_id]
-        annotation_number = len(annotations_this_image)
-        ann['id'] = image_id + '_' + str(annotation_number).zfill(2)        
-        ann['category_id'] = category_id
-        annotations_this_image.append(ann)
-        
-        extra_info = {}
-        for s in wi_extra_annotation_columns:            
-            v = row[s]
-            if not isnan(v):
-                extra_info[s] = v
-        ann['wi_extra_info'] = extra_info
-        
-    # ...for each row
-    
-    images = list(image_id_to_image.values())
-    categories = []
-    for category_name in category_name_to_id:
-        category_id = category_name_to_id[category_name]
-        categories.append({'id':category_id,'name':category_name})
-    annotations = []
-    for image_id in image_id_to_annotations:
-        annotations_this_image = image_id_to_annotations[image_id]
-        for ann in annotations_this_image:
-            annotations.append(ann)
-    info = {'version':'1.00','description':'converted from WI export'}
-    info['source_file'] = csv_file_in
-    coco_data = {}
-    coco_data['info'] = info
-    coco_data['images'] = images
-    coco_data['annotations'] = annotations
-    coco_data['categories'] = categories
-    
-    
-    ##%% Validate images, add sizes        
-    
-    if validate_images:
-        
-        print('Validating images')
-        # TODO: trivially parallelizable
-        
-        assert os.path.isdir(image_folder), \
-            'Must specify a valid image folder if you specify validate_images=True'
-            
-        # im = images[0]
-        for im in tqdm(images):
-            file_name_relative = im['file_name']
-            file_name_abs = os.path.join(image_folder,file_name_relative)
-            assert os.path.isfile(file_name_abs)
-                
-            im['corrupt'] = False
-            try:
-                pil_im = vis_utils.load_image(file_name_abs)
-            except Exception:
-                im['corrupt'] = True
-            if not im['corrupt']:
-                im['width'] = pil_im.width
-                im['height'] = pil_im.height
-    
-    
-    ##%% Write output json
-        
-    if coco_file_out is None:
-        
-        coco_file_out = csv_file_in + '.json'
-        
-        with open(coco_file_out,'w') as f:
-            json.dump(coco_data,f,indent=1)
-
-
-    ##%% Validate output
-    
-    from data_management.databases.integrity_check_json_db import \
-        IntegrityCheckOptions,integrity_check_json_db
-    options = IntegrityCheckOptions()
-    options.baseDir = image_folder
-    options.bCheckImageExistence = True
-    options.verbose = verbose
-    _ = integrity_check_json_db(coco_file_out,options)
-    
-
-    
-#%% Interactive driver
-
-if False:
-    
-    #%%
-    
-    base_folder = r'a/b/c'
-    csv_file_in = os.path.join(base_folder,'images.csv')
-    coco_file_out = None
-    gs_prefix = 'a_b_c_main/'
-    image_folder = os.path.join(base_folder,'images')
-    validate_images = False
-    verbose = True
-    category_remappings = default_category_remappings
-
-
-#%% Command-line driver
-
-# TODO
+"""
+
+wi_download_csv_to_coco.py
+
+Converts a .csv file from a Wildlife Insights project export to a COCO camera traps .json file.
+
+Currently assumes that common names are unique identifiers, which is convenient but unreliable.
+
+"""
+
+#%% Imports and constants
+
+import os
+import json
+import pandas as pd
+
+from tqdm import tqdm
+from collections import defaultdict
+
+from md_visualization import visualization_utils as vis_utils
+from md_utils.ct_utils import isnan
+
+wi_extra_annotation_columns = \
+    ('is_blank','identified_by','wi_taxon_id','class','order','family','genus','species','uncertainty',
+             'number_of_objects','age','sex','animal_recognizable','individual_id','individual_animal_notes',
+             'behavior','highlighted','markings')
+
+wi_extra_image_columns = ('project_id','deployment_id')
+
+def _make_location_id(project_id,deployment_id):    
+    return 'project_' + str(project_id) + '_deployment_' + deployment_id
+
+default_category_remappings = {
+    'Homo Species':'Human',
+    'Human-Camera Trapper':'Human',
+    'No CV Result':'Unknown'
+}
+
+
+#%% Main function
+
+def wi_download_csv_to_coco(csv_file_in,
+                            coco_file_out=None,
+                            image_folder=None,
+                            validate_images=False,
+                            gs_prefix=None,
+                            verbose=True,
+                            category_remappings=default_category_remappings):
+    """
+    Converts a .csv file from a Wildlife Insights project export to a COCO 
+    Camera Traps .json file.
+    
+    Args:
+        csv_file_in (str): the downloaded .csv file we should convert to COCO
+        coco_file_out (str, optional): the .json file we should write; if [coco_file_out] is None, 
+            uses [csv_file_in].json
+        image_folder (str, optional): the folder where images live, only relevant if 
+            [validate_images] is True
+        validate_images (bool, optional): whether to check images for corruption and load
+            image sizes; if this is True, [image_folder] must be a valid folder
+        gs_prefix (str, optional): a string to remove from GS URLs to convert to path names... 
+            for example, if your gs:// URLs look like:
+    
+            `gs://11234134_xyz/deployment/55554/dfadfasdfs.jpg`
+    
+            ...and you specify gs_prefix='11234134_xyz/deployment/', the filenames in
+            the .json file will look like:
+        
+            `55554/dfadfasdfs.jpg`
+        verbose (bool, optional): enable additional debug console output
+        category_remappings (dict, optional): str --> str dict that maps any number of
+            WI category names to output category names; for example defaults to mapping
+            "Homo Species" to "Human", but leaves 99.99% of categories unchanged.        
+            
+    Returns: 
+        dict: COCO-formatted data, identical to what's written to [coco_file_out]
+    """
+    
+    ##%% Create COCO dictionaries
+    
+    category_name_to_id = {}
+    category_name_to_id['empty'] = 0
+    
+    df = pd.read_csv(csv_file_in)
+    
+    print('Read {} rows from {}'.format(len(df),csv_file_in))
+    
+    image_id_to_image = {}
+    image_id_to_annotations = defaultdict(list)
+    
+    # i_row = 0; row = df.iloc[i_row]
+    for i_row,row in df.iterrows():
+        
+        image_id = row['image_id']
+        
+        if image_id not in image_id_to_image:
+            
+            im = {}
+            image_id_to_image[image_id] = im
+            
+            im['id'] = image_id
+            
+            gs_url = row['location']
+            assert gs_url.startswith('gs://')
+            
+            file_name = gs_url.replace('gs://','')
+            if gs_prefix is not None:
+                file_name = file_name.replace(gs_prefix,'')
+                
+            location_id = _make_location_id(row['project_id'],row['deployment_id'])
+            im['file_name'] = file_name
+            im['location'] = location_id
+            im['datetime'] = row['timestamp']
+            
+            im['wi_image_info'] = {}
+            for s in wi_extra_image_columns:
+                im['wi_image_info'][s] = str(row[s])
+            
+        else:
+            
+            im = image_id_to_image[image_id]
+            assert im['datetime'] == row['timestamp']
+            location_id = _make_location_id(row['project_id'],row['deployment_id'])
+            assert im['location'] == location_id
+            
+        category_name = row['common_name']
+        if category_remappings is not None and category_name in category_remappings:
+            category_name = category_remappings[category_name]
+            
+        if category_name == 'Blank':
+            category_name = 'empty'
+            assert row['is_blank'] == 1
+        else:
+            assert row['is_blank'] == 0
+        assert isinstance(category_name,str)
+        if category_name in category_name_to_id:
+            category_id = category_name_to_id[category_name]
+        else:
+            category_id = len(category_name_to_id)
+            category_name_to_id[category_name] = category_id
+        
+        ann = {}
+        ann['image_id'] = image_id
+        annotations_this_image = image_id_to_annotations[image_id]
+        annotation_number = len(annotations_this_image)
+        ann['id'] = image_id + '_' + str(annotation_number).zfill(2)        
+        ann['category_id'] = category_id
+        annotations_this_image.append(ann)
+        
+        extra_info = {}
+        for s in wi_extra_annotation_columns:            
+            v = row[s]
+            if not isnan(v):
+                extra_info[s] = v
+        ann['wi_extra_info'] = extra_info
+        
+    # ...for each row
+    
+    images = list(image_id_to_image.values())
+    categories = []
+    for category_name in category_name_to_id:
+        category_id = category_name_to_id[category_name]
+        categories.append({'id':category_id,'name':category_name})
+    annotations = []
+    for image_id in image_id_to_annotations:
+        annotations_this_image = image_id_to_annotations[image_id]
+        for ann in annotations_this_image:
+            annotations.append(ann)
+    info = {'version':'1.00','description':'converted from WI export'}
+    info['source_file'] = csv_file_in
+    coco_data = {}
+    coco_data['info'] = info
+    coco_data['images'] = images
+    coco_data['annotations'] = annotations
+    coco_data['categories'] = categories
+    
+    
+    ##%% Validate images, add sizes        
+    
+    if validate_images:
+        
+        print('Validating images')
+        # TODO: trivially parallelizable
+        
+        assert os.path.isdir(image_folder), \
+            'Must specify a valid image folder if you specify validate_images=True'
+            
+        # im = images[0]
+        for im in tqdm(images):
+            file_name_relative = im['file_name']
+            file_name_abs = os.path.join(image_folder,file_name_relative)
+            assert os.path.isfile(file_name_abs)
+                
+            im['corrupt'] = False
+            try:
+                pil_im = vis_utils.load_image(file_name_abs)
+            except Exception:
+                im['corrupt'] = True
+            if not im['corrupt']:
+                im['width'] = pil_im.width
+                im['height'] = pil_im.height
+    
+    
+    ##%% Write output json
+        
+    if coco_file_out is None:        
+        coco_file_out = csv_file_in + '.json'
+        
+    with open(coco_file_out,'w') as f:
+        json.dump(coco_data,f,indent=1)
+
+
+    ##%% Validate output
+    
+    from data_management.databases.integrity_check_json_db import \
+        IntegrityCheckOptions,integrity_check_json_db
+    options = IntegrityCheckOptions()
+    options.baseDir = image_folder
+    options.bCheckImageExistence = True
+    options.verbose = verbose
+    _ = integrity_check_json_db(coco_file_out,options)
+    
+    return coco_data
+
+# ...def wi_download_csv_to_coco(...)    
+
+
+#%% Interactive driver
+
+if False:
+    
+    #%%
+    
+    base_folder = r'a/b/c'
+    csv_file_in = os.path.join(base_folder,'images.csv')
+    coco_file_out = None
+    gs_prefix = 'a_b_c_main/'
+    image_folder = os.path.join(base_folder,'images')
+    validate_images = False
+    verbose = True
+    category_remappings = default_category_remappings
+
+
+#%% Command-line driver
+
+# TODO
```

### Comparing `megadetector-5.0.8/data_management/yolo_output_to_md_output.py` & `megadetector-5.0.9/data_management/yolo_output_to_md_output.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,73 +1,73 @@
-########
-#
-# yolo_output_to_md_output.py
-#
-# Converts the output of YOLOv5's detect.py or val.py to the MD API output format.
-#
-# Command-line driver not done yet, this has only been run interactively.
-#
-########
-
-### Converting .txt files ###
-
-#
-# detect.py writes a .txt file per image, in YOLO training format.  Converting from this
-# format does not currently support recursive results, since detect.py doesn't save filenames
-# in a way that allows easy inference of folder names.  Requires access to the input
-# images, because the YOLO format uses the *absence* of a results file to indicate that
-# no detections are present.
-#
-# YOLOv5 output has one text file per image, like so:
-#
-# 0 0.0141693 0.469758 0.0283385 0.131552 0.761428 
-#
-# That's [class, x_center, y_center, width_of_box, height_of_box, confidence]
-#
-# val.py can write in this format as well, using the --save-txt argument.
-#
-# In both cases, a confidence value is only written to each line if you include the --save-conf
-# argument.  Confidence values are required by this conversion script.
-#
-
-### Converting .json files ###
-
-#
-# val.py can also write a .json file in COCO-ish format.  It's "COCO-ish" because it's
-# just the "images" portion of a COCO .json file.
-#
-# Converting from this format also requires access to the original images, since the format
-# written by YOLOv5 uses absolute coordinates, but MD results are in relative coordinates.
-#
+"""
+
+yolo_output_to_md_output.py
+
+Converts the output of YOLOv5's detect.py or val.py to the MD API output format.
+
+**Converting .txt files**
+
+detect.py writes a .txt file per image, in YOLO training format.  Converting from this
+format does not currently support recursive results, since detect.py doesn't save filenames
+in a way that allows easy inference of folder names.  Requires access to the input
+images, because the YOLO format uses the *absence* of a results file to indicate that
+no detections are present.
+
+YOLOv5 output has one text file per image, like so:
+
+0 0.0141693 0.469758 0.0283385 0.131552 0.761428 
+
+That's [class, x_center, y_center, width_of_box, height_of_box, confidence]
+
+val.py can write in this format as well, using the --save-txt argument.
+
+In both cases, a confidence value is only written to each line if you include the --save-conf
+argument.  Confidence values are required by this conversion script.
+
+
+**Converting .json files**
+
+val.py can also write a .json file in COCO-ish format.  It's "COCO-ish" because it's
+just the "images" portion of a COCO .json file.
+
+Converting from this format also requires access to the original images, since the format
+written by YOLOv5 uses absolute coordinates, but MD results are in relative coordinates.
+
+"""
 
 #%% Imports and constants
 
 import json
 import csv
 import os
 import re
 
 from collections import defaultdict
 from tqdm import tqdm
 
 from md_utils import path_utils
 from md_utils import ct_utils
-
 from md_visualization import visualization_utils as vis_utils
-
 from detection.run_detector import CONF_DIGITS, COORD_DIGITS
 
 
 #%% Support functions
 
 def read_classes_from_yolo_dataset_file(fn):
     """
-    Read a dictionary mapping integer class IDs to class names from a YOLOv5/YOLOv8
+    Reads a dictionary mapping integer class IDs to class names from a YOLOv5/YOLOv8
     dataset.yaml file or a .json file.  A .json file should contain a dictionary mapping
     integer category IDs to string category names.
+    
+    Args:
+        fn (str): YOLOv5/YOLOv8 dataset file with a .yml or .yaml extension, or a .json file 
+            mapping integer category IDs to category names.
+            
+    Returns:
+        dict: a mapping from integer category IDs to category names
     """
         
     if fn.endswith('.yml') or fn.endswith('.yaml'):
         
         with open(fn,'r') as f:
             lines = f.readlines()
                 
@@ -88,53 +88,50 @@
                 category_id_to_name[int(k)] = d_in[k]
         
     else:
         
         raise ValueError('Unrecognized category file type: {}'.format(fn))
         
     assert len(category_id_to_name) > 0, 'Failed to read class mappings from {}'.format(fn)
+    
     return category_id_to_name
     
 
-def yolo_json_output_to_md_output(yolo_json_file, image_folder,
-                                  output_file, yolo_category_id_to_name,                              
+def yolo_json_output_to_md_output(yolo_json_file, 
+                                  image_folder,
+                                  output_file, 
+                                  yolo_category_id_to_name,                              
                                   detector_name='unknown',
                                   image_id_to_relative_path=None,
                                   offset_yolo_class_ids=True,
                                   truncate_to_standard_md_precision=True,
                                   image_id_to_error=None):
     """
-    Convert a YOLOv5 .json file to MD .json format.
+    Converts a YOLOv5/YOLOv8 .json file to MD .json format.
     
     Args:
         
-    - yolo_json_file: the .json file to convert from YOLOv5 format to MD output format.
-    
-    - image_folder: the .json file contains relative path names, this is the path base.
-    
-    - yolo_category_id_to_name: the .json file contains only numeric identifiers for
-      categories, but we want names and numbers for the output format; this is a 
-      dict mapping numbers to names.  Can also be a YOLOv5 dataset.yaml file.
-    
-    - detector_name: a string that gets put in the output file, not otherwise used within
-      this function.
-    
-    - image_id_to_relative_path: YOLOv5 .json uses only basenames (e.g. abc1234.JPG);
-      by default these will be appended to the input path to create pathnames, so if you
-      have a flat folder, this is fine.  If you want to map base names to relative paths, use
-      this dict.
-    
-    - offset_yolo_class_ids: YOLOv5 class IDs always start at zero; if you want to make the 
-      output classes start at 1, set offset_yolo_class_ids to True.
-    
-    - truncate_to_standard_md_precision: YOLOv5 .json includes lots of (not-super-meaningful)
-      precision, set this to truncate to COORD_DIGITS and CONF_DIGITS.
-    
-    - image_id_to_error: if you want to include image IDs in the output file for which you couldn't
-      prepare the input file in the first place due to errors, include them here.
+        yolo_json_file (str): the .json file to convert from YOLOv5 format to MD output format
+        image_folder (str): the .json file contains relative path names, this is the path base
+        yolo_category_id_to_name (str or dict): the .json results file contains only numeric 
+            identifiers for categories, but we want names and numbers for the output format; 
+            yolo_category_id_to_name provides that mapping either as a dict or as a YOLOv5 
+            dataset.yaml file.
+        detector_name (str, optional): a string that gets put in the output file, not otherwise 
+            used within this function
+        image_id_to_relative_path (dict, optional): YOLOv5 .json uses only basenames (e.g. 
+            abc1234.JPG); by default these will be appended to the input path to create pathnames.
+            If you have a flat folder, this is fine.  If you want to map base names to relative paths in
+            a more complicated way, use this parameter.   
+        offset_yolo_class_ids (bool, optional): YOLOv5 class IDs always start at zero; if you want to 
+            make the output classes start at 1, set offset_yolo_class_ids to True.    
+        truncate_to_standard_md_precision (bool, optional): YOLOv5 .json includes lots of 
+            (not-super-meaningful) precision, set this to truncate to COORD_DIGITS and CONF_DIGITS.
+        image_id_to_error (dict, optional): if you want to include image IDs in the output file for which 
+            you couldn't prepare the input file in the first place due to errors, include them here.
     """    
         
     assert os.path.isfile(yolo_json_file), \
         'Could not find YOLO .json file {}'.format(yolo_json_file)
     assert os.path.isdir(image_folder), \
         'Could not find image folder {}'.format(image_folder)
       
@@ -310,22 +307,33 @@
     
     with open(output_file,'w') as f:
         json.dump(d,f,indent=1)
             
 # ...def yolo_json_output_to_md_output(...)
     
 
-def yolo_txt_output_to_md_output(input_results_folder, image_folder,
-                                 output_file, detector_tag=None):
+def yolo_txt_output_to_md_output(input_results_folder, 
+                                 image_folder,
+                                 output_file, 
+                                 detector_tag=None):
     """
-    Converts a folder of YOLO-outptu .txt files to MD .json format.
+    Converts a folder of YOLO-output .txt files to MD .json format.
     
     Less finished than the .json conversion function; this .txt conversion assumes 
     a hard-coded mapping representing the standard MD categories (in MD indexing, 
     1/2/3=animal/person/vehicle; in YOLO indexing, 0/1/2=animal/person/vehicle).
+    
+    Args:
+        input_results_folder (str): the folder containing YOLO-output .txt files
+        image_folder (str): the folder where images live, may be the same as
+            [input_results_folder]
+        output_file (str): the MD-formatted .json file to which we should write
+            results
+        detector_tag (str, optional): string to put in the 'detector' field in the
+            output file            
     """
     
     assert os.path.isdir(input_results_folder)
     assert os.path.isdir(image_folder)
     
     ## Enumerate results files and image files
     
@@ -422,7 +430,12 @@
 
     #%%    
     
     input_results_folder = os.path.expanduser('~/tmp/model-version-experiments/pt-test-kru/exp/labels')
     image_folder = os.path.expanduser('~/data/KRU-test')
     output_file = os.path.expanduser('~/data/mdv5a-yolo-pt-kru.json')    
     yolo_txt_output_to_md_output(input_results_folder,image_folder,output_file)
+
+
+#%% Command-line driver
+
+# TODO
```

### Comparing `megadetector-5.0.8/detection/detector_training/model_main_tf2.py` & `megadetector-5.0.9/detection/detector_training/model_main_tf2.py`

 * *Files identical despite different names*

### Comparing `megadetector-5.0.8/detection/process_video.py` & `megadetector-5.0.9/detection/process_video.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,24 @@
-########
-#
-# process_video.py
-#
-# Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,
-# and optionally stitch together results into a new video with detection boxes.
-#
-########
+"""
+
+process_video.py
+
+Splits a video (or folder of videos) into frames, runs the frames through run_detector_batch.py,
+and optionally stitches together results into a new video with detection boxes.
+
+Operates by separating the video into frames, typically sampling every Nth frame, and writing
+those frames to disk, before running MD.  This approach clearly has a downside: it requires
+a bunch more disk space, compared to extracting frames and running MD on them without ever
+writing them to disk.  The upside, though, is that this approach allows you to run repeat
+detection elimination after running MegaDetector, and it allows allows more efficient re-use
+of frames if you end up running MD more than once, or running multiple versions of MD.
+
+TODO: optionally skip writing frames to disk, and process frames in memory.
+
+"""
 
 #%% Imports
 
 import os
 import sys
 import tempfile
 import argparse
@@ -25,81 +34,121 @@
 from detection.video_utils import frame_results_to_video_results
 from detection.video_utils import video_folder_to_frames
 from uuid import uuid1
 
 from detection.video_utils import default_fourcc
 
 
-#%% Options classes
+#%% Classes
 
 class ProcessVideoOptions:
-
-    # Can be a model filename (.pt or .pb) or a model name (e.g. "MDV5A")
+    """
+    Options controlling the behavior of process_video()
+    """
+    
+    #: Can be a model filename (.pt or .pb) or a model name (e.g. "MDV5A")
     model_file = 'MDV5A'
     
-    # Can be a file or a folder
+    #: Video (of folder of videos) to process
     input_video_file = ''
 
+    #: .json file to which we should write results
     output_json_file = None
     
-    # Only relevant if render_output_video is True
+    #: File to which we should write a video with boxes, only relevant if 
+    #: render_output_video is True
     output_video_file = None
     
-    # Folder to use for extracted frames
+    #: Folder to use for extracted frames; will use a folder in system temp space
+    #: if this is None
     frame_folder = None
     
-    # Folder to use for rendered frames (if rendering output video)
+    # Folder to use for rendered frames (if rendering output video); will use a folder 
+    #: in system temp space if this is None
     frame_rendering_folder = None
     
-    # Should we render a video with detection boxes?
-    #
-    # Only supported when processing a single video, not a folder.
+    #: Should we render a video with detection boxes?
+    #:
+    #: Only supported when processing a single video, not a folder.
     render_output_video = False
     
-    # If we are rendering boxes to a new video, should we keep the temporary
-    # rendered frames?
+    #: If we are rendering boxes to a new video, should we keep the temporary
+    #: rendered frames?
     keep_rendered_frames = False
     
-    # Should we keep the extracted frames?
+    #: Should we keep the extracted frames?
     keep_extracted_frames = False
     
-    # Should we delete the entire folder the extracted frames are written to?
-    #
-    # By default, we delete the frame files but leave the (probably-empty) folder in place.
+    #: Should we delete the entire folder the extracted frames are written to?
+    #:
+    #: By default, we delete the frame files but leave the (probably-empty) folder in place, 
+    #: for no reason other than being paranoid about deleting folders.
     force_extracted_frame_folder_deletion = False
     
-    # Should we delete the entire folder the rendered frames are written to?
-    #
-    # By default, we delete the frame files but leave the (probably-empty) folder in place.
+    #: Should we delete the entire folder the rendered frames are written to?
+    #:
+    #: By default, we delete the frame files but leave the (probably-empty) folder in place,
+    #: for no reason other than being paranoid about deleting folders.
     force_rendered_frame_folder_deletion = False
-        
+     
+    #: If we've already run MegaDetector on this video or folder of videos, i.e. if we 
+    #: find a corresponding MD results file, should we re-use it?  Defaults to reprocessing.
     reuse_results_if_available = False
+    
+    #: If we've already split this video or folder of videos into frames, should we 
+    #: we re-use those extracted frames?  Defaults to reprocessing.
     reuse_frames_if_available = False
     
+    #: If [input_video_file] is a folder, should we search for videos recursively?
     recursive = False 
+    
+    #: Enable additional debug console output
     verbose = False
     
+    #: fourcc code to use for writing videos; only relevant if render_output_video is True
     fourcc = None
 
+    #: Confidence threshold to use for writing videos with boxes, only relevant if
+    #: if render_output_video is True.  Defaults to choosing a reasonable threshold
+    #: based on the model version.
     rendering_confidence_threshold = None
+    
+    #: Detections below this threshold will not be included in the output file.
     json_confidence_threshold = 0.005
+    
+    #: Sample every Nth frame; set to None (default) or 1 to sample every frame.  Typically
+    #: we sample down to around 3 fps, so for typical 30 fps videos, frame_sample=10 is a 
+    #: typical value.
     frame_sample = None
     
+    #: Number of workers to use for parallelization; set to <= 1 to disable parallelization
     n_cores = 1
 
+    #: For debugging only, stop processing after a certain number of frames.
     debug_max_frames = -1
     
+    #: File containing non-standard categories, typically only used if you're running a non-MD
+    #: detector.
     class_mapping_filename = None
 
+# ...class ProcessVideoOptions
+
 
 #%% Functions
 
 def process_video(options):
     """
-    Process a single video
+    Process a single video through MD, optionally writing a new video with boxes
+    
+    Args: 
+        options (ProcessVideoOptions): all the parameters used to control this process,
+            including filenames; see ProcessVideoOptions for details
+            
+    Returns:
+        dict: frame-level MegaDetector results, identical to what's in the output .json file
     """
 
     if options.output_json_file is None:
         options.output_json_file = options.input_video_file + '.json'
 
     if options.render_output_video and (options.output_video_file is None):
         options.output_video_file = options.input_video_file + '.detections.mp4'
@@ -225,15 +274,19 @@
     return results
 
 # ...process_video()
 
 
 def process_video_folder(options):
     """
-    Process a folder of videos    
+    Process a folder of videos through MD
+    
+    Args: 
+        options (ProcessVideoOptions): all the parameters used to control this process,
+            including filenames; see ProcessVideoOptions for details            
     """
     
     ## Validate options
 
     assert os.path.isdir(options.input_video_file), \
         '{} is not a folder'.format(options.input_video_file)
            
@@ -424,16 +477,15 @@
             else:
                 for frame_fn in image_file_names:
                     os.remove(frame_fn)
         except Exception as e:
             print('Warning: error deleting frames from folder {}:\n{}'.format(
                 frame_output_folder,str(e)))
             pass
-        
-    
+
 # ...process_video_folder()
 
 
 def options_to_command(options):
     
     cmd = 'python process_video.py'
     cmd += ' "' + options.model_file + '"'
@@ -543,15 +595,15 @@
 #%% Command-line driver
 
 def main():
 
     default_options = ProcessVideoOptions()
     
     parser = argparse.ArgumentParser(description=(
-        'Run MegaDetector on each frame in a video (or every Nth frame), optionally '\
+        'Run MegaDetector on each frame (or every Nth frame) in a video (or folder of videos), optionally '\
         'producing a new video with detections annotated'))
 
     parser.add_argument('model_file', type=str,
                         help='MegaDetector model file (.pt or .pb) or model name (e.g. "MDV5A")')
 
     parser.add_argument('input_video_file', type=str,
                         help='video file (or folder) to process')
```

### Comparing `megadetector-5.0.8/detection/pytorch_detector.py` & `megadetector-5.0.9/detection/pytorch_detector.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-########
-#
-# pytorch_detector.py
-#
-# Module to run MegaDetector v5, a PyTorch YOLOv5 animal detection model.
-#
-########
+"""
+
+pytorch_detector.py
 
-#%% Imports
+Module to run MegaDetector v5, a PyTorch YOLOv5 animal detection model.
+
+"""
+
+#%% Imports and constants
 
 import torch
 import numpy as np
 import traceback
 
 from detection.run_detector import CONF_DIGITS, COORD_DIGITS, FAILURE_INFER
 from md_utils import ct_utils
@@ -100,20 +100,27 @@
 print(f'Using PyTorch version {torch.__version__}')
 
     
 #%% Classes
 
 class PTDetector:
 
-    IMAGE_SIZE = 1280  # image size used in training
+    #: Image size passed to YOLOv5's letterbox() function; 1280 means "1280 on the long side, preserving 
+    #: aspect ratio"
+    #:
+    #: :meta private:
+    IMAGE_SIZE = 1280
+    
+    #: Stride size passed to YOLOv5's letterbox() function
+    #:
+    #: :meta private:
     STRIDE = 64
 
-    def __init__(self, model_path: str, 
-                 force_cpu: bool = False,
-                 use_model_native_classes: bool = False):
+    def __init__(self, model_path, force_cpu=False, use_model_native_classes= False):
+        
         self.device = 'cpu'
         if not force_cpu:
             if torch.cuda.is_available():
                 self.device = torch.device('cuda:0')
             try:
                 if torch.backends.mps.is_built and torch.backends.mps.is_available():
                     self.device = 'mps'
@@ -158,29 +165,34 @@
             
         return model
 
     def generate_detections_one_image(self, img_original, image_id='unknown', 
                                       detection_threshold=0.00001, image_size=None,
                                       skip_image_resizing=False):
         """
-        Apply the detector to an image.
+        Applies the detector to an image.
 
         Args:
-            img_original: the PIL Image object with EXIF rotation taken into account
-            image_id: a path to identify the image; will be in the "file" field of the output object
-            detection_threshold: confidence above which to include the detection proposal
-            skip_image_resizing: whether to skip internal image resizing and rely on external resizing
+            img_original (Image): the PIL Image object with EXIF rotation taken into account
+            image_id (str, optional): a path to identify the image; will be in the "file" field 
+                of the output object
+            detection_threshold (float, optional): only detections above this confidence threshold 
+                will be included in the return value
+            image_size (tuple, optional): image size to use for inference, only mess with this
+                if (a) you're using a model other than MegaDetector or (b) you know what you're
+                doing
+            skip_image_resizing (bool, optional): whether to skip internal image resizing (and rely on external 
+                resizing)
 
         Returns:
-        A dict with the following fields, see the 'images' key in https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing#batch-processing-api-output-format
-            - 'file' (always present)
-            - 'max_detection_conf' (removed from MegaDetector output by default, but generated here)
-            - 'detections', which is a list of detection objects containing keys 'category', 
-              'conf' and 'bbox'
-            - 'failure'
+            dict: a dictionary with the following fields:
+                - 'file' (filename, always present)
+                - 'max_detection_conf' (removed from MegaDetector output files by default, but generated here)
+                - 'detections' (a list of detection objects containing keys 'category', 'conf', and 'bbox')
+                - 'failure' (a failure string, or None if everything went fine)
         """
 
         result = {
             'file': image_id
         }
         detections = []
         max_conf = 0.0
@@ -293,21 +305,27 @@
             traceback.print_exc(e)
 
         result['max_detection_conf'] = max_conf
         result['detections'] = detections
 
         return result
 
+    # ...def generate_detections_one_image(...)
+
+# ...class PTDetector
+
 
 #%% Command-line driver
 
+# For testing only... you don't really want to run this module directly.
+
 if __name__ == '__main__':
-    
-    # For testing only... you don't really want to run this module directly
 
+    pass
+    
     #%%
     
     import md_visualization.visualization_utils as vis_utils
     import os
     
     model_file = 'MDV5A'
     im_file = os.path.expanduser('~/git/MegaDetector/images/nacti.jpg')
```

### Comparing `megadetector-5.0.8/detection/run_detector.py` & `megadetector-5.0.9/detection/run_detector.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,44 +1,30 @@
-########
-#
-# run_detector.py
-#
-# Module to run an animal detection model on images.
-# 
-# The main function in this script also renders the predicted
-# bounding boxes on images and saves the resulting images (with bounding boxes).
-# 
-# This script is not a good way to process lots of images (tens of thousands,
-# say). It does not facilitate checkpointing the results so if it crashes you
-# would have to start from scratch. If you want to run a detector (e.g., ours)
-# on lots of images, you should check out run_detector_batch.py.
-# 
-# To run this script, we recommend you set up a conda virtual environment
-# following instructions in the Installation section on the main README, using
-# `environment-detector.yml` as the environment file where asked.
-# 
-# This is a good way to test our detector on a handful of images and get
-# super-satisfying, graphical results.  It's also a good way to see how fast a
-# detector model will run on a particular machine.
-# 
-# If you would like to *not* use the GPU on the machine, set the environment
-# variable CUDA_VISIBLE_DEVICES to "-1".
-# 
-# If no output directory is specified, writes detections for c:\foo\bar.jpg to
-# c:\foo\bar_detections.jpg.
-# 
-# This script will only consider detections with > 0.005 confidence at all times.
-# The `threshold` you provide is only for rendering the results. If you need to
-# see lower-confidence detections, you can change
-# DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD.
-# 
-# Reference:
-# https://github.com/tensorflow/models/blob/master/research/object_detection/inference/detection_inference.py
-# 
-########
+"""
+
+run_detector.py
+
+Module to run an animal detection model on images.  The main function in this script also renders 
+the predicted bounding boxes on images and saves the resulting images (with bounding boxes).
+
+**This script is not a good way to process lots of images**.  It does not produce a useful
+output format, and it does not facilitate checkpointing the results so if it crashes you
+would have to start from scratch. **If you want to run a detector on lots of images, you should 
+check out run_detector_batch.py**.
+
+That said, this script (run_detector.py) is a good way to test our detector on a handful of images 
+and get super-satisfying, graphical results.
+
+If you would like to *not* use the GPU on the machine, set the environment
+variable CUDA_VISIBLE_DEVICES to "-1".
+
+This script will only consider detections with > 0.005 confidence at all times.
+The threshold you provide is only for rendering the results. If you need to
+see lower-confidence detections, you can change DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD.
+
+"""
 
 #%% Constants, imports, environment
 
 import argparse
 import os
 import statistics
 import sys
@@ -159,32 +145,46 @@
 }
     
 
 #%% Utility functions
 
 def convert_to_tf_coords(array):
     """
-    From [x1, y1, width, height] to [y1, x1, y2, x2], where x1 is x_min, x2 is x_max
-
-    This is only used to keep the interface of the synchronous API.
+    Converts a bounding box from [x1, y1, width, height] to [y1, x1, y2, x2].  This 
+    is mostly not helpful, this function only exists to maintain backwards compatibility
+    in the synchronous API, which possibly zero people in the world are using.
+    
+    Args:
+        array (list): a bounding box in [x,y,w,h] format
+        
+    Returns:
+        list: a bounding box in [y1,x1,y2,x2] format
     """
     
     x1 = array[0]
     y1 = array[1]
     width = array[2]
     height = array[3]
     x2 = x1 + width
     y2 = y1 + height
+    
     return [y1, x1, y2, x2]
 
 
 def get_detector_metadata_from_version_string(detector_version):
     """
-    Given a MegaDetector version string (e.g. "v4.1.0"), return the metadata for
+    Given a MegaDetector version string (e.g. "v4.1.0"), returns the metadata for
     the model.  Used for writing standard defaults to batch output files.
+    
+    Args:
+        detector_version (str): a detection version string, e.g. "v4.1.0", which you
+            can extract from a filename using get_detector_version_from_filename()
+    
+    Returns:
+        dict: metadata for this model, suitable for writing to a MD output file
     """
     
     if detector_version not in DETECTOR_METADATA:
         print('Warning: no metadata for unknown detector version {}'.format(detector_version))
         default_detector_metadata = {
             'megadetector_version':'unknown',
             'typical_detection_threshold':0.5,
@@ -192,28 +192,34 @@
         }
         return default_detector_metadata
     else:
         return DETECTOR_METADATA[detector_version]
 
 
 def get_detector_version_from_filename(detector_filename):
-    """
-    Get the version number component of the detector from the model filename.  
+    r"""
+    Gets the version number component of the detector from the model filename.  
     
-    *detector_filename* will almost always end with one of the following:
+    [detector_filename] will almost always end with one of the following:
         
-    megadetector_v2.pb
-    megadetector_v3.pb
-    megadetector_v4.1 (not produed by run_detector_batch.py, only found in Azure Batch API output files)
-    md_v4.1.0.pb
-    md_v5a.0.0.pt
-    md_v5b.0.0.pt
+    * megadetector_v2.pb
+    * megadetector_v3.pb
+    * megadetector_v4.1 (not produed by run_detector_batch.py, only found in output files from the deprecated Azure Batch API)
+    * md_v4.1.0.pb
+    * md_v5a.0.0.pt
+    * md_v5b.0.0.pt
     
-    ...for which we identify the version number as "v2.0.0", "v3.0.0", "v4.1.0", 
+    This function identifies the version number as "v2.0.0", "v3.0.0", "v4.1.0", 
     "v4.1.0", "v5a.0.0", and "v5b.0.0", respectively.
+    
+    Args:
+        detector_filename (str): model filename, e.g. c:/x/z/md_v5a.0.0.pt
+    
+    Returns:
+        str: a detector version string, e.g. "v5a.0.0", or "multiple" if I'm confused
     """
     
     fn = os.path.basename(detector_filename).lower()
     matches = []
     for s in model_string_to_model_version.keys():
         if s in fn:
             matches.append(s)
@@ -224,18 +230,28 @@
         print('Warning: multiple MegaDetector versions for model file {}'.format(detector_filename))
         return 'multiple'
     else:
         return model_string_to_model_version[matches[0]]
     
 
 def estimate_md_images_per_second(model_file, device_name=None):
-    """
-    Estimate how fast MegaDetector will run based on benchmarks.  Defaults to querying
+    r"""
+    Estimates how fast MegaDetector will run, based on benchmarks.  Defaults to querying
     the current device.  Returns None if no data is available for the current card/model.
-    Estimates only available for a small handful of GPUs.
+    Estimates only available for a small handful of GPUs.  Uses an absurdly simple lookup
+    approach, e.g. if the string "4090" appears in the device name, congratulations,
+    you have an RTX 4090.
+    
+    Args:
+        model_file (str): model filename, e.g. c:/x/z/md_v5a.0.0.pt
+        device_name (str, optional): device name, e.g. blah-blah-4090-blah-blah
+    
+    Returns:
+        float: the approximate number of images this model version can process on this
+        device per second
     """
     
     if device_name is None:
         try:
             import torch
             device_name = torch.cuda.get_device_name()
         except Exception as e:
@@ -267,16 +283,22 @@
     else:
         print('Could not estimate inference speed for model file {}'.format(model_file))
         return None
     
     
 def get_typical_confidence_threshold_from_results(results):
     """
-    Given the .json data loaded from a MD results file, determine a typical confidence
+    Given the .json data loaded from a MD results file, returns a typical confidence
     threshold based on the detector version.
+    
+    Args:
+        results (dict): a dict of MD results, as it would be loaded from a MD results .json file
+    
+    Returns:
+        float: a sensible default threshold for this model
     """
     
     if 'detector_metadata' in results['info'] and \
         'typical_detection_threshold' in results['info']['detector_metadata']:
         default_threshold = results['info']['detector_metadata']['typical_detection_threshold']
     elif ('detector' not in results['info']) or (results['info']['detector'] is None):
         print('Warning: detector version not available in results file, using MDv5 defaults')
@@ -289,18 +311,24 @@
         detector_metadata = get_detector_metadata_from_version_string(detector_version)
         default_threshold = detector_metadata['typical_detection_threshold']
 
     return default_threshold    
 
     
 def is_gpu_available(model_file):
-    """
-    Decide whether a GPU is available, importing PyTorch or TF depending on the extension
+    r"""
+    Determines whether a GPU is available, importing PyTorch or TF depending on the extension
     of model_file.  Does not actually load model_file, just uses that to determine how to check 
-    for GPU availability.
+    for GPU availability (PT vs. TF).
+    
+    Args:
+        model_file (str): model filename, e.g. c:/x/z/md_v5a.0.0.pt
+    
+    Returns:
+        bool: whether a GPU is available
     """
     
     if model_file.endswith('.pb'):
         import tensorflow.compat.v1 as tf
         gpu_available = tf.test.is_gpu_available()
         print('TensorFlow version:', tf.__version__)
         print('tf.test.is_gpu_available:', gpu_available)                
@@ -319,16 +347,22 @@
                 pass
         return gpu_available
     else:
         raise ValueError('Unrecognized model file extension for model {}'.format(model_file))
 
 
 def load_detector(model_file, force_cpu=False):
-    """
-    Load a TF or PT detector, depending on the extension of model_file.
+    r"""
+    Loads a TF or PT detector, depending on the extension of model_file.
+    
+    Args:
+        model_file (str): model filename, e.g. c:/x/z/md_v5a.0.0.pt
+    
+    Returns:
+        object: loaded detector object
     """
     
     # Possibly automatically download the model
     model_file = try_download_known_detector(model_file)
     
     start_time = time.time()
     if model_file.endswith('.pb'):
@@ -340,27 +374,49 @@
     elif model_file.endswith('.pt'):
         from detection.pytorch_detector import PTDetector
         detector = PTDetector(model_file, force_cpu, USE_MODEL_NATIVE_CLASSES)        
     else:
         raise ValueError('Unrecognized model format: {}'.format(model_file))
     elapsed = time.time() - start_time
     print('Loaded model in {}'.format(humanfriendly.format_timespan(elapsed)))
+    
     return detector
 
 
 #%% Main function
 
-def load_and_run_detector(model_file, image_file_names, output_dir,
+def load_and_run_detector(model_file, 
+                          image_file_names,
+                          output_dir,
                           render_confidence_threshold=DEFAULT_RENDERING_CONFIDENCE_THRESHOLD,
-                          crop_images=False, box_thickness=DEFAULT_BOX_THICKNESS, 
-                          box_expansion=DEFAULT_BOX_EXPANSION, image_size=None,
-                          label_font_size=DEFAULT_LABEL_FONT_SIZE
+                          crop_images=False, 
+                          box_thickness=DEFAULT_BOX_THICKNESS, 
+                          box_expansion=DEFAULT_BOX_EXPANSION,
+                          image_size=None,
+                          label_font_size=DEFAULT_LABEL_FONT_SIZE                          
                           ):
-    """
-    Load and run detector on target images, and visualize the results.
+    r"""
+    Loads and runs a detector on target images, and visualizes the results.
+    
+    Args:
+        model_file (str): model filename, e.g. c:/x/z/md_v5a.0.0.pt, or a known model
+            string, e.g. "MDV5A"
+        image_file_names (list): list of absolute paths to process
+        output_dir (str): folder to write visualized images to
+        render_confidence_threshold (float, optional): only render boxes for detections
+            above this threshold
+        crop_images (bool, optional): whether to crop detected objects to individual images
+            (default is to render images with boxes, rather than cropping)
+        box_thickness (float, optional): thickness in pixels for box rendering
+        box_expansion (float, optional): box expansion in pixels
+        image_size (tuple, optional): image size to use for inference, only mess with this
+            if (a) you're using a model other than MegaDetector or (b) you know what you're
+            doing
+        label_font_size (float, optional): font size to use for displaying class names
+            and confidence values in the rendered images        
     """
     
     if len(image_file_names) == 0:
         print('Warning: no files available')
         return
 
     # Possibly automatically download the model
@@ -503,15 +559,20 @@
                                                       std_dev_time_infer))
 
 # ...def load_and_run_detector()
 
 
 def download_model(model_name,force_download=False):
     """
-    Download one of the known models to local temp space if it hasn't already been downloaded
+    Downloads one of the known models to local temp space if it hasn't already been downloaded.
+    
+    Args:
+        model_name (str): a known model string, e.g. "MDV5A"
+        force_download (bool, optional): whether download the model even if the local target 
+            file already exists
     """
     
     import tempfile
     from md_utils.url_utils import download_url
     model_tempdir = os.path.join(tempfile.gettempdir(), 'megadetector_models')    
     os.makedirs(model_tempdir,exist_ok=True)
     
@@ -532,17 +593,25 @@
     local_file = download_url(url, destination_filename=destination_filename, progress_updater=None, 
                      force_download=force_download, verbose=True)
     return local_file
 
 
 def try_download_known_detector(detector_file):
     """
-    Check whether detector_file is really the name of a known model, in which case we will
+    Checks whether detector_file is really the name of a known model, in which case we will
     either read the actual filename from the corresponding environment variable or download
     (if necessary) to local temp space.  Otherwise just returns the input string.
+    
+    Args:
+        detector_file (str): a known model string (e.g. "MDV5A"), or any other string (in which
+            case this function is a no-op)
+    
+    Returns:
+        str: the local filename to which the model was downloaded, or the same string that
+        was passed in, if it's not recognized as a well-known model name
     """
     
     if detector_file in downloadable_models:
         if detector_file in os.environ:
             fn = os.environ[detector_file]
             print('Reading MD location from environment variable {}: {}'.format(
                 detector_file,fn))
@@ -602,15 +671,15 @@
         help=('Confidence threshold between 0 and 1.0; only render' + 
               ' boxes above this confidence (defaults to {})'.format(
               DEFAULT_RENDERING_CONFIDENCE_THRESHOLD)))
     
     parser.add_argument(
         '--crop',
         default=False,
-        action="store_true",
+        action='store_true',
         help=('If set, produces separate output images for each crop, '
               'rather than adding bounding boxes to the original image'))
     
     parser.add_argument(
         '--box_thickness',
         type=int,
         default=DEFAULT_BOX_THICKNESS,
@@ -626,15 +695,22 @@
     
     parser.add_argument(
         '--label_font_size',
         type=int,
         default=DEFAULT_LABEL_FONT_SIZE,
         help=('Label font size (defaults to {})'.format(
               DEFAULT_LABEL_FONT_SIZE)))
-        
+    
+    parser.add_argument(
+        '--process_likely_output_images',
+        action='store_true',
+        help=('By default, we skip images that end in {}, because they probably came from this script. '\
+              .format(DETECTION_FILENAME_INSERT) + \
+              'This option disables that behavior.'))
+    
     if len(sys.argv[1:]) == 0:
         parser.print_help()
         parser.exit()
 
     args = parser.parse_args()
 
     # If the specified detector file is really the name of a known model, find 
@@ -646,14 +722,24 @@
     assert 0.0 < args.threshold <= 1.0, 'Confidence threshold needs to be between 0 and 1'
 
     if args.image_file:
         image_file_names = [args.image_file]
     else:
         image_file_names = path_utils.find_images(args.image_dir, args.recursive)
 
+    # Optionally skip images that were probably generated by this script
+    if not args.process_likely_output_images:
+        image_file_names_valid = []
+        for fn in image_file_names:
+            if os.path.splitext(fn)[0].endswith(DETECTION_FILENAME_INSERT):
+                print('Skipping likely output image {}'.format(fn))
+            else:
+                image_file_names_valid.append(fn)
+        image_file_names = image_file_names_valid
+                
     print('Running detector on {} images...'.format(len(image_file_names)))
 
     if args.output_dir:
         os.makedirs(args.output_dir, exist_ok=True)
     else:
         if args.image_dir:
             args.output_dir = args.image_dir
@@ -667,15 +753,14 @@
                           render_confidence_threshold=args.threshold,
                           box_thickness=args.box_thickness,
                           box_expansion=args.box_expansion,                          
                           crop_images=args.crop,
                           image_size=args.image_size,
                           label_font_size=args.label_font_size)
 
-
 if __name__ == '__main__':
     main()
 
 
 #%% Interactive driver
 
 if False:
```

### Comparing `megadetector-5.0.8/detection/run_detector_batch.py` & `megadetector-5.0.9/detection/run_detector_batch.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,42 +1,47 @@
-########
-#
-# run_detector_batch.py
-#
-# Module to run MegaDetector on lots of images, writing the results
-# to a file in the same format produced by our batch API:
-# 
-# https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing
-# 
-# This enables the results to be used in our post-processing pipeline; see
-# api/batch_processing/postprocessing/postprocess_batch_results.py .
-# 
-# This script can save results to checkpoints intermittently, in case disaster
-# strikes. To enable this, set --checkpoint_frequency to n > 0, and results 
-# will be saved as a checkpoint every n images. Checkpoints will be written 
-# to a file in the same directory as the output_file, and after all images
-# are processed and final results file written to output_file, the temporary
-# checkpoint file will be deleted. If you want to resume from a checkpoint, set
-# the checkpoint file's path using --resume_from_checkpoint.
-# 
-# The `threshold` you can provide as an argument is the confidence threshold above
-# which detections will be included in the output file.
-# 
-# Has preliminary multiprocessing support for CPUs only; if a GPU is available, it will
-# use the GPU instead of CPUs, and the --ncores option will be ignored.  Checkpointing
-# is not supported when using a GPU.
-# 
-# Does not have a command-line option to bind the process to a particular GPU, but you can 
-# prepend with "CUDA_VISIBLE_DEVICES=0 ", for example, to bind to GPU 0, e.g.:
-# 
-# CUDA_VISIBLE_DEVICES=0 python detection/run_detector_batch.py md_v4.1.0.pb ~/data ~/mdv4test.json 
-#
-# You can disable GPU processing entirely by setting CUDA_VISIBLE_DEVICES=''.
-#
-########
+"""
+
+run_detector_batch.py
+
+Module to run MegaDetector on lots of images, writing the results
+to a file in the MegaDetector results format.
+
+https://github.com/agentmorris/MegaDetector/tree/main/api/batch_processing#megadetector-batch-output-format
+
+This enables the results to be used in our post-processing pipeline; see
+api/batch_processing/postprocessing/postprocess_batch_results.py .
+
+This script can save results to checkpoints intermittently, in case disaster
+strikes. To enable this, set --checkpoint_frequency to n > 0, and results 
+will be saved as a checkpoint every n images. Checkpoints will be written 
+to a file in the same directory as the output_file, and after all images
+are processed and final results file written to output_file, the temporary
+checkpoint file will be deleted. If you want to resume from a checkpoint, set
+the checkpoint file's path using --resume_from_checkpoint.
+
+The `threshold` you can provide as an argument is the confidence threshold above
+which detections will be included in the output file.
+
+Has multiprocessing support for CPUs only; if a GPU is available, it will
+use the GPU instead of CPUs, and the --ncores option will be ignored.  Checkpointing
+is not supported when using a GPU.
+
+The lack of GPU multiprocessing support might sound annoying, but in practice we
+run a gazillion MegaDetector images on multiple GPUs using this script, we just only use
+one GPU *per invocation of this script*.  Dividing a big batch of images into one chunk
+per GPU happens outside of this script.
+
+Does not have a command-line option to bind the process to a particular GPU, but you can 
+prepend with "CUDA_VISIBLE_DEVICES=0 ", for example, to bind to GPU 0, e.g.:
+
+CUDA_VISIBLE_DEVICES=0 python detection/run_detector_batch.py md_v4.1.0.pb ~/data ~/mdv4test.json 
+
+You can disable GPU processing entirely by setting CUDA_VISIBLE_DEVICES=''.
+
+"""
 
 #%% Constants, imports, environment
 
 import argparse
 import json
 import os
 import sys
@@ -87,15 +92,15 @@
 exif_options = read_exif.ReadExifOptions()
 exif_options.processing_library = 'pil'
 exif_options.byte_handling = 'convert_to_string'
 
 
 #%% Support functions for multiprocessing
 
-def producer_func(q,image_files):
+def _producer_func(q,image_files):
     """ 
     Producer function; only used when using the (optional) image queue.
     
     Reads up to N images from disk and puts them on the blocking queue for processing.
     """
     
     if verbose:
@@ -116,15 +121,15 @@
         q.put([im_file,image])
     
     q.put(None)
         
     print('Finished image loading'); sys.stdout.flush()
     
     
-def consumer_func(q,return_queue,model_file,confidence_threshold,image_size=None):
+def _consumer_func(q,return_queue,model_file,confidence_threshold,image_size=None):
     """ 
     Consumer function; only used when using the (optional) image queue.
     
     Pulls images from a blocking queue and processes them.
     """
     
     if verbose:
@@ -173,45 +178,58 @@
 def run_detector_with_image_queue(image_files,model_file,confidence_threshold,
                                   quiet=False,image_size=None):
     """
     Driver function for the (optional) multiprocessing-based image queue; only used 
     when --use_image_queue is specified.  Starts a reader process to read images from disk, but 
     processes images in the  process from which this function is called (i.e., does not currently
     spawn a separate consumer process).
+    
+    Args:
+        image_files (str): list of absolute paths to images
+        model_file (str): filename or model identifier (e.g. "MDV5A")
+        confidence_threshold (float): minimum confidence detection to include in
+            output
+        quiet (bool, optional): suppress per-image console printouts
+        image_size (tuple, optional): image size to use for inference, only mess with this
+            if (a) you're using a model other than MegaDetector or (b) you know what you're
+            doing
+            
+    Returns:
+        list: list of dicts in the format returned by process_image()
     """
     
     q = multiprocessing.JoinableQueue(max_queue_size)
     return_queue = multiprocessing.Queue(1)
     
     if use_threads_for_queue:
-        producer = Thread(target=producer_func,args=(q,image_files,))
+        producer = Thread(target=_producer_func,args=(q,image_files,))
     else:
-        producer = Process(target=producer_func,args=(q,image_files,))
+        producer = Process(target=_producer_func,args=(q,image_files,))
     producer.daemon = False
     producer.start()
  
     # The queue system is a little more elegant if we start one thread for reading and one
     # for processing, and this works fine on Windows, but because we import TF at module load,
     # CUDA will only work in the main process, so currently the consumer function runs here.
     #
     # To enable proper multi-GPU support, we may need to move the TF import to a separate module
     # that isn't loaded until very close to where inference actually happens.
     run_separate_consumer_process = False
 
     if run_separate_consumer_process:
         if use_threads_for_queue:
-            consumer = Thread(target=consumer_func,args=(q,return_queue,model_file,
+            consumer = Thread(target=_consumer_func,args=(q,return_queue,model_file,
                                                          confidence_threshold,image_size,))
         else:
-            consumer = Process(target=consumer_func,args=(q,return_queue,model_file,
+            consumer = Process(target=_consumer_func,args=(q,return_queue,model_file,
                                                           confidence_threshold,image_size,))
         consumer.daemon = True
         consumer.start()
     else:
-        consumer_func(q,return_queue,model_file,confidence_threshold,image_size)
+        _consumer_func(q,return_queue,model_file,confidence_threshold,image_size)
 
     producer.join()
     print('Producer finished')
    
     if run_separate_consumer_process:
         consumer.join()
         print('Consumer finished')
@@ -222,43 +240,57 @@
     results = return_queue.get()
     
     return results
 
 
 #%% Other support functions
 
-def chunks_by_number_of_chunks(ls, n):
+def _chunks_by_number_of_chunks(ls, n):
     """
     Splits a list into n even chunks.
+    
+    External callers should use ct_utils.split_list_into_n_chunks().
 
-    Args
-    - ls: list
-    - n: int, # of chunks
+    Args:
+        ls (list): list to break up into chunks
+        n (int): number of chunks
     """
     
     for i in range(0, n):
         yield ls[i::n]
 
 
 #%% Image processing functions
 
 def process_images(im_files, detector, confidence_threshold, use_image_queue=False, 
-                   quiet=False, image_size=None, checkpoint_queue=None, include_image_size=False,
-                   include_image_timestamp=False, include_exif_data=False):
-    """
-    Runs MegaDetector over a list of image files.  As of 3/2024, this entry point is used when the
-    image queue is enabled, but not in the standard inference path (which loops over process_image()).
-
-    Args
-    - im_files: list of str, paths to image files
-    - detector: loaded model or str (path to .pb/.pt model file)
-    - confidence_threshold: float, only detections above this threshold are returned
+                   quiet=False, image_size=None, checkpoint_queue=None, 
+                   include_image_size=False, include_image_timestamp=False, 
+                   include_exif_data=False):
+    """
+    Runs a detector (typically MegaDetector) over a list of image files.  
+    As of 3/2024, this entry point is used when the image queue is enabled, but not in the 
+    standard inference path (which instead loops over process_image()).
+
+    Args:
+        im_files (list: paths to image files                                   
+        detector (str or detector object): loaded model or str; if this is a string, it can be a
+            path to a .pb/.pt model file or a known model identifier (e.g. "MDV5A")
+        confidence_threshold (float): only detections above this threshold are returned
+        use_image_queue (bool, optional): separate image loading onto a dedicated worker process
+        quiet (bool, optional): suppress per-image printouts
+        image_size (tuple, optional): image size to use for inference, only mess with this
+            if (a) you're using a model other than MegaDetector or (b) you know what you're
+            doing
+        checkpoint_queue (Queue, optional): internal parameter used to pass image queues around
+        include_image_size (bool, optional): should we include image size in the output for each image?
+        include_image_timestamp (bool, optional): should we include image timestamps in the output for each image?
+        include_exif_data (bool, optional): should we include EXIF data in the output for each image?        
 
-    Returns
-    - results: list of dict, each dict represents detections on one image
+    Returns:
+        list: list of dicts, in which each dict represents detections on one image,
         see the 'images' key in https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing#batch-processing-api-output-format
     """
     
     if isinstance(detector, str):
         start_time = time.time()
         detector = load_detector(detector)
         elapsed = time.time() - start_time
@@ -289,25 +321,34 @@
 
 
 def process_image(im_file, detector, confidence_threshold, image=None, 
                   quiet=False, image_size=None, include_image_size=False,
                   include_image_timestamp=False, include_exif_data=False,
                   skip_image_resizing=False):
     """
-    Runs MegaDetector on a single image file.
+    Runs a detector (typically MegaDetector) on a single image file.
 
-    Args
-    - im_file: str, path to image file
-    - detector: loaded model
-    - confidence_threshold: float, only detections above this threshold are returned
-    - image: previously-loaded image, if available
-    - skip_image_resizing: whether to skip internal image resizing and rely on external resizing
+    Args:
+        im_file (str): path to image file
+        detector (detector object): loaded model, this can no longer be a string by the time 
+            you get this far down the pipeline
+        confidence_threshold (float): only detections above this threshold are returned
+        image (Image, optional): previously-loaded image, if available, used when a worker
+            thread is handling image loads
+        quiet (bool, optional): suppress per-image printouts
+        image_size (tuple, optional): image size to use for inference, only mess with this
+            if (a) you're using a model other than MegaDetector or (b) you know what you're
+            doing        
+        include_image_size (bool, optional): should we include image size in the output for each image?
+        include_image_timestamp (bool, optional): should we include image timestamps in the output for each image?
+        include_exif_data (bool, optional): should we include EXIF data in the output for each image?                
+        skip_image_resizing (bool, optional): whether to skip internal image resizing and rely on external resizing
 
     Returns:
-    - result: dict representing detections on one image
+        dict: dict representing detections on one image,
         see the 'images' key in 
         https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing#batch-processing-api-output-format
     """
     
     if not quiet:
         print('Processing image {}'.format(im_file))
     
@@ -347,15 +388,15 @@
         result['exif_metadata'] = read_exif.read_pil_exif(image,exif_options)
 
     return result
 
 # ...def process_image(...)
 
 
-def load_custom_class_mapping(class_mapping_filename):
+def _load_custom_class_mapping(class_mapping_filename):
     """
     This is an experimental hack to allow the use of non-MD YOLOv5 models through
     the same infrastructure; it disables the code that enforces MDv5-like class lists.
     
     Should be a .json file that maps int-strings to strings, or a YOLOv5 dataset.yaml file.
     """
     
@@ -385,42 +426,58 @@
 def load_and_run_detector_batch(model_file, image_file_names, checkpoint_path=None,
                                 confidence_threshold=run_detector.DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD,
                                 checkpoint_frequency=-1, results=None, n_cores=1,
                                 use_image_queue=False, quiet=False, image_size=None, 
                                 class_mapping_filename=None, include_image_size=False, 
                                 include_image_timestamp=False, include_exif_data=False):
     """
-    Args
-    - model_file: path to model file, or supported model string (e.g. "MDV5A")
-    - image_file_names: list of strings (image filenames), a single image filename, 
-                        a folder to recursively search for images in, or a .json or .txt file
-                        containing a list of images.
-    - checkpoint_path: str, path to JSON checkpoint file
-    - confidence_threshold: float, only detections above this threshold are returned
-    - checkpoint_frequency: int, write results to JSON checkpoint file every N images
-    - results: list of dict, existing results loaded from checkpoint
-    - n_cores: int, # of CPU cores to use
-    - class_mapping_filename: str, use a non-default class mapping supplied in a .json file
-      or YOLOv5 dataset.yaml file.
-
-    Returns
-    - results: list of dicts; each dict represents detections on one image
+    Load a model file and run it on a list of images.
+    
+    Args:
+        
+        model_file (str): path to model file, or supported model string (e.g. "MDV5A")
+        image_file_names (list or str): list of strings (image filenames), a single image filename, 
+            a folder to recursively search for images in, or a .json or .txt file containing a list 
+            of images.
+        checkpoint_path (str, optional), path to use for checkpoints (if None, checkpointing
+            is disabled)
+        confidence_threshold (float, optional): only detections above this threshold are returned
+        checkpoint_frequency (int, optional): int, write results to JSON checkpoint file every N 
+            images, -1 disabled checkpointing
+        results (list, optional): list of dicts, existing results loaded from checkpoint; generally 
+            not useful if you're using this function outside of the CLI
+        n_cores (int, optional): number of parallel worker to use, ignored if we're running on a GPU
+        use_image_queue (bool, optional): use a dedicated worker for image loading
+        quiet (bool, optional): disable per-image console output
+        image_size (tuple, optional): image size to use for inference, only mess with this
+            if (a) you're using a model other than MegaDetector or (b) you know what you're
+            doing
+        class_mapping_filename (str, optional), use a non-default class mapping supplied in a .json 
+            file or YOLOv5 dataset.yaml file
+        include_image_size (bool, optional): should we include image size in the output for each image?
+        include_image_timestamp (bool, optional): should we include image timestamps in the output for each image?
+        include_exif_data (bool, optional): should we include EXIF data in the output for each image?        
+        
+    Returns:
+        results: list of dicts; each dict represents detections on one image
     """
     
+    # Validate input arguments
     if n_cores is None:
         n_cores = 1
     
     if confidence_threshold is None:
         confidence_threshold=run_detector.DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD
-        
-    if checkpoint_frequency is None:
+    
+    # Disable checkpointing if checkpoint_path is None
+    if checkpoint_frequency is None or checkpoint_path is None:
         checkpoint_frequency = -1
 
     if class_mapping_filename is not None:
-        load_custom_class_mapping(class_mapping_filename)
+        _load_custom_class_mapping(class_mapping_filename)
         
     # Handle the case where image_file_names is not yet actually a list
     if isinstance(image_file_names,str):
         
         # Find the images to score; images can be a directory, may need to recurse
         if os.path.isdir(image_file_names):
             image_dir = image_file_names
@@ -447,15 +504,16 @@
             else:
                 raise ValueError(
                     'File {} supplied as [image_file_names] argument, but extension is neither .json nor .txt'\
                         .format(
                         list_file))
         else:            
             raise ValueError(
-                '{} supplied as [image_file_names] argument, but it does not appear to be a file or folder')
+                '{} supplied as [image_file_names] argument, but it does not appear to be a file or folder'.format(
+                    image_file_names))
             
     if results is None:
         results = []
 
     already_processed = set([i['file'] for i in results])
 
     model_file = try_download_known_detector(model_file)
@@ -511,20 +569,20 @@
                                    confidence_threshold, quiet=quiet, 
                                    image_size=image_size, include_image_size=include_image_size,
                                    include_image_timestamp=include_image_timestamp,
                                    include_exif_data=include_exif_data)
             results.append(result)
 
             # Write a checkpoint if necessary
-            if checkpoint_frequency != -1 and count % checkpoint_frequency == 0:
+            if (checkpoint_frequency != -1) and ((count % checkpoint_frequency) == 0):
                 
                 print('Writing a new checkpoint after having processed {} images since '
                       'last restart'.format(count))
                 
-                write_checkpoint(checkpoint_path, results)
+                _write_checkpoint(checkpoint_path, results)
             
     else:
         
         # Multiprocessing is enabled at this point
         
         # When using multiprocessing, tell the workers to load the model on each
         # process, by passing the model_file string as the "model" argument to
@@ -536,28 +594,28 @@
         if len(already_processed) > 0:
             n_images_all = len(image_file_names)
             image_file_names = [fn for fn in image_file_names if fn not in already_processed]
             print('Loaded {} of {} images from checkpoint'.format(
                 len(already_processed),n_images_all))
         
         # Divide images into chunks; we'll send one chunk to each worker process   
-        image_batches = list(chunks_by_number_of_chunks(image_file_names, n_cores))
+        image_batches = list(_chunks_by_number_of_chunks(image_file_names, n_cores))
                 
         pool = workerpool(n_cores)
 
         if checkpoint_path is not None:
             
             # Multiprocessing and checkpointing are both enabled at this point
             
             checkpoint_queue = Manager().Queue()
             
             # Pass the "results" array (which may already contain images loaded from an existing
             # checkpoint) to the checkpoint queue handler function, which will append results to 
             # the list as they become available.
-            checkpoint_thread = Thread(target=checkpoint_queue_handler, 
+            checkpoint_thread = Thread(target=_checkpoint_queue_handler, 
                                        args=(checkpoint_path, checkpoint_frequency,
                                              checkpoint_queue, results), daemon=True)
             checkpoint_thread.start()
 
             pool.map(partial(process_images, detector=detector,
                                     confidence_threshold=confidence_threshold,
                                     image_size=image_size, 
@@ -593,15 +651,15 @@
     # 'results' may have been modified in place, but we also return it for
     # backwards-compatibility.
     return results
 
 # ...def load_and_run_detector_batch(...)
 
 
-def checkpoint_queue_handler(checkpoint_path, checkpoint_frequency, checkpoint_queue, results):
+def _checkpoint_queue_handler(checkpoint_path, checkpoint_frequency, checkpoint_queue, results):
     """
     Thread function to accumulate results and write checkpoints when checkpointing and
     multiprocessing are both enabled.
     """
     
     result_count = 0
     while True:
@@ -613,23 +671,23 @@
         results.append(result)
 
         if (checkpoint_frequency != -1) and (result_count % checkpoint_frequency == 0):
                 
             print('Writing a new checkpoint after having processed {} images since '
                     'last restart'.format(result_count))
             
-            write_checkpoint(checkpoint_path, results)
+            _write_checkpoint(checkpoint_path, results)
 
 
-def write_checkpoint(checkpoint_path, results):
+def _write_checkpoint(checkpoint_path, results):
     """
     Writes the 'images' field in the dict 'results' to a json checkpoint file.
     """
     
-    assert checkpoint_path is not None              
+    assert checkpoint_path is not None             
             
     # Back up any previous checkpoints, to protect against crashes while we're writing
     # the checkpoint file.
     checkpoint_tmp_path = None
     if os.path.isfile(checkpoint_path):
         checkpoint_tmp_path = checkpoint_path + '_tmp'
         shutil.copyfile(checkpoint_path,checkpoint_tmp_path)
@@ -641,17 +699,22 @@
     # Remove the backup checkpoint if it exists
     if checkpoint_tmp_path is not None:
         os.remove(checkpoint_tmp_path)
 
 
 def get_image_datetime(image):
     """
-    Returns the EXIF datetime from [image] (a PIL Image object), if available, as a string.
+    Reads EXIF datetime from a PIL Image object.
     
-    [im_file] is used only for error reporting.
+    Args:
+        image (Image): the PIL Image object from which we should read datetime information
+        
+    Returns:
+        str: the EXIF datetime from [image] (a PIL Image object), if available, as a string;
+        returns None if EXIF datetime is not available.
     """
     
     exif_tags = read_exif.read_pil_exif(image,exif_options)
     
     try:
         datetime_str = exif_tags['DateTimeOriginal']
         _ = time.strptime(datetime_str, '%Y:%m:%d %H:%M:%S')
@@ -665,28 +728,32 @@
                           detector_file=None, info=None, include_max_conf=False,
                           custom_metadata=None, force_forward_slashes=True):
     """
     Writes list of detection results to JSON output file. Format matches:
 
     https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing#batch-processing-api-output-format
 
-    Args
-    - results: list of dict, each dict represents detections on one image
-    - output_file: str, path to JSON output file, should end in '.json'
-    - relative_path_base: str, path to a directory as the base for relative paths
-    - detector_file: filename of the detector used to generate these results, only
-        used to pull out a version number for the "info" field
-    - info: dictionary to use instead of the default "info" field
-    - include_max_conf: old files (version 1.2 and earlier) included a "max_conf" field
-        in each image; this was removed in version 1.3.  Set this flag to force the inclusion
-        of this field.
-    - custom_metadata: additional data to include as info['custom_metadata'].  Typically
-        a dictionary, but no format checks are performed.
-        
-    Returns the complete output dictionary that was written to the output file.
+    Args:
+        results (list):  list of dict, each dict represents detections on one image
+        output_file (str): path to JSON output file, should end in '.json'
+        relative_path_base (str, optional): path to a directory as the base for relative paths, can
+            be None if the paths in [results] are absolute
+        detector_file (str, optional): filename of the detector used to generate these results, only
+            used to pull out a version number for the "info" field
+        info (dict, optional): dictionary to put in the results file instead of the default "info" field
+        include_max_conf (bool, optional): old files (version 1.2 and earlier) included a "max_conf" field
+            in each image; this was removed in version 1.3.  Set this flag to force the inclusion
+            of this field.
+        custom_metadata (object, optional): additional data to include as info['custom_metadata']; typically
+            a dictionary, but no type/format checks are performed
+        force_forward_slashes (bool, optional): convert all slashes in filenames within [results] to
+            forward slashes
+                    
+    Returns:
+        dict: the MD-formatted dictionary that was written to [output_file]
     """
     
     if relative_path_base is not None:
         results_relative = []
         for r in results:
             r_relative = copy.copy(r)
             r_relative['file'] = os.path.relpath(r_relative['file'], start=relative_path_base)
@@ -993,15 +1060,15 @@
 
     if len(output_dir) > 0:
         os.makedirs(output_dir,exist_ok=True)
         
     assert not os.path.isdir(args.output_file), 'Specified output file is a directory'
     
     if args.class_mapping_filename is not None:
-        load_custom_class_mapping(args.class_mapping_filename)
+        _load_custom_class_mapping(args.class_mapping_filename)
     
     # Load the checkpoint if available
     #
     # Relative file names are only output at the end; all file paths in the checkpoint are
     # still absolute paths.
     if args.resume_from_checkpoint is not None:
         if args.resume_from_checkpoint == 'auto':
@@ -1142,12 +1209,11 @@
     write_results_to_file(results, args.output_file, relative_path_base=relative_path_base,
                           detector_file=args.detector_file,include_max_conf=args.include_max_conf)
 
     if checkpoint_path and os.path.isfile(checkpoint_path):
         os.remove(checkpoint_path)
         print('Deleted checkpoint file {}'.format(checkpoint_path))
 
-    print('Done!')
-
+    print('Done, thanks for MegaDetect\'ing!')
 
 if __name__ == '__main__':
     main()
```

### Comparing `megadetector-5.0.8/detection/run_inference_with_yolov5_val.py` & `megadetector-5.0.9/detection/run_inference_with_yolov5_val.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,43 +1,51 @@
-########
-# 
-# run_inference_with_yolov5_val.py
-#
-# Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's 
-# val.py, converting the output to the standard MD format.  The main goal is to leverage
-# YOLO's test-time augmentation tools.
-#
-# YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work 
-# when you have typical camera trap images like:
-#
-# a/b/c/RECONYX0001.JPG
-# d/e/f/RECONYX0001.JPG
-#
-# ...so this script jumps through a bunch of hoops to put a symlinks in a flat
-# folder, run YOLOv5 on that folder, and map the results back to the real files.
-#
-# Currently requires the user to supply the path where a working YOLOv5 install lives,
-# and assumes that the current conda environment is all set up for YOLOv5.
-#
-# By default, this script uses symlinks to format the input images in a way that YOLOv5's 
-# val.py likes.  This requires admin privileges on Windows... actually technically this only 
-# requires permissions to create symbolic links, but I've never seen a case where someone has
-# that permission and *doesn't* have admin privileges.  If you are running this script on
-# Windows and you don't have admin privileges, use --no_use_symlinks.
-#
-# TODO:
-#
-# * Multiple GPU support
-#
-# * Checkpointing
-#
-# * Support alternative class names at the command line (currently defaults to MD classes,
-#   though other class names can be supplied programmatically)
-#
-########
+"""
+
+run_inference_with_yolov5_val.py
+
+Runs a folder of images through MegaDetector (or another YOLOv5/YOLOv8 model) with YOLO's
+val.py, converting the output to the standard MD format.  The reasons this script exists,
+as an alternative to the standard run_detector_batch.py are:
+    
+* This script provides access to YOLO's test-time augmentation tools.
+* This script serves a reference implementation: by any reasonable definition, YOLOv5's
+  val.py produces the "correct" result for any image, since it matches what was used in 
+  training.
+* This script works for any Ultralytics detection model, including YOLOv8 models  
+
+YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work 
+when you have typical camera trap images like:
+
+* a/b/c/RECONYX0001.JPG
+* d/e/f/RECONYX0001.JPG
+
+...both of which would just be "RECONYX0001.JPG".  So this script jumps through a bunch of 
+hoops to put a symlinks in a flat folder, run YOLOv5 on that folder, and map the results back 
+to the real files.
+
+If you are running a YOLOv5 model, this script currently requires the caller to supply the path
+where a working YOLOv5 install lives, and assumes that the current conda environment is all set up for 
+YOLOv5.  If you are running a YOLOv8 model, the folder doesn't matter, but it assumes that ultralytics
+tools are available in the current environment.
+
+By default, this script uses symlinks to format the input images in a way that YOLO's 
+val.py likes, as per above.  This requires admin privileges on Windows... actually technically this 
+only requires permissions to create symbolic links, but I've never seen a case where someone has
+that permission and *doesn't* have admin privileges.  If you are running this script on
+Windows and you don't have admin privileges, use --no_use_symlinks, which will make copies of images,
+rather than using symlinks.
+
+TODO:
+
+* Multiple GPU support
+* Checkpointing
+* Support alternative class names at the command line (currently defaults to MD classes,
+  though other class names can be supplied programmatically)
+
+"""
 
 #%% Imports
 
 import os
 import sys
 import uuid
 import glob
@@ -56,67 +64,120 @@
 default_image_size_with_augmentation = int(1280 * 1.3)
 default_image_size_with_no_augmentation = 1280
 
 
 #%% Options class
 
 class YoloInferenceOptions:
-        
+    """
+    Parameters that control the behavior of run_inference_with_yolov5_val(), including 
+    the input/output filenames.
+    """
+    
     ## Required ##
     
+    #: Folder of images to process
     input_folder = None
+    
+    #: Model filename (ending in .pt), or a well-known model name (e.g. "MDV5A")
     model_filename = None
+    
+    #: .json output file, in MD results format
     output_file = None
     
+    
     ## Optional ##
     
-    # Required for older YOLOv5 inference, not for newer ulytralytics inference
+    #: Required for older YOLOv5 inference, not for newer ulytralytics/YOLOv8 inference
     yolo_working_folder = None
     
-    # Currently 'yolov5' and 'ultralytics' are supported, and really these are proxies for
-    # "the yolov5 repo" and "the ultralytics repo" (typically YOLOv8).
+    #: Currently 'yolov5' and 'ultralytics' are supported, and really these are proxies for
+    #: "the yolov5 repo" and "the ultralytics repo".
     model_type = 'yolov5' 
 
+    #: Image size to use; this is a single int, which in ultralytics's terminology means
+    #: "scale the long side of the image to this size, and preserve aspect ratio".
     image_size = default_image_size_with_augmentation
+    
+    #: Detections below this threshold will not be included in the output file
     conf_thres = '0.001'
+    
+    #: Batch size... has no impact on results, but may create memory issues if you set
+    #: this to large values
     batch_size = 1
+    
+    #: Device string: typically '0' for GPU 0, '1' for GPU 1, etc., or 'cpu'
     device_string = '0'
+    
+    #: Should we enable test-time augmentation?
     augment = True
+    
+    #: Should we enable half-precision inference?
     half_precision_enabled = None
     
+    #: Where should we stash the temporary symlinks used to give unique identifiers to image files?
+    #:
+    #: If this is None, we'll create a folder in system temp space.
     symlink_folder = None
+    
+    #: Should we use symlinks to give unique identifiers to image files (vs. copies)?
     use_symlinks = True
     
+    #: Temporary folder to stash intermediate YOLO results.
+    #:
+    #: If this is None, we'll create a folder in system temp space.    
     yolo_results_folder = None
     
+    #: Should we remove the symlink folder when we're done?
     remove_symlink_folder = True
+    
+    #: Should we remove the intermediate results folder when we're done?
     remove_yolo_results_folder = True
     
-    # These are deliberately offset from the standard MD categories; YOLOv5
-    # needs categories IDs to start at 0.
-    #
-    # This can also be a string that points to a YOLOv5 dataset.yaml file.
+    #: These are deliberately offset from the standard MD categories; YOLOv5
+    #: needs categories IDs to start at 0.
+    #:
+    #: This can also be a string that points to a YOLO dataset.yaml file.
     yolo_category_id_to_name = {0:'animal',1:'person',2:'vehicle'}
     
-    # 'error','skip','overwrite'
+    #: What should we do if the output file already exists?
+    #:
+    #: Can be 'error', 'skip', or 'overwrite'.
     overwrite_handling = 'skip'
     
+    #: If True, we'll do a dry run that lets you preview the YOLO val command, without
+    #: actually running it.
     preview_yolo_command_only = False
     
+    #: By default, if any errors occur while we're copying images or creating symlinks, it's
+    #: game over.  If this is True, those errors become warnings, and we plow ahead.
     treat_copy_failures_as_warnings = False
     
+    #: Save YOLO console output
     save_yolo_debug_output = False
     
+    #: Whether to search for images recursively within [input_folder]
     recursive = True
             
     
+# ...YoloInferenceOptions()
+
+    
 #%% Main function
 
 def run_inference_with_yolo_val(options):
-
+    """
+    Runs a folder of images through MegaDetector (or another YOLOv5/YOLOv8 model) with YOLO's
+    val.py, converting the output to the standard MD format.
+    
+    Args: 
+        options (YoloInferenceOptions): all the parameters used to control this process,
+            including filenames; see YoloInferenceOptions for details            
+    """
+    
     ##%% Input and path handling
     
     if options.model_type == 'yolov8':
         
         print('Warning: model type "yolov8" supplied, "ultralytics" is the preferred model type string for YOLOv8 models')
         options.model_type = 'ultralytics'
         
@@ -602,16 +663,15 @@
     options.remove_symlink_folder = (not options.no_remove_symlink_folder)
     options.remove_yolo_results_folder = (not options.no_remove_yolo_results_folder)
     options.use_symlinks = (not options.no_use_symlinks)
     options.augment = (options.augment_enabled > 0)        
             
     print(options.__dict__)
     
-    run_inference_with_yolo_val(options)
-    
+    run_inference_with_yolo_val(options)    
 
 if __name__ == '__main__':
     main()
 
 
 #%% Scrap
```

### Comparing `megadetector-5.0.8/detection/run_tiled_inference.py` & `megadetector-5.0.9/detection/run_tiled_inference.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,28 +1,30 @@
-########
-#
-# run_tiled_inference.py
-#
-# Run inference on a folder, fist splitting each image up into tiles of size
-# MxN (typically the native inference size of your detector), writing those
-# tiles out to a temporary folder, then de-duplicating the results before merging
-# them back into a set of detections that make sense on the original images.
-#
-# This approach will likely fail to detect very large animals, so if you expect both large 
-# and small animals (in terms of pixel size), this script is best used in 
-# conjunction with a traditional inference pass that looks at whole images.
-#
-# Currently requires temporary storage at least as large as the input data, generally
-# a lot more than that (depending on the overlap between adjacent tiles).  This is 
-# inefficient, but easy to debug.
-#
-# Programmatic invocation supports using YOLOv5's inference scripts (and test-time
-# augmentation); the command-line interface only supports standard inference right now.
-#
-########
+"""
+
+run_tiled_inference.py
+
+**This script is experimental, YMMV.**
+
+Runs inference on a folder, fist splitting each image up into tiles of size
+MxN (typically the native inference size of your detector), writing those
+tiles out to a temporary folder, then de-duplicating the resulting detections before 
+merging them back into a set of detections that make sense on the original images.
+
+This approach will likely fail to detect very large animals, so if you expect both large 
+and small animals (in terms of pixel size), this script is best used in 
+conjunction with a traditional inference pass that looks at whole images.
+
+Currently requires temporary storage at least as large as the input data, generally
+a lot more than that (depending on the overlap between adjacent tiles).  This is 
+inefficient, but easy to debug.
+
+Programmatic invocation supports using YOLOv5's inference scripts (and test-time
+augmentation); the command-line interface only supports standard inference right now.
+
+"""
 
 #%% Imports and constants
 
 import os
 import json
 
 from tqdm import tqdm
@@ -50,25 +52,32 @@
 parallelization_uses_threads = False
 
 
 #%% Support functions
 
 def get_patch_boundaries(image_size,patch_size,patch_stride=None):
     """
-    Get a list of patch starting coordinates (x,y) given an image size (w,h)
-    and a stride (x,y).  Stride defaults to half the patch size.
+    Computes a list of patch starting coordinates (x,y) given an image size (w,h)
+    and a stride (x,y)
     
-    patch_stride can also be a single float, in which case that is interpreted 
-    as the stride relative to the patch size (0.1 == 10% stride).
-    
-    Patch size is guaranteed, stride may deviate to make sure all pixels are covered.
+    Patch size is guaranteed, but the stride may deviate to make sure all pixels are covered.
     I.e., we move by regular strides until the current patch walks off the right/bottom,
     at which point it backs up to one patch from the end.  So if your image is 15
     pixels wide and you have a stride of 10 pixels, you will get starting positions 
     of 0 (from 0 to 9) and 5 (from 5 to 14).
+    
+    Args:
+        image_size (tuple): size of the image you want to divide into patches, as a length-2 tuple (w,h)
+        patch_size (tuple): patch size into which you want to divide an image, as a length-2 tuple (w,h)
+        patch_stride (tuple or float, optional): stride between patches, as a length-2 tuple (x,y), or a 
+            float; if this is a float, it's interpreted as the stride relative to the patch size 
+            (0.1 == 10% stride).  Defaults to half the patch size.
+
+    Returns:
+        list: list of length-2 tuples, each representing the x/y start position of a patch        
     """
     
     if patch_stride is None:
         patch_stride = (round(patch_size[0]*(1.0-default_patch_overlap)),
                         round(patch_size[1]*(1.0-default_patch_overlap)))
     elif isinstance(patch_stride,float):
         patch_stride = (round(patch_size[0]*(patch_stride)),
@@ -159,31 +168,58 @@
     
     return patch_start_positions
 
 # ...get_patch_boundaries()
 
 
 def patch_info_to_patch_name(image_name,patch_x_min,patch_y_min):
+    """
+    Gives a unique string name to an x/y coordinate, e.g. turns ("a.jpg",10,20) into
+    "a.jpg_0010_0020".
+    
+    Args:
+        image_name (str): image identifier
+        patch_x_min (int): x coordinate
+        patch_y_min (int): y coordinate
     
+    Returns:
+        str: name for this patch, e.g. "a.jpg_0010_0020"
+    """
     patch_name = image_name + '_' + \
         str(patch_x_min).zfill(4) + '_' + str(patch_y_min).zfill(4)
     return patch_name
 
 
-def extract_patch_from_image(im,patch_xy,patch_size,
-                             patch_image_fn=None,patch_folder=None,image_name=None,overwrite=True):
+def extract_patch_from_image(im,
+                             patch_xy,
+                             patch_size,
+                             patch_image_fn=None,
+                             patch_folder=None,
+                             image_name=None,
+                             overwrite=True):
     """
-    Extracts a patch from the provided image, writing the patch out to patch_image_fn.
-    [im] can be a string or a PIL image.
-    
-    patch_xy is a length-2 tuple specifying the upper-left corner of the patch.
+    Extracts a patch from the provided image, and writes that patch out to a new file.
     
-    image_name and patch_folder are only required if patch_image_fn is None.
+    Args:
+        im (str or Image): image from which we should extract a patch, can be a filename or
+            a PIL Image object.
+        patch_xy (tuple): length-2 tuple of ints (x,y) representing the upper-left corner 
+            of the patch to extract
+        patch_size (tuple): length-2 tuple of ints (w,h) representing the size of the 
+            patch to extract
+        patch_image_fn (str, optional): image filename to write the patch to; if this is None
+            the filename will be generated from [image_name] and the patch coordinates
+        patch_folder (str, optional): folder in which the image lives; only used to generate
+            a patch filename, so only required if [patch_image_fn] is None
+        image_name (str, optional): the identifier of the source image; only used to generate
+            a patch filename, so only required if [patch_image_fn] is None
+        overwrite (bool, optional): whether to overwrite an existing patch image
     
-    Returns a dictionary with fields xmin,xmax,ymin,ymax,patch_fn.
+    Returns:
+        dict: a dictionary with fields xmin,xmax,ymin,ymax,patch_fn
     """
     
     if isinstance(im,str):
         pil_im = vis_utils.open_image(im)
     else:
         pil_im = im
         
@@ -219,18 +255,28 @@
     patch_info['xmax'] = patch_x_max
     patch_info['ymin'] = patch_y_min
     patch_info['ymax'] = patch_y_max
     patch_info['patch_fn'] = patch_image_fn
     
     return patch_info
 
+# ...def extract_patch_from_image(...)
+
 
 def in_place_nms(md_results, iou_thres=0.45, verbose=True):
     """
-    Run torch.ops.nms in-place on MD-formatted detection results    
+    Run torch.ops.nms in-place on MD-formatted detection results.
+    
+    Args:
+        md_results (dict): detection results for a list of images, in MD results format (i.e., 
+            containing a list of image dicts with the key 'images', each of which has a list
+            of detections with the key 'detections')
+        iou_thres (float, optional): IoU threshold above which we will treat two detections as
+            redundant
+        verbose (bool, optional): enable additional debug console output
     """
     
     n_detections_before = 0
     n_detections_after = 0
     
     # i_image = 18; im = md_results['images'][i_image]
     for i_image,im in tqdm(enumerate(md_results['images']),total=len(md_results['images'])):
@@ -339,15 +385,15 @@
                         tile_size_x=1280, tile_size_y=1280, tile_overlap=0.5,
                         checkpoint_path=None, checkpoint_frequency=-1, remove_tiles=False, 
                         yolo_inference_options=None,
                         n_patch_extraction_workers=default_n_patch_extraction_workers,
                         overwrite_tiles=True,
                         image_list=None):
     """
-    Run inference using [model_file] on the images in [image_folder], fist splitting each image up 
+    Runs inference using [model_file] on the images in [image_folder], fist splitting each image up 
     into tiles of size [tile_size_x] x [tile_size_y], writing those tiles to [tiling_folder],
     then de-duplicating the results before merging them back into a set of detections that make 
     sense on the original images and writing those results to [output_file].  
     
     [tiling_folder] can be any folder, but this function reserves the right to do whatever it wants
     within that folder, including deleting everything, so it's best if it's a new folder.  
     Conceptually this folder is temporary, it's just helpful in this case to not actually
@@ -356,15 +402,40 @@
     
     tile_overlap is the fraction of overlap between tiles.
     
     Optionally removes the temporary tiles.
     
     if yolo_inference_options is supplied, it should be an instance of YoloInferenceOptions; in 
     this case the model will be run with run_inference_with_yolov5_val.  This is typically used to 
-    run the model with test-time augmentation.          
+    run the model with test-time augmentation.
+    
+    Args:
+        model_file (str): model filename (ending in .pt), or a well-known model name (e.g. "MDV5A")
+        image_folder (str): the folder of images to proess (always recursive)
+        tiling_folder (str): folder for temporary tile storage; see caveats above
+        output_file (str): .json file to which we should write MD-formatted results
+        tile_size_x (int, optional): tile width
+        tile_size_y (int, optional): tile height
+        tile_overlap (float, optional): overlap between adjacenet tiles, as a fraction of the
+            tile size
+        checkpoint_path (str, optional): checkpoint path; passed directly to run_detector_batch; see
+            run_detector_batch for details
+        checkpoint_frequency (int, optional): checkpoint frequency; passed directly to run_detector_batch; see
+            run_detector_batch for details
+        remove_tiles (bool, optional): whether to delete the tiles when we're done
+        yolo_inference_options (YoloInferenceOptions, optional): if not None, will run inference with
+            run_inference_with_yolov5_val.py, rather than with run_detector_batch.py, using these options
+        n_patch_extraction_workers (int, optional): number of workers to use for patch extraction;
+            set to <= 1 to disable parallelization
+        image_list (list, optional): .json file containing a list of specific images to process.  If 
+            this is supplied, and the paths are absolute, [image_folder] will be ignored. If this is supplied,
+            and the paths are relative, they should be relative to [image_folder].
+    
+    Returns:
+        dict: MD-formatted results dictionary, identical to what's written to [output_file]
     """
 
     ##%% Validate arguments
     
     assert tile_overlap < 1 and tile_overlap >= 0, \
         'Illegal tile overlap value {}'.format(tile_overlap)
```

### Comparing `megadetector-5.0.8/detection/tf_detector.py` & `megadetector-5.0.9/detection/tf_detector.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,57 +1,63 @@
-########
-#
-# tf_detector.py
-#
-# Module containing the class TFDetector for loading a TensorFlow detection model and
-# running inference.
-# 
-########
+"""
+
+tf_detector.py
+
+Module containing the class TFDetector, for loading and running a TensorFlow detection model.
+
+"""
+
+#%% Imports and constants
 
 import numpy as np
 
 from detection.run_detector import CONF_DIGITS, COORD_DIGITS, FAILURE_INFER
 from md_utils.ct_utils import truncate_float
 
 import tensorflow.compat.v1 as tf
 
 print('TensorFlow version:', tf.__version__)
 print('Is GPU available? tf.test.is_gpu_available:', tf.test.is_gpu_available())
 
 
+#%% Classes
+
 class TFDetector:
     """
     A detector model loaded at the time of initialization. It is intended to be used with
-    the MegaDetector (TF). The inference batch size is set to 1; code needs to be modified
-    to support larger batch sizes, including resizing appropriately.
+    TensorFlow-based versions of MegaDetector (v2, v3, or v4).  If someone can find v1, I 
+    suppose you could use this class for v1 also.
     """
     
-    # MegaDetector was trained with batch size of 1, and the resizing function is a part
-    # of the inference graph
+    #: TF versions of MD were trained with batch size of 1, and the resizing function is a 
+    #: part of the inference graph, so this is fixed.
+    #:
+    #: :meta private:  
     BATCH_SIZE = 1
 
 
     def __init__(self, model_path):
         """
-        Loads model from model_path and starts a tf.Session with this graph. Obtains
+        Loads a model from [model_path] and starts a tf.Session with this graph. Obtains
         input and output tensor handles.
         """
         
         detection_graph = TFDetector.__load_model(model_path)
         self.tf_session = tf.Session(graph=detection_graph)
-
         self.image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
         self.box_tensor = detection_graph.get_tensor_by_name('detection_boxes:0')
         self.score_tensor = detection_graph.get_tensor_by_name('detection_scores:0')
         self.class_tensor = detection_graph.get_tensor_by_name('detection_classes:0')
 
+
     @staticmethod
-    def round_and_make_float(d, precision=4):
+    def __round_and_make_float(d, precision=4):
         return truncate_float(float(d), precision=precision)
 
+
     @staticmethod
     def __convert_coords(tf_coords):
         """
         Converts coordinates from the model's output format [y1, x1, y2, x2] to the
         format used by our API and MegaDB: [x1, y1, width, height]. All coordinates
         (including model outputs) are normalized in the range [0, 1].
 
@@ -66,17 +72,18 @@
         width = tf_coords[3] - tf_coords[1]
         height = tf_coords[2] - tf_coords[0]
 
         new = [tf_coords[1], tf_coords[0], width, height]  # must be a list instead of np.array
 
         # convert numpy floats to Python floats
         for i, d in enumerate(new):
-            new[i] = TFDetector.round_and_make_float(d, precision=COORD_DIGITS)
+            new[i] = TFDetector.__round_and_make_float(d, precision=COORD_DIGITS)
         return new
 
+
     @staticmethod
     def __load_model(model_path):
         """
         Loads a detection model (i.e., create a graph) from a .pb file.
 
         Args:
             model_path: .pb file of the model.
@@ -92,53 +99,64 @@
                 serialized_graph = fid.read()
                 od_graph_def.ParseFromString(serialized_graph)
                 tf.import_graph_def(od_graph_def, name='')
         print('TFDetector: Detection graph loaded.')
 
         return detection_graph
 
+
     def _generate_detections_one_image(self, image):
+        """
+        Runs the detector on a single image.
+        """
+        
         np_im = np.asarray(image, np.uint8)
         im_w_batch_dim = np.expand_dims(np_im, axis=0)
 
         # need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
         # np_images = [np.asarray(image, np.uint8) for image in images]
         # images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)
 
         # performs inference
         (box_tensor_out, score_tensor_out, class_tensor_out) = self.tf_session.run(
             [self.box_tensor, self.score_tensor, self.class_tensor],
             feed_dict={self.image_tensor: im_w_batch_dim})
 
         return box_tensor_out, score_tensor_out, class_tensor_out
 
+
     def generate_detections_one_image(self, image, image_id, detection_threshold, image_size=None,
                                       skip_image_resizing=False):
         """
-        Apply the detector to an image.
+        Runs the detector on an image.
 
         Args:
-            image: the PIL Image object
-            image_id: a path to identify the image; will be in the "file" field of the output object
-            detection_threshold: confidence above which to include the detection proposal
+            image (Image): the PIL Image object on which we should run the detector
+            image_id (str): a path to identify the image; will be in the "file" field of the output object            
+            detection_threshold (float): only detections above this threshold will be included in the return
+                value
+            image_size (tuple, optional): image size to use for inference, only mess with this
+                if (a) you're using a model other than MegaDetector or (b) you know what you're
+                doing
+            skip_image_resizing (bool, optional): whether to skip internal image resizing (and rely on external 
+                resizing)... not currently supported, but included here for compatibility with PTDetector.
 
         Returns:
-        A dict with the following fields, see the 'images' key in:
-        https://github.com/agentmorris/MegaDetector/tree/master/api/batch_processing#batch-processing-api-output-format
-            - 'file' (always present)
-            - 'max_detection_conf'
-            - 'detections', which is a list of detection objects containing keys 'category', 'conf' and 'bbox'
-            - 'failure'
+            dict: a dictionary with the following fields:
+                - 'file' (filename, always present)
+                - 'max_detection_conf' (removed from MegaDetector output files by default, but generated here)
+                - 'detections' (a list of detection objects containing keys 'category', 'conf', and 'bbox')
+                - 'failure' (a failure string, or None if everything went fine)
         """
         
         assert image_size is None, 'Image sizing not supported for TF detectors'
         assert not skip_image_resizing, 'Image sizing not supported for TF detectors'
-        result = {
-            'file': image_id
-        }
+        
+        result = { 'file': image_id }
+        
         try:
             b_box, b_score, b_class = self._generate_detections_one_image(image)
 
             # our batch size is 1; need to loop the batch dim if supporting batch size > 1
             boxes, scores, classes = b_box[0], b_score[0], b_class[0]
 
             detections_cur_image = []  # will be empty for an image with no confident detections
@@ -160,7 +178,11 @@
             result['detections'] = detections_cur_image
 
         except Exception as e:
             result['failure'] = FAILURE_INFER
             print('TFDetector: image {} failed during inference: {}'.format(image_id, str(e)))
 
         return result
+
+    # ...def generate_detections_one_image(...)
+    
+# ...class TFDetector
```

### Comparing `megadetector-5.0.8/detection/video_utils.py` & `megadetector-5.0.9/detection/video_utils.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,521 +1,606 @@
-########
-#
-# video_utils.py
-#
-# Utilities for splitting, rendering, and assembling videos.
-#
-########
-
-#%% Constants, imports, environment
-
-import os
-import cv2
-import glob
-import json
-
-from collections import defaultdict
-from multiprocessing.pool import ThreadPool
-from multiprocessing.pool import Pool
-from tqdm import tqdm
-from typing import Container,Iterable,List
-from functools import partial
-
-from md_utils import path_utils
-    
-from md_visualization import visualization_utils as vis_utils
-
-default_fourcc = 'h264'
-
-
-#%% Path utilities
-
-VIDEO_EXTENSIONS = ('.mp4','.avi','.mpeg','.mpg')
-
-def is_video_file(s: str, video_extensions: Container[str] = VIDEO_EXTENSIONS
-                  ) -> bool:
-    """
-    Checks a file's extension against a hard-coded set of video file
-    extensions.
-    """
-    
-    ext = os.path.splitext(s)[1]
-    return ext.lower() in video_extensions
-
-
-def find_video_strings(strings: Iterable[str]) -> List[str]:
-    """
-    Given a list of strings that are potentially video file names, looks for
-    strings that actually look like video file names (based on extension).
-    """
-    
-    return [s for s in strings if is_video_file(s.lower())]
-
-
-def find_videos(dirname: str, recursive: bool = False,
-                convert_slashes: bool=False,
-                return_relative_paths: bool=False) -> List[str]:
-    """
-    Finds all files in a directory that look like video file names. Returns
-    absolute paths unless return_relative_paths is set.  Uses the native
-    path separator unless convert_slashes is set.
-    """
-    
-    if recursive:
-        files = glob.glob(os.path.join(dirname, '**', '*.*'), recursive=True)
-    else:
-        files = glob.glob(os.path.join(dirname, '*.*'))
-        
-    if return_relative_paths:
-        files = [os.path.relpath(fn,dirname) for fn in files]
-
-    if convert_slashes:
-        files = [fn.replace('\\', '/') for fn in files]
-    
-    return find_video_strings(files)
-
-
-#%% Function for rendering frames to video and vice-versa
-
-# http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html
-
-def frames_to_video(images, Fs, output_file_name, codec_spec=default_fourcc):
-    """
-    Given a list of image files and a sample rate, concatenate those images into
-    a video and write to [output_file_name].
-    
-    Note to self: h264 is a sensible default and generally works on Windows, but when this
-    fails (which is around 50% of the time on Linux), I fall back to mp4v.
-    """
-    
-    if codec_spec is None:
-        codec_spec = 'h264'
-        
-    if len(images) == 0:
-        return
-
-    # Determine the width and height from the first image
-    frame = cv2.imread(images[0])
-    cv2.imshow('video',frame)
-    height, width, channels = frame.shape
-
-    # Define the codec and create VideoWriter object
-    fourcc = cv2.VideoWriter_fourcc(*codec_spec)
-    out = cv2.VideoWriter(output_file_name, fourcc, Fs, (width, height))
-
-    for image in images:
-        frame = cv2.imread(image)
-        out.write(frame)
-
-    out.release()
-    cv2.destroyAllWindows()
-
-
-def get_video_fs(input_video_file):
-    """
-    Get the frame rate of [input_video_file]
-    """
-    
-    assert os.path.isfile(input_video_file), 'File {} not found'.format(input_video_file)    
-    vidcap = cv2.VideoCapture(input_video_file)
-    Fs = vidcap.get(cv2.CAP_PROP_FPS)
-    vidcap.release()
-    return Fs
-
-
-def frame_number_to_filename(frame_number):
-    return 'frame{:06d}.jpg'.format(frame_number)
-
-
-def video_to_frames(input_video_file, output_folder, overwrite=True, 
-                    every_n_frames=None, verbose=False):
-    """
-    Render every frame of [input_video_file] to a .jpg in [output_folder]
-    
-    With help from:
-        
-    https://stackoverflow.com/questions/33311153/python-extracting-and-saving-video-frames
-    """
-    
-    assert os.path.isfile(input_video_file), 'File {} not found'.format(input_video_file)
-    
-    vidcap = cv2.VideoCapture(input_video_file)
-    n_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))
-    Fs = vidcap.get(cv2.CAP_PROP_FPS)
-    
-    # If we're not over-writing, check whether all frame images already exist
-    if overwrite == False:
-        
-        missing_frame_number = None
-        frame_filenames = []
-        
-        for frame_number in range(0,n_frames):
-            
-            if every_n_frames is not None:
-                if frame_number % every_n_frames != 0:
-                    continue
-            
-            frame_filename = frame_number_to_filename(frame_number)
-            frame_filename = os.path.join(output_folder,frame_filename)
-            frame_filenames.append(frame_filename)
-            if os.path.isfile(frame_filename):
-                continue
-            else:
-                missing_frame_number = frame_number
-                break
-    
-        # OpenCV seems to over-report the number of frames by 1 in some cases, or fails
-        # to read the last frame; either way, I'm allowing one missing frame.
-        allow_last_frame_missing = True
-        
-        if missing_frame_number is None or \
-            (allow_last_frame_missing and (missing_frame_number == n_frames-1)):
-            if verbose:
-                print('Skipping video {}, all output frames exist'.format(input_video_file))
-            return frame_filenames,Fs
-        else:
-            pass
-            # print("Rendering video {}, couldn't find frame {}".format(
-            #    input_video_file,missing_frame_number))
-    
-    # ...if we need to check whether to skip this video entirely
-        
-    if verbose:
-        print('Reading {} frames at {} Hz from {}'.format(n_frames,Fs,input_video_file))
-
-    frame_filenames = []
-
-    # for frame_number in tqdm(range(0,n_frames)):
-    for frame_number in range(0,n_frames):
-
-        success,image = vidcap.read()
-        if not success:
-            assert image is None
-            if verbose:
-                print('Read terminating at frame {} of {}'.format(frame_number,n_frames))
-            break
-
-        if every_n_frames is not None:
-            if frame_number % every_n_frames != 0:
-                continue
-            
-        frame_filename = frame_number_to_filename(frame_number)
-        frame_filename = os.path.join(output_folder,frame_filename)
-        frame_filenames.append(frame_filename)
-        
-        if overwrite == False and os.path.isfile(frame_filename):
-            # print('Skipping frame {}'.format(frame_filename))
-            pass            
-        else:
-            try:
-                if frame_filename.isascii():
-                    cv2.imwrite(os.path.normpath(frame_filename),image)
-                else:
-                    is_success, im_buf_arr = cv2.imencode('.jpg', image)
-                    im_buf_arr.tofile(frame_filename)
-                assert os.path.isfile(frame_filename), \
-                    'Output frame {} unavailable'.format(frame_filename)
-            except KeyboardInterrupt:
-                vidcap.release()
-                raise
-            except Exception as e:
-                print('Error on frame {} of {}: {}'.format(frame_number,n_frames,str(e)))
-
-    if verbose:
-        print('\nExtracted {} of {} frames'.format(len(frame_filenames),n_frames))
-
-    vidcap.release()    
-    return frame_filenames,Fs
-
-
-def _video_to_frames_for_folder(relative_fn,input_folder,output_folder_base,every_n_frames,overwrite,verbose):
-    """
-    Internal function to call video_to_frames in the context of video_folder_to_frames; 
-    makes sure the right output folder exists, then calls video_to_frames.
-    """    
-    
-    input_fn_absolute = os.path.join(input_folder,relative_fn)
-    assert os.path.isfile(input_fn_absolute),\
-        'Could not find file {}'.format(input_fn_absolute)
-
-    # Create the target output folder
-    output_folder_video = os.path.join(output_folder_base,relative_fn)
-    os.makedirs(output_folder_video,exist_ok=True)
-
-    # Render frames
-    # input_video_file = input_fn_absolute; output_folder = output_folder_video
-    frame_filenames,fs = video_to_frames(input_fn_absolute,output_folder_video,
-                                         overwrite=overwrite,every_n_frames=every_n_frames,
-                                         verbose=verbose)
-    
-    return frame_filenames,fs
-
-
-def video_folder_to_frames(input_folder:str, output_folder_base:str, 
-                           recursive:bool=True, overwrite:bool=True,
-                           n_threads:int=1, every_n_frames:int=None,
-                           verbose=False, parallelization_uses_threads=True):
-    """
-    For every video file in input_folder, create a folder within output_folder_base, and 
-    render every frame of the video to .jpg in that folder.
-    
-    return frame_filenames_by_video,fs_by_video,input_files_full_paths
-    """
-    
-    # Recursively enumerate video files
-    input_files_full_paths = find_videos(input_folder,recursive=recursive)
-    print('Found {} videos in folder {}'.format(len(input_files_full_paths),input_folder))
-    if len(input_files_full_paths) == 0:
-        return [],[],[]
-    
-    input_files_relative_paths = [os.path.relpath(s,input_folder) for s in input_files_full_paths]
-    input_files_relative_paths = [s.replace('\\','/') for s in input_files_relative_paths]
-    
-    os.makedirs(output_folder_base,exist_ok=True)    
-    
-    frame_filenames_by_video = []
-    fs_by_video = []
-    
-    if n_threads == 1:
-        # For each video
-        #
-        # input_fn_relative = input_files_relative_paths[0]
-        for input_fn_relative in tqdm(input_files_relative_paths):
-        
-            frame_filenames,fs = \
-                _video_to_frames_for_folder(input_fn_relative,input_folder,output_folder_base,
-                                            every_n_frames,overwrite,verbose)
-            frame_filenames_by_video.append(frame_filenames)
-            fs_by_video.append(fs)
-    else:
-        if parallelization_uses_threads:
-            print('Starting a worker pool with {} threads'.format(n_threads))
-            pool = ThreadPool(n_threads)
-        else:
-            print('Starting a worker pool with {} processes'.format(n_threads))
-            pool = Pool(n_threads)
-        process_video_with_options = partial(_video_to_frames_for_folder, 
-                                             input_folder=input_folder,
-                                             output_folder_base=output_folder_base,
-                                             every_n_frames=every_n_frames,
-                                             overwrite=overwrite,
-                                             verbose=verbose)
-        results = list(tqdm(pool.imap(
-            partial(process_video_with_options),input_files_relative_paths), 
-                            total=len(input_files_relative_paths)))
-        frame_filenames_by_video = [x[0] for x in results]
-        fs_by_video = [x[1] for x in results]
-        
-    return frame_filenames_by_video,fs_by_video,input_files_full_paths
-  
-
-class FrameToVideoOptions:
-    
-    # One-indexed, i.e. "1" means "use the confidence value from the highest-confidence frame"
-    nth_highest_confidence = 1
-    
-    # 'error' or 'skip_with_warning'
-    non_video_behavior = 'error'
-    
-    
-def frame_results_to_video_results(input_file,output_file,options:FrameToVideoOptions = None):
-    """
-    Given an API output file produced at the *frame* level, corresponding to a directory 
-    created with video_folder_to_frames, map those frame-level results back to the 
-    video level for use in Timelapse.
-    
-    Preserves everything in the input .json file other than the images.
-    """
-
-    if options is None:
-        options = FrameToVideoOptions()
-        
-    # Load results
-    with open(input_file,'r') as f:
-        input_data = json.load(f)
-
-    images = input_data['images']
-    detection_categories = input_data['detection_categories']
-    
-    ## Break into videos
-    
-    video_to_frames = defaultdict(list) 
-    
-    # im = images[0]
-    for im in tqdm(images):
-        
-        fn = im['file']
-        video_name = os.path.dirname(fn)
-        if not is_video_file(video_name):
-            if options.non_video_behavior == 'error':
-                raise ValueError('{} is not a video file'.format(video_name))
-            elif options.non_video_behavior == 'skip_with_warning':
-                print('Warning: {} is not a video file'.format(video_name))
-                continue
-            else:
-                raise ValueError('Unrecognized non-video handling behavior: {}'.format(
-                    options.non_video_behavior))
-        video_to_frames[video_name].append(im)
-    
-    print('Found {} unique videos in {} frame-level results'.format(
-        len(video_to_frames),len(images)))
-    
-    output_images = []
-    
-    ## For each video...
-    
-    # video_name = list(video_to_frames.keys())[0]
-    for video_name in tqdm(video_to_frames):
-        
-        frames = video_to_frames[video_name]
-        
-        all_detections_this_video = []
-        
-        # frame = frames[0]
-        for frame in frames:
-            if frame['detections'] is not None:
-                all_detections_this_video.extend(frame['detections'])
-            
-        # At most one detection for each category for the whole video
-        canonical_detections = []
-            
-        # category_id = list(detection_categories.keys())[0]
-        for category_id in detection_categories:
-            
-            category_detections = [det for det in all_detections_this_video if \
-                                   det['category'] == category_id]
-            
-            # Find the nth-highest-confidence video to choose a confidence value
-            if len(category_detections) >= options.nth_highest_confidence:
-                
-                category_detections_by_confidence = sorted(category_detections, 
-                                                           key = lambda i: i['conf'],reverse=True)
-                canonical_detection = category_detections_by_confidence[options.nth_highest_confidence-1]
-                canonical_detections.append(canonical_detection)
-                                      
-        # Prepare the output representation for this video
-        im_out = {}
-        im_out['file'] = video_name
-        im_out['detections'] = canonical_detections
-        
-        # 'max_detection_conf' is no longer included in output files by default
-        if False:
-            im_out['max_detection_conf'] = 0
-            if len(canonical_detections) > 0:
-                confidences = [d['conf'] for d in canonical_detections]
-                im_out['max_detection_conf'] = max(confidences)
-        
-        output_images.append(im_out)
-        
-    # ...for each video
-    
-    output_data = input_data
-    output_data['images'] = output_images
-    s = json.dumps(output_data,indent=1)
-    
-    # Write the output file
-    with open(output_file,'w') as f:
-        f.write(s)
-    
-        
-#%% Test driver
-
-if False:
-
-    #%% Constants
-    
-    Fs = 30.01
-    confidence_threshold = 0.75
-    input_folder = 'z:\\'
-    frame_folder_base = r'e:\video_test\frames'
-    detected_frame_folder_base = r'e:\video_test\detected_frames'
-    rendered_videos_folder_base = r'e:\video_test\rendered_videos'
-    
-    results_file = r'results.json'
-    os.makedirs(detected_frame_folder_base,exist_ok=True)
-    os.makedirs(rendered_videos_folder_base,exist_ok=True)
-    
-    
-    #%% Split videos into frames
-        
-    frame_filenames_by_video,fs_by_video,video_filenames = \
-        video_folder_to_frames(input_folder,frame_folder_base,recursive=True)
-    
-    
-    #%% List image files, break into folders
-    
-    frame_files = path_utils.find_images(frame_folder_base,True)
-    frame_files = [s.replace('\\','/') for s in frame_files]
-    print('Enumerated {} total frames'.format(len(frame_files)))
-    
-    Fs = 30.01
-    # Find unique folders
-    folders = set()
-    # fn = frame_files[0]
-    for fn in frame_files:
-        folders.add(os.path.dirname(fn))
-    folders = [s.replace('\\','/') for s in folders]
-    print('Found {} folders for {} files'.format(len(folders),len(frame_files)))
-    
-        
-    #%% Load detector output
-    
-    with open(results_file,'r') as f:
-        detection_results = json.load(f)
-    detections = detection_results['images']
-    detector_label_map = detection_results['detection_categories']
-    for d in detections:
-        d['file'] = d['file'].replace('\\','/').replace('video_frames/','')
-
-
-    #%% Render detector frames
-    
-    # folder = list(folders)[0]
-    for folder in folders:
-        
-        frame_files_this_folder = [fn for fn in frame_files if folder in fn]
-        folder_relative = folder.replace((frame_folder_base + '/').replace('\\','/'),'')
-        detection_results_this_folder = [d for d in detections if folder_relative in d['file']]
-        print('Found {} detections in folder {}'.format(len(detection_results_this_folder),folder))
-        assert len(frame_files_this_folder) == len(detection_results_this_folder)
-        
-        rendered_frame_output_folder = os.path.join(detected_frame_folder_base,folder_relative)
-        os.makedirs(rendered_frame_output_folder,exist_ok=True)
-        
-        # d = detection_results_this_folder[0]
-        for d in tqdm(detection_results_this_folder):
-            
-            input_file = os.path.join(frame_folder_base,d['file'])
-            output_file = os.path.join(detected_frame_folder_base,d['file'])
-            os.makedirs(os.path.dirname(output_file),exist_ok=True)
-            vis_utils.draw_bounding_boxes_on_file(input_file,output_file,d['detections'],
-                                                  confidence_threshold)
-        
-        # ...for each file in this folder
-            
-    # ...for each folder
-
-
-    #%% Render output videos
-            
-    # folder = list(folders)[0]
-    for folder in tqdm(folders):
-        
-        folder_relative = folder.replace((frame_folder_base + '/').replace('\\','/'),'')
-        rendered_detector_output_folder = os.path.join(detected_frame_folder_base,folder_relative)
-        assert os.path.isdir(rendered_detector_output_folder)
-        
-        frame_files_relative = os.listdir(rendered_detector_output_folder)
-        frame_files_absolute = [os.path.join(rendered_detector_output_folder,s) \
-                                for s in frame_files_relative]
-        
-        output_video_filename = os.path.join(rendered_videos_folder_base,folder_relative)
-        os.makedirs(os.path.dirname(output_video_filename),exist_ok=True)
-        
-        original_video_filename = output_video_filename.replace(
-            rendered_videos_folder_base,input_folder)
-        assert os.path.isfile(original_video_filename)
-        Fs = get_video_fs(original_video_filename)
-                
-        frames_to_video(frame_files_absolute, Fs, output_video_filename)
-
-    # ...for each video
+"""
+
+video_utils.py
+
+Utilities for splitting, rendering, and assembling videos.
+
+"""
+
+#%% Constants, imports, environment
+
+import os
+import cv2
+import glob
+import json
+
+from collections import defaultdict
+from multiprocessing.pool import ThreadPool
+from multiprocessing.pool import Pool
+from tqdm import tqdm
+from functools import partial
+
+from md_utils import path_utils    
+from md_visualization import visualization_utils as vis_utils
+
+default_fourcc = 'h264'
+
+
+#%% Path utilities
+
+VIDEO_EXTENSIONS = ('.mp4','.avi','.mpeg','.mpg')
+
+def is_video_file(s,video_extensions=VIDEO_EXTENSIONS):
+    """
+    Checks a file's extension against a set of known video file
+    extensions to determine whether it's a video file.  Performs a
+    case-insensitive comparison.
+    
+    Args:
+        s (str): filename to check for probable video-ness
+        video_extensions (list, optional): list of video file extensions
+    
+    Returns:
+        bool: True if this looks like a video file, else False
+    """
+    
+    ext = os.path.splitext(s)[1]
+    return ext.lower() in video_extensions
+
+
+def find_video_strings(strings):
+    """
+    Given a list of strings that are potentially video file names, looks for
+    strings that actually look like video file names (based on extension).
+    
+    Args:
+        strings (list): list of strings to check for video-ness
+    
+    Returns:
+        list: a subset of [strings] that looks like they are video filenames
+    """
+    
+    return [s for s in strings if is_video_file(s.lower())]
+
+
+def find_videos(dirname, 
+                recursive=False,
+                convert_slashes=True,
+                return_relative_paths=False):
+    """
+    Finds all files in a directory that look like video file names.
+    
+    Args:
+        dirname (str): folder to search for video files
+        recursive (bool, optional): whether to search [dirname] recursively
+        convert_slashes (bool, optional): forces forward slashes in the returned files,
+            otherwise uses the native path separator
+        return_relative_paths (bool, optional): forces the returned filenames to be 
+            relative to [dirname], otherwise returns absolute paths
+    
+    Returns:
+        A list of filenames within [dirname] that appear to be videos
+    """
+    
+    if recursive:
+        files = glob.glob(os.path.join(dirname, '**', '*.*'), recursive=True)
+    else:
+        files = glob.glob(os.path.join(dirname, '*.*'))
+        
+    if return_relative_paths:
+        files = [os.path.relpath(fn,dirname) for fn in files]
+
+    if convert_slashes:
+        files = [fn.replace('\\', '/') for fn in files]
+    
+    return find_video_strings(files)
+
+
+#%% Function for rendering frames to video and vice-versa
+
+# http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html
+
+def frames_to_video(images, Fs, output_file_name, codec_spec=default_fourcc):
+    """
+    Given a list of image files and a sample rate, concatenates those images into
+    a video and writes to a new video file.
+    
+    Args:
+        images (list): a list of frame file names to concatenate into a video
+        Fs (float): the frame rate in fps
+        output_file_name (str): the output video file, no checking is performed to make
+            sure the extension is compatible with the codec
+        codec_spec (str, optional):  codec to use for encoding; h264 is a sensible default 
+            and generally works on Windows, but when this fails (which is around 50% of the time 
+            on Linux), mp4v is a good second choice
+    """
+    
+    if codec_spec is None:
+        codec_spec = 'h264'
+        
+    if len(images) == 0:
+        return
+
+    # Determine the width and height from the first image
+    frame = cv2.imread(images[0])
+    cv2.imshow('video',frame)
+    height, width, channels = frame.shape
+
+    # Define the codec and create VideoWriter object
+    fourcc = cv2.VideoWriter_fourcc(*codec_spec)
+    out = cv2.VideoWriter(output_file_name, fourcc, Fs, (width, height))
+
+    for image in images:
+        frame = cv2.imread(image)
+        out.write(frame)
+
+    out.release()
+    cv2.destroyAllWindows()
+
+
+def get_video_fs(input_video_file):
+    """
+    Retrieves the frame rate of [input_video_file].
+    
+    Args:
+        input_video_file (str): video file for which we want the frame rate
+        
+    Returns:
+        float: the frame rate of [input_video_file]
+    """
+    
+    assert os.path.isfile(input_video_file), 'File {} not found'.format(input_video_file)    
+    vidcap = cv2.VideoCapture(input_video_file)
+    Fs = vidcap.get(cv2.CAP_PROP_FPS)
+    vidcap.release()
+    return Fs
+
+
+def _frame_number_to_filename(frame_number):
+    """
+    Ensures that frame images are given consistent filenames.
+    """
+    
+    return 'frame{:06d}.jpg'.format(frame_number)
+
+
+def video_to_frames(input_video_file, output_folder, overwrite=True, 
+                    every_n_frames=None, verbose=False):
+    """
+    Renders frames from [input_video_file] to a .jpg in [output_folder].
+    
+    With help from:
+        
+    https://stackoverflow.com/questions/33311153/python-extracting-and-saving-video-frames
+    
+    Args:
+        input_video_file (str): video file to split into frames
+        output_folder (str): folder to put frame images in
+        overwrite (bool, optional): whether to overwrite existing frame images
+        every_n_frames (int, optional): sample every Nth frame starting from the first frame;
+            if this is None or 1, every frame is extracted
+        verbose (bool, optional): enable additional debug console output
+    
+    Returns:
+        tuple: length-2 tuple containing (list of frame filenames,frame rate)
+    """
+    
+    assert os.path.isfile(input_video_file), 'File {} not found'.format(input_video_file)
+    
+    vidcap = cv2.VideoCapture(input_video_file)
+    n_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))
+    Fs = vidcap.get(cv2.CAP_PROP_FPS)
+    
+    # If we're not over-writing, check whether all frame images already exist
+    if overwrite == False:
+        
+        missing_frame_number = None
+        frame_filenames = []
+        
+        for frame_number in range(0,n_frames):
+            
+            if every_n_frames is not None:
+                if frame_number % every_n_frames != 0:
+                    continue
+            
+            frame_filename = _frame_number_to_filename(frame_number)
+            frame_filename = os.path.join(output_folder,frame_filename)
+            frame_filenames.append(frame_filename)
+            if os.path.isfile(frame_filename):
+                continue
+            else:
+                missing_frame_number = frame_number
+                break
+    
+        # OpenCV seems to over-report the number of frames by 1 in some cases, or fails
+        # to read the last frame; either way, I'm allowing one missing frame.
+        allow_last_frame_missing = True
+        
+        if missing_frame_number is None or \
+            (allow_last_frame_missing and (missing_frame_number == n_frames-1)):
+            if verbose:
+                print('Skipping video {}, all output frames exist'.format(input_video_file))
+            return frame_filenames,Fs
+        else:
+            pass
+            # print("Rendering video {}, couldn't find frame {}".format(
+            #    input_video_file,missing_frame_number))
+    
+    # ...if we need to check whether to skip this video entirely
+        
+    if verbose:
+        print('Reading {} frames at {} Hz from {}'.format(n_frames,Fs,input_video_file))
+
+    frame_filenames = []
+
+    # for frame_number in tqdm(range(0,n_frames)):
+    for frame_number in range(0,n_frames):
+
+        success,image = vidcap.read()
+        if not success:
+            assert image is None
+            if verbose:
+                print('Read terminating at frame {} of {}'.format(frame_number,n_frames))
+            break
+
+        if every_n_frames is not None:
+            if frame_number % every_n_frames != 0:
+                continue
+            
+        frame_filename = _frame_number_to_filename(frame_number)
+        frame_filename = os.path.join(output_folder,frame_filename)
+        frame_filenames.append(frame_filename)
+        
+        if overwrite == False and os.path.isfile(frame_filename):
+            # print('Skipping frame {}'.format(frame_filename))
+            pass            
+        else:
+            try:
+                if frame_filename.isascii():
+                    cv2.imwrite(os.path.normpath(frame_filename),image)
+                else:
+                    is_success, im_buf_arr = cv2.imencode('.jpg', image)
+                    im_buf_arr.tofile(frame_filename)
+                assert os.path.isfile(frame_filename), \
+                    'Output frame {} unavailable'.format(frame_filename)
+            except KeyboardInterrupt:
+                vidcap.release()
+                raise
+            except Exception as e:
+                print('Error on frame {} of {}: {}'.format(frame_number,n_frames,str(e)))
+
+    if verbose:
+        print('\nExtracted {} of {} frames'.format(len(frame_filenames),n_frames))
+
+    vidcap.release()    
+    return frame_filenames,Fs
+
+# ...def video_to_frames(...)
+
+
+def _video_to_frames_for_folder(relative_fn,input_folder,output_folder_base,every_n_frames,overwrite,verbose):
+    """
+    Internal function to call video_to_frames in the context of video_folder_to_frames; 
+    makes sure the right output folder exists, then calls video_to_frames.
+    """    
+    
+    input_fn_absolute = os.path.join(input_folder,relative_fn)
+    assert os.path.isfile(input_fn_absolute),\
+        'Could not find file {}'.format(input_fn_absolute)
+
+    # Create the target output folder
+    output_folder_video = os.path.join(output_folder_base,relative_fn)
+    os.makedirs(output_folder_video,exist_ok=True)
+
+    # Render frames
+    # input_video_file = input_fn_absolute; output_folder = output_folder_video
+    frame_filenames,fs = video_to_frames(input_fn_absolute,output_folder_video,
+                                         overwrite=overwrite,every_n_frames=every_n_frames,
+                                         verbose=verbose)
+    
+    return frame_filenames,fs
+
+
+def video_folder_to_frames(input_folder, output_folder_base, 
+                           recursive=True, overwrite=True,
+                           n_threads=1, every_n_frames=None,
+                           verbose=False, parallelization_uses_threads=True):
+    """
+    For every video file in input_folder, creates a folder within output_folder_base, and 
+    renders frame of that video to images in that folder.
+    
+    Args:
+        input_folder (str): folder to process
+        output_folder_base (str): root folder for output images; subfolders will be
+            created for each input video
+        recursive (bool, optional): whether to recursively process videos in [input_folder]
+        overwrite (bool, optional): whether to overwrite existing frame images
+        n_threads (int, optional): number of concurrent workers to use; set to <= 1 to disable
+            parallelism
+        every_n_frames (int, optional): sample every Nth frame starting from the first frame;
+            if this is None or 1, every frame is extracted
+        verbose (bool, optional): enable additional debug console output
+        parallelization_uses_threads (bool, optional): whether to use threads (True) or
+            processes (False) for parallelization; ignored if n_threads <= 1
+            
+    Returns:
+        tuple: a length-3 tuple containing:
+            - list of lists of frame filenames; the Nth list of frame filenames corresponds to 
+              the Nth video
+            - list of video frame rates; the Nth value corresponds to the Nth video
+            - list of video filenames    
+    """
+    
+    # Recursively enumerate video files
+    input_files_full_paths = find_videos(input_folder,recursive=recursive)
+    print('Found {} videos in folder {}'.format(len(input_files_full_paths),input_folder))
+    if len(input_files_full_paths) == 0:
+        return [],[],[]
+    
+    input_files_relative_paths = [os.path.relpath(s,input_folder) for s in input_files_full_paths]
+    input_files_relative_paths = [s.replace('\\','/') for s in input_files_relative_paths]
+    
+    os.makedirs(output_folder_base,exist_ok=True)    
+    
+    frame_filenames_by_video = []
+    fs_by_video = []
+    
+    if n_threads == 1:
+        # For each video
+        #
+        # input_fn_relative = input_files_relative_paths[0]
+        for input_fn_relative in tqdm(input_files_relative_paths):
+        
+            frame_filenames,fs = \
+                _video_to_frames_for_folder(input_fn_relative,input_folder,output_folder_base,
+                                            every_n_frames,overwrite,verbose)
+            frame_filenames_by_video.append(frame_filenames)
+            fs_by_video.append(fs)
+    else:
+        if parallelization_uses_threads:
+            print('Starting a worker pool with {} threads'.format(n_threads))
+            pool = ThreadPool(n_threads)
+        else:
+            print('Starting a worker pool with {} processes'.format(n_threads))
+            pool = Pool(n_threads)
+        process_video_with_options = partial(_video_to_frames_for_folder, 
+                                             input_folder=input_folder,
+                                             output_folder_base=output_folder_base,
+                                             every_n_frames=every_n_frames,
+                                             overwrite=overwrite,
+                                             verbose=verbose)
+        results = list(tqdm(pool.imap(
+            partial(process_video_with_options),input_files_relative_paths), 
+                            total=len(input_files_relative_paths)))
+        frame_filenames_by_video = [x[0] for x in results]
+        fs_by_video = [x[1] for x in results]
+        
+    return frame_filenames_by_video,fs_by_video,input_files_full_paths
+  
+# ...def video_folder_to_frames(...)
+
+
+class FrameToVideoOptions:
+    """
+    Options controlling the conversion of frame-level results to video-level results via
+    frame_results_to_video_results()    
+    """
+    
+    #: One-indexed indicator of which frame-level confidence value to use to determine detection confidence
+    #: for the whole video, i.e. "1" means "use the confidence value from the highest-confidence frame"
+    nth_highest_confidence = 1
+    
+    #: What to do if a file referred to in a .json results file appears not to be a 
+    #: video; can be 'error' or 'skip_with_warning'
+    non_video_behavior = 'error'
+    
+    
+def frame_results_to_video_results(input_file,output_file,options=None):
+    """
+    Given an MD results file produced at the *frame* level, corresponding to a directory 
+    created with video_folder_to_frames, maps those frame-level results back to the 
+    video level for use in Timelapse.
+    
+    Preserves everything in the input .json file other than the images.
+    
+    Args:
+        input_file (str): the frame-level MD results file to convert to video-level results
+        output_file (str): the .json file to which we should write video-level results
+        options (FrameToVideoOptions, optional): parameters for converting frame-level results
+            to video-level results, see FrameToVideoOptions for details            
+    """
+
+    if options is None:
+        options = FrameToVideoOptions()
+        
+    # Load results
+    with open(input_file,'r') as f:
+        input_data = json.load(f)
+
+    images = input_data['images']
+    detection_categories = input_data['detection_categories']
+    
+    ## Break into videos
+    
+    video_to_frames = defaultdict(list) 
+    
+    # im = images[0]
+    for im in tqdm(images):
+        
+        fn = im['file']
+        video_name = os.path.dirname(fn)
+        if not is_video_file(video_name):
+            if options.non_video_behavior == 'error':
+                raise ValueError('{} is not a video file'.format(video_name))
+            elif options.non_video_behavior == 'skip_with_warning':
+                print('Warning: {} is not a video file'.format(video_name))
+                continue
+            else:
+                raise ValueError('Unrecognized non-video handling behavior: {}'.format(
+                    options.non_video_behavior))
+        video_to_frames[video_name].append(im)
+    
+    print('Found {} unique videos in {} frame-level results'.format(
+        len(video_to_frames),len(images)))
+    
+    output_images = []
+    
+    ## For each video...
+    
+    # video_name = list(video_to_frames.keys())[0]
+    for video_name in tqdm(video_to_frames):
+        
+        frames = video_to_frames[video_name]
+        
+        all_detections_this_video = []
+        
+        # frame = frames[0]
+        for frame in frames:
+            if frame['detections'] is not None:
+                all_detections_this_video.extend(frame['detections'])
+            
+        # At most one detection for each category for the whole video
+        canonical_detections = []
+            
+        # category_id = list(detection_categories.keys())[0]
+        for category_id in detection_categories:
+            
+            category_detections = [det for det in all_detections_this_video if \
+                                   det['category'] == category_id]
+            
+            # Find the nth-highest-confidence video to choose a confidence value
+            if len(category_detections) >= options.nth_highest_confidence:
+                
+                category_detections_by_confidence = sorted(category_detections, 
+                                                           key = lambda i: i['conf'],reverse=True)
+                canonical_detection = category_detections_by_confidence[options.nth_highest_confidence-1]
+                canonical_detections.append(canonical_detection)
+                                      
+        # Prepare the output representation for this video
+        im_out = {}
+        im_out['file'] = video_name
+        im_out['detections'] = canonical_detections
+        
+        # 'max_detection_conf' is no longer included in output files by default
+        if False:
+            im_out['max_detection_conf'] = 0
+            if len(canonical_detections) > 0:
+                confidences = [d['conf'] for d in canonical_detections]
+                im_out['max_detection_conf'] = max(confidences)
+        
+        output_images.append(im_out)
+        
+    # ...for each video
+    
+    output_data = input_data
+    output_data['images'] = output_images
+    s = json.dumps(output_data,indent=1)
+    
+    # Write the output file
+    with open(output_file,'w') as f:
+        f.write(s)
+    
+# ...def frame_results_to_video_results(...)
+
+
+#%% Test driver
+
+if False:
+
+    #%% Constants
+    
+    Fs = 30.01
+    confidence_threshold = 0.75
+    input_folder = 'z:\\'
+    frame_folder_base = r'e:\video_test\frames'
+    detected_frame_folder_base = r'e:\video_test\detected_frames'
+    rendered_videos_folder_base = r'e:\video_test\rendered_videos'
+    
+    results_file = r'results.json'
+    os.makedirs(detected_frame_folder_base,exist_ok=True)
+    os.makedirs(rendered_videos_folder_base,exist_ok=True)
+    
+    
+    #%% Split videos into frames
+        
+    frame_filenames_by_video,fs_by_video,video_filenames = \
+        video_folder_to_frames(input_folder,frame_folder_base,recursive=True)
+    
+    
+    #%% List image files, break into folders
+    
+    frame_files = path_utils.find_images(frame_folder_base,True)
+    frame_files = [s.replace('\\','/') for s in frame_files]
+    print('Enumerated {} total frames'.format(len(frame_files)))
+    
+    Fs = 30.01
+    # Find unique folders
+    folders = set()
+    # fn = frame_files[0]
+    for fn in frame_files:
+        folders.add(os.path.dirname(fn))
+    folders = [s.replace('\\','/') for s in folders]
+    print('Found {} folders for {} files'.format(len(folders),len(frame_files)))
+    
+        
+    #%% Load detector output
+    
+    with open(results_file,'r') as f:
+        detection_results = json.load(f)
+    detections = detection_results['images']
+    detector_label_map = detection_results['detection_categories']
+    for d in detections:
+        d['file'] = d['file'].replace('\\','/').replace('video_frames/','')
+
+
+    #%% Render detector frames
+    
+    # folder = list(folders)[0]
+    for folder in folders:
+        
+        frame_files_this_folder = [fn for fn in frame_files if folder in fn]
+        folder_relative = folder.replace((frame_folder_base + '/').replace('\\','/'),'')
+        detection_results_this_folder = [d for d in detections if folder_relative in d['file']]
+        print('Found {} detections in folder {}'.format(len(detection_results_this_folder),folder))
+        assert len(frame_files_this_folder) == len(detection_results_this_folder)
+        
+        rendered_frame_output_folder = os.path.join(detected_frame_folder_base,folder_relative)
+        os.makedirs(rendered_frame_output_folder,exist_ok=True)
+        
+        # d = detection_results_this_folder[0]
+        for d in tqdm(detection_results_this_folder):
+            
+            input_file = os.path.join(frame_folder_base,d['file'])
+            output_file = os.path.join(detected_frame_folder_base,d['file'])
+            os.makedirs(os.path.dirname(output_file),exist_ok=True)
+            vis_utils.draw_bounding_boxes_on_file(input_file,output_file,d['detections'],
+                                                  confidence_threshold)
+        
+        # ...for each file in this folder
+            
+    # ...for each folder
+
+
+    #%% Render output videos
+            
+    # folder = list(folders)[0]
+    for folder in tqdm(folders):
+        
+        folder_relative = folder.replace((frame_folder_base + '/').replace('\\','/'),'')
+        rendered_detector_output_folder = os.path.join(detected_frame_folder_base,folder_relative)
+        assert os.path.isdir(rendered_detector_output_folder)
+        
+        frame_files_relative = os.listdir(rendered_detector_output_folder)
+        frame_files_absolute = [os.path.join(rendered_detector_output_folder,s) \
+                                for s in frame_files_relative]
+        
+        output_video_filename = os.path.join(rendered_videos_folder_base,folder_relative)
+        os.makedirs(os.path.dirname(output_video_filename),exist_ok=True)
+        
+        original_video_filename = output_video_filename.replace(
+            rendered_videos_folder_base,input_folder)
+        assert os.path.isfile(original_video_filename)
+        Fs = get_video_fs(original_video_filename)
+                
+        frames_to_video(frame_files_absolute, Fs, output_video_filename)
+
+    # ...for each video
```

### Comparing `megadetector-5.0.8/md_utils/azure_utils.py` & `megadetector-5.0.9/md_utils/azure_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-########
-#
-# azure_utils.py
-#
-# Miscellaneous Azure Blob Storage utilities
-#
-# Requires azure-storage-blob>=12.4.0
-#
-########
+"""
+
+azure_utils.py
+
+Miscellaneous Azure Blob Storage utilities
+
+Requires azure-storage-blob>=12.4.0
+
+"""
 
 import json
 from md_utils import path_utils
 from typing import Any, Iterable, List, Optional, Tuple, Union
 
 from azure.storage.blob import BlobPrefix, ContainerClient
```

### Comparing `megadetector-5.0.8/md_utils/ct_utils.py` & `megadetector-5.0.9/md_utils/ct_utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,18 +1,17 @@
-########
-#
-# ct_utils.py
-#
-# Numeric/geometry utility functions
-#
-########
+"""
+
+ct_utils.py
+
+Numeric/geometry/array utility functions.
+
+"""
 
 #%% Imports and constants
 
-import argparse
 import inspect
 import json
 import math
 import os
 
 import jsonpickle
 import numpy as np
@@ -22,20 +21,23 @@
 image_extensions = ['.jpg', '.jpeg', '.gif', '.png']
 
 
 #%% Functions
 
 def truncate_float_array(xs, precision=3):
     """
-    Vectorized version of truncate_float(...)
+    Vectorized version of truncate_float(...), truncates the fractional portion of each
+    floating-point value to a specific number of floating-point digits.
 
     Args:
-    xs        (list of float) List of floats to truncate
-    precision (int)           The number of significant digits to preserve, should be
-                              greater or equal 1
+        xs (list): list of floats to truncate
+        precision (int, optional): the number of significant digits to preserve, should be >= 1            
+            
+    Returns:
+        list: list of truncated floats
     """
 
     return [truncate_float(x, precision=precision) for x in xs]
 
 
 def truncate_float(x, precision=3):
     """
@@ -47,17 +49,19 @@
         truncate_float(0.0003214884) --> 0.000321
         truncate_float(1.0003214884) --> 1.000321
     
     This function is primarily used to achieve a certain float representation
     before exporting to JSON.
 
     Args:
-    x         (float) Scalar to truncate
-    precision (int)   The number of significant digits to preserve, should be
-                      greater or equal 1
+        x (float): scalar to truncate
+        precision (int, optional): the number of significant digits to preserve, should be >= 1
+        
+    Returns:
+        float: truncated version of [x]
     """
 
     assert precision > 0
 
     if np.isclose(x, 0):
         
         return 0
@@ -74,146 +78,158 @@
         factor = math.pow(10, precision - 1 - math.floor(math.log10(abs(x))))
         
         # Shift decimal point by multiplication with factor, flooring, and
         # division by factor.
         return math.floor(x * factor)/factor
 
 
-def args_to_object(args: argparse.Namespace, obj: object) -> None:
+def args_to_object(args, obj):
     """
     Copies all fields from a Namespace (typically the output from parse_args) to an
     object. Skips fields starting with _. Does not check existence in the target
     object.
 
     Args:
-        args: argparse.Namespace
-        obj: class or object whose whose attributes will be updated
+        args (argparse.Namespace): the namespace to convert to an object
+        obj (object): object whose whose attributes will be updated
+        
+    Returns:
+        object: the modified object (modified in place, but also returned)
     """
     
     for n, v in inspect.getmembers(args):
         if not n.startswith('_'):
             setattr(obj, n, v)
 
+    return obj
+
 
 def pretty_print_object(obj, b_print=True):
     """
-    Prints an arbitrary object as .json
+    Converts an arbitrary object to .json, optionally printing the .json representation.
+    
+    Args:
+        obj (object): object to print
+        b_print (bool, optional): whether to print the object
+        
+    Returns:
+        str: .json reprepresentation of [obj]
     """
 
     # _ = pretty_print_object(obj)
 
-    # Sloppy that I'm making a module-wide change here...
+    # TODO: it's sloppy that I'm making a module-wide change here.
     jsonpickle.set_encoder_options('json', sort_keys=True, indent=2)
     a = jsonpickle.encode(obj)
     s = '{}'.format(a)
     if b_print:
         print(s)
     return s
 
 
-def is_list_sorted(L,reverse=False):
+def is_list_sorted(L, reverse=False):
     """
-    Returns true if the list L appears to be sorted, otherwise False.
+    Returns True if the list L appears to be sorted, otherwise False.
     
     Calling is_list_sorted(L,reverse=True) is the same as calling
     is_list_sorted(L.reverse(),reverse=False).
+    
+    Args:
+        L (list): list to evaluate
+        reverse (bool, optional): whether to reverse the list before evaluating sort status 
+    
+    Returns:
+        bool: True if the list L appears to be sorted, otherwise False
     """
     
     if reverse:
         return all(L[i] >= L[i + 1] for i in range(len(L)-1))
     else:
         return all(L[i] <= L[i + 1] for i in range(len(L)-1))
         
 
 def write_json(path, content, indent=1):
     """
-    Standardized wrapper for json.dump
+    Standardized wrapper for json.dump().
+    
+    Args:
+        path (str): filename to write to
+        content (object): object to dump
+        indent (int, optional): indentation depth passed to json.dump
     """
     
     with open(path, 'w') as f:
         json.dump(content, f, indent=indent)
 
 
-def is_image_file(s):
-    """
-    Checks a file's extension against a hard-coded set of image file extensions; 
-    return True if it appears to be an image.
-    """
-
-    ext = os.path.splitext(s)[1]
-    return ext.lower() in image_extensions
-
-
 def convert_yolo_to_xywh(yolo_box):
     """
     Converts a YOLO format bounding box to [x_min, y_min, width_of_box, height_of_box].
 
     Args:
-        yolo_box: bounding box of format [x_center, y_center, width_of_box, height_of_box].
+        yolo_box (list): bounding box of format [x_center, y_center, width_of_box, height_of_box]
 
     Returns:
-        bbox with coordinates represented as [x_min, y_min, width_of_box, height_of_box].
+        list: bbox with coordinates represented as [x_min, y_min, width_of_box, height_of_box]
     """
     
     x_center, y_center, width_of_box, height_of_box = yolo_box
     x_min = x_center - width_of_box / 2.0
     y_min = y_center - height_of_box / 2.0
     return [x_min, y_min, width_of_box, height_of_box]
 
 
 def convert_xywh_to_tf(api_box):
     """
-    Converts an xywh bounding box to an [y_min, x_min, y_max, x_max] box that the TensorFlow
-    Object Detection API uses
+    Converts an xywh bounding box (the format used in MD output) to the [y_min, x_min, y_max, x_max] 
+    format that the TensorFlow Object Detection API uses.
 
     Args:
         api_box: bbox output by the batch processing API [x_min, y_min, width_of_box, height_of_box]
 
     Returns:
-        bbox with coordinates represented as [y_min, x_min, y_max, x_max]
+        list: bbox with coordinates represented as [y_min, x_min, y_max, x_max]
     """
     
     x_min, y_min, width_of_box, height_of_box = api_box
     x_max = x_min + width_of_box
     y_max = y_min + height_of_box
     return [y_min, x_min, y_max, x_max]
 
 
 def convert_xywh_to_xyxy(api_bbox):
     """
-    Converts an xywh bounding box to an xyxy bounding box.
+    Converts an xywh bounding box (the MD output format) to an xyxy bounding box.
 
-    Note that this is also different from the TensorFlow Object Detection API coords format.
-    
     Args:
-        api_bbox: bbox output by the batch processing API [x_min, y_min, width_of_box, height_of_box]
+        api_bbox (list): bbox formatted as [x_min, y_min, width_of_box, height_of_box]
 
     Returns:
-        bbox with coordinates represented as [x_min, y_min, x_max, y_max]
+        list: bbox formatted as [x_min, y_min, x_max, y_max]
     """
 
     x_min, y_min, width_of_box, height_of_box = api_bbox
     x_max, y_max = x_min + width_of_box, y_min + height_of_box
     return [x_min, y_min, x_max, y_max]
 
 
 def get_iou(bb1, bb2):
     """
-    Calculates the Intersection over Union (IoU) of two bounding boxes.
+    Calculates the intersection over union (IoU) of two bounding boxes.
 
     Adapted from:
         
     https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation
 
     Args:
-        bb1: [x_min, y_min, width_of_box, height_of_box]
-        bb2: [x_min, y_min, width_of_box, height_of_box]
+        bb1 (list): [x_min, y_min, width_of_box, height_of_box]
+        bb2 (list): [x_min, y_min, width_of_box, height_of_box]
 
     Returns:
-        intersection_over_union, a float in [0, 1]
+        float: intersection_over_union, a float in [0, 1]
     """
 
     bb1 = convert_xywh_to_xyxy(bb1)
     bb2 = convert_xywh_to_xyxy(bb2)
 
     assert bb1[0] < bb1[2], 'Malformed bounding box (x2 >= x1)'
     assert bb1[1] < bb1[3], 'Malformed bounding box (y2 >= y1)'
@@ -257,42 +273,62 @@
         confidences = [det['conf'] for det in detections]
         max_conf = max(confidences)
     return max_conf
 
 
 def get_max_conf(im):
     """
-    Given an image dict in the format used by the batch API, compute the maximum detection
-    confidence for any class.  Returns 0.0 (not None) if there was a failure and 'detections'
-    isn't present.
+    Given an image dict in the MD output format, computes the maximum detection confidence for any 
+    class.  Returns 0.0 (rather than None) if there was a failure or 'detections' isn't present.
+    
+    Args:
+        im (dict): image dictionary in the MD output format (with a 'detections' field)
+        
+    Returns:
+        float: the maximum detection confidence across all classes
     """
     
     max_conf = 0.0
     if 'detections' in im and im['detections'] is not None and len(im['detections']) > 0:
         max_conf = _get_max_conf_from_detections(im['detections'])
     return max_conf
 
 
 def point_dist(p1,p2):
     """
-    Distance between two points, represented as length-two tuples.
+    Computes the distance between two points, represented as length-two tuples.
+    
+    Args:
+        p1: point, formatted as (x,y)
+        p2: point, formatted as (x,y)
+        
+    Returns:
+        float: the Euclidean distance between p1 and p2
     """
     
     return math.sqrt( ((p1[0]-p2[0])**2) + ((p1[1]-p2[1])**2) )
 
 
 def rect_distance(r1, r2, format='x0y0x1y1'):
     """
-    Minimum distance between two axis-aligned rectangles, each represented as 
+    Computes the minimum distance between two axis-aligned rectangles, each represented as 
     (x0,y0,x1,y1) by default.
     
-    Can also specify "format" as x0y0wh for MD-style bbox formatting (x0,y0,w,h).    
+    Can also specify "format" as x0y0wh for MD-style bbox formatting (x0,y0,w,h).
+    
+    Args:
+        r1: rectangle, formatted as (x0,y0,x1,y1) or (x0,y0,xy,y1)
+        r2: rectangle, formatted as (x0,y0,x1,y1) or (x0,y0,xy,y1)
+        format (str, optional): whether the boxes are formatted as 'x0y0x1y1' (default) or 'x0y0wh'
+        
+    Returns:
+        float: the minimum distance between r1 and r2
     """
     
-    assert format in ('x0y0x1y1','x0y0wh')
+    assert format in ('x0y0x1y1','x0y0wh'), 'Illegal rectangle format {}'.format(format)
     
     if format == 'x0y0wh':
         # Convert to x0y0x1y1 without modifying the original rectangles
         r1 = [r1[0],r1[1],r1[0]+r1[2],r1[1]+r1[3]]
         r2 = [r2[0],r2[1],r2[0]+r2[2],r2[1]+r2[3]]
         
     # https://stackoverflow.com/a/26178015
@@ -318,39 +354,46 @@
         return y1 - y2b
     elif top:
         return y2 - y1b
     else:
         return 0.0
 
 
-def list_is_sorted(l):
-    """
-    Returns True if the list [l] is sorted, else False.
-    """
-    
-    return all(l[i] <= l[i+1] for i in range(len(l)-1))
-
-
 def split_list_into_fixed_size_chunks(L,n):
     """
     Split the list or tuple L into chunks of size n (allowing chunks of size n-1 if necessary,
-    i.e. len(L) does not have to be a multiple of n.
+    i.e. len(L) does not have to be a multiple of n).
+    
+    Args:
+        L (list): list to split into chunks
+        n (int): preferred chunk size
+        
+    Returns:
+        list: list of chunks, where each chunk is a list of length n or n-1
     """
     
     return [L[i * n:(i + 1) * n] for i in range((len(L) + n - 1) // n )]
 
 
 def split_list_into_n_chunks(L, n, chunk_strategy='greedy'):
     """
     Splits the list or tuple L into n equally-sized chunks (some chunks may be one 
-    element smaller than others, i.e. len(L) does not have to be a multiple of n.
+    element smaller than others, i.e. len(L) does not have to be a multiple of n).
     
     chunk_strategy can be "greedy" (default, if there are k samples per chunk, the first
     k go into the first chunk) or "balanced" (alternate between chunks when pulling
     items from the list).
+                                              
+    Args:
+        L (list): list to split into chunks
+        n (int): number of chunks
+        chunk_strategy (str, optiopnal): "greedy" or "balanced"; see above
+        
+    Returns:
+        list: list of chunks, each of which is a list
     """
     
     if chunk_strategy == 'greedy':
         k, m = divmod(len(L), n)
         return list(L[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))
     elif chunk_strategy == 'balanced':
         chunks = [ [] for _ in range(n) ]
@@ -361,56 +404,84 @@
     else:
         raise ValueError('Invalid chunk strategy: {}'.format(chunk_strategy))
 
 
 def sort_dictionary_by_key(d,reverse=False):
     """
     Sorts the dictionary [d] by key.
+    
+    Args:
+        d (dict): dictionary to sort
+        reverse (bool, optional): whether to sort in reverse (descending) order
+        
+    Returns:
+        dict: sorted copy of [d]
     """
     
     d = dict(sorted(d.items(),reverse=reverse))
     return d
     
 
 def sort_dictionary_by_value(d,sort_values=None,reverse=False):
     """
     Sorts the dictionary [d] by value.  If sort_values is None, uses d.values(),
     otherwise uses the dictionary sort_values as the sorting criterion.
+    
+    Args:
+        d (dict): dictionary to sort
+        sort_values (dict, optional): dictionary mapping keys in [d] to sort values (defaults 
+            to None, uses [d] itself for sorting)
+        reverse (bool, optional): whether to sort in reverse (descending) order
+    
+    Returns:
+        dict: sorted copy of [d]
     """
     
     if sort_values is None:
         d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=reverse)}
     else:
         d = {k: v for k, v in sorted(d.items(), key=lambda item: sort_values[item[0]], reverse=reverse)}
     return d
 
 
 def invert_dictionary(d):
     """
-    Create a new dictionary that maps d.values() to d.keys().  Does not check
-    uniqueness.    
+    Creates a new dictionary that maps d.values() to d.keys().  Does not check
+    uniqueness.
+    
+    Args:
+        d (dict): dictionary to invert
+    
+    Returns:
+        dict: inverted copy of [d]
     """
     
     return {v: k for k, v in d.items()}
 
 
 def image_file_to_camera_folder(image_fn):
-    """
-    Remove common overflow folders (e.g. RECNX101, RECNX102) from paths, i.e. turn:
+    r"""
+    Removes common overflow folders (e.g. RECNX101, RECNX102) from paths, i.e. turn:
         
     a\b\c\RECNX101\image001.jpg
     
     ...into:
         
     a\b\c
 
     Returns the same thing as os.dirname() (i.e., just the folder name) if no overflow folders are 
     present.
 
     Always converts backslashes to slashes.
+    
+    Args:
+        image_fn (str): the image filename from which we should remove overflow folders
+        
+    Returns:
+        str: a version of [image_fn] from which camera overflow folders have been removed
     """
     
     import re
     
     # 100RECNX is the overflow folder style for Reconyx cameras
     # 100EK113 is (for some reason) the overflow folder style for Bushnell cameras
     # 100_BTCF is the overflow folder style for Browning cameras
@@ -421,14 +492,103 @@
     for pat in patterns:
         image_fn = re.sub(pat,'/',image_fn)
     camera_folder = os.path.dirname(image_fn)
     
     return camera_folder
     
 
+def is_float(v):
+    """
+    Determines whether v is either a float or a string representation of a float.
+    
+    Args:
+        v (object): object to evaluate
+        
+    Returns:
+        bool: True if [v] is a float or a string representation of a float, otherwise False
+    """
+    
+    try:
+        _ = float(v)
+        return True
+    except ValueError:
+        return False
+
+
+def is_iterable(x):
+    """
+    Uses duck typing to assess whether [x] is iterable (list, set, dict, etc.).
+    
+    Args:
+        x (object): the object to test
+    
+    Returns:
+        bool: True if [x] appears to be iterable, otherwise False
+    """
+    
+    try:
+        _ = iter(x)
+    except:
+       return False
+    return True
+
+
+def is_empty(v):
+    """
+    A common definition of "empty" used throughout the repo, particularly when loading
+    data from .csv files.  "empty" includes None, '', and NaN.
+    
+    Args:
+        v: the object to evaluate for emptiness
+        
+    Returns:
+        bool: True if [v] is None, '', or NaN, otherwise False
+    """
+    if v is None:
+        return True
+    if isinstance(v,str) and v == '':
+        return True
+    if isinstance(v,float) and np.isnan(v):
+        return True
+    return False
+
+
+def isnan(v):
+    """
+    Returns True if v is a nan-valued float, otherwise returns False.
+    
+    Args:
+        v: the object to evaluate for nan-ness
+    
+    Returns:
+        bool: True if v is a nan-valued float, otherwise False
+    """
+    
+    try:        
+        return np.isnan(v)
+    except Exception:
+        return False
+
+
+def sets_overlap(set1, set2):
+    """
+    Determines whether two sets overlap.
+    
+    Args:
+        set1 (set): the first set to compare (converted to a set if it's not already)
+        set2 (set): the second set to compare (converted to a set if it's not already)
+        
+    Returns:
+        bool: True if any elements are shared between set1 and set2
+    """
+    
+    return not set(set1).isdisjoint(set(set2))
+
+
+
 #%% Test drivers
 
 if False:
     
     pass
     
     #%% Test image_file_to_camera_folder()
```

### Comparing `megadetector-5.0.8/md_utils/directory_listing.py` & `megadetector-5.0.9/md_utils/directory_listing.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,70 +1,49 @@
-########
-#
-# directory_listing.py
-#
-# Script for creating HTML directory listings for a local directory and
-# all its subdirectories.  Primarily intended for use on mounted blob
-# containers, so it includes the ability to set the content-type property
-# on the generated html using the blob SDK, so it can be browser-viewable.
-#
-# Also includes a preview of a jpg file (the first in an alphabetical list),
-# if present.
-#
-# Sample invocation:
-#
-# python directory_listing.py /naipout/v002 --basepath /naipout/v002 \
-#   --enable_overwrite \
-#   --sas_url "https://naipblobs.blob.core.windows.net/naip/v002?sv=..."
-#
-########
+"""
+
+directory_listing.py
+
+Script for creating Apache-style HTML directory listings for a local directory 
+and all its subdirectories.
+
+Also includes a preview of a jpg file (the first in an alphabetical list),
+if present.
+
+"""
 
 #%% Imports
 
 import os
 import sys
 import argparse
 import re
 
 import azure.common
 from azure.storage.blob import BlobServiceClient, ContentSettings
 
+from md_utils.path_utils import is_image_file
 
-#%% Directory enumeration functions
-
-def is_image_file(path):
-    """
-    Checks whether the provided file path points to an image by checking the
-    file extension. The following file extensions are considered images: jpg,
-    jpeg.  The check is case-insensitive.
-
-    Args:
-        path: string, path to image file or just the file name
-
-    Returns:
-        boolean, True if the file is an image
-    """
-
-    return os.path.splitext(path)[1].lower()[1:] in ['jpg', 'jpeg'] #, 'gif', 'tiff', 'tif', 'png']
 
+#%% Directory enumeration functions
 
 def create_plain_index(root, dirs, files, dirname=None):
     """
-    Creates the fairly plain HTML folder index
-    including a preview of a single image file, if any is present.
-    Returns the HTML source as string.
+    Creates the fairly plain HTML folder index including a preview of a single image file, 
+    if any is present.
 
     Args:
-        root: string, path to the root directory, all paths in *dirs* and
-            *files* are relative to this one
-        dirs: list of strings, the directories in *root*
-        files: list of string, the files in *root*
-        dirname: name to print in the html, which may be different than *root*
+        root (str): path to the root directory, all paths in [dirs] and
+            [files] are relative to this root folder
+        dirs (list): list of strings, the directories in [root]
+        files (list): list of strings, the files in [root]
+        dirname (str, optional): name to print in the html, 
+            which may be different than [root]
 
-    Returns: HTML source of the directory listing
+    Returns: 
+        str: HTML source of the directory listing
     """
 
     if dirname is None:
         dirname = root or '/'
 
     html = "<!DOCTYPE html>\n"
     html += "<html lang='en'><head>"
@@ -124,33 +103,39 @@
         html += "<p style = 'padding-left:1em;'>No files</p>\n"
 
     # Add some space at the bottom because the browser's status bar might hide stuff
     html += "<p style='margin:2em;'>&nbsp;</p>\n"
     html += "</body></html>\n"
     return html
 
+# ...def create_plain_index(...)
+
 
 def traverse_and_create_index(dir, sas_url=None, overwrite_files=False,
                               template_fun=create_plain_index, basepath=None):
     """
-    Recursively traverses the local directory *dir* and generates a index
-    file for each folder using *template_fun* to generate the HTML output.
+    Recursively traverses the local directory [dir] and generates a index
+    file for each folder using [template_fun] to generate the HTML output.
     Excludes hidden files.
 
     Args:
-        dir: string, path to directory
-        template_fun: function taking three arguments (string, list of string, list of string)
-            representing the current root, the list of folders, and the list of files.
-            Should return the HTML source of the index file
-
-    Return:
-        None
+        dir (str): directory to process
+        sas_url (str, optional): write-capable SAS URL that points to the same place as 
+            [dir], used for the very esoteric scenario where [dir] is really a mounted
+            blob container, and we want to set the content-type on each file so the resulting
+            index can be viewed in a browser
+        overwrite_files (bool, optional): whether to over-write existing index file
+        template_fun (func, optional): function taking three arguments (string, 
+            list of string, list of string) representing the current root, the list of folders, 
+            and the list of files.  Should return the HTML source of the index file.
+        basepath (str, optional): if not None, the name used for each subfolder in [dir]
+            in the output files will be relative to [basepath]
     """
 
-    print("Traversing {}".format(dir))
+    print('Traversing {}'.format(dir))
 
     # Make sure we remove the trailing /
     dir = os.path.normpath(dir)
 
     # If we want to set the content type in blob storage using a SAS URL
     if sas_url:
 
@@ -218,34 +203,44 @@
             try:
                 blob_client = blob_service.get_blob_client(container_name, output_blob_path)
                 blob_client.set_http_headers(content_settings=target_settings)
             except azure.common.AzureMissingResourceHttpError:
                 print('ERROR: It seems the SAS URL is incorrect or does not allow setting properties.')
                 return
 
+# ...def traverse_and_create_index(...)
 
-#%% Command-line driver
 
-if __name__ == '__main__':
+#%% Command-line driver
 
+def main():
+    
     parser = argparse.ArgumentParser()
-    parser.add_argument("directory", type=str, help='Path to directory which should be traversed.')
-    parser.add_argument("--basepath", type=str, help='Folder names will be printed relative to basepath, if specified', default=None)
-    parser.add_argument("--sas_url", type=str, help='Blobfuse does not set the content-type property ' + \
-        'properly and hence index.html won\'t be accessible in the browser. If you want to set the ' + \
-        'content-type in the corresponding blob storage, provide the SAS URL that corresponds to the ' + \
-        "directory, e.g. if *directory* is /mountpoint/path/to/folder, then *--sas_url* looks like " + \
-        "'https://accname.blob.core.windows.net/bname/path/to/folder?st=...&se=...&sp=...&...'")
+    
+    parser.add_argument("directory", type=str, 
+                        help='Path to directory which should be traversed.')
+    parser.add_argument("--basepath", type=str, 
+                        help='Folder names will be printed relative to basepath, if specified', 
+                        default=None)
+    parser.add_argument("--sas_url", type=str, 
+                        help='Blobfuse does not set the content-type property ' + \
+                             'properly and hence index.html won\'t be accessible in the browser. If you want to set the ' + \
+                              'content-type in the corresponding blob storage, provide the SAS URL that corresponds to the ' + \
+                              'directory, e.g. if *directory* is /mountpoint/path/to/folder, then *--sas_url* looks like ' + \
+                              '"https://accname.blob.core.windows.net/bname/path/to/folder?st=...&se=...&sp=...&..."')
     parser.add_argument("--enable_overwrite", action='store_true', default=False,
                         help='If set, the script will overwrite existing index.html files.')
 
     if len(sys.argv[1:]) == 0:
         parser.print_help()
         parser.exit()
 
     args = parser.parse_args()
 
     assert os.path.isdir(args.directory), "{} is not a valid directory".format(args.directory)
     assert re.match('https?://[^\.]+\.blob\.core\.windows\.net/.+', args.sas_url), "--sas_url does not " + \
         "match the format https://accname.blob.core.windows.net/bname/path/to/folder?..."
 
     traverse_and_create_index(args.directory, overwrite_files=args.enable_overwrite, sas_url=args.sas_url, basepath=args.basepath)
+
+if __name__ == '__main__':
+    main()
```

### Comparing `megadetector-5.0.8/md_utils/md_tests.py` & `megadetector-5.0.9/md_utils/md_tests.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,871 +1,968 @@
-########
-#
-# md_tests.py
-#
-# A series of tests to validate basic repo functionality and verify either "correct"
-# inference behavior, or - when operating in environments other than the training
-# environment - acceptable deviation from the correct results.
-#
-# This module should not depend on anything else in this repo outside of the 
-# tests themselves, even if it means some duplicated code (e.g. for downloading files),
-# since much of what it tries to test is, e.g., imports.
-#
-########
-
-#%% Imports and constants
-
-### Only standard imports belong here, not MD-specific imports ###
-
-import os
-import json
-import glob
-import tempfile
-import urllib
-import urllib.request
-import zipfile
-import subprocess
-import argparse
-
-
-#%% Classes
-
-class MDTestOptions:
-        
-    ## Required ##
-    
-    disable_gpu = False
-    cpu_execution_is_error = False
-    skip_video_tests = False
-    skip_python_tests = False
-    skip_cli_tests = False
-    scratch_dir = None
-    test_data_url = 'https://lila.science/public/md-test-package.zip'
-    force_data_download = False
-    force_data_unzip = False
-    warning_mode = False
-    test_image_subdir = 'md-test-images'
-    max_coord_error = 0.001
-    max_conf_error = 0.005
-    cli_working_dir = None
-    yolo_working_folder = None
-
-
-#%% Support functions
-
-def get_expected_results_filename(gpu_is_available):
-    """
-    Expected results vary just a little across inference environments, particularly
-    between PT 1.x and 2.x, so when making sure things are working acceptably, we 
-    compare to a reference file that matches the current environment.
-    """
-    
-    if gpu_is_available:
-        hw_string = 'gpu'
-    else:
-        hw_string = 'cpu'
-    import torch
-    torch_version = str(torch.__version__)
-    if torch_version.startswith('1'):
-        assert torch_version == '1.10.1', 'Only tested against PT 1.10.1 and PT 2.x'
-        pt_string = 'pt1.10.1'
-    else:
-        assert torch_version.startswith('2'), 'Unknown torch version: {}'.format(torch_version)
-        pt_string = 'pt2.x'
-    
-    # A hack for now to account for the fact that even with acceleration enabled and PT2 
-    # installed, Apple silicon appears to provide the same results as CPU/PT1 inference
-    try:
-        import torch
-        m1_inference = torch.backends.mps.is_built and torch.backends.mps.is_available()
-        if m1_inference:
-            hw_string = 'cpu'
-        pt_string = 'pt1.10.1'
-    except Exception:
-        pass
-    
-    return 'md-test-results-{}-{}.json'.format(hw_string,pt_string)
-    
-    
-def download_test_data(options=None):
-    """
-    Download the test zipfile if necessary, unzip if necessary.
-    """
-
-    if options is None:
-        options = MDTestOptions()
-        
-    if options.scratch_dir is None:        
-        tempdir_base = tempfile.gettempdir()
-        scratch_dir = os.path.join(tempdir_base,'md-tests')
-    else:
-        scratch_dir = options.scratch_dir
-    
-    os.makedirs(scratch_dir,exist_ok=True)    
-    
-    # See whether we've already downloaded the data zipfile
-    download_zipfile = True        
-    if not options.force_data_download:
-        local_zipfile = os.path.join(scratch_dir,options.test_data_url.split('/')[-1])
-        if os.path.isfile(local_zipfile):
-            url_info = urllib.request.urlopen(options.test_data_url).info()
-            remote_size = int(url_info['Content-Length'])
-            target_file_size = os.path.getsize(local_zipfile)
-            if remote_size == target_file_size:
-                download_zipfile = False
-    
-    if download_zipfile:
-        print('Downloading test data zipfile')
-        urllib.request.urlretrieve(options.test_data_url, local_zipfile)
-        print('Finished download to {}'.format(local_zipfile))
-    else:
-        print('Bypassing test data zipfile download for {}'.format(local_zipfile))
-    
-    
-    ## Unzip data
-    
-    zipf = zipfile.ZipFile(local_zipfile)    
-    zip_contents = zipf.filelist
-    
-    # file_info = zip_contents[1]
-    for file_info in zip_contents:
-        
-        expected_size = file_info.file_size
-        if expected_size == 0:
-            continue
-        fn_relative = file_info.filename
-        target_file = os.path.join(scratch_dir,fn_relative)
-        unzip_file = True
-        if (not options.force_data_unzip) and os.path.isfile(target_file):
-            existing_file_size = os.path.getsize(target_file)
-            if existing_file_size == expected_size:
-                unzip_file = False
-        if unzip_file:
-            os.makedirs(os.path.dirname(target_file),exist_ok=True)
-            with open(target_file,'wb') as f:
-                f.write(zipf.read(fn_relative))
-            
-    # ...for each file in the zipfile
-    
-    # Warn if file are present that aren't expected
-    test_files = glob.glob(os.path.join(scratch_dir,'**/*'), recursive=True)
-    test_files = [os.path.relpath(fn,scratch_dir).replace('\\','/') for fn in test_files]
-    test_files_set = set(test_files)
-    expected_images_set = set(zipf.namelist())
-    for fn in expected_images_set:
-        if fn.endswith('/'):
-            continue
-        assert fn in test_files_set, 'File {} is missing from the test image folder'.format(fn)
-    
-    # Populate the test options with test data information
-    options.scratch_dir = scratch_dir
-    options.all_test_files = test_files
-    options.test_images = [fn for fn in test_files if os.path.splitext(fn.lower())[1] in ('.jpg','.jpeg','.png')]
-    options.test_videos = [fn for fn in test_files if os.path.splitext(fn.lower())[1] in ('.mp4','.avi')]    
-    options.test_videos = [fn for fn in options.test_videos if 'rendered' not in fn]
-        
-    print('Finished unzipping and enumerating test data')
-    
-# ...def download_test_data(...)
-
-
-def is_gpu_available(verbose=True):
-    """
-    Check whether a GPU (including M1/M2 MPS) is available.
-    """
-    
-    # Import torch inside this function, so we have a chance to set CUDA_VISIBLE_DEVICES
-    # before checking GPU availability.
-    import torch
-    gpu_available = torch.cuda.is_available()
-    
-    if gpu_available:
-        if verbose:
-            print('CUDA available: {}'.format(gpu_available))
-            device_ids = list(range(torch.cuda.device_count()))
-            if len(device_ids) > 1:
-                print('Found multiple devices: {}'.format(str(device_ids)))
-    else:
-        try:
-            gpu_available = torch.backends.mps.is_built and torch.backends.mps.is_available()
-        except AttributeError:
-            pass
-        if gpu_available:
-            print('Metal performance shaders available')
-    
-    if not gpu_available:
-        print('No GPU available')
-        
-    return gpu_available            
-        
-    
-#%% CLI functions
-
-# These are copied from process_utils.py to avoid imports outside of the test
-# functions.
-
-os.environ["PYTHONUNBUFFERED"] = "1"
-
-def execute(cmd):
-    """
-    Run [cmd] (a single string) in a shell, yielding each line of output to the caller.
-    """
- 
-    # https://stackoverflow.com/questions/4417546/constantly-print-subprocess-output-while-process-is-running
-    popen = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
-                             shell=True, universal_newlines=True)
-    for stdout_line in iter(popen.stdout.readline, ""):
-        yield stdout_line
-    popen.stdout.close()
-    return_code = popen.wait()
-    if return_code:
-        raise subprocess.CalledProcessError(return_code, cmd)
-
-
-def execute_and_print(cmd,print_output=True):
-    """
-    Run [cmd] (a single string) in a shell, capturing and printing output.  Returns
-    a dictionary with fields "status" and "output".
-    """
-
-    to_return = {'status':'unknown','output':''}
-    output=[]
-    try:
-        for s in execute(cmd):
-            output.append(s)
-            if print_output:
-                print(s,end='',flush=True)
-        to_return['status'] = 0
-    except subprocess.CalledProcessError as cpe:
-        print('execute_and_print caught error: {}'.format(cpe.output))
-        to_return['status'] = cpe.returncode
-    to_return['output'] = output
-   
-    return to_return
-
-
-#%% Python tests
-
-def run_python_tests(options):
-
-    print('\n*** Starting module tests ***\n')
-    
-    ## Prepare data
-    
-    download_test_data(options)
-    
-    
-    ## Run inference on an image
-    
-    from detection import run_detector
-    from md_visualization import visualization_utils as vis_utils
-    model_file = 'MDV5A'
-    image_fn = os.path.join(options.scratch_dir,options.test_images[0])
-    model = run_detector.load_detector(model_file)
-    pil_im = vis_utils.load_image(image_fn)
-    result = model.generate_detections_one_image(pil_im) # noqa
-
-    
-    ## Run inference on a folder
-    
-    from detection.run_detector_batch import load_and_run_detector_batch,write_results_to_file
-    from md_utils import path_utils
-
-    image_folder = os.path.join(options.scratch_dir,'md-test-images')
-    assert os.path.isdir(image_folder), 'Test image folder {} is not available'.format(image_folder)
-    inference_output_file = os.path.join(options.scratch_dir,'folder_inference_output.json')
-    image_file_names = path_utils.find_images(image_folder,recursive=True)
-    results = load_and_run_detector_batch('MDV5A', image_file_names, quiet=True)
-    _ = write_results_to_file(results,inference_output_file,
-                              relative_path_base=image_folder,detector_file=model_file)
-
-    # Read results
-    with open(inference_output_file,'r') as f:
-        results_from_file = json.load(f) # noqa
-    
-
-    ## Verify results
-
-    # Read expected results
-    expected_results_filename = get_expected_results_filename(is_gpu_available(verbose=False))
-    
-    with open(os.path.join(options.scratch_dir,expected_results_filename),'r') as f:
-        expected_results = json.load(f)
-            
-    filename_to_results = {im['file'].replace('\\','/'):im for im in results_from_file['images']}
-    filename_to_results_expected = {im['file'].replace('\\','/'):im for im in expected_results['images']}
-    
-    assert len(filename_to_results) == len(filename_to_results_expected), \
-        'Error: expected {} files in results, found {}'.format(
-            len(filename_to_results_expected),
-            len(filename_to_results))
-    
-    max_coord_error = 0
-    max_conf_error = 0
-    
-    # fn = next(iter(filename_to_results.keys()))
-    for fn in filename_to_results.keys():
-                
-        actual_image_results = filename_to_results[fn]
-        expected_image_results = filename_to_results_expected[fn]
-        
-        if 'failure' in actual_image_results:
-            assert 'failure' in expected_image_results and \
-                'detections' not in actual_image_results and \
-                'detections' not in expected_image_results
-            continue
-        assert 'failure' not in expected_image_results
-        
-        actual_detections = actual_image_results['detections']
-        expected_detections = expected_image_results['detections']
-        
-        s = 'expected {} detections for file {}, found {}'.format(
-            len(expected_detections),fn,len(actual_detections))
-        s += '\nExpected results file: {}\nActual results file: {}'.format(
-            expected_results_filename,inference_output_file)
-        
-        if options.warning_mode:
-            if len(actual_detections) != len(expected_detections):
-                print('Warning: {}'.format(s))
-            continue
-        assert len(actual_detections) == len(expected_detections), \
-            'Error: {}'.format(s)
-        
-        # i_det = 0
-        for i_det in range(0,len(actual_detections)):
-            actual_det = actual_detections[i_det]
-            expected_det = expected_detections[i_det]
-            assert actual_det['category'] == expected_det['category']
-            conf_err = abs(actual_det['conf'] - expected_det['conf'])
-            coord_differences = []
-            for i_coord in range(0,4):
-                coord_differences.append(abs(actual_det['bbox'][i_coord]-expected_det['bbox'][i_coord]))
-            coord_err = max(coord_differences)
-            
-            if conf_err > max_conf_error:
-                max_conf_error = conf_err
-            if coord_err > max_coord_error:
-                max_coord_error = coord_err
-        
-        # ...for each detection
-        
-    # ...for each image
-    
-    if not options.warning_mode:
-        
-        assert max_conf_error <= options.max_conf_error, \
-            'Confidence error {} is greater than allowable ({})'.format(
-                max_conf_error,options.max_conf_error)
-        
-        assert max_coord_error <= options.max_coord_error, \
-            'Coord error {} is greater than allowable ({})'.format(
-                max_coord_error,options.max_coord_error)
-        
-    print('Max conf error: {}'.format(max_conf_error))
-    print('Max coord error: {}'.format(max_coord_error))
-
-
-    ## Postprocess results
-    
-    from api.batch_processing.postprocessing.postprocess_batch_results import \
-        PostProcessingOptions,process_batch_results
-    postprocessing_options = PostProcessingOptions()
-    
-    postprocessing_options.api_output_file = inference_output_file
-    postprocessing_options.output_dir = os.path.join(options.scratch_dir,'postprocessing_output')
-    postprocessing_options.image_base_dir = image_folder
-    
-    postprocessing_results = process_batch_results(postprocessing_options)
-    assert os.path.isfile(postprocessing_results.output_html_file), \
-        'Postprocessing output file {} not found'.format(postprocessing_results.output_html_file)
-    
-    
-    ## Partial RDE test
-    
-    from api.batch_processing.postprocessing.repeat_detection_elimination.repeat_detections_core import \
-        RepeatDetectionOptions,find_repeat_detections
-    
-    rde_options = RepeatDetectionOptions()
-    rde_options.occurrenceThreshold = 2
-    rde_options.confidenceMin = 0.001
-    rde_options.outputBase = os.path.join(options.scratch_dir,'rde_working_dir')
-    rde_options.imageBase = image_folder
-    rde_output_file = inference_output_file.replace('.json','_filtered.json')
-    assert rde_output_file != inference_output_file
-    rde_results = find_repeat_detections(inference_output_file, rde_output_file, rde_options)
-    assert os.path.isfile(rde_results.filterFile),\
-        'Could not find RDE output file {}'.format(rde_results.filterFile)
-        
-    
-    # TODO: add remove_repeat_detections test here
-    #
-    # It's already tested in the CLI tests, so this is not urgent.
-    
-
-    ## Video test (single video)
-    
-    from detection.process_video import ProcessVideoOptions, process_video
-    
-    video_options = ProcessVideoOptions()
-    video_options.model_file = 'MDV5A'
-    video_options.input_video_file = os.path.join(options.scratch_dir,options.test_videos[0])
-    video_options.output_json_file = os.path.join(options.scratch_dir,'single_video_output.json')
-    video_options.output_video_file = os.path.join(options.scratch_dir,'video_scratch/rendered_video.mp4')
-    video_options.frame_folder = os.path.join(options.scratch_dir,'video_scratch/frame_folder')
-    video_options.frame_rendering_folder = os.path.join(options.scratch_dir,'video_scratch/rendered_frame_folder')    
-    video_options.render_output_video = True
-    # video_options.keep_rendered_frames = False
-    # video_options.keep_rendered_frames = False
-    video_options.force_extracted_frame_folder_deletion = True
-    video_options.force_rendered_frame_folder_deletion = True
-    # video_options.reuse_results_if_available = False
-    # video_options.reuse_frames_if_available = False
-    video_options.recursive = True
-    video_options.verbose = False
-    video_options.fourcc = 'mp4v'
-    # video_options.rendering_confidence_threshold = None
-    # video_options.json_confidence_threshold = 0.005
-    video_options.frame_sample = 5    
-    video_options.n_cores = 5
-    # video_options.debug_max_frames = -1
-    # video_options.class_mapping_filename = None
-    
-    _ = process_video(video_options)
-
-    assert os.path.isfile(video_options.output_video_file), \
-        'Python video test failed to render output video file'
-    assert os.path.isfile(video_options.output_json_file), \
-        'Python video test failed to render output .json file'
-        
-    
-    ## Video test (folder)
-    
-    from detection.process_video import ProcessVideoOptions, process_video_folder
-    
-    video_options = ProcessVideoOptions()
-    video_options.model_file = 'MDV5A'
-    video_options.input_video_file = os.path.join(options.scratch_dir,
-                                                  os.path.dirname(options.test_videos[0]))
-    video_options.output_json_file = os.path.join(options.scratch_dir,'video_folder_output.json')
-    # video_options.output_video_file = None
-    video_options.frame_folder = os.path.join(options.scratch_dir,'video_scratch/frame_folder')
-    video_options.frame_rendering_folder = os.path.join(options.scratch_dir,'video_scratch/rendered_frame_folder')    
-    video_options.render_output_video = False
-    # video_options.keep_rendered_frames = False
-    # video_options.keep_rendered_frames = False
-    video_options.force_extracted_frame_folder_deletion = True
-    video_options.force_rendered_frame_folder_deletion = True
-    # video_options.reuse_results_if_available = False
-    # video_options.reuse_frames_if_available = False
-    video_options.recursive = True
-    video_options.verbose = False
-    # video_options.fourcc = None
-    # video_options.rendering_confidence_threshold = None
-    # video_options.json_confidence_threshold = 0.005
-    video_options.frame_sample = 5    
-    video_options.n_cores = 5
-    # video_options.debug_max_frames = -1
-    # video_options.class_mapping_filename = None
-    
-    _ = process_video_folder(video_options)
-
-    assert os.path.isfile(video_options.output_json_file), \
-        'Python video test failed to render output .json file'
-    
-    
-    print('\n*** Finished module tests ***\n')
-
-# ...def run_python_tests(...)
-
-
-#%% Command-line tests
-
-def run_cli_tests(options):
-    
-    print('\n*** Starting CLI tests ***\n')
-    
-    ## chdir if necessary
-    
-    if options.cli_working_dir is not None:
-        os.chdir(options.cli_working_dir)
-    
-    
-    ## Prepare data
-    
-    download_test_data(options)
-    
-    
-    ## Run inference on an image
-    
-    model_file = 'MDV5A'
-    image_fn = os.path.join(options.scratch_dir,options.test_images[0])
-    output_dir = os.path.join(options.scratch_dir,'single_image_test')
-    if options.cli_working_dir is None:
-        cmd = 'python -m detection.run_detector'
-    else:
-        cmd = 'python detection/run_detector.py'
-    cmd += ' {} --image_file {} --output_dir {}'.format(
-        model_file,image_fn,output_dir)
-    print('Running: {}'.format(cmd))
-    cmd_results = execute_and_print(cmd)
-    
-    if options.cpu_execution_is_error:
-        gpu_available_via_cli = False
-        for s in cmd_results['output']:
-            if 'GPU available: True' in s:
-                gpu_available_via_cli = True
-                break
-        if not gpu_available_via_cli:
-            raise Exception('GPU execution is required, but not available')
-
-    
-    ## Run inference on a folder
-    
-    image_folder = os.path.join(options.scratch_dir,'md-test-images')
-    assert os.path.isdir(image_folder), 'Test image folder {} is not available'.format(image_folder)
-    inference_output_file = os.path.join(options.scratch_dir,'folder_inference_output.json')
-    if options.cli_working_dir is None:
-        cmd = 'python -m detection.run_detector_batch'
-    else:
-        cmd = 'python detection/run_detector_batch.py'
-    cmd += ' {} {} {} --recursive'.format(
-        model_file,image_folder,inference_output_file)
-    cmd += ' --output_relative_filenames --quiet --include_image_size'
-    cmd += ' --include_image_timestamp --include_exif_data'
-    print('Running: {}'.format(cmd))
-    cmd_results = execute_and_print(cmd)
-    
-    # Make sure a coherent file got written out, but don't verify the results, leave that
-    # to the Python tests.
-    with open(inference_output_file,'r') as f:
-        results_from_file = json.load(f) # noqa
-    
-
-    ## Postprocessing
-    
-    postprocessing_output_dir = os.path.join(options.scratch_dir,'postprocessing_output_cli')
-    
-    if options.cli_working_dir is None:
-        cmd = 'python -m api.batch_processing.postprocessing.postprocess_batch_results'
-    else:
-        cmd = 'python api/batch_processing/postprocessing/postprocess_batch_results.py'
-    cmd += ' {} {}'.format(
-        inference_output_file,postprocessing_output_dir)
-    cmd += ' --image_base_dir {}'.format(image_folder)
-    print('Running: {}'.format(cmd))
-    cmd_results = execute_and_print(cmd)
-                
-    
-    ## RDE
-    
-    rde_output_dir = os.path.join(options.scratch_dir,'rde_output_cli')
-    
-    if options.cli_working_dir is None:
-        cmd = 'python -m api.batch_processing.postprocessing.repeat_detection_elimination.find_repeat_detections'
-    else:
-        cmd = 'python  api/batch_processing/postprocessing/repeat_detection_elimination/find_repeat_detections.py'
-    cmd += ' {}'.format(inference_output_file)
-    cmd += ' --imageBase {}'.format(image_folder)
-    cmd += ' --outputBase {}'.format(rde_output_dir)
-    cmd += ' --occurrenceThreshold 1' # Use an absurd number here to make sure we get some suspicious detections
-    print('Running: {}'.format(cmd))
-    cmd_results = execute_and_print(cmd)    
-    
-    # Find the latest filtering folder
-    filtering_output_dir = os.listdir(rde_output_dir)
-    filtering_output_dir = [fn for fn in filtering_output_dir if fn.startswith('filtering_')]
-    filtering_output_dir = [os.path.join(rde_output_dir,fn) for fn in filtering_output_dir]
-    filtering_output_dir = [fn for fn in filtering_output_dir if os.path.isdir(fn)]
-    filtering_output_dir = sorted(filtering_output_dir)[-1]
-    
-    print('Using RDE filtering folder {}'.format(filtering_output_dir))
-    
-    filtered_output_file = inference_output_file.replace('.json','_filtered.json')
-    
-    if options.cli_working_dir is None:
-        cmd = 'python -m api.batch_processing.postprocessing.repeat_detection_elimination.remove_repeat_detections'
-    else:
-        cmd = 'python  api/batch_processing/postprocessing/repeat_detection_elimination/remove_repeat_detections.py'
-    cmd += ' {} {} {}'.format(inference_output_file,filtered_output_file,filtering_output_dir)
-    print('Running: {}'.format(cmd))
-    cmd_results = execute_and_print(cmd)
-    
-    assert os.path.isfile(filtered_output_file), \
-        'Could not find RDE output file {}'.format(filtered_output_file)
-    
-    
-    ## Run inference on a folder (tiled)
-    
-    image_folder = os.path.join(options.scratch_dir,'md-test-images')
-    tiling_folder = os.path.join(options.scratch_dir,'tiling-folder')
-    inference_output_file_tiled = os.path.join(options.scratch_dir,'folder_inference_output_tiled.json')
-    if options.cli_working_dir is None:
-        cmd = 'python -m detection.run_tiled_inference'
-    else:
-        cmd = 'python detection/run_tiled_inference.py'
-    cmd += ' {} {} {} {}'.format(
-        model_file,image_folder,tiling_folder,inference_output_file_tiled)
-    cmd += ' --overwrite_handling overwrite'
-    print('Running: {}'.format(cmd))
-    cmd_results = execute_and_print(cmd)
-    
-    with open(inference_output_file_tiled,'r') as f:
-        results_from_file = json.load(f) # noqa
-        
-    
-    ## Run inference on a folder (augmented)
-    
-    if options.yolo_working_folder is None:
-        
-        print('Bypassing YOLOv5 val tests, no yolo folder supplied')
-        
-    else:
-    
-        image_folder = os.path.join(options.scratch_dir,'md-test-images')
-        yolo_results_folder = os.path.join(options.scratch_dir,'yolo-output-folder')
-        yolo_symlink_folder = os.path.join(options.scratch_dir,'yolo-symlink_folder')
-        inference_output_file_yolo_val = os.path.join(options.scratch_dir,'folder_inference_output_yolo_val.json')
-        if options.cli_working_dir is None:
-            cmd = 'python -m detection.run_inference_with_yolov5_val'
-        else:
-            cmd = 'python detection/run_inference_with_yolov5_val.py'
-        cmd += ' {} {} {}'.format(
-            model_file,image_folder,inference_output_file_yolo_val)
-        cmd += ' --yolo_working_folder {}'.format(options.yolo_working_folder)
-        cmd += ' --yolo_results_folder {}'.format(yolo_results_folder)
-        cmd += ' --symlink_folder {}'.format(yolo_symlink_folder)
-        cmd += ' --augment_enabled 1'
-        # cmd += ' --no_use_symlinks'
-        cmd += ' --overwrite_handling overwrite'
-        print('Running: {}'.format(cmd))
-        cmd_results = execute_and_print(cmd)
-        
-        with open(inference_output_file_yolo_val,'r') as f:
-            results_from_file = json.load(f) # noqa
-        
-        
-    ## Video test
-    
-    model_file = 'MDV5A'
-    video_inference_output_file = os.path.join(options.scratch_dir,'video_inference_output.json')
-    output_video_file = os.path.join(options.scratch_dir,'video_scratch/cli_rendered_video.mp4')
-    frame_folder = os.path.join(options.scratch_dir,'video_scratch/frame_folder_cli')
-    frame_rendering_folder = os.path.join(options.scratch_dir,'video_scratch/rendered_frame_folder_cli')        
-    
-    video_fn = os.path.join(options.scratch_dir,options.test_videos[-1])
-    output_dir = os.path.join(options.scratch_dir,'single_video_test_cli')
-    if options.cli_working_dir is None:
-        cmd = 'python -m detection.process_video'
-    else:
-        cmd = 'python detection/process_video.py'
-    cmd += ' {} {}'.format(model_file,video_fn)
-    cmd += ' --frame_folder {} --frame_rendering_folder {} --output_json_file {} --output_video_file {}'.format(
-        frame_folder,frame_rendering_folder,video_inference_output_file,output_video_file)
-    cmd += ' --render_output_video --fourcc mp4v'
-    cmd += ' --force_extracted_frame_folder_deletion --force_rendered_frame_folder_deletion --n_cores 5 --frame_sample 3'
-    print('Running: {}'.format(cmd))
-    cmd_results = execute_and_print(cmd)
-    
-    
-    ## Run inference on a folder (again, so we can do a comparison)
-    
-    image_folder = os.path.join(options.scratch_dir,'md-test-images')
-    model_file = 'MDV5B'
-    inference_output_file_alt = os.path.join(options.scratch_dir,'folder_inference_output_alt.json')
-    if options.cli_working_dir is None:
-        cmd = 'python -m detection.run_detector_batch'
-    else:
-        cmd = 'python detection/run_detector_batch.py'
-    cmd += ' {} {} {} --recursive'.format(
-        model_file,image_folder,inference_output_file_alt)
-    cmd += ' --output_relative_filenames --quiet --include_image_size'
-    cmd += ' --include_image_timestamp --include_exif_data'
-    print('Running: {}'.format(cmd))
-    cmd_results = execute_and_print(cmd)
-    
-    with open(inference_output_file_alt,'r') as f:
-        results_from_file = json.load(f) # noqa
-    
-    
-    ## Compare the two files
-    
-    comparison_output_folder = os.path.join(options.scratch_dir,'results_comparison')
-    image_folder = os.path.join(options.scratch_dir,'md-test-images')
-    results_files_string = '"{}" "{}"'.format(
-        inference_output_file,inference_output_file_alt)
-    if options.cli_working_dir is None:
-        cmd = 'python -m api.batch_processing.postprocessing.compare_batch_results'
-    else:
-        cmd = 'python api/batch_processing/postprocessing/compare_batch_results.py'
-    cmd += ' {} {} {}'.format(comparison_output_folder,image_folder,results_files_string)
-    print('Running: {}'.format(cmd))
-    cmd_results = execute_and_print(cmd)
-    
-    assert cmd_results['status'] == 0, 'Error generating comparison HTML'
-    assert os.path.isfile(os.path.join(comparison_output_folder,'index.html')), \
-        'Failed to generate comparison HTML'
-    
-    print('\n*** Finished CLI tests ***\n')
-    
-# ...def run_cli_tests(...)
-
-
-#%% Main test wrapper
-
-def run_tests(options):
-    
-    # Prepare data folder
-    download_test_data(options)    
-    
-    if options.disable_gpu:
-        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
-        
-    # Verify GPU
-    gpu_available = is_gpu_available()
-    
-    # If the GPU is required and isn't available, error
-    if options.cpu_execution_is_error and (not gpu_available):
-        raise ValueError('GPU not available, and cpu_execution_is_error is set')
-    
-    # If the GPU should be disabled, verify that it is
-    if options.disable_gpu:
-        assert (not gpu_available), 'CPU execution specified, but the GPU appears to be available'
-        
-    # Run python tests
-    if not options.skip_python_tests:
-        run_python_tests(options)
-    
-    # Run CLI tests
-    if not options.skip_cli_tests:
-        run_cli_tests(options)
-
-
-#%% Interactive driver
-
-if False:
-    
-    pass
-
-    #%%
-    
-    options = MDTestOptions()
-    
-    options.disable_gpu = False
-    options.cpu_execution_is_error = False
-    options.skip_video_tests = False
-    options.skip_python_tests = False
-    options.skip_cli_tests = False
-    options.scratch_dir = None
-    options.test_data_url = 'https://lila.science/public/md-test-package.zip'
-    options.force_data_download = False
-    options.force_data_unzip = False
-    options.warning_mode = True
-    options.test_image_subdir = 'md-test-images'
-    options.max_coord_error = 0.001
-    options.max_conf_error = 0.005
-    options.cli_working_dir = r'c:\git\MegaDetector'
-    options.yolo_working_folder = r'c:\git\yolov5'
-
-
-    #%%
-    
-    run_tests(options)
-    
-    
-#%% Command-line driver
-
-def main():
-
-    options = MDTestOptions()
-    
-    parser = argparse.ArgumentParser(
-        description='MegaDetector test suite')
-    
-    parser.add_argument(
-        '--disable_gpu',
-        action='store_true',
-        help='Disable GPU operation')
-    
-    parser.add_argument(
-        '--cpu_execution_is_error',
-        action='store_true',
-        help='Fail if the GPU appears not to be available')
-    
-    parser.add_argument(
-        '--scratch_dir',
-        default=None,
-        type=str,
-        help='Directory for temporary storage (defaults to system temp dir)')
-    
-    parser.add_argument(
-        '--skip_video_tests',
-        action='store_true',
-        help='Skip tests related to video (which can be slow)')
-        
-    parser.add_argument(
-        '--skip_python_tests',
-        action='store_true',
-        help='Skip python tests')
-        
-    parser.add_argument(
-        '--skip_cli_tests',
-        action='store_true',
-        help='Skip CLI tests')
-        
-    parser.add_argument(
-        '--force_data_download',
-        action='store_true',
-        help='Force download of the test data file, even if it\'s already available')
-    
-    parser.add_argument(
-        '--force_data_unzip',
-        action='store_true',
-        help='Force extraction of all files in the test data file, even if they\'re already available')
-    
-    parser.add_argument(
-        '--warning_mode',
-        action='store_true',
-        help='Turns numeric/content errors into warnings')
-    
-    parser.add_argument(
-        '--max_conf_error',
-        type=float,
-        default=options.max_conf_error,
-        help='Maximum tolerable confidence value deviation from expected (default {})'.format(
-            options.max_conf_error))
-    
-    parser.add_argument(
-        '--max_coord_error',
-        type=float,
-        default=options.max_coord_error,
-        help='Maximum tolerable coordinate value deviation from expected (default {})'.format(
-            options.max_coord_error))
-
-    parser.add_argument(
-        '--cli_working_dir',
-        type=str,
-        default=None,
-        help='Working directory for CLI tests')
-
-    # token used for linting
-    #
-    # no_arguments_required
-        
-    args = parser.parse_args()
-        
-    options.disable_gpu = args.disable_gpu
-    options.cpu_execution_is_error = args.cpu_execution_is_error
-    options.skip_video_tests = args.skip_video_tests
-    options.skip_python_tests = args.skip_python_tests
-    options.skip_cli_tests = args.skip_cli_tests
-    options.scratch_dir = args.scratch_dir
-    options.warning_mode = args.warning_mode
-    options.force_data_download = args.force_data_download
-    options.max_conf_error = args.max_conf_error
-    options.max_coord_error = args.max_coord_error
-    options.cli_working_dir = args.cli_working_dir
-
-    run_tests(options)
-    
-    
-if __name__ == '__main__':
-    main()
+"""
+
+md_tests.py
+
+A series of tests to validate basic repo functionality and verify either "correct"
+inference behavior, or - when operating in environments other than the training
+environment - acceptable deviation from the correct results.
+
+This module should not depend on anything else in this repo outside of the 
+tests themselves, even if it means some duplicated code (e.g. for downloading files),
+since much of what it tries to test is, e.g., imports.
+
+"""
+
+#%% Imports and constants
+
+### Only standard imports belong here, not MD-specific imports ###
+
+import os
+import json
+import glob
+import tempfile
+import urllib
+import urllib.request
+import zipfile
+import subprocess
+import argparse
+
+
+#%% Classes
+
+class MDTestOptions:
+    """
+    Options controlling test behavior.
+    """
+    
+    ## Required ##
+    
+    #: Force CPU execution
+    disable_gpu = False
+    
+    #: If GPU execution is requested, but a GPU is not available, should we error?
+    cpu_execution_is_error = False
+    
+    #: Skip tests related to video processing
+    skip_video_tests = False
+    
+    #: Skip tests launched via Python functions (as opposed to CLIs)
+    skip_python_tests = False
+    
+    #: Skip CLI tests
+    skip_cli_tests = False
+    
+    #: Force a specific folder for temporary input/output
+    scratch_dir = None
+    
+    #: Where does the test data live?
+    test_data_url = 'https://lila.science/public/md-test-package.zip'
+    
+    #: Download test data even if it appears to have already been downloaded
+    force_data_download = False
+    
+    #: Unzip test data even if it appears to have already been unzipped
+    force_data_unzip = False
+    
+    #: By default, any unexpected behavior is an error; this forces most errors to
+    #: be treated as warnings.
+    warning_mode = False
+    
+    #: How much deviation from the expected detection coordinates should we allow before
+    #: a disrepancy becomes an error?
+    max_coord_error = 0.001
+    
+    #: How much deviation from the expected confidence values should we allow before
+    #: a disrepancy becomes an error?    
+    max_conf_error = 0.005
+    
+    #: Current working directory when running CLI tests
+    cli_working_dir = None
+    
+    #: YOLOv5 installation, only relevant if we're testing run_inference_with_yolov5_val. 
+    #:
+    #: If this is None, we'll skip that test.
+    yolo_working_folder = None
+
+# ...class MDTestOptions()
+
+
+#%% Support functions
+
+def get_expected_results_filename(gpu_is_available):
+    """
+    Expected results vary just a little across inference environments, particularly
+    between PT 1.x and 2.x, so when making sure things are working acceptably, we 
+    compare to a reference file that matches the current environment.
+    
+    This function gets the correct filename to compare to current results, depending
+    on whether a GPU is available.
+    
+    Args:
+        gpu_is_available (bool): whether a GPU is available
+        
+    Returns:
+        str: relative filename of the results file we should use (within the test
+        data zipfile)
+    """
+    
+    if gpu_is_available:
+        hw_string = 'gpu'
+    else:
+        hw_string = 'cpu'
+    import torch
+    torch_version = str(torch.__version__)
+    if torch_version.startswith('1'):
+        assert torch_version == '1.10.1', 'Only tested against PT 1.10.1 and PT 2.x'
+        pt_string = 'pt1.10.1'
+    else:
+        assert torch_version.startswith('2'), 'Unknown torch version: {}'.format(torch_version)
+        pt_string = 'pt2.x'
+    
+    # A hack for now to account for the fact that even with acceleration enabled and PT2 
+    # installed, Apple silicon appears to provide the same results as CPU/PT1 inference
+    try:
+        import torch
+        m1_inference = torch.backends.mps.is_built and torch.backends.mps.is_available()
+        if m1_inference:
+            hw_string = 'cpu'
+        pt_string = 'pt1.10.1'
+    except Exception:
+        pass
+    
+    return 'md-test-results-{}-{}.json'.format(hw_string,pt_string)
+    
+    
+def download_test_data(options=None):
+    """
+    Downloads the test zipfile if necessary, unzips if necessary.
+    
+    Args:
+        options (MDTestOptions, optional): see MDTestOptions for details
+        
+    Returns:
+        MDTestOptions: the same object passed in as input, or the options that
+        were used if [options] was supplied as None
+    """
+
+    if options is None:
+        options = MDTestOptions()
+        
+    if options.scratch_dir is None:        
+        tempdir_base = tempfile.gettempdir()
+        scratch_dir = os.path.join(tempdir_base,'md-tests')
+    else:
+        scratch_dir = options.scratch_dir
+    
+    os.makedirs(scratch_dir,exist_ok=True)    
+    
+    # See whether we've already downloaded the data zipfile
+    download_zipfile = True        
+    if not options.force_data_download:
+        local_zipfile = os.path.join(scratch_dir,options.test_data_url.split('/')[-1])
+        if os.path.isfile(local_zipfile):
+            url_info = urllib.request.urlopen(options.test_data_url).info()
+            remote_size = int(url_info['Content-Length'])
+            target_file_size = os.path.getsize(local_zipfile)
+            if remote_size == target_file_size:
+                download_zipfile = False
+    
+    if download_zipfile:
+        print('Downloading test data zipfile')
+        urllib.request.urlretrieve(options.test_data_url, local_zipfile)
+        print('Finished download to {}'.format(local_zipfile))
+    else:
+        print('Bypassing test data zipfile download for {}'.format(local_zipfile))
+    
+    
+    ## Unzip data
+    
+    zipf = zipfile.ZipFile(local_zipfile)    
+    zip_contents = zipf.filelist
+    
+    # file_info = zip_contents[1]
+    for file_info in zip_contents:
+        
+        expected_size = file_info.file_size
+        if expected_size == 0:
+            continue
+        fn_relative = file_info.filename
+        target_file = os.path.join(scratch_dir,fn_relative)
+        unzip_file = True
+        if (not options.force_data_unzip) and os.path.isfile(target_file):
+            existing_file_size = os.path.getsize(target_file)
+            if existing_file_size == expected_size:
+                unzip_file = False
+        if unzip_file:
+            os.makedirs(os.path.dirname(target_file),exist_ok=True)
+            with open(target_file,'wb') as f:
+                f.write(zipf.read(fn_relative))
+            
+    # ...for each file in the zipfile
+    
+    # Warn if file are present that aren't expected
+    test_files = glob.glob(os.path.join(scratch_dir,'**/*'), recursive=True)
+    test_files = [os.path.relpath(fn,scratch_dir).replace('\\','/') for fn in test_files]
+    test_files_set = set(test_files)
+    expected_images_set = set(zipf.namelist())
+    for fn in expected_images_set:
+        if fn.endswith('/'):
+            continue
+        assert fn in test_files_set, 'File {} is missing from the test image folder'.format(fn)
+    
+    # Populate the test options with test data information
+    options.scratch_dir = scratch_dir
+    options.all_test_files = test_files
+    options.test_images = [fn for fn in test_files if os.path.splitext(fn.lower())[1] in ('.jpg','.jpeg','.png')]
+    options.test_videos = [fn for fn in test_files if os.path.splitext(fn.lower())[1] in ('.mp4','.avi')]    
+    options.test_videos = [fn for fn in options.test_videos if 'rendered' not in fn]
+        
+    print('Finished unzipping and enumerating test data')
+    
+    return options
+
+# ...def download_test_data(...)
+
+
+def is_gpu_available(verbose=True):
+    """
+    Checks whether a GPU (including M1/M2 MPS) is available.
+    
+    Args:
+        verbose (bool, optional): enable additional debug console output
+    
+    Returns:
+        bool: whether a GPU is available
+    """
+    
+    # Import torch inside this function, so we have a chance to set CUDA_VISIBLE_DEVICES
+    # before checking GPU availability.
+    import torch
+    gpu_available = torch.cuda.is_available()
+    
+    if gpu_available:
+        if verbose:
+            print('CUDA available: {}'.format(gpu_available))
+            device_ids = list(range(torch.cuda.device_count()))
+            if len(device_ids) > 1:
+                print('Found multiple devices: {}'.format(str(device_ids)))
+    else:
+        try:
+            gpu_available = torch.backends.mps.is_built and torch.backends.mps.is_available()
+        except AttributeError:
+            pass
+        if gpu_available:
+            print('Metal performance shaders available')
+    
+    if not gpu_available:
+        print('No GPU available')
+        
+    return gpu_available            
+        
+    
+#%% CLI functions
+
+# These are copied from process_utils.py to avoid imports outside of the test
+# functions.
+
+os.environ["PYTHONUNBUFFERED"] = "1"
+
+def execute(cmd):
+    """
+    Runs [cmd] (a single string) in a shell, yielding each line of output to the caller.
+    
+    Args:
+        cmd (str): command to run
+    
+    Returns:
+        int: the command's return code, always zero, otherwise a CalledProcessError is raised
+    """
+ 
+    # https://stackoverflow.com/questions/4417546/constantly-print-subprocess-output-while-process-is-running
+    popen = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
+                             shell=True, universal_newlines=True)
+    for stdout_line in iter(popen.stdout.readline, ""):
+        yield stdout_line
+    popen.stdout.close()
+    return_code = popen.wait()
+    if return_code:
+        raise subprocess.CalledProcessError(return_code, cmd)
+    return return_code
+
+
+def execute_and_print(cmd,print_output=True):
+    """
+    Runs [cmd] (a single string) in a shell, capturing (and optionally printing) output.
+    
+    Args:
+        cmd (str): command to run
+        print_output (bool, optional): whether to print output from [cmd]
+    
+    Returns:
+        dict: a dictionary with fields "status" (the process return code) and "output"
+        (the content of stdout)
+    """
+
+    to_return = {'status':'unknown','output':''}
+    output=[]
+    try:
+        for s in execute(cmd):
+            output.append(s)
+            if print_output:
+                print(s,end='',flush=True)
+        to_return['status'] = 0
+    except subprocess.CalledProcessError as cpe:
+        print('execute_and_print caught error: {}'.format(cpe.output))
+        to_return['status'] = cpe.returncode
+    to_return['output'] = output
+   
+    return to_return
+
+
+#%% Python tests
+
+def run_python_tests(options):
+    """
+    Runs Python-based (as opposed to CLI-based) package tests.
+    
+    Args:
+        options (MDTestOptions): see MDTestOptions for details
+    """
+    
+    print('\n*** Starting module tests ***\n')
+    
+    ## Prepare data
+    
+    download_test_data(options)
+    
+    
+    ## Run inference on an image
+    
+    from detection import run_detector
+    from md_visualization import visualization_utils as vis_utils
+    model_file = 'MDV5A'
+    image_fn = os.path.join(options.scratch_dir,options.test_images[0])
+    model = run_detector.load_detector(model_file)
+    pil_im = vis_utils.load_image(image_fn)
+    result = model.generate_detections_one_image(pil_im) # noqa
+
+    
+    ## Run inference on a folder
+    
+    from detection.run_detector_batch import load_and_run_detector_batch,write_results_to_file
+    from md_utils import path_utils
+
+    image_folder = os.path.join(options.scratch_dir,'md-test-images')
+    assert os.path.isdir(image_folder), 'Test image folder {} is not available'.format(image_folder)
+    inference_output_file = os.path.join(options.scratch_dir,'folder_inference_output.json')
+    image_file_names = path_utils.find_images(image_folder,recursive=True)
+    results = load_and_run_detector_batch('MDV5A', image_file_names, quiet=True)
+    _ = write_results_to_file(results,inference_output_file,
+                              relative_path_base=image_folder,detector_file=model_file)
+
+    # Read results
+    with open(inference_output_file,'r') as f:
+        results_from_file = json.load(f) # noqa
+    
+
+    ## Verify results
+
+    # Read expected results
+    expected_results_filename = get_expected_results_filename(is_gpu_available(verbose=False))
+    
+    with open(os.path.join(options.scratch_dir,expected_results_filename),'r') as f:
+        expected_results = json.load(f)
+            
+    filename_to_results = {im['file'].replace('\\','/'):im for im in results_from_file['images']}
+    filename_to_results_expected = {im['file'].replace('\\','/'):im for im in expected_results['images']}
+    
+    assert len(filename_to_results) == len(filename_to_results_expected), \
+        'Error: expected {} files in results, found {}'.format(
+            len(filename_to_results_expected),
+            len(filename_to_results))
+    
+    max_coord_error = 0
+    max_conf_error = 0
+    
+    # fn = next(iter(filename_to_results.keys()))
+    for fn in filename_to_results.keys():
+                
+        actual_image_results = filename_to_results[fn]
+        expected_image_results = filename_to_results_expected[fn]
+        
+        if 'failure' in actual_image_results:
+            assert 'failure' in expected_image_results and \
+                'detections' not in actual_image_results and \
+                'detections' not in expected_image_results
+            continue
+        assert 'failure' not in expected_image_results
+        
+        actual_detections = actual_image_results['detections']
+        expected_detections = expected_image_results['detections']
+        
+        s = 'expected {} detections for file {}, found {}'.format(
+            len(expected_detections),fn,len(actual_detections))
+        s += '\nExpected results file: {}\nActual results file: {}'.format(
+            expected_results_filename,inference_output_file)
+        
+        if options.warning_mode:
+            if len(actual_detections) != len(expected_detections):
+                print('Warning: {}'.format(s))
+            continue
+        assert len(actual_detections) == len(expected_detections), \
+            'Error: {}'.format(s)
+        
+        # i_det = 0
+        for i_det in range(0,len(actual_detections)):
+            actual_det = actual_detections[i_det]
+            expected_det = expected_detections[i_det]
+            assert actual_det['category'] == expected_det['category']
+            conf_err = abs(actual_det['conf'] - expected_det['conf'])
+            coord_differences = []
+            for i_coord in range(0,4):
+                coord_differences.append(abs(actual_det['bbox'][i_coord]-expected_det['bbox'][i_coord]))
+            coord_err = max(coord_differences)
+            
+            if conf_err > max_conf_error:
+                max_conf_error = conf_err
+            if coord_err > max_coord_error:
+                max_coord_error = coord_err
+        
+        # ...for each detection
+        
+    # ...for each image
+    
+    if not options.warning_mode:
+        
+        assert max_conf_error <= options.max_conf_error, \
+            'Confidence error {} is greater than allowable ({})'.format(
+                max_conf_error,options.max_conf_error)
+        
+        assert max_coord_error <= options.max_coord_error, \
+            'Coord error {} is greater than allowable ({})'.format(
+                max_coord_error,options.max_coord_error)
+        
+    print('Max conf error: {}'.format(max_conf_error))
+    print('Max coord error: {}'.format(max_coord_error))
+
+
+    ## Postprocess results
+    
+    from api.batch_processing.postprocessing.postprocess_batch_results import \
+        PostProcessingOptions,process_batch_results
+    postprocessing_options = PostProcessingOptions()
+    
+    postprocessing_options.api_output_file = inference_output_file
+    postprocessing_options.output_dir = os.path.join(options.scratch_dir,'postprocessing_output')
+    postprocessing_options.image_base_dir = image_folder
+    
+    postprocessing_results = process_batch_results(postprocessing_options)
+    assert os.path.isfile(postprocessing_results.output_html_file), \
+        'Postprocessing output file {} not found'.format(postprocessing_results.output_html_file)
+    
+    
+    ## Partial RDE test
+    
+    from api.batch_processing.postprocessing.repeat_detection_elimination.repeat_detections_core import \
+        RepeatDetectionOptions,find_repeat_detections
+    
+    rde_options = RepeatDetectionOptions()
+    rde_options.occurrenceThreshold = 2
+    rde_options.confidenceMin = 0.001
+    rde_options.outputBase = os.path.join(options.scratch_dir,'rde_working_dir')
+    rde_options.imageBase = image_folder
+    rde_output_file = inference_output_file.replace('.json','_filtered.json')
+    assert rde_output_file != inference_output_file
+    rde_results = find_repeat_detections(inference_output_file, rde_output_file, rde_options)
+    assert os.path.isfile(rde_results.filterFile),\
+        'Could not find RDE output file {}'.format(rde_results.filterFile)
+        
+    
+    # TODO: add remove_repeat_detections test here
+    #
+    # It's already tested in the CLI tests, so this is not urgent.
+    
+    if not options.skip_video_tests:
+        
+        ## Video test (single video)
+       
+        from detection.process_video import ProcessVideoOptions, process_video
+        
+        video_options = ProcessVideoOptions()
+        video_options.model_file = 'MDV5A'
+        video_options.input_video_file = os.path.join(options.scratch_dir,options.test_videos[0])
+        video_options.output_json_file = os.path.join(options.scratch_dir,'single_video_output.json')
+        video_options.output_video_file = os.path.join(options.scratch_dir,'video_scratch/rendered_video.mp4')
+        video_options.frame_folder = os.path.join(options.scratch_dir,'video_scratch/frame_folder')
+        video_options.frame_rendering_folder = os.path.join(options.scratch_dir,'video_scratch/rendered_frame_folder')    
+        video_options.render_output_video = True
+        # video_options.keep_rendered_frames = False
+        # video_options.keep_rendered_frames = False
+        video_options.force_extracted_frame_folder_deletion = True
+        video_options.force_rendered_frame_folder_deletion = True
+        # video_options.reuse_results_if_available = False
+        # video_options.reuse_frames_if_available = False
+        video_options.recursive = True
+        video_options.verbose = False
+        video_options.fourcc = 'mp4v'
+        # video_options.rendering_confidence_threshold = None
+        # video_options.json_confidence_threshold = 0.005
+        video_options.frame_sample = 5    
+        video_options.n_cores = 5
+        # video_options.debug_max_frames = -1
+        # video_options.class_mapping_filename = None
+        
+        _ = process_video(video_options)
+    
+        assert os.path.isfile(video_options.output_video_file), \
+            'Python video test failed to render output video file'
+        assert os.path.isfile(video_options.output_json_file), \
+            'Python video test failed to render output .json file'
+            
+        
+        ## Video test (folder)
+        
+        from detection.process_video import ProcessVideoOptions, process_video_folder
+        
+        video_options = ProcessVideoOptions()
+        video_options.model_file = 'MDV5A'
+        video_options.input_video_file = os.path.join(options.scratch_dir,
+                                                      os.path.dirname(options.test_videos[0]))
+        video_options.output_json_file = os.path.join(options.scratch_dir,'video_folder_output.json')
+        # video_options.output_video_file = None
+        video_options.frame_folder = os.path.join(options.scratch_dir,'video_scratch/frame_folder')
+        video_options.frame_rendering_folder = os.path.join(options.scratch_dir,'video_scratch/rendered_frame_folder')    
+        video_options.render_output_video = False
+        # video_options.keep_rendered_frames = False
+        # video_options.keep_rendered_frames = False
+        video_options.force_extracted_frame_folder_deletion = True
+        video_options.force_rendered_frame_folder_deletion = True
+        # video_options.reuse_results_if_available = False
+        # video_options.reuse_frames_if_available = False
+        video_options.recursive = True
+        video_options.verbose = False
+        # video_options.fourcc = None
+        # video_options.rendering_confidence_threshold = None
+        # video_options.json_confidence_threshold = 0.005
+        video_options.frame_sample = 5    
+        video_options.n_cores = 5
+        # video_options.debug_max_frames = -1
+        # video_options.class_mapping_filename = None
+        
+        _ = process_video_folder(video_options)
+    
+        assert os.path.isfile(video_options.output_json_file), \
+            'Python video test failed to render output .json file'
+        
+    # ...if we're not skipping video tests
+    
+    print('\n*** Finished module tests ***\n')
+
+# ...def run_python_tests(...)
+
+
+#%% Command-line tests
+
+def run_cli_tests(options):
+    """
+    Runs CLI (as opposed to Python-based) package tests.
+    
+    Args:
+        options (MDTestOptions): see MDTestOptions for details
+    """
+    
+    print('\n*** Starting CLI tests ***\n')
+    
+    ## chdir if necessary
+    
+    if options.cli_working_dir is not None:
+        os.chdir(options.cli_working_dir)
+    
+    
+    ## Prepare data
+    
+    download_test_data(options)
+    
+    
+    ## Run inference on an image
+    
+    model_file = 'MDV5A'
+    image_fn = os.path.join(options.scratch_dir,options.test_images[0])
+    output_dir = os.path.join(options.scratch_dir,'single_image_test')
+    if options.cli_working_dir is None:
+        cmd = 'python -m detection.run_detector'
+    else:
+        cmd = 'python detection/run_detector.py'
+    cmd += ' {} --image_file {} --output_dir {}'.format(
+        model_file,image_fn,output_dir)
+    print('Running: {}'.format(cmd))
+    cmd_results = execute_and_print(cmd)
+    
+    if options.cpu_execution_is_error:
+        gpu_available_via_cli = False
+        for s in cmd_results['output']:
+            if 'GPU available: True' in s:
+                gpu_available_via_cli = True
+                break
+        if not gpu_available_via_cli:
+            raise Exception('GPU execution is required, but not available')
+
+    
+    ## Run inference on a folder
+    
+    image_folder = os.path.join(options.scratch_dir,'md-test-images')
+    assert os.path.isdir(image_folder), 'Test image folder {} is not available'.format(image_folder)
+    inference_output_file = os.path.join(options.scratch_dir,'folder_inference_output.json')
+    if options.cli_working_dir is None:
+        cmd = 'python -m detection.run_detector_batch'
+    else:
+        cmd = 'python detection/run_detector_batch.py'
+    cmd += ' {} {} {} --recursive'.format(
+        model_file,image_folder,inference_output_file)
+    cmd += ' --output_relative_filenames --quiet --include_image_size'
+    cmd += ' --include_image_timestamp --include_exif_data'
+    print('Running: {}'.format(cmd))
+    cmd_results = execute_and_print(cmd)
+    
+    # Make sure a coherent file got written out, but don't verify the results, leave that
+    # to the Python tests.
+    with open(inference_output_file,'r') as f:
+        results_from_file = json.load(f) # noqa
+    
+
+    ## Postprocessing
+    
+    postprocessing_output_dir = os.path.join(options.scratch_dir,'postprocessing_output_cli')
+    
+    if options.cli_working_dir is None:
+        cmd = 'python -m api.batch_processing.postprocessing.postprocess_batch_results'
+    else:
+        cmd = 'python api/batch_processing/postprocessing/postprocess_batch_results.py'
+    cmd += ' {} {}'.format(
+        inference_output_file,postprocessing_output_dir)
+    cmd += ' --image_base_dir {}'.format(image_folder)
+    print('Running: {}'.format(cmd))
+    cmd_results = execute_and_print(cmd)
+                
+    
+    ## RDE
+    
+    rde_output_dir = os.path.join(options.scratch_dir,'rde_output_cli')
+    
+    if options.cli_working_dir is None:
+        cmd = 'python -m api.batch_processing.postprocessing.repeat_detection_elimination.find_repeat_detections'
+    else:
+        cmd = 'python  api/batch_processing/postprocessing/repeat_detection_elimination/find_repeat_detections.py'
+    cmd += ' {}'.format(inference_output_file)
+    cmd += ' --imageBase {}'.format(image_folder)
+    cmd += ' --outputBase {}'.format(rde_output_dir)
+    cmd += ' --occurrenceThreshold 1' # Use an absurd number here to make sure we get some suspicious detections
+    print('Running: {}'.format(cmd))
+    cmd_results = execute_and_print(cmd)    
+    
+    # Find the latest filtering folder
+    filtering_output_dir = os.listdir(rde_output_dir)
+    filtering_output_dir = [fn for fn in filtering_output_dir if fn.startswith('filtering_')]
+    filtering_output_dir = [os.path.join(rde_output_dir,fn) for fn in filtering_output_dir]
+    filtering_output_dir = [fn for fn in filtering_output_dir if os.path.isdir(fn)]
+    filtering_output_dir = sorted(filtering_output_dir)[-1]
+    
+    print('Using RDE filtering folder {}'.format(filtering_output_dir))
+    
+    filtered_output_file = inference_output_file.replace('.json','_filtered.json')
+    
+    if options.cli_working_dir is None:
+        cmd = 'python -m api.batch_processing.postprocessing.repeat_detection_elimination.remove_repeat_detections'
+    else:
+        cmd = 'python  api/batch_processing/postprocessing/repeat_detection_elimination/remove_repeat_detections.py'
+    cmd += ' {} {} {}'.format(inference_output_file,filtered_output_file,filtering_output_dir)
+    print('Running: {}'.format(cmd))
+    cmd_results = execute_and_print(cmd)
+    
+    assert os.path.isfile(filtered_output_file), \
+        'Could not find RDE output file {}'.format(filtered_output_file)
+    
+    
+    ## Run inference on a folder (tiled)
+    
+    image_folder = os.path.join(options.scratch_dir,'md-test-images')
+    tiling_folder = os.path.join(options.scratch_dir,'tiling-folder')
+    inference_output_file_tiled = os.path.join(options.scratch_dir,'folder_inference_output_tiled.json')
+    if options.cli_working_dir is None:
+        cmd = 'python -m detection.run_tiled_inference'
+    else:
+        cmd = 'python detection/run_tiled_inference.py'
+    cmd += ' {} {} {} {}'.format(
+        model_file,image_folder,tiling_folder,inference_output_file_tiled)
+    cmd += ' --overwrite_handling overwrite'
+    print('Running: {}'.format(cmd))
+    cmd_results = execute_and_print(cmd)
+    
+    with open(inference_output_file_tiled,'r') as f:
+        results_from_file = json.load(f) # noqa
+        
+    
+    ## Run inference on a folder (augmented)
+    
+    if options.yolo_working_folder is None:
+        
+        print('Bypassing YOLOv5 val tests, no yolo folder supplied')
+        
+    else:
+    
+        image_folder = os.path.join(options.scratch_dir,'md-test-images')
+        yolo_results_folder = os.path.join(options.scratch_dir,'yolo-output-folder')
+        yolo_symlink_folder = os.path.join(options.scratch_dir,'yolo-symlink_folder')
+        inference_output_file_yolo_val = os.path.join(options.scratch_dir,'folder_inference_output_yolo_val.json')
+        if options.cli_working_dir is None:
+            cmd = 'python -m detection.run_inference_with_yolov5_val'
+        else:
+            cmd = 'python detection/run_inference_with_yolov5_val.py'
+        cmd += ' {} {} {}'.format(
+            model_file,image_folder,inference_output_file_yolo_val)
+        cmd += ' --yolo_working_folder {}'.format(options.yolo_working_folder)
+        cmd += ' --yolo_results_folder {}'.format(yolo_results_folder)
+        cmd += ' --symlink_folder {}'.format(yolo_symlink_folder)
+        cmd += ' --augment_enabled 1'
+        # cmd += ' --no_use_symlinks'
+        cmd += ' --overwrite_handling overwrite'
+        print('Running: {}'.format(cmd))
+        cmd_results = execute_and_print(cmd)
+        
+        with open(inference_output_file_yolo_val,'r') as f:
+            results_from_file = json.load(f) # noqa
+        
+        
+    if not options.skip_video_tests:
+            
+        ## Video test
+        
+        model_file = 'MDV5A'
+        video_inference_output_file = os.path.join(options.scratch_dir,'video_inference_output.json')
+        output_video_file = os.path.join(options.scratch_dir,'video_scratch/cli_rendered_video.mp4')
+        frame_folder = os.path.join(options.scratch_dir,'video_scratch/frame_folder_cli')
+        frame_rendering_folder = os.path.join(options.scratch_dir,'video_scratch/rendered_frame_folder_cli')        
+        
+        video_fn = os.path.join(options.scratch_dir,options.test_videos[-1])
+        output_dir = os.path.join(options.scratch_dir,'single_video_test_cli')
+        if options.cli_working_dir is None:
+            cmd = 'python -m detection.process_video'
+        else:
+            cmd = 'python detection/process_video.py'
+        cmd += ' {} {}'.format(model_file,video_fn)
+        cmd += ' --frame_folder {} --frame_rendering_folder {} --output_json_file {} --output_video_file {}'.format(
+            frame_folder,frame_rendering_folder,video_inference_output_file,output_video_file)
+        cmd += ' --render_output_video --fourcc mp4v'
+        cmd += ' --force_extracted_frame_folder_deletion --force_rendered_frame_folder_deletion --n_cores 5 --frame_sample 3'
+        print('Running: {}'.format(cmd))
+        cmd_results = execute_and_print(cmd)
+
+    # ...if we're not skipping video tests
+    
+    
+    ## Run inference on a folder (again, so we can do a comparison)
+    
+    image_folder = os.path.join(options.scratch_dir,'md-test-images')
+    model_file = 'MDV5B'
+    inference_output_file_alt = os.path.join(options.scratch_dir,'folder_inference_output_alt.json')
+    if options.cli_working_dir is None:
+        cmd = 'python -m detection.run_detector_batch'
+    else:
+        cmd = 'python detection/run_detector_batch.py'
+    cmd += ' {} {} {} --recursive'.format(
+        model_file,image_folder,inference_output_file_alt)
+    cmd += ' --output_relative_filenames --quiet --include_image_size'
+    cmd += ' --include_image_timestamp --include_exif_data'
+    print('Running: {}'.format(cmd))
+    cmd_results = execute_and_print(cmd)
+    
+    with open(inference_output_file_alt,'r') as f:
+        results_from_file = json.load(f) # noqa
+    
+    
+    ## Compare the two files
+    
+    comparison_output_folder = os.path.join(options.scratch_dir,'results_comparison')
+    image_folder = os.path.join(options.scratch_dir,'md-test-images')
+    results_files_string = '"{}" "{}"'.format(
+        inference_output_file,inference_output_file_alt)
+    if options.cli_working_dir is None:
+        cmd = 'python -m api.batch_processing.postprocessing.compare_batch_results'
+    else:
+        cmd = 'python api/batch_processing/postprocessing/compare_batch_results.py'
+    cmd += ' {} {} {}'.format(comparison_output_folder,image_folder,results_files_string)
+    print('Running: {}'.format(cmd))
+    cmd_results = execute_and_print(cmd)
+    
+    assert cmd_results['status'] == 0, 'Error generating comparison HTML'
+    assert os.path.isfile(os.path.join(comparison_output_folder,'index.html')), \
+        'Failed to generate comparison HTML'
+    
+    print('\n*** Finished CLI tests ***\n')
+    
+# ...def run_cli_tests(...)
+
+
+#%% Main test wrapper
+
+def run_tests(options):
+    """
+    Runs Python-based and/or CLI-based package tests.
+    
+    Args:
+        options (MDTestOptions): see MDTestOptions for details
+    """
+    
+    # Prepare data folder
+    download_test_data(options)    
+    
+    if options.disable_gpu:
+        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
+        
+    # Verify GPU
+    gpu_available = is_gpu_available()
+    
+    # If the GPU is required and isn't available, error
+    if options.cpu_execution_is_error and (not gpu_available):
+        raise ValueError('GPU not available, and cpu_execution_is_error is set')
+    
+    # If the GPU should be disabled, verify that it is
+    if options.disable_gpu:
+        assert (not gpu_available), 'CPU execution specified, but the GPU appears to be available'
+        
+    # Run python tests
+    if not options.skip_python_tests:
+        run_python_tests(options)
+    
+    # Run CLI tests
+    if not options.skip_cli_tests:
+        run_cli_tests(options)
+
+
+#%% Interactive driver
+
+if False:
+    
+    pass
+
+    #%%
+    
+    options = MDTestOptions()
+    
+    options.disable_gpu = False
+    options.cpu_execution_is_error = False
+    options.skip_video_tests = False
+    options.skip_python_tests = False
+    options.skip_cli_tests = False
+    options.scratch_dir = None
+    options.test_data_url = 'https://lila.science/public/md-test-package.zip'
+    options.force_data_download = False
+    options.force_data_unzip = False
+    options.warning_mode = True
+    options.max_coord_error = 0.001
+    options.max_conf_error = 0.005
+    options.cli_working_dir = r'c:\git\MegaDetector'
+    options.yolo_working_folder = r'c:\git\yolov5'
+
+
+    #%%
+    
+    run_tests(options)
+    
+    
+#%% Command-line driver
+
+def main():
+
+    options = MDTestOptions()
+    
+    parser = argparse.ArgumentParser(
+        description='MegaDetector test suite')
+    
+    parser.add_argument(
+        '--disable_gpu',
+        action='store_true',
+        help='Disable GPU operation')
+    
+    parser.add_argument(
+        '--cpu_execution_is_error',
+        action='store_true',
+        help='Fail if the GPU appears not to be available')
+    
+    parser.add_argument(
+        '--scratch_dir',
+        default=None,
+        type=str,
+        help='Directory for temporary storage (defaults to system temp dir)')
+    
+    parser.add_argument(
+        '--skip_video_tests',
+        action='store_true',
+        help='Skip tests related to video (which can be slow)')
+        
+    parser.add_argument(
+        '--skip_python_tests',
+        action='store_true',
+        help='Skip python tests')
+        
+    parser.add_argument(
+        '--skip_cli_tests',
+        action='store_true',
+        help='Skip CLI tests')
+        
+    parser.add_argument(
+        '--force_data_download',
+        action='store_true',
+        help='Force download of the test data file, even if it\'s already available')
+    
+    parser.add_argument(
+        '--force_data_unzip',
+        action='store_true',
+        help='Force extraction of all files in the test data file, even if they\'re already available')
+    
+    parser.add_argument(
+        '--warning_mode',
+        action='store_true',
+        help='Turns numeric/content errors into warnings')
+    
+    parser.add_argument(
+        '--max_conf_error',
+        type=float,
+        default=options.max_conf_error,
+        help='Maximum tolerable confidence value deviation from expected (default {})'.format(
+            options.max_conf_error))
+    
+    parser.add_argument(
+        '--max_coord_error',
+        type=float,
+        default=options.max_coord_error,
+        help='Maximum tolerable coordinate value deviation from expected (default {})'.format(
+            options.max_coord_error))
+
+    parser.add_argument(
+        '--cli_working_dir',
+        type=str,
+        default=None,
+        help='Working directory for CLI tests')
+
+    # token used for linting
+    #
+    # no_arguments_required
+        
+    args = parser.parse_args()
+        
+    options.disable_gpu = args.disable_gpu
+    options.cpu_execution_is_error = args.cpu_execution_is_error
+    options.skip_video_tests = args.skip_video_tests
+    options.skip_python_tests = args.skip_python_tests
+    options.skip_cli_tests = args.skip_cli_tests
+    options.scratch_dir = args.scratch_dir
+    options.warning_mode = args.warning_mode
+    options.force_data_download = args.force_data_download
+    options.max_conf_error = args.max_conf_error
+    options.max_coord_error = args.max_coord_error
+    options.cli_working_dir = args.cli_working_dir
+
+    run_tests(options)
+    
+if __name__ == '__main__':
+    main()
```

### Comparing `megadetector-5.0.8/md_utils/path_utils.py` & `megadetector-5.0.9/data_management/yolo_to_coco.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,718 +1,676 @@
-########
-# 
-# path_utils.py
-#
-# Miscellaneous useful utils for path manipulation, things that could *almost*
-# be in os.path, but aren't.
-#
-########
+"""
+
+yolo_to_coco.py
+
+Converts a folder of YOLO-formatted annotation files to a COCO-formatted dataset. 
+
+"""
 
 #%% Imports and constants
 
-import glob
-import ntpath
-import os
-import sys
-import platform
-import posixpath
-import string
 import json
-import shutil
-import unicodedata
-import zipfile
-import webbrowser
-import subprocess
-import re
-
-from zipfile import ZipFile
-from datetime import datetime
-from typing import Container, Iterable, List, Optional, Tuple, Sequence
-from multiprocessing.pool import Pool, ThreadPool
+import os
+
+from multiprocessing.pool import ThreadPool
+from multiprocessing.pool import Pool
 from functools import partial
+
 from tqdm import tqdm
 
-IMG_EXTENSIONS = ('.jpg', '.jpeg', '.gif', '.png', '.tif', '.tiff', '.bmp')
+from md_utils.path_utils import find_images
+from md_utils.path_utils import recursive_file_list
+from md_utils.path_utils import find_image_strings
+from md_utils.ct_utils import invert_dictionary
+from md_visualization.visualization_utils import open_image
+from data_management.yolo_output_to_md_output import read_classes_from_yolo_dataset_file
 
-VALID_FILENAME_CHARS = f"~-_.() {string.ascii_letters}{string.digits}"
-SEPARATOR_CHARS = r":\/"
-VALID_PATH_CHARS = VALID_FILENAME_CHARS + SEPARATOR_CHARS
-CHAR_LIMIT = 255
 
+#%% Support functions
+
+def _filename_to_image_id(fn):
+    """
+    Image IDs can't have spaces in them, replae spaces with underscores
+    """
+    return fn.replace(' ','_')
 
-#%% General path functions
 
-def recursive_file_list(base_dir, convert_slashes=True, 
-                        return_relative_paths=False, sort_files=True,
-                        recursive=True):
-    r"""
-    Enumerate files (not directories) in [base_dir], optionally converting
-    \ to /
+def _process_image(fn_abs,input_folder,category_id_to_name):
+    """
+    Internal support function for processing one image's labels.
     """
     
-    assert os.path.isdir(base_dir), '{} is not a folder'.format(base_dir)
+    # Create the image object for this image
+    fn_relative = os.path.relpath(fn_abs,input_folder)
+    image_id = _filename_to_image_id(fn_relative)
     
-    all_files = []
-
-    if recursive:
-        for root, _, filenames in os.walk(base_dir):
-            for filename in filenames:
-                full_path = os.path.join(root, filename)
-                all_files.append(full_path)
-    else:
-        all_files_relative = os.listdir(base_dir)
-        all_files = [os.path.join(base_dir,fn) for fn in all_files_relative]
-        all_files = [fn for fn in all_files if os.path.isfile(fn)]
-        
-    if return_relative_paths:
-        all_files = [os.path.relpath(fn,base_dir) for fn in all_files]
-
-    if convert_slashes:
-        all_files = [fn.replace('\\', '/') for fn in all_files]
+    # This is done in a separate loop now
+    #
+    # assert image_id not in image_ids, \
+    #    'Oops, you have hit a very esoteric case where you have the same filename ' + \
+    #    'with both spaces and underscores, this is not currently handled.'
+    # image_ids.add(image_id)
     
-    if sort_files:
-        all_files = sorted(all_files)
+    im = {}
+    im['file_name'] = fn_relative    
+    im['id'] = image_id
+    
+    annotations_this_image = []
+    
+    try:        
+        pil_im = open_image(fn_abs)
+        im_width, im_height = pil_im.size
+        im['width'] = im_width
+        im['height'] = im_height
+        im['error'] = None
+    except Exception as e:
+        print('Warning: error reading {}:\n{}'.format(fn_relative,str(e)))
+        im['width'] = -1
+        im['height'] = -1
+        im['error'] = str(e)
+        return (im,annotations_this_image)
+        
+    # Is there an annotation file for this image?
+    annotation_file = os.path.splitext(fn_abs)[0] + '.txt'
+    if not os.path.isfile(annotation_file):
+        annotation_file = os.path.splitext(fn_abs)[0] + '.TXT'
+    
+    if os.path.isfile(annotation_file):
+        
+        with open(annotation_file,'r') as f:
+            lines = f.readlines()
+        lines = [s.strip() for s in lines]
         
-    return all_files
-
-
-def file_list(base_dir, convert_slashes=True, return_relative_paths=False, sort_files=True, 
-              recursive=False):
-    """
-    Trivial wrapper for recursive_file_list, which was a poor function name choice at the time, 
-    it doesn't really make sense to have a "recursive" option in a function called "recursive_file_list".
-    """
+        # s = lines[0]
+        annotation_number = 0
+        
+        for s in lines:
+            
+            if len(s.strip()) == 0:
+                continue
+            
+            tokens = s.split()
+            assert len(tokens) == 5
+            category_id = int(tokens[0])
+            assert category_id in category_id_to_name, \
+                'Unrecognized category ID {} in annotation file {}'.format(
+                    category_id,annotation_file)
+            ann = {}
+            ann['id'] = im['id'] + '_' + str(annotation_number)
+            ann['image_id'] = im['id']
+            ann['category_id'] = category_id
+            ann['sequence_level_annotation'] = False
+            
+            # COCO: [x_min, y_min, width, height] in absolute coordinates
+            # YOLO: [class, x_center, y_center, width, height] in normalized coordinates
+            
+            yolo_bbox = [float(x) for x in tokens[1:]]
+            
+            normalized_x_center = yolo_bbox[0]
+            normalized_y_center = yolo_bbox[1]
+            normalized_width = yolo_bbox[2]
+            normalized_height = yolo_bbox[3]
+            
+            absolute_x_center = normalized_x_center * im_width 
+            absolute_y_center = normalized_y_center * im_height
+            absolute_width = normalized_width * im_width
+            absolute_height = normalized_height * im_height
+            absolute_x_min = absolute_x_center - absolute_width / 2
+            absolute_y_min = absolute_y_center - absolute_height / 2
+            
+            coco_bbox = [absolute_x_min, absolute_y_min, absolute_width, absolute_height]
+            
+            ann['bbox'] = coco_bbox
+            annotation_number += 1
+            
+            annotations_this_image.append(ann)                
+            
+        # ...for each annotation 
+        
+    # ...if this image has annotations
     
-    return recursive_file_list(base_dir,convert_slashes,return_relative_paths,sort_files,
-                               recursive=recursive)
-
+    return (im,annotations_this_image)
 
-def split_path(path: str) -> List[str]:
-    r"""
-    Splits [path] into all its constituent tokens.
+# ...def _process_image(...)
 
-    Non-recursive version of:
-    http://nicks-liquid-soapbox.blogspot.com/2011/03/splitting-path-to-list-in-python.html
 
-    Examples
-    >>> split_path(r'c:\dir\subdir\file.txt')
-    ['c:\\', 'dir', 'subdir', 'file.txt']
-    >>> split_path('/dir/subdir/file.jpg')
-    ['/', 'dir', 'subdir', 'file.jpg']
-    >>> split_path('c:\\')
-    ['c:\\']
-    >>> split_path('/')
-    ['/']
+def load_yolo_class_list(class_name_file):
     """
+    Loads a dictionary mapping zero-indexed IDs to class names from the text/yaml file
+    [class_name_file].    
     
-    parts = []
-    while True:
-        # ntpath seems to do the right thing for both Windows and Unix paths
-        head, tail = ntpath.split(path)
-        if head == '' or head == path:
-            break
-        parts.append(tail)
-        path = head
-    parts.append(head or tail)
-    return parts[::-1]  # reverse
-
-
-def fileparts(path: str) -> Tuple[str, str, str]:
-    r"""
-    Breaks down a path into the directory path, filename, and extension.
-
-    Note that the '.' lives with the extension, and separators are removed.
-
-    Examples
-    >>> fileparts('file')
-    ('', 'file', '')
-    >>> fileparts(r'c:\dir\file.jpg')
-    ('c:\\dir', 'file', '.jpg')
-    >>> fileparts('/dir/subdir/file.jpg')
-    ('/dir/subdir', 'file', '.jpg')
-
+    Args:
+        class_name_file (str or list): this can be:
+            - a .yaml or .yaml file in YOLO's dataset.yaml format
+            - a .txt or .data file containing a flat list of class names
+            - a list of class names
+            
     Returns:
-        p: str, directory path
-        n: str, filename without extension
-        e: str, extension including the '.'
+        dict: A dict mapping zero-indexed integer IDs to class names
     """
     
-    # ntpath seems to do the right thing for both Windows and Unix paths
-    p = ntpath.dirname(path)
-    basename = ntpath.basename(path)
-    n, e = ntpath.splitext(basename)
-    return p, n, e
-
+    # class_name_file can also be a list of class names
+    if isinstance(class_name_file,list):
+        category_id_to_name = {}
+        for i_name,name in enumerate(class_name_file):
+            category_id_to_name[i_name] = name
+        return category_id_to_name
+            
+    ext = os.path.splitext(class_name_file)[1][1:]
+    assert ext in ('yml','txt','yaml','data'), 'Unrecognized class name file type {}'.format(
+        class_name_file)
+    
+    if ext in ('txt','data'):
+        
+        with open(class_name_file,'r') as f:
+            lines = f.readlines()
+        assert len(lines) > 0, 'Empty class name file {}'.format(class_name_file)
+        class_names = [s.strip() for s in lines]
+        assert len(lines[0]) > 0, 'Empty class name file {} (empty first line)'.format(class_name_file)
+        
+        # Blank lines should only appear at the end
+        b_found_blank = False
+        for s in lines:
+            if len(s) == 0:
+                b_found_blank = True
+            elif b_found_blank:
+                raise ValueError('Invalid class name file {}, non-blank line after the last blank line'.format(
+                    class_name_file))
+    
+        category_id_to_name = {}        
+        for i_category_id,category_name in enumerate(class_names):
+            assert len(category_name) > 0
+            category_id_to_name[i_category_id] = category_name
+            
+    else:
+        
+        assert ext in ('yml','yaml')
+        category_id_to_name = read_classes_from_yolo_dataset_file(class_name_file)
+        
+    return category_id_to_name
 
-def insert_before_extension(filename: str, s: str = '', separator='.') -> str:
-    """
-    Insert string [s] before the extension in [filename], separated with [separator].
+# ...load_yolo_class_list(...)
 
-    If [s] is empty, generates a date/timestamp. If [filename] has no extension,
-    appends [s].
 
-    Examples
-    >>> insert_before_extension('/dir/subdir/file.ext', 'insert')
-    '/dir/subdir/file.insert.ext'
-    >>> insert_before_extension('/dir/subdir/file', 'insert')
-    '/dir/subdir/file.insert'
-    >>> insert_before_extension('/dir/subdir/file')
-    '/dir/subdir/file.2020.07.20.10.54.38'
-    """
+def validate_label_file(label_file,category_id_to_name=None,verbose=False):
+    """"
+    Verifies that [label_file] is a valid YOLO label file.  Does not check the extension.
     
-    assert len(filename) > 0
-    if len(s) == 0:
-        s = datetime.now().strftime('%Y.%m.%d.%H.%M.%S')
-    name, ext = os.path.splitext(filename)
-    return f'{name}{separator}{s}{ext}'
-
-
-def top_level_folder(p: str, windows: Optional[bool] = None) -> str:
-    """
-    Gets the top-level folder from path [p].
-
-    This function behaves differently for Windows vs. Unix paths. Set
-    windows=True if [p] is a Windows path. Set windows=None (default) to treat
-    [p] as a native system path.
-
-    On Windows, will use the top-level folder that isn't the drive.
-    >>> top_level_folder(r'c:\blah\foo')
-    'c:\blah'
-
-    On Unix, does not include the leaf node.
-    >>> top_level_folder('/blah/foo')
-    '/blah'
-    """
+    Args:
+        label_file (str): the .txt file to validate
+        category_id_to_name (dict, optional): a dict mapping integer category IDs to names;
+            if this is not None, this function errors if the file uses a category that's not
+            in this dict
+        verbose (bool, optional): enable additional debug console output
     
-    if p == '':
-        return ''
-
-    default_lib = os.path  # save default os.path
-    if windows is not None:
-        os.path = ntpath if windows else posixpath
-
-    # Path('/blah').parts is ('/', 'blah')
-    parts = split_path(p)
-
-    drive = os.path.splitdrive(p)[0]
-    if len(parts) > 1 and (
-            parts[0] == drive
-            or parts[0] == drive + '/'
-            or parts[0] == drive + '\\'
-            or parts[0] in ['\\', '/']):
-        result = os.path.join(parts[0], parts[1])
-    else:
-        result = parts[0]
-
-    os.path = default_lib  # restore default os.path
-    return result
-
-
-def safe_create_link(link_exists,link_new):
+    Returns:
+        dict: a dict with keys 'file' (the same as [label_file]) and 'errors' (a list of 
+        errors (if any) that we found in this file)
     """
-    Create a symlink at link_new pointing to link_exists.
     
-    If link_new already exists, make sure it's a link (not a file),
-    and if it has a different target than link_exists, remove and re-create
-    it.
+    label_result = {}
+    label_result['file'] = label_file
+    label_result['errors'] = []
     
-    Errors if link_new already exists but it's not a link.
-    """
+    try:
+        with open(label_file,'r') as f:
+            lines = f.readlines()
+    except Exception as e:
+        label_result['errors'].append('Read error: {}'.format(str(e)))
+        return label_result
     
-    if os.path.exists(link_new) or os.path.islink(link_new):
-        assert os.path.islink(link_new)
-        if not os.readlink(link_new) == link_exists:
-            os.remove(link_new)
-            os.symlink(link_exists,link_new)
-    else:
-        os.symlink(link_exists,link_new)
+    # i_line 0; line = lines[i_line]
+    for i_line,line in enumerate(lines):
+        s = line.strip()
+        if len(s) == 0 or s[0] == '#':
+            continue
         
-
-#%% Image-related path functions
-
-def is_image_file(s: str, img_extensions: Container[str] = IMG_EXTENSIONS
-                  ) -> bool:
-    """
-    Checks a file's extension against a hard-coded set of image file
-    extensions.
+        try:
+        
+            tokens = s.split()
+            assert len(tokens) == 5, '{} tokens'.format(len(tokens))                
+            
+            if category_id_to_name is not None:
+                category_id = int(tokens[0])
+                assert category_id in category_id_to_name, \
+                    'Unrecognized category ID {}'.format(category_id)
+        
+            yolo_bbox = [float(x) for x in tokens[1:]]
+            
+        except Exception as e:
+            label_result['errors'].append('Token error at line {}: {}'.format(i_line,str(e)))
+            continue
+                            
+        normalized_x_center = yolo_bbox[0]
+        normalized_y_center = yolo_bbox[1]
+        normalized_width = yolo_bbox[2]
+        normalized_height = yolo_bbox[3]
+        
+        normalized_x_min = normalized_x_center - normalized_width / 2.0
+        normalized_x_max = normalized_x_center + normalized_width / 2.0
+        normalized_y_min = normalized_y_center - normalized_height / 2.0
+        normalized_y_max = normalized_y_center + normalized_height / 2.0
+        
+        if normalized_x_min < 0 or normalized_y_min < 0 or \
+            normalized_x_max > 1 or normalized_y_max > 1:
+            label_result['errors'].append('Invalid bounding box: {} {} {} {}'.format(
+                normalized_x_min,normalized_y_min,normalized_x_max,normalized_y_max))
+        
+    # ...for each line
     
-    Does not check whether the file exists, only determines whether the filename
-    implies it's an image file.
-    """
+    if verbose:
+        if len(label_result['errors']) > 0:
+            print('Errors for {}:'.format(label_file))
+            for error in label_result['errors']:
+                print(error)
+                
+    return label_result
+    
+# ...def validate_label_file(...)
+
+    
+def validate_yolo_dataset(input_folder, class_name_file, n_workers=1, pool_type='thread', verbose=False):
+    """
+    Verifies all the labels in a YOLO dataset folder.  
+    
+    Looks for:
+        
+    * Image files without label files
+    * Text files without image files
+    * Illegal classes in label files
+    * Invalid boxes in label files
+
+    Args:
+        input_folder (str): the YOLO dataset folder to validate
+        class_name_file (str or list): a list of classes, a flat text file, or a yolo 
+            dataset.yml/.yaml file.  If it's a dataset.yml file, that file should point to 
+            input_folder as the base folder, though this is not explicitly checked.
+        n_workers (int, optional): number of concurrent workers, set to <= 1 to disable
+            parallelization
+        pool_type (str, optional): 'thread' or 'process', worker type to use for parallelization;
+            not used if [n_workers] <= 1
+        verbose (bool, optional): enable additional debug console output
     
-    ext = os.path.splitext(s)[1]
-    return ext.lower() in img_extensions
-
-
-def find_image_strings(strings: Iterable[str]) -> List[str]:
-    """
-    Given a list of strings that are potentially image file names, looks for
-    strings that actually look like image file names (based on extension).
+    Returns:
+        dict: validation results, as a dict with fields:        
+        
+        - image_files_without_label_files (list)
+        - label_files_without_image_files (list)
+        - label_results (list of dicts with field 'filename', 'errors') (list)
     """
     
-    return [s for s in strings if is_image_file(s)]
-
-
-def find_images(dirname: str, recursive: bool = False, 
-                return_relative_paths: bool = False, 
-                convert_slashes: bool = False) -> List[str]:
-    """
-    Finds all files in a directory that look like image file names. Returns
-    absolute paths unless return_relative_paths is set.  Uses the OS-native
-    path separator unless convert_slashes is set, in which case will always
-    use '/'.
-    """
+    # Validate arguments
+    assert os.path.isdir(input_folder), 'Could not find input folder {}'.format(input_folder)
+    if n_workers > 1:
+        assert pool_type in ('thread','process'), 'Illegal pool type {}'.format(pool_type)
+        
+    category_id_to_name = load_yolo_class_list(class_name_file)
     
-    assert os.path.isdir(dirname), '{} is not a folder'.format(dirname)
+    print('Enumerating files in {}'.format(input_folder))
     
-    if recursive:
-        strings = glob.glob(os.path.join(dirname, '**', '*.*'), recursive=True)
-    else:
-        strings = glob.glob(os.path.join(dirname, '*.*'))
+    all_files = recursive_file_list(input_folder,recursive=True,return_relative_paths=False,
+                                    convert_slashes=True)
+    label_files = [fn for fn in all_files if fn.endswith('.txt')]
+    image_files = find_image_strings(all_files)
+    print('Found {} images files and {} label files in {}'.format(
+        len(image_files),len(label_files),input_folder))
     
-    image_files = find_image_strings(strings)
+    label_files_set = set(label_files)
     
-    if return_relative_paths:
-        image_files = [os.path.relpath(fn,dirname) for fn in image_files]
+    image_files_without_extension = set()
+    for fn in image_files:
+        image_file_without_extension = os.path.splitext(fn)[0]
+        assert image_file_without_extension not in image_files_without_extension, \
+            'Duplicate image file, likely with different extensions: {}'.format(fn)
+        image_files_without_extension.add(image_file_without_extension)
+        
+    print('Looking for missing image/label files')
     
-    image_files = sorted(image_files)
+    image_files_without_label_files = []
+    label_files_without_images = []
     
-    if convert_slashes:
-        image_files = [fn.replace('\\', '/') for fn in image_files]
-        
-    return image_files
-
-
-#%% Filename cleaning functions
-
-def clean_filename(filename: str, allow_list: str = VALID_FILENAME_CHARS,
-                   char_limit: int = CHAR_LIMIT, force_lower: bool = False) -> str:
-    r"""
-    Removes non-ASCII and other invalid filename characters (on any
-    reasonable OS) from a filename, then trims to a maximum length.
-
-    Does not allow :\/ by default, use clean_path if you want to preserve those.
+    for image_file in tqdm(image_files):
+        expected_label_file = os.path.splitext(image_file)[0] + '.txt'
+        if expected_label_file not in label_files_set:
+            image_files_without_label_files.append(image_file)
+            
+    for label_file in tqdm(label_files):
+        expected_image_file_without_extension = os.path.splitext(label_file)[0]
+        if expected_image_file_without_extension not in image_files_without_extension:
+            label_files_without_images.append(label_file)
+            
+    print('Found {} image files without labels, {} labels without images'.format(
+        len(image_files_without_label_files),len(label_files_without_images)))
 
-    Adapted from
-    https://gist.github.com/wassname/1393c4a57cfcbf03641dbc31886123b8
-    """
+    print('Validating label files')
     
-    # keep only valid ascii chars
-    cleaned_filename = (unicodedata.normalize('NFKD', filename)
-                        .encode('ASCII', 'ignore').decode())
-
-    # keep only allow-listed chars
-    cleaned_filename = ''.join([c for c in cleaned_filename if c in allow_list])
-    if char_limit is not None:
-        cleaned_filename = cleaned_filename[:char_limit]
-    if force_lower:
-        cleaned_filename = cleaned_filename.lower()
-    return cleaned_filename
-
-
-def clean_path(pathname: str, allow_list: str = VALID_PATH_CHARS,
-               char_limit: int = CHAR_LIMIT, force_lower: bool = False) -> str:
-    """
-    Removes non-ASCII and other invalid path characters (on any reasonable
-    OS) from a path, then trims to a maximum length.
-    """
+    if n_workers <= 1:
+        
+        label_results = []        
+        for fn_abs in tqdm(label_files):                
+            label_results.append(validate_label_file(fn_abs,
+                                                      category_id_to_name=category_id_to_name,
+                                                      verbose=verbose))
+            
+    else:
+        
+        assert pool_type in ('process','thread'), 'Illegal pool type {}'.format(pool_type)
+        
+        if pool_type == 'thread':
+            pool = ThreadPool(n_workers)
+        else:
+            pool = Pool(n_workers)
+        
+        print('Starting a {} pool of {} workers'.format(pool_type,n_workers))
+        
+        p = partial(validate_label_file,
+                    category_id_to_name=category_id_to_name,
+                    verbose=verbose)
+        label_results = list(tqdm(pool.imap(p, label_files),
+                                  total=len(label_files)))        
+    
+    assert len(label_results) == len(label_files)
+    
+    validation_results = {}
+    validation_results['image_files_without_label_files'] = image_files_without_label_files
+    validation_results['label_files_without_images'] = label_files_without_images
+    validation_results['label_results'] = label_results
+    
+    return validation_results
+    
+# ...validate_yolo_dataset(...)    
+
+
+#%% Main conversion function
+
+def yolo_to_coco(input_folder,
+                 class_name_file,
+                 output_file=None,
+                 empty_image_handling='no_annotations',
+                 empty_image_category_name='empty',
+                 error_image_handling='no_annotations',
+                 allow_images_without_label_files=True,
+                 n_workers=1,
+                 pool_type='thread',
+                 recursive=True,
+                 exclude_string=None,
+                 include_string=None):
+    """
+    Converts a YOLO-formatted dataset to a COCO-formatted dataset.
+    
+    All images will be assigned an "error" value, usually None.    
+    
+    Args:
+        input_folder (str): the YOLO dataset folder to validate
+        class_name_file (str or list): a list of classes, a flat text file, or a yolo 
+            dataset.yml/.yaml file.  If it's a dataset.yml file, that file should point to 
+            input_folder as the base folder, though this is not explicitly checked.
+        output_file (str, optional): .json file to which we should write COCO .json data
+        empty_image_handling (str, optional): how to handle images with no boxes; whether
+            this includes images with no .txt files depending on the value of 
+            [allow_images_without_label_files].  Can be:
+                
+            - 'no_annotations': include the image in the image list, with no annotations
+            - 'empty_annotations': include the image in the image list, and add an annotation without
+              any bounding boxes, using a category called [empty_image_category_name].
+            - 'skip': don't include the image in the image list
+            - 'error': there shouldn't be any empty images            
+        error_image_handling (str, optional): how to handle images that don't load properly; can
+            be:
+            
+            - 'skip': don't include the image at all
+            - 'no_annotations': include with no annotations
+            
+        n_workers (int, optional): number of concurrent workers, set to <= 1 to disable
+            parallelization
+        pool_type (str, optional): 'thread' or 'process', worker type to use for parallelization;
+            not used if [n_workers] <= 1
+        recursive (bool, optional): whether to recurse into [input_folder]
+        exclude_string (str, optional): exclude any images whose filename contains a string
+        include_string (str, optional): include only images whose filename contains a string
     
-    return clean_filename(pathname, allow_list=allow_list, 
-                          char_limit=char_limit, force_lower=force_lower)
-
-
-def flatten_path(pathname: str, separator_chars: str = SEPARATOR_CHARS) -> str:
-    """
-    Removes non-ASCII and other invalid path characters (on any reasonable
-    OS) from a path, then trims to a maximum length. Replaces all valid
-    separators with '~'.
+    Returns:
+        dict: COCO-formatted data, the same as what's written to [output_file]
     """
     
-    s = clean_path(pathname)
-    for c in separator_chars:
-        s = s.replace(c, '~')
-    return s
-
-
-#%% Platform-independent way to open files in their associated application
-
-def environment_is_wsl():
-    """
-    Returns True if we're running in WSL
-    """
+    ## Validate input
     
-    if sys.platform not in ('linux','posix'):
-        return False
-    platform_string = ' '.join(platform.uname()).lower()
-    return 'microsoft' in platform_string and 'wsl' in platform_string
+    assert os.path.isdir(input_folder)
+    assert os.path.isfile(class_name_file)
     
+    assert empty_image_handling in \
+        ('no_annotations','empty_annotations','skip','error'), \
+            'Unrecognized empty image handling spec: {}'.format(empty_image_handling)
+     
+            
+    ## Read class names
+    
+    category_id_to_name = load_yolo_class_list(class_name_file)
+    
+    
+    # Find or create the empty image category, if necessary
+    empty_category_id = None
+    
+    if (empty_image_handling == 'empty_annotations'):
+        category_name_to_id = invert_dictionary(category_id_to_name)
+        if empty_image_category_name in category_name_to_id:
+            empty_category_id = category_name_to_id[empty_image_category_name]
+            print('Using existing empty image category with name {}, ID {}'.format(
+                empty_image_category_name,empty_category_id))            
+        else:
+            empty_category_id = len(category_id_to_name)
+            print('Adding an empty category with name {}, ID {}'.format(
+                empty_image_category_name,empty_category_id))
+            category_id_to_name[empty_category_id] = empty_image_category_name
+            
+            
+    ## Enumerate images
+    
+    print('Enumerating images...')
+    
+    image_files_abs = find_images(input_folder,recursive=recursive,convert_slashes=True)
 
-def wsl_path_to_windows_path(filename):
-    """
-    Converts a WSL path to a Windows path, or returns None if that's not possible.  E.g.
-    converts:
+    n_files_original = len(image_files_abs)
+    
+    # Optionally include/exclude images matching specific strings
+    if exclude_string is not None:
+        image_files_abs = [fn for fn in image_files_abs if exclude_string not in fn]
+    if include_string is not None:
+        image_files_abs = [fn for fn in image_files_abs if include_string in fn]
+    
+    if len(image_files_abs) != n_files_original or exclude_string is not None or include_string is not None:
+        n_excluded = n_files_original - len(image_files_abs)
+        print('Excluded {} of {} images based on filenames'.format(n_excluded,n_files_original))
         
-    /mnt/e/a/b/c
+    categories = []
     
-    ...to:
+    for category_id in category_id_to_name:
+        categories.append({'id':category_id,'name':category_id_to_name[category_id]})
         
-    e:\a\b\c
-    """
+    info = {}
+    info['version'] = '1.0'
+    info['description'] = 'Converted from YOLO format'
     
-    result = subprocess.run(['wslpath', '-w', filename], text=True, capture_output=True)
-    if result.returncode != 0:
-        print('Could not convert path {} from WSL to Windows'.format(filename))
-        return None
-    return result.stdout.strip()
+    image_ids = set()
     
     
-def open_file(filename, attempt_to_open_in_wsl_host=False, browser_name=None):
-    """
-    Opens [filename] in the default OS file handler for this file type.
+    ## If we're expected to have labels for every image, check before we process all the images
     
-    If attempt_to_open_in_wsl_host is True, and we're in WSL, attempts to open
-    [filename] in the Windows host environment.
+    if not allow_images_without_label_files:
+        print('Verifying that label files exist')
+        for image_file_abs in tqdm(image_files_abs):
+            label_file_abs = os.path.splitext(image_file_abs)[0] + '.txt'
+            assert os.path.isfile(label_file_abs), \
+                'No annotation file for {}'.format(image_file_abs)
     
-    If browser_name is not None, uses the webbrowser module to open the filename
-    in the specified browser; see https://docs.python.org/3/library/webbrowser.html
-    for supported browsers.  Falls back to the default file handler if webbrowser.open()
-    fails.  In this case, attempt_to_open_in_wsl_host is ignored unless webbrowser.open() fails.
     
-    If browser_name is 'default', use the system default.  This is different from the 
-    parameter to webbrowser.get(), where None implies the system default.
-    """
+    ## Initial loop to make sure image IDs will be unique
     
-    if browser_name is not None:
-        if browser_name == 'chrome':
-            browser_name = 'google-chrome'
-        elif browser_name == 'default':
-            browser_name = None
-        try:
-            result = webbrowser.get(using=browser_name).open(filename)
-        except Exception:
-            result = False
-        if result:
-            return
+    print('Validating image IDs...')
+    
+    for fn_abs in tqdm(image_files_abs):
         
-    if sys.platform == 'win32':
+        fn_relative = os.path.relpath(fn_abs,input_folder)
+        image_id = _filename_to_image_id(fn_relative)
+        assert image_id not in image_ids, \
+            'Oops, you have hit a very esoteric case where you have the same filename ' + \
+            'with both spaces and underscores, this is not currently handled.'
+        image_ids.add(image_id)
+    
+    
+    ## Main loop to process labels
+    
+    print('Processing labels...')
+    
+    if n_workers <= 1:
         
-        os.startfile(filename)
-
-    elif sys.platform == 'darwin':
-      
-        opener = 'open'
-        subprocess.call([opener, filename])
+        image_results = []        
+        for fn_abs in tqdm(image_files_abs):                
+            image_results.append(_process_image(fn_abs,input_folder,category_id_to_name))
             
-    elif attempt_to_open_in_wsl_host and environment_is_wsl():
+    else:
         
-        windows_path = wsl_path_to_windows_path(filename)
+        assert pool_type in ('process','thread'), 'Illegal pool type {}'.format(pool_type)
         
-        # Fall back to xdg-open
-        if windows_path is None:
-            subprocess.call(['xdg-open', filename])
-            
-        if os.path.isdir(filename):            
-            subprocess.run(["explorer.exe", windows_path])
+        if pool_type == 'thread':
+            pool = ThreadPool(n_workers)
         else:
-            os.system("cmd.exe /C start %s" % (re.escape(windows_path)))    
-        
-    else:
+            pool = Pool(n_workers)
         
-        opener = 'xdg-open'        
-        subprocess.call([opener, filename])
+        print('Starting a {} pool of {} workers'.format(pool_type,n_workers))
         
-
-#%% File list functions
-
-def write_list_to_file(output_file: str, strings: Sequence[str]) -> None:
-    """
-    Writes a list of strings to either a JSON file or text file,
-    depending on extension of the given file name.
-    """
+        p = partial(_process_image,input_folder=input_folder,
+                    category_id_to_name=category_id_to_name)
+        image_results = list(tqdm(pool.imap(p, image_files_abs),
+                                  total=len(image_files_abs)))
+                
     
-    with open(output_file, 'w') as f:
-        if output_file.endswith('.json'):
-            json.dump(strings, f, indent=1)
-        else:
-            f.write('\n'.join(strings))
-
-
-def read_list_from_file(filename: str) -> List[str]:
-    """
-    Reads a json-formatted list of strings from a file.
-    """
+    assert len(image_results) == len(image_files_abs)
     
-    assert filename.endswith('.json')
-    with open(filename, 'r') as f:
-        file_list = json.load(f)
-    assert isinstance(file_list, list)
-    for s in file_list:
-        assert isinstance(s, str)
-    return file_list
-
-
-def _copy_file(input_output_tuple,overwrite=True,verbose=False):
-    assert len(input_output_tuple) == 2
-    source_fn = input_output_tuple[0]
-    target_fn = input_output_tuple[1]
-    if (not overwrite) and (os.path.isfile(target_fn)):
-        if verbose:
-            print('Skipping existing file {}'.format(target_fn))
-        return
-    os.makedirs(os.path.dirname(target_fn),exist_ok=True)
-    shutil.copyfile(source_fn,target_fn)
     
-
-def parallel_copy_files(input_file_to_output_file, max_workers=16, 
-                        use_threads=True, overwrite=False, verbose=False):
-    """
-    Copy files from source to target according to the dict input_file_to_output_file.
-    """
-
-    n_workers = min(max_workers,len(input_file_to_output_file))
+    ## Re-assembly of results into a COCO dict
     
-    # Package the dictionary as a set of 2-tuples
-    input_output_tuples = []
-    for input_fn in input_file_to_output_file:
-        input_output_tuples.append((input_fn,input_file_to_output_file[input_fn]))
-
-    if use_threads:
-        pool = ThreadPool(n_workers)
-    else:
-        pool = Pool(n_workers)
-
-    with tqdm(total=len(input_output_tuples)) as pbar:
-        for i,_ in enumerate(pool.imap_unordered(partial(_copy_file,overwrite=overwrite,verbose=verbose),
-                                                 input_output_tuples)):
-            pbar.update()
-
-# ...def parallel_copy_files(...)
-
-
-def get_file_sizes(base_dir, convert_slashes=True):
-    """
-    Get sizes recursively for all files in base_dir, returning a dict mapping
-    relative filenames to size.
+    print('Assembling labels...')
     
-    TODO: merge the functionality here with parallel_get_file_sizes, which uses slightly
-    different semantics.
-    """
+    images = []
+    annotations = []
     
-    relative_filenames = recursive_file_list(base_dir, convert_slashes=convert_slashes, 
-                                             return_relative_paths=True)
+    for image_result in tqdm(image_results):
     
-    fn_to_size = {}
-    for fn_relative in tqdm(relative_filenames):
-        fn_abs = os.path.join(base_dir,fn_relative)
-        fn_to_size[fn_relative] = os.path.getsize(fn_abs)
-                   
-    return fn_to_size
+        im = image_result[0]
+        annotations_this_image = image_result[1]
+           
+        # If we have annotations for this image
+        if len(annotations_this_image) > 0:
+            assert im['error'] is None
+            images.append(im)
+            for ann in annotations_this_image:
+                annotations.append(ann)
+                
+        # If this image failed to read
+        elif im['error'] is not None:
+            
+            if error_image_handling == 'skip':
+                pass
+            elif error_image_handling == 'no_annotations':
+                images.append(im)            
+                
+        # If this image read successfully, but there are no annotations
+        else:
+            
+            if empty_image_handling == 'skip':
+                pass
+            elif empty_image_handling == 'no_annotations':
+                images.append(im)
+            elif empty_image_handling == 'empty_annotations':
+                assert empty_category_id  is not None
+                ann = {}
+                ann['id'] = im['id'] + '_0'
+                ann['image_id'] = im['id']
+                ann['category_id'] = empty_category_id
+                ann['sequence_level_annotation'] = False
+                # This would also be a reasonable thing to do, but it's not the convention
+                # we're adopting.
+                # ann['bbox'] = [0,0,0,0]
+                annotations.append(ann)
+                images.append(im)        
         
-
-def _get_file_size(filename,verbose=False):
-    """
-    Internal function for safely getting the size of a file.  Returns a (filename,size)
-    tuple, where size is None if there is an error.
-    """
+    # ...for each image result
     
-    try:
-        size = os.path.getsize(filename)
-    except Exception as e:
-        if verbose:
-            print('Error reading file size for {}: {}'.format(filename,str(e)))
-        size = None
-    return (filename,size)
-
+    print('Read {} annotations for {} images'.format(len(annotations),
+                                                     len(images)))
     
-def parallel_get_file_sizes(filenames, max_workers=16, 
-                        use_threads=True, verbose=False,
-                        recursive=True):
-    """
-    Return a dictionary mapping every file in [filenames] to the corresponding file size,
-    or None for errors.  If [filenames] is a folder, will enumerate the folder (optionally recursively).
-    """
+    d = {}
+    d['images'] = images
+    d['annotations'] = annotations
+    d['categories'] = categories
+    d['info'] = info
 
-    n_workers = min(max_workers,len(filenames))
-    
-    if isinstance(filenames,str) and os.path.isdir(filenames):
-        filenames = recursive_file_list(filenames,recursive=recursive,return_relative_paths=False)
-    
-    if use_threads:
-        pool = ThreadPool(n_workers)
-    else:
-        pool = Pool(n_workers)
+    if output_file is not None:
+        print('Writing to {}'.format(output_file))
+        with open(output_file,'w') as f:
+            json.dump(d,f,indent=1)
 
-    resize_results = list(tqdm(pool.imap(
-        partial(_get_file_size,verbose=verbose),filenames), total=len(filenames)))
-    
-    to_return = {}
-    for r in resize_results:
-        to_return[r[0]] = r[1]
+    return d
 
-    return to_return
+# ...def yolo_to_coco()
 
 
-#%% Zip functions
+#%% Interactive driver
 
-def zip_file(input_fn, output_fn=None, overwrite=False, verbose=False, compresslevel=9):
-    """
-    Zip a single file, by default writing to a new file called [input_fn].zip
-    """
-    
-    basename = os.path.basename(input_fn)
-    
-    if output_fn is None:
-        output_fn = input_fn + '.zip'
-        
-    if (not overwrite) and (os.path.isfile(output_fn)):
-        print('Skipping existing file {}'.format(output_fn))
-        return
+if False:
     
-    if verbose:
-        print('Zipping {} to {} with level {}'.format(input_fn,output_fn,compresslevel))
-    
-    with ZipFile(output_fn,'w',zipfile.ZIP_DEFLATED) as zipf:
-        zipf.write(input_fn,arcname=basename,compresslevel=compresslevel,
-                   compress_type=zipfile.ZIP_DEFLATED)
-
-    return output_fn
+    pass
 
-
-def zip_files_into_single_zipfile(input_files, output_fn, arc_name_base,
-                                  overwrite=False, verbose=False, compresslevel=9):
-    """
-    Zip all the files in [input_files] into [output_fn].  Archive names are relative to 
-    arc_name_base.
-    """
+    #%% Convert YOLO folders to COCO
     
-    if not overwrite:
-        if os.path.isfile(output_fn):
-            print('Zip file {} exists, skipping'.format(output_fn))
-            return            
-        
-    if verbose:
-        print('Zipping {} files to {} (compression level {})'.format(
-            len(input_files),output_fn,compresslevel))
-        
-    with ZipFile(output_fn,'w',zipfile.ZIP_DEFLATED) as zipf:
-        for input_fn_abs in tqdm(input_files,disable=(not verbose)):
-            input_fn_relative = os.path.relpath(input_fn_abs,arc_name_base)
-            zipf.write(input_fn_abs,
-                       arcname=input_fn_relative,
-                       compresslevel=compresslevel,
-                       compress_type=zipfile.ZIP_DEFLATED)
+    preview_folder = '/home/user/data/noaa-fish/val-coco-conversion-preview'
+    input_folder = '/home/user/data/noaa-fish/val'
+    output_file = '/home/user/data/noaa-fish/val.json'
+    class_name_file = '/home/user/data/noaa-fish/AllImagesWithAnnotations/classes.txt'
 
-    return output_fn
-    
-    
-def zip_folder(input_folder, output_fn=None, overwrite=False, verbose=False, compresslevel=9):
-    """
-    Recursively zip everything in [input_folder] into a single zipfile, storing outputs as relative 
-    paths.
-    
-    Defaults to writing to [input_folder].zip
-    """
-    
-    if output_fn is None:
-        output_fn = input_folder + '.zip'
-        
-    if not overwrite:
-        if os.path.isfile(output_fn):
-            print('Zip file {} exists, skipping'.format(output_fn))
-            return            
+    d = yolo_to_coco(input_folder,class_name_file,output_file)
         
-    if verbose:
-        print('Zipping {} to {} (compression level {})'.format(
-            input_folder,output_fn,compresslevel))
+    input_folder = '/home/user/data/noaa-fish/train'
+    output_file = '/home/user/data/noaa-fish/train.json'
+    class_name_file = '/home/user/data/noaa-fish/AllImagesWithAnnotations/classes.txt'
+
+    d = yolo_to_coco(input_folder,class_name_file,output_file)
     
-    relative_filenames = recursive_file_list(input_folder,return_relative_paths=True)
     
-    with ZipFile(output_fn,'w',zipfile.ZIP_DEFLATED) as zipf:
-        for input_fn_relative in tqdm(relative_filenames,disable=(not verbose)):
-            input_fn_abs = os.path.join(input_folder,input_fn_relative)            
-            zipf.write(input_fn_abs,
-                       arcname=input_fn_relative,
-                       compresslevel=compresslevel,
-                       compress_type=zipfile.ZIP_DEFLATED)
+    #%% Check DB integrity
 
-    return output_fn
+    from data_management.databases import integrity_check_json_db
 
-        
-def parallel_zip_files(input_files, max_workers=16, use_threads=True, compresslevel=9, 
-                       overwrite=False, verbose=False):
-    """
-    Zip one or more files to separate output files in parallel, leaving the 
-    original files in place.  Each file is zipped to [filename].zip.
-    """
+    options = integrity_check_json_db.IntegrityCheckOptions()
+    options.baseDir = input_folder
+    options.bCheckImageSizes = False
+    options.bCheckImageExistence = True
+    options.bFindUnusedImages = True
 
-    n_workers = min(max_workers,len(input_files))
+    _, _, _ = integrity_check_json_db.integrity_check_json_db(output_file, options)
 
-    if use_threads:
-        pool = ThreadPool(n_workers)
-    else:
-        pool = Pool(n_workers)
-
-    with tqdm(total=len(input_files)) as pbar:
-        for i,_ in enumerate(pool.imap_unordered(partial(zip_file,
-          output_fn=None,overwrite=overwrite,verbose=verbose,compresslevel=compresslevel),
-          input_files)):
-            pbar.update()
 
+    #%% Preview some images
 
-def parallel_zip_folders(input_folders, max_workers=16, use_threads=True,
-                         compresslevel=9, overwrite=False, verbose=False):
-    """
-    Zip one or more folders to separate output files in parallel, leaving the 
-    original folders in place.  Each folder is zipped to [folder_name].zip.
-    """
+    from md_visualization import visualize_db
 
-    n_workers = min(max_workers,len(input_folders))
+    viz_options = visualize_db.DbVizOptions()
+    viz_options.num_to_visualize = None
+    viz_options.trim_to_images_with_bboxes = False
+    viz_options.add_search_links = False
+    viz_options.sort_by_filename = False
+    viz_options.parallelize_rendering = True
+    viz_options.include_filename_links = True
 
-    if use_threads:
-        pool = ThreadPool(n_workers)
-    else:
-        pool = Pool(n_workers)
+    html_output_file, _ = visualize_db.visualize_db(db_path=output_file,
+                                                        output_dir=preview_folder,
+                                                        image_base_dir=input_folder,
+                                                        options=viz_options)
     
-    with tqdm(total=len(input_folders)) as pbar:
-        for i,_ in enumerate(pool.imap_unordered(
-                partial(zip_folder,overwrite=overwrite,
-                        compresslevel=compresslevel,verbose=verbose),
-                input_folders)):
-            pbar.update()
+    from md_utils.path_utils import open_file
+    open_file(html_output_file)
 
 
-def zip_each_file_in_folder(folder_name,recursive=False,max_workers=16,use_threads=True,
-                            compresslevel=9,overwrite=False,required_token=None,verbose=False,
-                            exclude_zip=True):
-    """
-    Zip each file in [folder_name] to its own zipfile (filename.zip), optionally recursing.  To zip a whole
-    folder into a single zipfile, use zip_folder().
-    
-    If required_token is not None, include only files that contain that token.
-    """
-    
-    assert os.path.isdir(folder_name), '{} is not a folder'.format(folder_name)
-    
-    input_files = recursive_file_list(folder_name,recursive=recursive,return_relative_paths=False)
-    
-    if required_token is not None:
-        input_files = [fn for fn in input_files if required_token in fn]
-    
-    if exclude_zip:
-        input_files = [fn for fn in input_files if (not fn.endswith('.zip'))]
-                                                    
-    parallel_zip_files(input_files=input_files,max_workers=max_workers,
-                       use_threads=use_threads,compresslevel=compresslevel,
-                       overwrite=overwrite,verbose=verbose)
-
+#%% Command-line driver
 
-def unzip_file(input_file, output_folder=None):
-    """
-    Unzip a zipfile to the specified output folder, defaulting to the same location as
-    the input file    
-    """
-    
-    if output_folder is None:
-        output_folder = os.path.dirname(input_file)
-        
-    with zipfile.ZipFile(input_file, 'r') as zf:
-        zf.extractall(output_folder)
+# TODO
```

### Comparing `megadetector-5.0.8/md_utils/sas_blob_utils.py` & `megadetector-5.0.9/md_utils/sas_blob_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,27 +1,27 @@
-########
-#
-# sas_blob_utils.py
-#
-# This module contains helper functions for dealing with Shared Access Signatures
-# (SAS) tokens for Azure Blob Storage.
-#
-# The default Azure Storage SAS URI format is:
-#
-# https://<account>.blob.core.windows.net/<container>/<blob>?<sas_token>
-#
-# This module assumes azure-storage-blob version 12.5.
-#
-# Documentation for Azure Blob Storage:
-# docs.microsoft.com/en-us/azure/developer/python/sdk/storage/storage-blob-readme
-#
-# Documentation for SAS:
-# docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview
-#
-########
+"""
+
+sas_blob_utils.py
+
+This module contains helper functions for dealing with Shared Access Signatures
+(SAS) tokens for Azure Blob Storage.
+
+The default Azure Storage SAS URI format is:
+
+https://<account>.blob.core.windows.net/<container>/<blob>?<sas_token>
+
+This module assumes azure-storage-blob version 12.5.
+
+Documentation for Azure Blob Storage:
+docs.microsoft.com/en-us/azure/developer/python/sdk/storage/storage-blob-readme
+
+Documentation for SAS:
+docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview
+
+"""
 
 #%% Imports
 
 from datetime import datetime, timedelta
 import io
 import re
 from typing import (Any, AnyStr, Dict, IO, Iterable, List, Optional, Set, Tuple, Union)
```

### Comparing `megadetector-5.0.8/md_utils/split_locations_into_train_val.py` & `megadetector-5.0.9/md_utils/split_locations_into_train_val.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-########
-#
-# split_locations_into_train_val.py
-#
-# Split a list of location IDs into training and validation, targeting a specific
-# train/val split for each category, but allowing some categories to be tighter or looser
-# than others.  Does nothing particularly clever, just randomly splits locations into 
-# train/val lots of times using the target val fraction, and picks the one that meets the 
-# specified constraints and minimizes weighted error, where "error" is defined as the
-# sum of each class's absolute divergence from the target val fraction.
-#
-########
+"""
+
+split_locations_into_train_val.py
+
+Splits a list of location IDs into training and validation, targeting a specific
+train/val split for each category, but allowing some categories to be tighter or looser
+than others.  Does nothing particularly clever, just randomly splits locations into 
+train/val lots of times using the target val fraction, and picks the one that meets the 
+specified constraints and minimizes weighted error, where "error" is defined as the
+sum of each class's absolute divergence from the target val fraction.
+
+"""
 
 #%% Imports/constants
 
 import random
 import numpy as np
 
 from collections import defaultdict
@@ -26,39 +26,52 @@
 def split_locations_into_train_val(location_to_category_counts,
                                    n_random_seeds=10000,
                                    target_val_fraction=0.15,
                                    category_to_max_allowable_error=None,                                   
                                    category_to_error_weight=None,
                                    default_max_allowable_error=0.1):
     """
-    Split a list of location IDs into training and validation, targeting a specific
+    Splits a list of location IDs into training and validation, targeting a specific
     train/val split for each category, but allowing some categories to be tighter or looser
     than others.  Does nothing particularly clever, just randomly splits locations into 
     train/val lots of times using the target val fraction, and picks the one that meets the 
     specified constraints and minimizes weighted error, where "error" is defined as the
     sum of each class's absolute divergence from the target val fraction.    
     
-    location_to_category_counts should be a dict mapping location IDs to dicts,
-    with each dict mapping a category name to a count.  Any categories not present in a 
-    particular dict are assumed to have a count of zero for that location.
-    
-    If not None, category_to_max_allowable_error should be a dict mapping category names
-    to maximum allowable errors.  These are hard constraints, but you can specify a subset
-    of categories.  Categories not included here have a maximum error of Inf.
-    
-    If not None, category_to_error_weight should be a dict mapping category names to
-    error weights.  You can specify a subset of categories.  Categories not included here
-    have a weight of 1.0.
-    
-    default_max_allowable_error is the maximum allowable error for categories not present in
-    category_to_max_allowable_error.  Set to None (or >= 1.0) to disable hard constraints for 
-    categories not present in category_to_max_allowable_error
-    
-    returns val_locations,category_to_val_fraction
-    
+    Args:
+        location_to_category_counts (dict): a dict mapping location IDs to dicts,
+            with each dict mapping a category name to a count.  Any categories not present 
+            in a particular dict are assumed to have a count of zero for that location.
+            
+            For example:
+                
+            .. code-block:: none
+
+                {'location-000': {'bear':4,'wolf':10},
+                 'location-001': {'bear':12,'elk':20}}
+    
+        n_random_seeds (int, optional): number of random seeds to try, always starting from zero
+        target_val_fraction (float, optional): fraction of images containing each species we'd
+            like to put in the val split
+        category_to_max_allowable_error (dict, optional): a dict mapping category names
+            to maximum allowable errors.  These are hard constraints (i.e., we will error
+            if we can't meet them).  Does not need to include all categories; categories not 
+            included will be assigned a maximum error according to [default_max_allowable_error].
+            If this is None, no hard constraints are applied.
+        category_to_error_weight (dict, optional): a dict mapping category names to
+            error weights.  You can specify a subset of categories; categories not included here
+            have a weight of 1.0.  If None, all categories have the same weight.
+        default_max_allowable_error (float, optional): the maximum allowable error for categories not 
+            present in [category_to_max_allowable_error].  Set to None (or >= 1.0) to disable hard 
+            constraints for categories not present in [category_to_max_allowable_error]
+    
+    Returns:
+        tuple: A two-element tuple:
+            - list of location IDs in the val split
+            - a dict mapping category names to the fraction of images in the val split                
     """
     
     location_ids = list(location_to_category_counts.keys())
     
     n_val_locations = int(target_val_fraction*len(location_ids))
     
     if category_to_max_allowable_error is None:
@@ -80,15 +93,15 @@
     
     print('Splitting {} categories over {} locations'.format(
         len(category_ids),len(location_ids)))
     
     # random_seed = 0
     def compute_seed_errors(random_seed):
         """
-        Compute the per-category error for a specific random seed.
+        Computes the per-category error for a specific random seed.
         
         returns weighted_average_error,category_to_val_fraction
         """
         
         # Randomly split into train/val
         random.seed(random_seed)
         val_locations = random.sample(location_ids,k=n_val_locations)
```

### Comparing `megadetector-5.0.8/md_utils/string_utils.py` & `megadetector-5.0.9/md_utils/string_utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,36 +1,53 @@
-########
-#
-# string_utils.py
-#
-# Miscellaneous string utilities
-#
-########
+"""
+
+string_utils.py
+
+Miscellaneous string utilities.
+
+"""
+
+#%% Imports
 
 import re
 
+
+#%% Functions
+
 def is_float(s):
     """ 
-    Checks whether a string represents a valid float
+    Checks whether [s] is an object (typically a string) that can be cast to a float
+    
+    Args:
+        s (object): object to evaluate
+        
+    Returns:
+        bool: True if s successfully casts to a float, otherwise False
     """
     
     try:
         _ = float(s)
     except ValueError:
         return False
     return True
 
 
 def human_readable_to_bytes(size):
     """
     Given a human-readable byte string (e.g. 2G, 10GB, 30MB, 20KB),
-    return the number of bytes.  Will return 0 if the argument has
+    returns the number of bytes.  Will return 0 if the argument has
     unexpected form.
     
     https://gist.github.com/beugley/ccd69945346759eb6142272a6d69b4e0
+    
+    Args:
+        size (str): string representing a size
+        
+    Returns:
+        int: the corresponding size in bytes
     """
     
     size = re.sub(r'\s+', '', size)
     
     if (size[-1] == 'B'):
         size = size[:-1]
         
@@ -57,13 +74,19 @@
             bytes = 0
             
     return bytes
 
 
 def remove_ansi_codes(s):
     """
-    Remove ANSI escape codes from a string.
+    Removes ANSI escape codes from a string.
     
     https://stackoverflow.com/questions/14693701/how-can-i-remove-the-ansi-escape-sequences-from-a-string-in-python#14693789
+    
+    Args:
+        s (str): the string to de-ANSI-i-fy
+        
+    Returns:
+        str: A copy of [s] without ANSI codes
     """
     ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
     return ansi_escape.sub('', s)
```

### Comparing `megadetector-5.0.8/md_utils/write_html_image_list.py` & `megadetector-5.0.9/md_utils/write_html_image_list.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,53 +1,60 @@
-########
-#
-# write_html_image_list.py
-#
-# Given a list of image file names, writes an HTML file that
-# shows all those images, with optional one-line headers above each.
-#
-# Each "filename" can also be a dict with elements 'filename','title',
-# 'imageStyle','textStyle', 'linkTarget'
-#
-########
+"""
+
+write_html_image_list.py
+
+Given a list of image file names, writes an HTML file that
+shows all those images, with optional one-line headers above each.
+
+Each "filename" can also be a dict with elements 'filename','title',
+'imageStyle','textStyle', 'linkTarget'
+
+"""
 
 #%% Constants and imports
 
 import os
 import math
 import urllib
 
 from md_utils import path_utils
 
 
 #%% write_html_image_list
 
 def write_html_image_list(filename=None,images=None,options=None):
     """
-    filename: the output file
-    
-    image: a list of image filenames or dictionaries with one or more of the following fields:
-        
-        filename
-        imageStyle
-        textStyle
-        title
-        linkTarget
-        
-    options: a dict with one or more of the following fields:
-        
-        fHtml
-        headerHtml
-        trailerHtml
-        defaultTextStyle
-        defaultImageStyle
-        maxFiguresPerHtmlFile
-        urlEncodeFilenames (default True, e.g. '#' will be replaced by '%23')
-        urlEncodeLinkTargets (default True, e.g. '#' will be replaced by '%23')
-        
+    Given a list of image file names, writes an HTML file that shows all those images, 
+    with optional one-line headers above each.
+
+    Args:
+        filename (str, optional): the .html output file; if None, just returns a valid 
+            options dict
+        images (list, optional): the images to write to the .html file; if None, just returns 
+            a valid options dict.  This can be a flat list of image filenames, or this can
+            be a list of dictionaries with one or more of the following fields:
+                
+            - filename (image filename) (required, all other fields are optional)
+            - imageStyle (css style for this image)
+            - textStyle (css style for the title associated with this image)
+            - title (text label for this image)
+            - linkTarget (URL to which this image should link on click)
+            
+        options (dict, optional): a dict with one or more of the following fields:        
+            
+            - fHtml (file pointer to write to, used for splitting write operations over multiple calls)
+            - headerHtml (html text to include before the image list)
+            - trailerHtml (html text to include after the image list)
+            - defaultImageStyle (default css style for images)
+            - defaultTextStyle (default css style for image titles)
+            - maxFiguresPerHtmlFile (max figures for a single HTML file; overflow will be handled by creating
+              multiple files and a TOC with links)
+            - urlEncodeFilenames (default True, e.g. '#' will be replaced by '%23')
+            - urlEncodeLinkTargets (default True, e.g. '#' will be replaced by '%23')
+            
     """
     
     # returns an options struct
     if options is None:
         options = {}
         
     if 'fHtml' not in options:
@@ -74,15 +81,15 @@
         options['urlEncodeLinkTargets'] = True
     
     # Possibly split the html output for figures into multiple files; Chrome gets sad with
     # thousands of images in a single tab.        
     if 'maxFiguresPerHtmlFile' not in options or options['maxFiguresPerHtmlFile'] is None:
         options['maxFiguresPerHtmlFile'] = math.inf    
     
-    if filename is None:
+    if filename is None or images is None:
         return options
     
     # images may be a list of images or a list of image/style/title dictionaries, 
     # enforce that it's the latter to simplify downstream code
     for iImage,imageInfo in enumerate(images):
         if isinstance(imageInfo,str):
             imageInfo = {'filename':imageInfo}
```

### Comparing `megadetector-5.0.8/md_visualization/plot_utils.py` & `megadetector-5.0.9/md_visualization/plot_utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,60 +1,57 @@
-########
-#
-# plot_utils.py
-# 
-# Utility functions for plotting.
-#
-# NOTE: Do NOT import matplotlib.pyplot here to avoid the interactive backend.
-# Thus, the matplotlib.figure.Figure objects returned by the functions here do not
-# need to be "closed" with `plt.close(fig)`.
-# 
-########
+"""
 
-#%% Imports
+plot_utils.py
+
+Utility functions for plotting, particularly for plotting confusion matrices
+and precision-recall curves.
 
-from typing import Any, Optional, Sequence, Tuple, Union
+"""
+
+#%% Imports
 
 import numpy as np
-import matplotlib.figure  # this also imports mpl.{cm, axes, colors}
+
+# This also imports mpl.{cm, axes, colors}
+import matplotlib.figure
 
 
 #%% Plotting functions
 
-def plot_confusion_matrix(
-        matrix: np.ndarray,
-        classes: Sequence[str],
-        normalize: bool = False,
-        title: str = 'Confusion matrix',
-        cmap: Union[str, matplotlib.colors.Colormap] = matplotlib.cm.Blues,
-        vmax: Optional[float] = None,
-        use_colorbar: bool = True,
-        y_label: bool = True,        
-        fmt: str = '{:.0f}',
-        fig=None
-        ) -> matplotlib.figure.Figure:
+def plot_confusion_matrix(matrix,
+                          classes,
+                          normalize=False,
+                          title='Confusion matrix',
+                          cmap=matplotlib.cm.Blues,
+                          vmax=None,
+                          use_colorbar=True,
+                          y_label=True,
+                          fmt= '{:.0f}',
+                          fig=None):
     """
-    Plot a confusion matrix. By default, assumes values in the given matrix
-    are percentages. If the matrix contains counts, normalization can be applied
-    by setting `normalize=True`.
+    Plots a confusion matrix.
 
     Args:
-        matrix: np.ndarray, shape [num_classes, num_classes], confusion matrix
-            where rows are ground-truth classes and cols are predicted classes.
-        classes: list of str, class names for each row/column
-        normalize: bool, whether to perform row-wise normalization to sum 1
-        title: str, figure title
-        cmap: colormap, default: matplotlib.cm.Blues
-        vmax: float, value corresponding s to the largest value of the colormap.
-            If None, the maximum value in *matrix* will be used. Default: None
-        use_colorbar: bool, whether to show colorbar
-        y_label: bool, whether to show class names on the y-axis
-        fmt: str, format string
-
-    Returns: matplotlib.figure.Figure, a reference to the figure
+        matrix (np.ndarray): shape [num_classes, num_classes], confusion matrix
+            where rows are ground-truth classes and columns are predicted classes
+        classes (list of str): class names for each row/column
+        normalize (bool, optional): whether to perform row-wise normalization; 
+            by default, assumes values in the confusion matrix are percentages
+        title (str, optional): figure title
+        cmap (matplotlib.colors.colormap): colormap for cell backgrounds
+        vmax (float, optional), value corresponding to the largest value of the colormap;
+            if None, the maximum value in [matrix] will be used
+        use_colorbar (bool, optional): whether to show colorbar
+        y_label (bool, optional): whether to show class names on the y axis
+        fmt (str): format string for rendering numeric values
+        fig (Figure): existing figure to which we should render, otherwise creates
+            a new figure
+        
+    Returns: 
+        matplotlib.figure.Figure: the figure we rendered to or created
     """
     
     num_classes = matrix.shape[0]
     assert matrix.shape[1] == num_classes
     assert len(classes) == num_classes
 
     normalized_matrix = matrix.astype(np.float64) / (
@@ -93,32 +90,37 @@
         ax.text(j, i, fmt.format(v),
                 horizontalalignment='center',
                 verticalalignment='center',
                 color='white' if normalized_matrix[i, j] > 0.5 else 'black')
 
     return fig
 
+# ...def plot_confusion_matrix(...)
 
-def plot_precision_recall_curve(
-        precisions: Sequence[float], recalls: Sequence[float],
-        title: str = 'Precision/recall curve',
-        xlim=(0.0,1.05),ylim=(0.0,1.05)
-        ) -> matplotlib.figure.Figure:
+
+def plot_precision_recall_curve(precisions, 
+                                recalls, 
+                                title='Precision/recall curve',
+                                xlim=(0.0,1.05),
+                                ylim=(0.0,1.05)):
     """
-    Plots the precision recall curve given lists of (ordered) precision
+    Plots a precision/recall curve given lists of (ordered) precision
     and recall values.
 
     Args:
-        precisions: list of float, precision for corresponding recall values,
-            should have same length as *recalls*.
-        recalls: list of float, recall for corresponding precision values,
-            should have same length as *precisions*.
-        title: str, plot title
+        precisions (list of float): precision for corresponding recall values,
+            should have same length as [recalls].
+        recalls (list of float): recall for corresponding precision values,
+            should have same length as [precisions].
+        title (str, optional): plot title
+        xlim (tuple, optional): x-axis limits as a length-2 tuple
+        ylim (tuple, optional): y-axis limits as a length-2 tuple
 
-    Returns: matplotlib.figure.Figure, reference to the figure
+    Returns: 
+        matplotlib.figure.Figure: the (new) figure
     """
     
     assert len(precisions) == len(recalls)
 
     fig = matplotlib.figure.Figure(tight_layout=True)
     ax = fig.subplots(1, 1)
     ax.step(recalls, precisions, color='b', alpha=0.2, where='post')
@@ -134,35 +136,32 @@
         ax.set_title(title)
         ax.set_xlim(xlim[0],xlim[1])
         ax.set_ylim(ylim[0],ylim[1])
         
     return fig
 
 
-def plot_stacked_bar_chart(data: np.ndarray,
-                           series_labels: Sequence[str],
-                           col_labels: Optional[Sequence[str]] = None,
-                           x_label: Optional[str] = None,
-                           y_label: Optional[str] = None,
-                           log_scale: bool = False
-                           ) -> matplotlib.figure.Figure:
+def plot_stacked_bar_chart(data, series_labels=None, col_labels=None,
+                           x_label=None, y_label=None, log_scale=False):
     """
-    For plotting e.g. species distribution across locations.
+    Plot a stacked bar chart, for plotting e.g. species distribution across locations.
+    
     Reference: https://stackoverflow.com/q/44309507
 
     Args:
-        data: 2-D np.ndarray or nested list, rows (series) are species, columns
+        data (np.ndarray or list of list): data to plot; rows (series) are species, columns
             are locations
-        series_labels: list of str, e.g., species names
-        col_labels: list of str, e.g., location names
-        x_label: str
-        y_label: str
-        log_scale: bool, whether to plot y-axis in log-scale
+        series_labels (list of str, optional): series labels, typically species names
+        col_labels (list of str, optional): column labels, typically location names
+        x_label (str, optional): x-axis label
+        y_label (str, optional): y-axis label
+        log_scale (bool, optional) whether to plot the y axis in log-scale
 
-    Returns: matplotlib.figure.Figure, reference to figure
+    Returns: 
+        matplotlib.figure.Figure: the (new) figure
     """
     
     data = np.asarray(data)
     num_series, num_columns = data.shape
     ind = np.arange(num_columns)
 
     fig = matplotlib.figure.Figure(tight_layout=True)
@@ -196,36 +195,37 @@
 
     # Put a legend to the right of the current axis
     ax.legend(loc='center left', bbox_to_anchor=(0.99, 0.5), frameon=False)
 
     return fig
 
 
-def calibration_ece(true_scores: Sequence[int], pred_scores: Sequence[float],
-                    num_bins: int) -> Tuple[np.ndarray, np.ndarray, float]:
-    """
+def calibration_ece(true_scores, pred_scores, num_bins):
+    r"""
     Expected calibration error (ECE) as defined in equation (3) of
-        Guo et al. "On Calibration of Modern Neural Networks." (2017).
+    Guo et al. "On Calibration of Modern Neural Networks." (2017).
 
     Implementation modified from sklearn.calibration.calibration_curve()
-    in order to implement ECE calculation. See
-        https://github.com/scikit-learn/scikit-learn/issues/18268
+    in order to implement ECE calculation. See:
+    
+    https://github.com/scikit-learn/scikit-learn/issues/18268
 
     Args:
-        pred_scores: list of float, length N, pred_scores[i] is the predicted
-            confidence that example i is positive
-        true_scores: list of int, length N, binary-valued (0 = neg, 1 = pos)
-        num_bins: int, number of bins to use (`M` in eq. (3) of Guo 2017)
+        true_scores (list of int): true values, length N, binary-valued (0 = neg, 1 = pos)
+        pred_scores (list of float): predicted confidence values, length N, pred_scores[i] is the 
+            predicted confidence that example i is positive
+        num_bins (int): number of bins to use (`M` in eq. (3) of Guo 2017)
 
     Returns:
-        accs: np.ndarray, shape [M], type float64, accuracy in each bin,
-            M <= num_bins because bins with no samples are not returned
-        confs: np.ndarray, shape [M], type float64, mean model confidence in
-            each bin
-        ece: float, expected calibration error
+        tuple: a length-three tuple containing:
+            - accs: np.ndarray, shape [M], type float64, accuracy in each bin,
+              M <= num_bins because bins with no samples are not returned        
+            - confs: np.ndarray, shape [M], type float64, mean model confidence in
+              each bin
+            - ece: float, expected calibration error
     """
     
     assert len(true_scores) == len(pred_scores)
 
     bins = np.linspace(0., 1. + 1e-8, num=num_bins + 1)
     binids = np.digitize(pred_scores, bins) - 1
 
@@ -238,40 +238,33 @@
     confs = bin_sums[nonzero] / bin_total[nonzero]
 
     weights = bin_total[nonzero] / len(true_scores)
     ece = np.abs(accs - confs) @ weights
     return accs, confs, ece
 
 
-def plot_calibration_curve(true_scores: Sequence[int],
-                           pred_scores: Sequence[float],
-                           num_bins: int,
-                           name: str = 'calibration',
-                           plot_perf: bool = True,
-                           plot_hist: bool = True,
-                           ax: Optional[matplotlib.axes.Axes] = None,
-                           **fig_kwargs: Any
-                           ) -> matplotlib.figure.Figure:
+def plot_calibration_curve(true_scores, pred_scores, num_bins,
+                           name='calibration', plot_perf=True, plot_hist=True,
+                           ax=None, **fig_kwargs):
     """
-    Plot a calibration curve.
-
-    Consider rewriting / removing this function if
-        https://github.com/scikit-learn/scikit-learn/pull/17443
-    is merged into an actual scikit-learn release.
+    Plots a calibration curve.
 
     Args:
-        see calibration_ece() for args
-        name: str, label in legend for the calibration curve
-        plot_perf: bool, whether to plot y=x line indicating perfect calibration
-        plot_hist: bool, whether to plot histogram of counts
-        ax: optional matplotlib Axes, if given then no legend is drawn, and
-            fig_kwargs are ignored
-        fig_kwargs: only used if ax is None
+        true_scores (list of int): true values, length N, binary-valued (0 = neg, 1 = pos)
+        pred_scores (list of float): predicted confidence values, length N, pred_scores[i] is the 
+            predicted confidence that example i is positive
+        num_bins (int): number of bins to use (`M` in eq. (3) of Guo 2017)
+        name (str, optional): label in legend for the calibration curve
+        plot_perf (bool, optional): whether to plot y=x line indicating perfect calibration
+        plot_hist (bool, optional): whether to plot histogram of counts
+        ax (Axes, optional): if given then no legend is drawn, and fig_kwargs are ignored
+        fig_kwargs (dict, optional): only used if [ax] is None
 
-    Returns: matplotlib Figure
+    Returns:
+        matplotlib.figure.Figure: the (new) figure
     """
     
     accs, confs, ece = calibration_ece(true_scores, pred_scores, num_bins)
 
     created_fig = False
     if ax is None:
         created_fig = True
```

### Comparing `megadetector-5.0.8/md_visualization/render_images_with_thumbnails.py` & `megadetector-5.0.9/md_visualization/render_images_with_thumbnails.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,49 +1,53 @@
-########
-# 
-# render_images_with_thumbnails.py
-# 
-# Render an output image with one primary and crops from many secondary images,
-# used to check whether suspicious detections are actually false positives or not.
-#
-########
+"""
 
-#%% Constants
+render_images_with_thumbnails.py
+
+Renders an output image with one primary image and crops from many secondary images,
+used primarily to check whether candidate repeat detections are actually false positives or not.
+
+"""
+
+#%% Imports
 
 import math
 import os
 import random
 
-from md_visualization import visualization_utils as vis_utils
 from PIL import Image
 
+from md_visualization import visualization_utils as vis_utils
+from md_utils import path_utils
+
 
 #%% Support functions
 
 def crop_image_with_normalized_coordinates(
         image,
         bounding_box):
     """
     Args:
-        image: image to crop
-        bounding_box: tuple formatted as (x,y,w,h), where (0,0) is the
-        upper-left of the image, and coordinates are normalized
-        (so (0,0,1,1) is a box containing the entire image).
+        image (PIL.Image): image to crop
+        bounding_box (tuple): tuple formatted as (x,y,w,h), where (0,0) is the
+            upper-left of the image, and coordinates are normalized
+            (so (0,0,1,1) is a box containing the entire image).
+            
+    Returns:
+        PIL.Image: cropped image
     """
     
     im_width, im_height = image.size
     (x_norm, y_norm, w_norm, h_norm) = bounding_box
     (x, y, w, h) = (x_norm * im_width,
                     y_norm * im_height,
                     w_norm * im_width,
                     h_norm * im_height)
     return image.crop((x, y, x+w, y+h))
 
 
-
 #%% Main function
 
 def render_images_with_thumbnails(
         primary_image_filename,
         primary_image_width,
         secondary_image_filename_list,
         secondary_image_bounding_box_list,
@@ -60,28 +64,27 @@
     The output file will be primary_image_width + cropped_grid_width pixels
     wide.
 
     The height of the output image will be determined by the original aspect
     ratio of the primary image. 
     
     Args:
-        primary_image_filename: filename of the primary image to load as str
-        primary_image_width: width at which to render the primary image; if this is 
-            None, will render at the original image width.
-        secondary_image_filename_list: list of strs that are the filenames of
-            the secondary images.
-        secondary_image_bounding_box_list: list of tuples, one per secondary
+        primary_image_filename (str): filename of the primary image to load as str
+        primary_image_width (int): width at which to render the primary image; if this is 
+            None, will render at the original image width
+        secondary_image_filename_list (list): list of filenames of the secondary images
+        secondary_image_bounding_box_list (list): list of tuples, one per secondary
             image. Each tuple is a bounding box of the secondary image,
             formatted as (x,y,w,h), where (0,0) is the upper-left of the image,
             and coordinates are normalized (so (0,0,1,1) is a box containing
             the entire image.
-        cropped_grid_width: width of all the cropped images
-        output_image_filename: str of the filename to write the output image        
-        primary_image_location: 'right' or left'; reserving 'top', 'bottom', etc.
-            for future use.
+        cropped_grid_width (int): width of the cropped-image area
+        output_image_filename (str): filename to write the output image        
+        primary_image_location (str, optional): 'right' or left'; reserving 'top', 'bottom', etc.
+            for future use
     """
 
     # Check to make sure the arguments are reasonable
     assert(len(secondary_image_filename_list) ==
            len(secondary_image_bounding_box_list)), \
            'Length of secondary image list and bounding box list should be equal'
 
@@ -179,14 +182,16 @@
             i_row += 1
             
     # ...for each crop
 
     # Write output image to disk
     output_image.save(output_image_filename)    
 
+# ...def render_images_with_thumbnails(...)
+
 
 #%% Interactive driver
 
 if False:
     
     pass
 
@@ -212,26 +217,23 @@
             primary_image_width,
             secondary_image_filename_list,
             secondary_image_bounding_box_list,
             cropped_grid_width,
             output_image_filename,
             primary_image_location='right')    
 
-    from md_utils import path_utils
     path_utils.open_file(output_image_filename)
     
     
 #%% Command-line driver
 
+# This is just a test driver, this module is not meant to be run from the command line.
+
 def main():
-    
-    #%%
-    
-    from md_utils import path_utils
-    
+        
     # Load images from a test directory.
     #
     # Make the first image in the directory the primary image, 
     # the remaining ones the comparison images.    
     test_input_folder = os.path.expanduser('~/data/KRU-test')
     output_image_filename = os.path.expanduser('~/tmp/thumbnail_test.jpg')
     
@@ -264,12 +266,10 @@
         secondary_image_filename_list,
         secondary_image_bounding_box_list,
         cropped_grid_width,
         output_image_filename, 'right')
     
     from md_utils import path_utils
     path_utils.open_file(output_image_filename)
-    
-    #%%
-    
+        
 if __name__ == '__main__':
     main()
```

### Comparing `megadetector-5.0.8/md_visualization/visualization_utils.py` & `megadetector-5.0.9/md_visualization/visualization_utils.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-########
-# 
-# visualization_utils.py
-# 
-# Core rendering functions shared across visualization scripts
-#
-########
+"""
+
+visualization_utils.py
+
+Rendering functions shared across visualization scripts
+
+"""
 
 #%% Constants and imports
 
 import time
 import numpy as np
 import requests
 import os
+import cv2
 
 from io import BytesIO
-from typing import Union
 from PIL import Image, ImageFile, ImageFont, ImageDraw
 from multiprocessing.pool import ThreadPool
 from multiprocessing.pool import Pool
 from tqdm import tqdm
 from functools import partial
 
 from md_utils.path_utils import find_images
@@ -43,41 +43,74 @@
 TEXTALIGN_RIGHT = 1
 
 # Convert category ID from int to str
 DEFAULT_DETECTOR_LABEL_MAP = {
     str(k): v for k, v in detector_bbox_category_id_to_name.items()
 }
 
-# Retry on blob storage read failures
+# Constants controlling retry behavior when fetching images from URLs
 n_retries = 10
 retry_sleep_time = 0.01
+
+# If we try to open an image from a URL, and we encounter any error in this list,
+# we'll retry, otherwise it's just an error.
 error_names_for_retry = ['ConnectionError']
 
 DEFAULT_BOX_THICKNESS = 4
 DEFAULT_LABEL_FONT_SIZE = 16
 
+# Default color map for mapping integer category IDs to colors when rendering bounding
+# boxes
+DEFAULT_COLORS = [
+    'AliceBlue', 'Red', 'RoyalBlue', 'Gold', 'Chartreuse', 'Aqua', 'Azure',
+    'Beige', 'Bisque', 'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue',
+    'AntiqueWhite', 'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson',
+    'Cyan', 'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',
+    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',
+    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',
+    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'GoldenRod',
+    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',
+    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',
+    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',
+    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',
+    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',
+    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',
+    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',
+    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',
+    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',
+    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',
+    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',
+    'RosyBrown', 'Aquamarine', 'SaddleBrown', 'Green', 'SandyBrown',
+    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',
+    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',
+    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',
+    'WhiteSmoke', 'Yellow', 'YellowGreen'
+]
+
 
 #%% Functions
 
-def open_image(input_file: Union[str, BytesIO], ignore_exif_rotation=False) -> Image:
+def open_image(input_file, ignore_exif_rotation=False):
     """
     Opens an image in binary format using PIL.Image and converts to RGB mode.
     
     Supports local files or URLs.
 
     This operation is lazy; image will not be actually loaded until the first
     operation that needs to load it (for example, resizing), so file opening
     errors can show up later.  load_image() is the non-lazy version of this function.
 
     Args:
-        input_file: str or BytesIO, either a path to an image file (anything
-            that PIL can open), or an image as a stream of bytes
+        input_file (str or BytesIO): can be a path to an image file (anything
+            that PIL can open), a URL, or an image as a stream of bytes
+        ignore_exif_rotation (bool, optional): don't rotate the loaded pixels,
+            even if we are loading a JPEG and that JPEG says it should be rotated
 
     Returns:
-        A PIL image object in RGB mode
+        PIL.Image.Image: A PIL Image object in RGB mode
     """
     
     if (isinstance(input_file, str)
             and input_file.startswith(('http://', 'https://'))):
         try:
             response = requests.get(input_file)
         except Exception as e:
@@ -100,14 +133,16 @@
             image = Image.open(BytesIO(response.content))
         except Exception as e:
             print(f'Error opening image {input_file}: {e}')
             raise
 
     else:
         image = Image.open(input_file)
+    
+    # Convert to RGB if necessary
     if image.mode not in ('RGBA', 'RGB', 'L', 'I;16'):
         raise AttributeError(
             f'Image {input_file} uses unsupported mode {image.mode}')
     if image.mode == 'RGBA' or image.mode == 'L':
         # PIL.Image.convert() returns a converted copy of this image
         image = image.convert(mode='RGB')
 
@@ -130,29 +165,32 @@
     return image
 
 # ...def open_image(...)
 
 
 def exif_preserving_save(pil_image,output_file,quality='keep',default_quality=85,verbose=False):
     """
-    Save [pil_image] to [output_file], making a moderate attempt to preserve EXIF
+    Saves [pil_image] to [output_file], making a moderate attempt to preserve EXIF
     data and JPEG quality.  Neither is guaranteed.
     
     Also see:
     
     https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/
      
     ...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.
 
-    The "quality" parameter should be "keep" (default), or an integer from 0 to 100. 
-    This is only used if PIL thinks the the source image is a JPEG.  If you load a JPEG
-    and resize it in memory, for example, it's no longer a JPEG.
-    
-    'default_quality' is used when quality == 'keep' and we are saving a non-JPEG source.
-    'keep' is only supported for JPEG sources.
+    Args:
+        pil_image (Image): the PIL Image objct to save
+        output_file (str): the destination file
+        quality (str or int, optional): can be "keep" (default), or an integer from 0 to 100. 
+            This is only used if PIL thinks the the source image is a JPEG.  If you load a JPEG
+            and resize it in memory, for example, it's no longer a JPEG.
+        default_quality (int, optional): determines output quality when quality == 'keep' and we are 
+            saving a non-JPEG source to a JPEG file
+        verbose (bool, optional): enable additional debug console output
     """
     
     # Read EXIF metadata
     exif = pil_image.info['exif'] if ('exif' in pil_image.info) else None
     
     # Quality preservation is only supported for JPEG sources.
     if pil_image.format != "JPEG":
@@ -181,50 +219,63 @@
             pil_image.save(output_file, exif=exif)            
         else:
             pil_image.save(output_file)
             
 # ...def exif_preserving_save(...)
 
 
-def load_image(input_file: Union[str, BytesIO], ignore_exif_rotation=False) -> Image:
+def load_image(input_file, ignore_exif_rotation=False):
     """
-    Loads the image at input_file as a PIL Image into memory.
-
-    Image.open() used in open_image() is lazy and errors will occur downstream
-    if not explicitly loaded.
-
+    Loads an image file.  This is the non-lazy version of open_file(); i.e., 
+    it forces image decoding before returning.
+    
     Args:
-        input_file: str or BytesIO, either a path to an image file (anything
-            that PIL can open), or an image as a stream of bytes
+        input_file (str or BytesIO): can be a path to an image file (anything
+            that PIL can open), a URL, or an image as a stream of bytes
+        ignore_exif_rotation (bool, optional): don't rotate the loaded pixels,
+            even if we are loading a JPEG and that JPEG says it should be rotated
 
-    Returns: PIL.Image.Image, in RGB mode
+    Returns: 
+        PIL.Image.Image: a PIL Image object in RGB mode
     """
     
     image = open_image(input_file, ignore_exif_rotation=ignore_exif_rotation)
     image.load()
     return image
 
 
-def resize_image(image, target_width, target_height=-1, output_file=None,
+def resize_image(image, target_width=-1, target_height=-1, output_file=None,
                  no_enlarge_width=False, verbose=False, quality='keep'):
     """
-    Resizes a PIL image object to the specified width and height; does not resize
+    Resizes a PIL Image object to the specified width and height; does not resize
     in place. If either width or height are -1, resizes with aspect ratio preservation.
     
-    None is equivalent to -1 for target_width and target_height.
-    
-    [image] can be a PIL image or a filename.
-    
     If target_width and target_height are both -1, does not modify the image, but 
     will write to output_file if supplied.
     
-    If no_enlarge_width is True, and the target width is larger than the original
-    image width, does not modify the image, but will write to output_file if supplied.
+    If no resizing is required, and an Image object is supplied, returns the original Image 
+    object (i.e., does not copy).
     
-    'quality' is passed to exif_preserving_save, see docs there.
+    Args:
+        image (Image or str): PIL Image object or a filename (local file or URL)
+        target_width (int, optional): width to which we should resize this image, or -1
+            to let target_height determine the size
+        target_height (int, optional): height to which we should resize this image, or -1
+            to let target_width determine the size
+        output_file (str, optional): file to which we should save this image; if None,
+            just returns the image without saving
+        no_enlarge_width (bool, optional): if [no_enlarge_width] is True, and 
+            [target width] is larger than the original image width, does not modify the image, 
+            but will write to output_file if supplied
+        verbose (bool, optional): enable additional debug output
+        quality (str or int, optional): passed to exif_preserving_save, see docs for more detail
+        
+    returns:
+        PIL.Image.Image: the resized image, which may be the original image if no resizing is 
+            required
     """
 
     image_fn = 'in_memory'
     if isinstance(image,str):
         image_fn = image
         image = load_image(image)
         
@@ -291,50 +342,30 @@
         exif_preserving_save(resized_image,output_file,quality=quality,verbose=verbose)
         
     return resized_image
 
 # ...def resize_image(...)
 
 
-DEFAULT_COLORS = [
-    'AliceBlue', 'Red', 'RoyalBlue', 'Gold', 'Chartreuse', 'Aqua', 'Azure',
-    'Beige', 'Bisque', 'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue',
-    'AntiqueWhite', 'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson',
-    'Cyan', 'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',
-    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',
-    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',
-    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'GoldenRod',
-    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',
-    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',
-    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',
-    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',
-    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',
-    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',
-    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',
-    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',
-    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',
-    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',
-    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',
-    'RosyBrown', 'Aquamarine', 'SaddleBrown', 'Green', 'SandyBrown',
-    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',
-    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',
-    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',
-    'WhiteSmoke', 'Yellow', 'YellowGreen'
-]
-
-
 def crop_image(detections, image, confidence_threshold=0.15, expansion=0):
     """
-    Crops detections above *confidence_threshold* from the PIL image *image*,
-    returning a list of PIL images.
-
-    *detections* should be a list of dictionaries with keys 'conf' and 'bbox';
-    see bbox format description below.  Normalized, [x,y,w,h], upper-left-origin.
+    Crops detections above [confidence_threshold] from the PIL image [image],
+    returning a list of PIL Images.
 
-    *expansion* specifies a number of pixels to include on each side of the box.
+    Args:
+        detections (list): a list of dictionaries with keys 'conf' and 'bbox';
+            boxes are length-four arrays formatted as [x,y,w,h], normalized, 
+            upper-left origin (this is the standard MD detection format)
+        image (Image): the PIL Image object from which we should crop detections
+        confidence_threshold (float, optional): only crop detections above this threshold
+        expansion (int, optional): a number of pixels to include on each side of a cropped
+            detection
+        
+    Returns:
+        list: a possibly-empty list of PIL Image objects    
     """
 
     ret_images = []
 
     for detection in detections:
 
         score = float(detection['conf'])
@@ -368,99 +399,120 @@
         # ...if this detection is above threshold
 
     # ...for each detection
 
     return ret_images
 
 
-def render_detection_bounding_boxes(detections, image,
-                                    label_map={}, 
+def render_detection_bounding_boxes(detections, 
+                                    image,
+                                    label_map='show_categories',
                                     classification_label_map=None, 
-                                    confidence_threshold=0.15, thickness=DEFAULT_BOX_THICKNESS, expansion=0,
+                                    confidence_threshold=0.15, 
+                                    thickness=DEFAULT_BOX_THICKNESS, 
+                                    expansion=0,
                                     classification_confidence_threshold=0.3,
                                     max_classifications=3,
-                                    colormap=DEFAULT_COLORS,
+                                    colormap=None,
                                     textalign=TEXTALIGN_LEFT,
                                     label_font_size=DEFAULT_LABEL_FONT_SIZE,
                                     custom_strings=None):
     """
-    Renders bounding boxes, label, and confidence on an image if confidence is above the threshold.
-
-    Boxes are in the format that's output from the batch processing API.
-
+    Renders bounding boxes (with labels and confidence values) on an image for all
+    detections above a threshold.
+    
     Renders classification labels if present.
+    
+    [image] is modified in place.
 
     Args:
 
-        detections: detections on the image, example content:
-            [
-                {
-                    "category": "2",
-                    "conf": 0.996,
-                    "bbox": [
-                        0.0,
-                        0.2762,
-                        0.1234,
-                        0.2458
-                    ]
-                }
-            ]
-
-            ...where the bbox coordinates are [x, y, box_width, box_height].
-
-            (0, 0) is the upper-left.  Coordinates are normalized.
-
-            Supports classification results, if *detections* has the format
-            [
-                {
-                    "category": "2",
-                    "conf": 0.996,
-                    "bbox": [
-                        0.0,
-                        0.2762,
-                        0.1234,
-                        0.2458
-                    ]
-                    "classifications": [
-                        ["3", 0.901],
-                        ["1", 0.071],
-                        ["4", 0.025]
-                    ]
-                }
-            ]
+        detections (list): list of detections in the MD output format, for example:
+            
+            .. code-block::none
+            
+                [
+                    {
+                        "category": "2",
+                        "conf": 0.996,
+                        "bbox": [
+                            0.0,
+                            0.2762,
+                            0.1234,
+                            0.2458
+                        ]
+                    }
+                ]
+    
+                ...where the bbox coordinates are [x, y, box_width, box_height].
+    
+                (0, 0) is the upper-left.  Coordinates are normalized.
+    
+            Supports classification results, in the standard format:
+            
+            .. code-block::none
+            
+                [
+                    {
+                        "category": "2",
+                        "conf": 0.996,
+                        "bbox": [
+                            0.0,
+                            0.2762,
+                            0.1234,
+                            0.2458
+                        ]
+                        "classifications": [
+                            ["3", 0.901],
+                            ["1", 0.071],
+                            ["4", 0.025]
+                        ]
+                    }
+                ]
 
-        image: PIL.Image object
+        image (PIL.Image.Image): image on which we should render detections
 
-        label_map: optional, mapping the numerical label to a string name. The type of the numerical label
-            (default string) needs to be consistent with the keys in label_map; no casting is carried out.
-            If this is None, no labels are shown (not even numbers and confidence values).  If you want
-            category numbers and confidence values without class labels, use {}.
+        label_map (dict, optional): optional, mapping the numeric label to a string name. The type of the 
+            numeric label (typically strings) needs to be consistent with the keys in label_map; no casting is 
+            carried out. If [label_map] is None, no labels are shown (not even numbers and confidence values).  
+            If you want category numbers and confidence values without class labels, use the default value, 
+            the string 'show_categories'.
 
-        classification_label_map: optional, mapping of the string class labels to the actual class names.
-            The type of the numerical label (default string) needs to be consistent with the keys in
-            label_map; no casting is carried out.  If this is None, no classification labels are shown.
+        classification_label_map (dict, optional): optional, mapping of the string class labels to the actual 
+            class names. The type of the numeric label (typically strings) needs to be consistent with the keys 
+            in label_map; no casting is  carried out. If [label_map] is None, no labels are shown (not even numbers 
+            and confidence values).
 
-        confidence_threshold: optional, threshold above which boxes are rendered.  Can also be a dictionary
-        mapping category IDs to thresholds.
+        confidence_threshold (float or dict, optional), threshold above which boxes are rendered.  Can also be a 
+            dictionary mapping category IDs to thresholds.
+        
+        thickness (int, optional): line thickness in pixels
+        
+        expansion (int, optional): number of pixels to expand bounding boxes on each side
+        
+        classification_confidence_threshold (float, optional): confidence above which classification results 
+            are displayed
         
-        thickness: line thickness in pixels. Default value is 4.
+        max_classifications (int, optional): maximum number of classification results rendered for one image
         
-        expansion: number of pixels to expand bounding boxes on each side.  Default is 0.
+        colormap (list, optional): list of color names, used to choose colors for categories by
+            indexing with the values in [classes]; defaults to a reasonable set of colors
         
-        classification_confidence_threshold: confidence above which classification result is retained.
+        textalign (int, optional): TEXTALIGN_LEFT or TEXTALIGN_RIGHT
         
-        max_classifications: maximum number of classification results retained for one image.
+        label_font_size (float, optional): font size for labels
         
         custom_strings: optional set of strings to append to detection labels, should have the
-        same length as [detections].  Appended before classification labels, if classification
-        data is provided.
-
-    image is modified in place.
+            same length as [detections].  Appended before any classification labels.
     """
 
+    # Input validation
+    if (label_map is not None) and (isinstance(label_map,str)) and (label_map == 'show_categories'):
+        label_map = {}
+        
     if custom_strings is not None:
         assert len(custom_strings) == len(detections), \
             '{} custom strings provided for {} detections'.format(
                 len(custom_strings),len(detections))
             
     display_boxes = []
     
@@ -473,27 +525,26 @@
     for i_detection,detection in enumerate(detections):
 
         score = detection['conf']
         
         if isinstance(confidence_threshold,dict):
             rendering_threshold = confidence_threshold[detection['category']]
         else:
-            rendering_threshold = confidence_threshold        
-            
+            rendering_threshold = confidence_threshold            
             
         # Always render objects with a confidence of "None", this is typically used
         # for ground truth data.        
         if score is None or score >= rendering_threshold:
             
             x1, y1, w_box, h_box = detection['bbox']
             display_boxes.append([y1, x1, y1 + h_box, x1 + w_box])
             clss = detection['category']
             
             # {} is the default, which means "show labels with no mapping", so don't use "if label_map" here
-            # if label_map:
+            # if label_map:                
             if label_map is not None:
                 label = label_map[clss] if clss in label_map else clss
                 if score is not None:
                     displayed_label = ['{}: {}%'.format(label, round(100 * score))]
                 else:
                     displayed_label = ['{}'.format(label)]
             else:
@@ -556,33 +607,38 @@
 
 def draw_bounding_boxes_on_image(image,
                                  boxes,
                                  classes,
                                  thickness=DEFAULT_BOX_THICKNESS,
                                  expansion=0,
                                  display_strs=None,
-                                 colormap=DEFAULT_COLORS,
+                                 colormap=None,
                                  textalign=TEXTALIGN_LEFT,
                                  label_font_size=DEFAULT_LABEL_FONT_SIZE):
     """
-    Draws bounding boxes on an image.
+    Draws bounding boxes on an image.  Modifies the image in place.
 
     Args:
-      image: a PIL.Image object.
-      boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).
-             The coordinates are in normalized format between [0, 1].
-      classes: a list of ints or strings (that can be cast to ints) corresponding to the
-               class labels of the boxes. This is only used for color selection.
-      thickness: line thickness in pixels. Default value is 4.
-      expansion: number of pixels to expand bounding boxes on each side.  Default is 0.
-      display_strs: list of list of strings.
-                             a list of strings for each bounding box.
-                             The reason to pass a list of strings for a
-                             bounding box is that it might contain
-                             multiple labels.
+        
+        image (PIL.Image): the image on which we should draw boxes
+        boxes (np.array): a two-dimensional numpy array of size [N, 4], where N is the 
+            number of boxes, and each row is (ymin, xmin, ymax, xmax).  Coordinates should be
+            normalized to image height/width.
+        classes (list): a list of ints or string-formatted ints corresponding to the
+             class labels of the boxes. This is only used for color selection.  Should have the same 
+             length as [boxes].
+        thickness (int, optional): line thickness in pixels
+        expansion (int, optional): number of pixels to expand bounding boxes on each side
+        display_strs (list, optional): list of list of strings (the outer list should have the
+            same length as [boxes]).  Typically this is used to show (possibly multiple) detection
+            or classification categories and/or confidence values.
+        colormap (list, optional): list of color names, used to choose colors for categories by
+            indexing with the values in [classes]; defaults to a reasonable set of colors
+        textalign (int, optional): TEXTALIGN_LEFT or TEXTALIGN_RIGHT
+        label_font_size (float, optional): font size for labels
     """
 
     boxes_shape = boxes.shape
     if not boxes_shape:
         return
     if len(boxes_shape) != 2 or boxes_shape[1] != 4:
         # print('Input must be of size [N, 4], but is ' + str(boxes_shape))
@@ -606,52 +662,60 @@
                                ymin,
                                xmin,
                                ymax,
                                xmax,
                                clss=None,
                                thickness=DEFAULT_BOX_THICKNESS,
                                expansion=0,
-                               display_str_list=(),
+                               display_str_list=None,
                                use_normalized_coordinates=True,
                                label_font_size=DEFAULT_LABEL_FONT_SIZE,
-                               colormap=DEFAULT_COLORS,
+                               colormap=None,
                                textalign=TEXTALIGN_LEFT):
     """
-    Adds a bounding box to an image.
+    Adds a bounding box to an image.  Modifies the image in place.
 
     Bounding box coordinates can be specified in either absolute (pixel) or
     normalized coordinates by setting the use_normalized_coordinates argument.
 
     Each string in display_str_list is displayed on a separate line above the
     bounding box in black text on a rectangle filled with the input 'color'.
     If the top of the bounding box extends to the edge of the image, the strings
     are displayed below the bounding box.
 
-    Args:
-    image: a PIL.Image object.
-    ymin: ymin of bounding box - upper left.
-    xmin: xmin of bounding box.
-    ymax: ymax of bounding box.
-    xmax: xmax of bounding box.    
-    clss: str, the class of the object in this bounding box; should be either an integer
-        or a string-formatted integer.
-    thickness: line thickness. Default value is 4.
-    expansion: number of pixels to expand bounding boxes on each side.  Default is 0.
-    display_str_list: list of strings to display in box
-        (each to be shown on its own line).
-        use_normalized_coordinates: If True (default), treat coordinates
-        ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat
-        coordinates as absolute.
-    label_font_size: font size 
-    
     Adapted from:
         
     https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py
+    
+    Args:
+        image (PIL.Image.Image): the image on which we should draw a box
+        ymin (float): ymin of bounding box
+        xmin (float): xmin of bounding box
+        ymax (float): ymax of bounding box
+        xmax (float): xmax of bounding box
+        clss (int, optional): the class index of the object in this bounding box, used for choosing
+            a color; should be either an integer or a string-formatted integer
+        thickness (int, optional): line thickness in pixels
+        expansion (int, optional): number of pixels to expand bounding boxes on each side
+        display_str_list (list, optional): list of strings to display above the box (each to be shown on its 
+            own line)
+        use_normalized_coordinates (bool, optional): if True (default), treat coordinates 
+            ymin, xmin, ymax, xmax as relative to the image, otherwise coordinates as absolute pixel values
+        label_font_size (float, optional): font size 
+        colormap (list, optional): list of color names, used to choose colors for categories by
+            indexing with the values in [classes]; defaults to a reasonable set of colors
+        textalign (int, optional): TEXTALIGN_LEFT or TEXTALIGN_RIGHT        
     """
     
+    if colormap is None:
+        colormap = DEFAULT_COLORS
+        
+    if display_str_list is None:
+        display_str_list = []
+        
     if clss is None:
         # Default to the MegaDetector animal class ID (1)
         color = colormap[1]
     else:
         color = colormap[int(clss) % len(colormap)]
 
     draw = ImageDraw.Draw(image)
@@ -754,69 +818,37 @@
             font=font)
 
         text_bottom -= (text_height + 2 * margin)
 
 # ...def draw_bounding_box_on_image(...)
 
 
-def render_iMerit_boxes(boxes, classes, image,
-                        label_map=annotation_constants.annotation_bbox_category_id_to_name):
-    """
-    Renders bounding boxes and their category labels on a PIL image.
-
-    Args:
-        boxes: bounding box annotations from iMerit, format is:
-            [x_rel, y_rel, w_rel, h_rel] (rel = relative coords)
-        classes: the class IDs of the predicted class of each box/object
-        image: PIL.Image object to annotate on
-        label_map: optional dict mapping classes to a string for display
-
-    Returns:
-        image will be altered in place
-    """
-
-    display_boxes = []
-    
-    # list of lists, one list of strings for each bounding box (to accommodate multiple labels)
-    display_strs = []  
-    
-    for box, clss in zip(boxes, classes):
-        if len(box) == 0:
-            assert clss == 5
-            continue
-        x_rel, y_rel, w_rel, h_rel = box
-        ymin, xmin = y_rel, x_rel
-        ymax = ymin + h_rel
-        xmax = xmin + w_rel
-
-        display_boxes.append([ymin, xmin, ymax, xmax])
-
-        if label_map:
-            clss = label_map[int(clss)]
-        display_strs.append([clss])
-
-    display_boxes = np.array(display_boxes)
-    draw_bounding_boxes_on_image(image, display_boxes, classes, display_strs=display_strs)
-
-
 def render_megadb_bounding_boxes(boxes_info, image):
     """
+    Render bounding boxes to an image, where those boxes are in the mostly-deprecated
+    MegaDB format, which looks like:
+    
+    .. code-block::none
+        
+        {
+            "category": "animal",
+            "bbox": [
+                0.739,
+                0.448,
+                0.187,
+                0.198
+            ]
+        }        
+        
     Args:
-        boxes_info: list of dict, each dict represents a single detection
-            {
-                "category": "animal",
-                "bbox": [
-                    0.739,
-                    0.448,
-                    0.187,
-                    0.198
-                ]
-            }
+        boxes_info (list): list of dicts, each dict represents a single detection
             where bbox coordinates are normalized [x_min, y_min, width, height]
-        image: PIL.Image.Image, opened image
+        image (PIL.Image.Image): image to modify
+    
+    :meta private:
     """
     
     display_boxes = []
     display_strs = []
     classes = []  # ints, for selecting colors
 
     for b in boxes_info:
@@ -829,23 +861,42 @@
 
     display_boxes = np.array(display_boxes)
     draw_bounding_boxes_on_image(image, display_boxes, classes, display_strs=display_strs)
 
 # ...def render_iMerit_boxes(...)
 
 
-def render_db_bounding_boxes(boxes, classes, image, original_size=None,
-                             label_map=None, thickness=DEFAULT_BOX_THICKNESS, expansion=0):
+def render_db_bounding_boxes(boxes,
+                             classes, 
+                             image, 
+                             original_size=None,
+                             label_map=None, 
+                             thickness=DEFAULT_BOX_THICKNESS, 
+                             expansion=0):
     """
-    Render bounding boxes (with class labels) on [image].  This is a wrapper for
+    Render bounding boxes (with class labels) on an image.  This is a wrapper for
     draw_bounding_boxes_on_image, allowing the caller to operate on a resized image
-    by providing the original size of the image; bboxes will be scaled accordingly.
+    by providing the original size of the image; boxes will be scaled accordingly.
+    
+    This function assumes that bounding boxes are in absolute coordinates, typically
+    because they come from COCO camera traps .json files.
     
-    This function assumes that bounding boxes are in the COCO camera traps format,
-    with absolute coordinates.
+    Args:
+        boxes (list): list of length-4 tuples, foramtted as (x,y,w,h) (in pixels)
+        classes (list): list of ints (or string-formatted ints), used to choose labels (either
+            by literally rendering the class labels, or by indexing into [label_map])
+        image (PIL.Image.Image): image object to modify
+        original_size (tuple, optional): if this is not None, and the size is different than 
+            the size of [image], we assume that [boxes] refer to the original size, and we scale
+            them accordingly before rendering
+        label_map (dict, optional): int --> str dictionary, typically mapping category IDs to
+            species labels; if None, category labels are rendered verbatim (typically as numbers)
+        thickness (int, optional): line width
+        expansion (int, optional): a number of pixels to include on each side of a cropped
+            detection
     """
 
     display_boxes = []
     display_strs = []
 
     if original_size is not None:
         image_size = original_size
@@ -869,105 +920,143 @@
         if label_map:
             clss = label_map[int(clss)]
             
         # need to be a string here because PIL needs to iterate through chars
         display_strs.append([str(clss)])  
 
     display_boxes = np.array(display_boxes)
-    draw_bounding_boxes_on_image(image, display_boxes, classes, display_strs=display_strs,
-                                 thickness=thickness, expansion=expansion)
+    
+    draw_bounding_boxes_on_image(image, 
+                                 display_boxes, 
+                                 classes, 
+                                 display_strs=display_strs,
+                                 thickness=thickness, 
+                                 expansion=expansion)
 
 # ...def render_db_bounding_boxes(...)
 
 
-def draw_bounding_boxes_on_file(input_file, output_file, detections, confidence_threshold=0.0,
+def draw_bounding_boxes_on_file(input_file, 
+                                output_file, 
+                                detections, 
+                                confidence_threshold=0.0,
                                 detector_label_map=DEFAULT_DETECTOR_LABEL_MAP,
-                                thickness=DEFAULT_BOX_THICKNESS, expansion=0,
-                                colormap=DEFAULT_COLORS,
+                                thickness=DEFAULT_BOX_THICKNESS, 
+                                expansion=0,
+                                colormap=None,
                                 label_font_size=DEFAULT_LABEL_FONT_SIZE,
-                                custom_strings=None,target_size=None,
+                                custom_strings=None,
+                                target_size=None,
                                 ignore_exif_rotation=False):
     """
-    Render detection bounding boxes on an image loaded from file, writing the results to a
-    new image file.
-    
-    "detections" is in the API results format:
-        
-    [{"category": "2","conf": 0.996,"bbox": [0.0,0.2762,0.1234,0.2458]}]
-    
-    ...where the bbox is:
-        
-    [x_min, y_min, width_of_box, height_of_box]
-    
-    Normalized, with the origin at the upper-left.
+    Renders detection bounding boxes on an image loaded from file, optionally writing the results to 
+    a new image file.
     
-    detector_label_map is a dict mapping category IDs to strings.  If this is None, 
-    no confidence values or identifiers are shown  If this is {}, just category indices and 
-    confidence values are shown.
-    
-    custom_strings: optional set of strings to append to detection labels, should have the
-    same length as [detections].  Appended before classification labels, if classification
-    data is provided.
-    
-    target_size: tuple of (target_width,target_height).  Either or both can be -1,
-    see resize_image for documentation.  If None or (-1,-1), uses the original image size.
+    Args:
+        input_file (str): filename or URL to load
+        output_file (str, optional): filename to which we should write the rendered image
+        detections (list): a list of dictionaries with keys 'conf' and 'bbox';
+            boxes are length-four arrays formatted as [x,y,w,h], normalized, 
+            upper-left origin (this is the standard MD detection format)
+        detector_label_map (dict, optional): a dict mapping category IDs to strings.  If this 
+            is None, no confidence values or identifiers are shown  If this is {}, just category 
+            indices and confidence values are shown.
+        thickness (int, optional): line width in pixels for box rendering
+        expansion (int, optional): box expansion in pixels
+        colormap (list, optional): list of color names, used to choose colors for categories by
+            indexing with the values in [classes]; defaults to a reasonable set of colors
+        label_font_size (float, optional): label font size
+        custom_strings (list, optional): set of strings to append to detection labels, should have the
+            same length as [detections].  Appended before any classification labels.
+        target_size (tuple, optional): tuple of (target_width,target_height).  Either or both can be -1,
+            see resize_image() for documentation.  If None or (-1,-1), uses the original image size.
+        ignore_exif_rotation (bool, optional): don't rotate the loaded pixels,
+            even if we are loading a JPEG and that JPEG says it should be rotated.
+            
+    Returns:
+        PIL.Image.Image: loaded and modified image
     """
     
     image = open_image(input_file, ignore_exif_rotation=ignore_exif_rotation)
     
     if target_size is not None:
         image = resize_image(image,target_size[0],target_size[1])
         
     render_detection_bounding_boxes(
             detections, image, label_map=detector_label_map,
             confidence_threshold=confidence_threshold,
             thickness=thickness,expansion=expansion,colormap=colormap,
             custom_strings=custom_strings,label_font_size=label_font_size)
 
-    image.save(output_file)
+    if output_file is not None:
+        image.save(output_file)
+    
+    return image
 
 
-def draw_db_boxes_on_file(input_file, output_file, boxes, classes=None, 
-                          label_map=None, thickness=DEFAULT_BOX_THICKNESS, expansion=0,
+def draw_db_boxes_on_file(input_file, 
+                          output_file, 
+                          boxes, 
+                          classes=None, 
+                          label_map=None, 
+                          thickness=DEFAULT_BOX_THICKNESS, 
+                          expansion=0,
                           ignore_exif_rotation=False):
     """
-    Render COCO bounding boxes (in absolute coordinates) on an image loaded from file, writing the
-    results to a new image file.
+    Render COCO-formatted bounding boxes (in absolute coordinates) on an image loaded from file, 
+    writing the results to a new image file.
 
-    classes is a list of integer category IDs.
+    Args:
+        input_file (str): image file to read
+        output_file (str): image file to write
+        boxes (list): list of length-4 tuples, foramtted as (x,y,w,h) (in pixels)
+        classes (list, optional): list of ints (or string-formatted ints), used to choose 
+            labels (either by literally rendering the class labels, or by indexing into [label_map])
+        label_map (dict, optional): int --> str dictionary, typically mapping category IDs to
+            species labels; if None, category labels are rendered verbatim (typically as numbers)
+        thickness (int, optional): line width
+        expansion (int, optional): a number of pixels to include on each side of a cropped
+            detection
+        ignore_exif_rotation (bool, optional): don't rotate the loaded pixels,
+            even if we are loading a JPEG and that JPEG says it should be rotated
     
-    detector_label_map is a dict mapping category IDs to strings.
+    Returns:
+        PIL.Image.Image: the loaded and modified image
     """
     
     image = open_image(input_file, ignore_exif_rotation=ignore_exif_rotation)
 
     if classes is None:
         classes = [0] * len(boxes)
         
     render_db_bounding_boxes(boxes, classes, image, original_size=None,
                                  label_map=label_map, thickness=thickness, expansion=expansion)
-
+    
     image.save(output_file)
     
-
+    return image
+    
 # ...def draw_bounding_boxes_on_file(...)
 
 
 def gray_scale_fraction(image,crop_size=(0.1,0.1)):
     """
-    Returns the fraction of the pixels in [image] that appear to be grayscale (R==G==B), 
+    Computes the fraction of the pixels in [image] that appear to be grayscale (R==G==B), 
     useful for approximating whether this is a night-time image when flash information is not
     available in EXIF data (or for video frames, where this information is often not available
     in structured metadata at all).
     
-    [image] can be a PIL image or a file name.
-    
-    crop_size should be a 2-element list/tuple, representing the fraction of the image 
-    to crop at the top and bottom, respectively, before analyzing (to minimize the possibility
-    of including color elements in the image chrome).
+    Args:
+        image (str or PIL.Image.Image): Image, filename, or URL to analyze
+        crop_size (optional): a 2-element list/tuple, representing the fraction of the 
+            image to crop at the top and bottom, respectively, before analyzing (to minimize 
+            the possibility of including color elements in the image overlay)
+            
+    Returns:
+        float: the fraction of pixels in [image] that appear to be grayscale (R==G==B)
     """
     
     if isinstance(image,str):
         image = Image.open(image)
     
     if image.mode == 'L':
         return 1.0
@@ -1055,15 +1144,15 @@
 # ...def _resize_relative_image(...)
 
 
 def _resize_absolute_image(input_output_files,
                           target_width,target_height,no_enlarge_width,verbose,quality):
     
     """
-    Internal wrappter for resize_image used in the context of a batch resize operation.
+    Internal wrapper for resize_image used in the context of a batch resize operation.
     """
     
     input_fn_abs = input_output_files[0]
     output_fn_abs = input_output_files[1]
     os.makedirs(os.path.dirname(output_fn_abs),exist_ok=True)
     try:
         _ = resize_image(input_fn_abs, 
@@ -1081,30 +1170,51 @@
     return {'input_fn':input_fn_abs,'output_fn':output_fn_abs,status:'status',
             'error':error}
 
 # ..._resize_absolute_image(...)
 
 
 def resize_images(input_file_to_output_file,
-                  target_width=-1, target_height=-1,
-                  no_enlarge_width=False, verbose=False, quality='keep',
-                  pool_type='process', n_workers=10):
+                  target_width=-1, 
+                  target_height=-1,
+                  no_enlarge_width=False, 
+                  verbose=False, 
+                  quality='keep',
+                  pool_type='process', 
+                  n_workers=10):
     """
-    Resize all images the dictionary [input_file_to_output_file].
-    
-    Defaults to parallelizing across processes.
-    
-    See resize_image() for parameter information.
-    
+    Resizes all images the dictionary [input_file_to_output_file].
+
     TODO: This is a little more redundant with resize_image_folder than I would like;
     refactor resize_image_folder to call resize_images.  Not doing that yet because
     at the time I'm writing this comment, a lot of code depends on resize_image_folder 
     and I don't want to rock the boat yet.
-    """
+    
+    Args:
+        input_file_to_output_file (dict): dict mapping images that exist to the locations
+            where the resized versions should be written
+        target_width (int, optional): width to which we should resize this image, or -1
+            to let target_height determine the size
+        target_height (int, optional): height to which we should resize this image, or -1
+            to let target_width determine the size
+        no_enlarge_width (bool, optional): if [no_enlarge_width] is True, and 
+            [target width] is larger than the original image width, does not modify the image, 
+            but will write to output_file if supplied
+        verbose (bool, optional): enable additional debug output
+        quality (str or int, optional): passed to exif_preserving_save, see docs for more detail
+        pool_type (str, optional): whether use use processes ('process') or threads ('thread') for
+            parallelization; ignored if n_workers <= 1
+        n_workers (int, optional): number of workers to use for parallel resizing; set to <=1
+            to disable parallelization
 
+    Returns:
+        list: a list of dicts with keys 'input_fn', 'output_fn', 'status', and 'error'.
+        'status' will be 'success' or 'error'; 'error' will be None for successful cases, 
+        otherwise will contain the image-specific error.
+    """
     
     assert pool_type in ('process','thread'), 'Illegal pool type {}'.format(pool_type)
     
     input_output_file_pairs = []
     
     # Reformat input files as (input,output) tuples
     for input_fn in input_file_to_output_file:
@@ -1142,45 +1252,82 @@
         results = list(tqdm(pool.imap(p, input_output_file_pairs),total=len(input_output_file_pairs)))
 
     return results
 
 # ...def resize_images(...)
 
 
-def resize_image_folder(input_folder, output_folder=None,
-                        target_width=-1, target_height=-1,
-                        no_enlarge_width=False, verbose=False, quality='keep',
-                        pool_type='process', n_workers=10, recursive=True,
+def resize_image_folder(input_folder, 
+                        output_folder=None,
+                        target_width=-1, 
+                        target_height=-1,
+                        no_enlarge_width=False, 
+                        verbose=False, 
+                        quality='keep',
+                        pool_type='process', 
+                        n_workers=10, 
+                        recursive=True,
                         image_files_relative=None):
     """
-    Resize all images in a folder (defaults to recursive)
+    Resize all images in a folder (defaults to recursive).
     
     Defaults to in-place resizing (output_folder is optional).
     
-    Defaults to parallelizing across processes.
-    
-    See resize_image() for parameter information.
+    Args:
+        input_folder (str): folder in which we should find images to resize
+        output_folder (str, optional): folder in which we should write resized images.  If
+            None, resizes images in place.  Otherwise, maintains relative paths in the target
+            folder.
+        target_width (int, optional): width to which we should resize this image, or -1
+            to let target_height determine the size
+        target_height (int, optional): height to which we should resize this image, or -1
+            to let target_width determine the size
+        no_enlarge_width (bool, optional): if [no_enlarge_width] is True, and 
+            [target width] is larger than the original image width, does not modify the image, 
+            but will write to output_file if supplied
+        verbose (bool, optional): enable additional debug output
+        quality (str or int, optional): passed to exif_preserving_save, see docs for more detail
+        pool_type (str, optional): whether use use processes ('process') or threads ('thread') for
+            parallelization; ignored if n_workers <= 1
+        n_workers (int, optional): number of workers to use for parallel resizing; set to <=1
+            to disable parallelization
+        recursive (bool, optional): whether to search [input_folder] recursively for images.
+        image_files_relative (list, optional): if not None, skips any relative paths not
+            in this list.
+            
+    Returns:
+        list: a list of dicts with keys 'input_fn', 'output_fn', 'status', and 'error'.
+        'status' will be 'success' or 'error'; 'error' will be None for successful cases, 
+        otherwise will contain the image-specific error.
     """
 
     assert os.path.isdir(input_folder), '{} is not a folder'.format(input_folder)
     
     if output_folder is None:
         output_folder = input_folder
     else:
         os.makedirs(output_folder,exist_ok=True)
         
     assert pool_type in ('process','thread'), 'Illegal pool type {}'.format(pool_type)
     
     if image_files_relative is None:
-        image_files_relative = find_images(input_folder,recursive=recursive,return_relative_paths=True)
+        
+        if verbose:
+            print('Enumerating images')
+            
+        image_files_relative = find_images(input_folder,recursive=recursive,
+                                           return_relative_paths=True,convert_slashes=True)
         if verbose:
             print('Found {} images'.format(len(image_files_relative)))
     
     if n_workers == 1:    
         
+        if verbose:
+            print('Resizing images')
+
         results = []
         for fn_relative in tqdm(image_files_relative):
             results.append(_resize_relative_image(fn_relative,
                                   input_folder=input_folder,
                                   output_folder=output_folder,
                                   target_width=target_width,
                                   target_height=target_height,
@@ -1211,21 +1358,180 @@
         results = list(tqdm(pool.imap(p, image_files_relative),total=len(image_files_relative)))
 
     return results
 
 # ...def resize_image_folder(...)
 
 
+#%% Image integrity checking functions
+
+def check_image_integrity(filename,modes=None):
+    """
+    Check whether we can successfully load an image via OpenCV and/or PIL.
+    
+    Args: 
+        filename (str): the filename to evaluate
+        modes (list, optional): a list containing one or more of:
+        
+            - 'cv'
+            - 'pil'
+            - 'skimage'
+            - 'jpeg_trailer' 
+                
+            'jpeg_trailer' checks that the binary data ends with ffd9.  It does not check whether
+            the image is actually a jpeg, and even if it is, there are lots of reasons the image might not
+            end with ffd9.  It's also true the JPEGs that cause "premature end of jpeg segment" issues
+            don't end with ffd9, so this may be a useful diagnostic.  High precision, very low recall
+            for corrupt jpegs.
+                
+            Set to None to use all modes.
+    
+    Returns:
+        dict: a dict with a key called 'file' (the value of [filename]), one key for each string in
+        [modes] (a success indicator for that mode, specifically a string starting with either
+        'success' or 'error').
+    """
+    
+    if modes is None:
+        modes = ('cv','pil','skimage','jpeg_trailer')
+    else:
+        if isinstance(modes,str):
+            modes = [modes]
+        for mode in modes:
+            assert mode in ('cv','pil','skimage'), 'Unrecognized mode {}'.format(mode)
+        
+    assert os.path.isfile(filename), 'Could not find file {}'.format(filename)
+    
+    result = {}
+    result['file'] = filename
+    
+    for mode in modes:
+        
+        result[mode] = 'unknown'
+        if mode == 'pil':
+            try:
+                pil_im = load_image(filename) # noqa
+                assert pil_im is not None
+                result[mode] = 'success'
+            except Exception as e:
+                result[mode] = 'error: {}'.format(str(e))
+        elif mode == 'cv':
+            try:
+                cv_im = cv2.imread(filename)
+                assert cv_im is not None, 'Unknown opencv read failure'
+                numpy_im = np.asarray(cv_im) # noqa
+                result[mode] = 'success'
+            except Exception as e:
+                result[mode] = 'error: {}'.format(str(e))
+        elif mode == 'skimage':            
+            try:
+                # This is not a standard dependency
+                from skimage import io as skimage_io # noqa
+            except Exception:
+                result[mode] = 'could not import skimage, run pip install scikit-image'
+                return result
+            try:
+                skimage_im = skimage_io.imread(filename) # noqa
+                assert skimage_im is not None
+                result[mode] = 'success'
+            except Exception as e:
+                result[mode] = 'error: {}'.format(str(e))
+        elif mode == 'jpeg_trailer':
+            # https://stackoverflow.com/a/48282863/16644970
+            try:
+                with open(filename, 'rb') as f:
+                    check_chars = f.read()[-2:]
+                if check_chars != b'\xff\xd9':
+                    result[mode] = 'invalid jpeg trailer: {}'.format(str(check_chars))
+                else:
+                    result[mode] = 'success'
+            except Exception as e:
+                result[mode] = 'error: {}'.format(str(e))
+                
+    # ...for each mode            
+    
+    return result
+
+# ...def check_image_integrity(...)
+
+
+def parallel_check_image_integrity(filenames,
+                                   modes=None, 
+                                   max_workers=16, 
+                                   use_threads=True, 
+                                   recursive=True):
+    """
+    Check whether we can successfully load a list of images via OpenCV and/or PIL.
+    
+    Args:
+        filenames (list or str): a list of image filenames or a folder
+        mode (list): see check_image_integrity() for documentation on the [modes] parameter
+        max_workers (int, optional): the number of parallel workers to use; set to <=1 to disable
+            parallelization
+        use_threads (bool, optional): whether to use threads (True) or processes (False) for
+            parallelization
+        recursive (bool, optional): if [filenames] is a folder, whether to search recursively for images.
+            Ignored if [filenames] is a list.
+            
+    Returns:
+        list: a list of dicts, each with a key called 'file' (the value of [filename]), one key for 
+        each string in [modes] (a success indicator for that mode, specifically a string starting 
+        with either 'success' or 'error').
+    """
+
+    n_workers = min(max_workers,len(filenames))
+    
+    if isinstance(filenames,str) and os.path.isdir(filenames):
+        filenames = find_images(filenames,recursive=recursive,return_relative_paths=False)
+    
+    print('Checking image integrity for {} filenames'.format(len(filenames)))
+    
+    if n_workers <= 1:
+        
+        results = []
+        for filename in filenames:
+            results.append(check_image_integrity(filename,modes=modes))
+        
+    else:
+        
+        if use_threads:
+            pool = ThreadPool(n_workers)
+        else:
+            pool = Pool(n_workers)
+    
+        results = list(tqdm(pool.imap(
+            partial(check_image_integrity,modes=modes),filenames), total=len(filenames)))
+    
+    return results
+
+
 #%% Test drivers
 
 if False:
     
     #%% Recursive resize test
     
     from md_visualization.visualization_utils import resize_image_folder # noqa
     
     input_folder = r"C:\temp\resize-test\in"
     output_folder = r"C:\temp\resize-test\out"
     
     resize_results = resize_image_folder(input_folder,output_folder,
                          target_width=1280,verbose=True,quality=85,no_enlarge_width=True,
-                         pool_type='process',n_workers=10)
+                         pool_type='process',n_workers=10)
+    
+    
+    #%% Integrity checking test
+    
+    from md_utils import md_tests
+    options = md_tests.download_test_data()
+    folder = options.scratch_dir
+    
+    results = parallel_check_image_integrity(folder,max_workers=8)
+    
+    modes = ['cv','pil','skimage','jpeg_trailer']
+    
+    for r in results:
+        for mode in modes:
+            if r[mode] != 'success':
+                s = r[mode]
+                print('Mode {} failed for {}:\n{}\n'.format(mode,r['file'],s))
```

### Comparing `megadetector-5.0.8/md_visualization/visualize_db.py` & `megadetector-5.0.9/md_visualization/visualize_db.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-########
-#
-# visualize_db.py
-# 
-# Outputs an HTML page visualizing annotations (class labels and/or bounding boxes)
-# on a sample of images in a database in the COCO Camera Traps format.
-#
-########
+"""
+
+visualize_db.py
+
+Outputs an HTML page visualizing annotations (class labels and/or bounding boxes)
+on a sample of images in a database in the COCO Camera Traps format.
+
+"""
 
 #%% Imports
 
 import argparse
 import inspect
+import random
 import json
 import math
 import os
 import sys
 import time
 
 import pandas as pd
@@ -32,100 +33,127 @@
 
 import md_visualization.visualization_utils as vis_utils
 
 
 #%% Settings
 
 class DbVizOptions:
+    """
+    Parameters controlling the behavior of visualize_db().
+    """
     
-    # Set to None to visualize all images
+    #: Number of images to sample from the database, or None to visualize all images
     num_to_visualize = None
     
-    # Target size for rendering; set either dimension to -1 to preserve aspect ratio
-    #
-    # If viz_size is None or (-1,-1), the original image size is used.
+    #: Target size for rendering; set either dimension to -1 to preserve aspect ratio.
+    #:
+    #: If viz_size is None or (-1,-1), the original image size is used.
     viz_size = (800, -1)
     
-    # The most relevant option one might want to set here is:
-    #
-    # htmlOptions['maxFiguresPerHtmlFile']
-    #
-    # ...which can be used to paginate previews to a number of images that will load well
-    # in a browser (5000 is a reasonable limit).
+    #: HTML rendering options; see write_html_image_list for details
+    #:
+    #:The most relevant option one might want to set here is:
+    #:
+    #: htmlOptions['maxFiguresPerHtmlFile']
+    #:
+    #: ...which can be used to paginate previews to a number of images that will load well
+    #: in a browser (5000 is a reasonable limit).
     htmlOptions = write_html_image_list()
     
+    #: Whether to sort images by filename (True) or randomly (False)
     sort_by_filename = True
+    
+    #: Only show images that contain bounding boxes
     trim_to_images_with_bboxes = False
     
-    random_seed = 0 # None
+    #: Random seed to use for sampling images
+    random_seed = 0
     
-    # Should we include Web search links for each category name?    
+    #: Should we include Web search links for each category name?    
     add_search_links = False
     
-    # Should each thumbnail image link back to the original image?
+    #: Should each thumbnail image link back to the original image?
     include_image_links = False
     
-    # Should there be a text link back to each original image?
+    #: Should there be a text link back to each original image?
     include_filename_links = False
     
+    #: Line width in pixels
     box_thickness = 4
+    
+    #: Number of pixels to expand each bounding box
     box_expansion = 0
     
-    # These are mutually exclusive; both are category names, not IDs
-    classes_to_exclude = None
+    #: Only include images that contain annotations with these class names (not IDs)
+    #:
+    #: Mutually exclusive with classes_to_exclude
     classes_to_include = None
     
-    # Special tag used to say "show me all images with multiple categories"
+    #: Exclude images that contain annotations with these class names (not IDs)
+    #:
+    #: Mutually exclusive with classes_to_include
+    classes_to_exclude = None
+    
+    #: Special tag used to say "show me all images with multiple categories"
+    #:
+    #: :meta private:
     multiple_categories_tag = '*multiple*'
 
-    # We sometimes flatten image directories by replacing a path separator with 
-    # another character.  Leave blank for the typical case where this isn't necessary.
+    #: We sometimes flatten image directories by replacing a path separator with 
+    #: another character.  Leave blank for the typical case where this isn't necessary.
     pathsep_replacement = '' # '~'
 
-    # Control rendering parallelization
-    parallelize_rendering_n_cores = 25
+    #: Parallelize rendering across multiple workers
+    parallelize_rendering = False
     
-    # Process-based parallelization in this function is currently unsupported
-    # due to pickling issues I didn't care to look at, but I'm going to just
-    # flip this with a warning, since I intend to support it in the future.
+    #: In theory, whether to parallelize with threads (True) or processes (False), but
+    #: process-based parallelization in this function is currently unsupported
     parallelize_rendering_with_threads = True
-    parallelize_rendering = False
     
-    # Should we show absolute (vs. relative) paths for each image?
+    #: Number of workers to use for parallelization; ignored if parallelize_rendering
+    #: is False
+    parallelize_rendering_n_cores = 25
+        
+    #: Should we show absolute (True) or relative (False) paths for each image?
     show_full_paths = False
     
-    # Set to False to skip existing images
+    #: Set to False to skip existing images
     force_rendering = True
     
+    #: Enable additionald debug console output
     verbose = False
     
 
 #%% Helper functions
 
-# Translate the file name in an image entry in the json database to a path, possibly doing
-# some manipulation of path separators
-def image_filename_to_path(image_file_name, image_base_dir, pathsep_replacement=''):
+def _image_filename_to_path(image_file_name, image_base_dir, pathsep_replacement=''):
+    """
+    Translates the file name in an image entry in the json database to a path, possibly doing
+    some manipulation of path separators.
+    """
     
     if len(pathsep_replacement) > 0:
         image_file_name = os.path.normpath(image_file_name).replace(os.pathsep,pathsep_replacement)        
     return os.path.join(image_base_dir, image_file_name)
 
 
 #%% Core functions
 
 def visualize_db(db_path, output_dir, image_base_dir, options=None):
     """
-    Writes images and html to output_dir to visualize the annotations in the json file
-    db_path.
+    Writes images and html to output_dir to visualize the annotations in a .json file.
     
-    db_path can also be a previously-loaded database.
+    Args:
+        db_path (str or dict): the .json filename to load, or a previously-loaded database
+        image_base_dir (str): the folder where the images live; filenames in [db_path] should
+            be relative to this folder.
+        options (DbVizOptions, optional): See DbVizOptions for details
     
-    Returns the html filename and the database:
-        
-    return htmlOutputFile,image_db
+    Returns:
+        tuple: A length-two tuple containing (the html filename) and (the loaded database).        
     """    
     
     if options is None:
         options = DbVizOptions()
     
     if not options.parallelize_rendering_with_threads:
         print('Warning: process-based parallelization is not yet supported by visualize_db')
@@ -245,15 +273,15 @@
         
         img_relative_path = img['file_name']
         
         if image_base_dir.startswith('http'):
             img_path = image_base_dir + img_relative_path
         else:
             img_path = os.path.join(image_base_dir, 
-                                    image_filename_to_path(img_relative_path, image_base_dir))
+                                    _image_filename_to_path(img_relative_path, image_base_dir))
     
         annos_i = df_anno.loc[df_anno['image_id'] == img_id, :] # all annotations on this image
     
         bboxes = []
         boxClasses = []
         
         # All the class labels we've seen for this image (with out without bboxes)
@@ -427,14 +455,16 @@
     elapsed = time.time() - start_time
     
     print('Rendered {} images in {} ({} successful)'.format(
         len(rendering_info),humanfriendly.format_timespan(elapsed),sum(rendering_success)))
         
     if options.sort_by_filename:    
         images_html = sorted(images_html, key=lambda x: x['filename'])
+    else:
+        random.shuffle(images_html)
         
     htmlOutputFile = os.path.join(output_dir, 'index.html')
     
     htmlOptions = options.htmlOptions
     if isinstance(db_path,str):
         htmlOptions['headerHtml'] = '<h1>Sample annotations from {}</h1>'.format(db_path)
     else:
@@ -496,17 +526,15 @@
     options = DbVizOptions()
     args_to_object(args, options)
     if options.random_sort:
         options.sort_by_filename = False
         
     visualize_db(options.db_path,options.output_dir,options.image_base_dir,options) 
 
-
-if __name__ == '__main__':
-    
+if __name__ == '__main__':    
     main()
 
 
 #%% Interactive driver
 
 if False:
```

### Comparing `megadetector-5.0.8/md_visualization/visualize_detector_output.py` & `megadetector-5.0.9/md_visualization/visualize_detector_output.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-########
-#
-# visualize_detector_output.py
-#
-# Render images with bounding boxes annotated on them to a folder, based on a
-# detector output result file (json), optionally writing an HTML index file.
-#
-########
+"""
+
+visualize_detector_output.py
+
+Render images with bounding boxes annotated on them to a folder, based on a
+detector output result file (.json), optionally writing an HTML index file.
+
+"""
 
 #%% Imports
 
 import argparse
 import json
 import os
 import random
@@ -34,19 +34,22 @@
 DEFAULT_DETECTOR_LABEL_MAP = {
     str(k): v for k, v in detector_bbox_category_id_to_name.items()
 }
 
 
 #%% Support functions
 
-def render_image(entry,
+def _render_image(entry,
                  detector_label_map,classification_label_map,
                  confidence_threshold,classification_confidence_threshold,
                  render_detections_only,preserve_path_structure,out_dir,images_dir,
                  output_image_width):
+    """
+    Internal function for rendering a single image.
+    """
     
     rendering_result = {'failed_image':False,'missing_image':False,
                         'skipped_image':False,'annotated_image_path':None,
                         'max_conf':None,'file':entry['file']}
     
     image_id = entry['file']
 
@@ -93,46 +96,63 @@
     rendering_result['annotated_image_path'] = annotated_img_path        
 
     return rendering_result
 
 
 #%% Main function
 
-def visualize_detector_output(detector_output_path: str,
-                              out_dir: str,
-                              images_dir: str,
-                              confidence_threshold: float = 0.15,
-                              sample: int = -1,
-                              output_image_width: int = 700,
-                              random_seed: int = None,
-                              render_detections_only: bool = False,
-                              classification_confidence_threshold = 0.1,
-                              html_output_file = None,
-                              html_output_options = None,
-                              preserve_path_structure = False,
-                              parallelize_rendering = False,
-                              parallelize_rendering_n_cores = 10,
-                              parallelize_rendering_with_threads = True) -> List[str]:
+def visualize_detector_output(detector_output_path,
+                              out_dir,
+                              images_dir,
+                              confidence_threshold=0.15,
+                              sample=-1,
+                              output_image_width=700,
+                              random_seed=None,
+                              render_detections_only=False,
+                              classification_confidence_threshold=0.1,
+                              html_output_file=None,
+                              html_output_options=None,
+                              preserve_path_structure=False,
+                              parallelize_rendering=False,
+                              parallelize_rendering_n_cores=10,
+                              parallelize_rendering_with_threads=True):
     
     """
-    Draw bounding boxes on images given the output of a detector.
+    Draws bounding boxes on images given the output of a detector.
 
     Args:
-        detector_output_path: str, path to detector output json file
-        out_dir: str, path to directory for saving annotated images
-        images_dir: str, path to images dir
-        confidence: float, threshold above which annotations will be rendered
-        sample: int, maximum number of images to annotate, -1 for all
-        random_seed: seed for sampling (not relevant if sample == -1)
-        output_image_width: int, width in pixels to resize images for display,
-            set to -1 to use original image width
-        random_seed: int, for deterministic image sampling when sample != -1
-        render_detections_only: bool, only render images with above-threshold detections
+        detector_output_path (str): path to detector output .json file
+        out_dir (str): path to directory for saving annotated images
+        images_dir (str): folder where the images live; filenames in 
+            [detector_output_path] should be relative to [image_dir]
+        confidence_threshold (float, optional): threshold above which detections will be rendered
+        sample (int, optional): maximum number of images to render, -1 for all
+        output_image_width (int, optional): width in pixels to resize images for display,
+            preserving aspect ration; set to -1 to use original image width
+        random_seed (int, optional): seed to use for choosing images when sample != -1
+        render_detections_only (bool): only render images with above-threshold detections
+        classification_confidence_threshold (float, optional): only show classifications
+            above this threshold; does not impact whether images are rendered, only whether
+            classification labels (not detection categories) are displayed
+        html_output_file (str, optional): output path for an HTML index file (not written
+            if None)
+        html_output_options (dict, optional): HTML formatting options; see write_html_image_list 
+            for details
+        preserve_path_structure (bool, optional): if False (default), writes images to unique
+            names in a flat structure in the output folder; if True, preserves relative paths
+            within the output folder
+        parallelize_rendering (bool, optional): whether to use concurrent workers for rendering
+        parallelize_rendering_n_cores (int, optional): number of concurrent workers to use 
+            (ignored if parallelize_rendering is False)
+        parallelize_rendering_with_threads (bool, optional): determines whether we use
+            threads (True) or processes (False) for parallelization (ignored if parallelize_rendering 
+            is False)
 
-    Returns: list of str, paths to annotated images
+    Returns:
+        list: list of paths to annotated images
     """
     
     assert os.path.exists(detector_output_path), \
         'Detector output file does not exist at {}'.format(detector_output_path)
 
     assert os.path.isdir(images_dir), \
         'Image folder {} is not available'.format(images_dir)
@@ -205,30 +225,30 @@
             if parallelize_rendering_with_threads:
                 pool = ThreadPool(parallelize_rendering_n_cores)
             else:
                 pool = Pool(parallelize_rendering_n_cores)
             print('Rendering images with {} {}'.format(parallelize_rendering_n_cores,
                                                        worker_string))            
         rendering_results = list(tqdm(pool.imap(
-                                 partial(render_image,detector_label_map=detector_label_map,
+                                 partial(_render_image,detector_label_map=detector_label_map,
                                          classification_label_map=classification_label_map,
                                          confidence_threshold=confidence_threshold,
                                          classification_confidence_threshold=classification_confidence_threshold,
                                          render_detections_only=render_detections_only,
                                          preserve_path_structure=preserve_path_structure,
                                          out_dir=out_dir,
                                          images_dir=images_dir,
                                          output_image_width=output_image_width),
                                  images), total=len(images)))
         
     else:
                 
         for entry in tqdm(images):
             
-            rendering_result = render_image(entry,detector_label_map,classification_label_map,
+            rendering_result = _render_image(entry,detector_label_map,classification_label_map,
                                             confidence_threshold,classification_confidence_threshold,
                                             render_detections_only,preserve_path_structure,out_dir,
                                             images_dir,output_image_width)
             rendering_results.append(rendering_result)
         
     # ...for each image
     
@@ -265,17 +285,16 @@
                                                     options=html_output_options)
         
     return annotated_image_paths
 
 
 #%% Command-line driver
 
-def main() -> None:
-    """Main function."""
-
+def main():
+    
     parser = argparse.ArgumentParser(
         formatter_class=argparse.ArgumentDefaultsHelpFormatter,
         description='Annotate the bounding boxes predicted by a detector above '
                     'some confidence threshold, and save the annotated images.')
     parser.add_argument(
         'detector_output_path', type=str,
         help='Path to json output file of the detector')
```

### Comparing `megadetector-5.0.8/megadetector.egg-info/PKG-INFO` & `megadetector-5.0.9/megadetector.egg-info/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: megadetector
-Version: 5.0.8
+Version: 5.0.9
 Summary: MegaDetector is an AI model that helps conservation folks spend less time doing boring things with camera trap images.
 Author-email: Your friendly neighborhood MegaDetector team <cameratraps@lila.science>
 Maintainer-email: Your friendly neighborhood MegaDetector team <cameratraps@lila.science>
 License:     MIT License
         
             Permission is hereby granted, free of charge, to any person obtaining a copy
             of this software and associated documentation files (the "Software"), to deal
@@ -21,14 +21,15 @@
             FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
             AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
             LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
             OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
             SOFTWARE.
         
 Project-URL: Homepage, https://github.com/agentmorris/MegaDetector
+Project-URL: Documentation, https://megadetector.readthedocs.io
 Project-URL: Bug Reports, https://github.com/agentmorris/MegaDetector/issues
 Project-URL: Source, https://github.com/agentmorris/MegaDetector
 Keywords: camera traps,conservation,wildlife,ai
 Classifier: Development Status :: 3 - Alpha
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Programming Language :: Python :: 3
 Requires-Python: <3.12,>=3.9
@@ -47,24 +48,24 @@
 Requires-Dist: scikit-learn>=1.3.1
 Requires-Dist: pandas>=2.1.1
 Requires-Dist: PyYAML>=6.0.1
 Requires-Dist: ultralytics-yolov5==0.1.1
 
 # MegaDetector
 
-This package is a pip-installable version of the support/inference code for [MegaDetector](https://github.com/agentmorris/MegaDetector), an object detection model that helps conservation biologists spend less time doing boring things with camera trap images.
+This package is a pip-installable version of the support/inference code for [MegaDetector](https://github.com/agentmorris/MegaDetector), an object detection model that helps conservation biologists spend less time doing boring things with camera trap images.  Complete documentation for this Python package is available at <megadetector.readthedocs.io>.
 
-If you want to learn more about what MegaDetector is all about, head over to the [MegaDetector repo](https://github.com/agentmorris/MegaDetector).
+If you aren't looking for the Python package specificaly, and you just want to learn more about what MegaDetector is all about, head over to the [MegaDetector repo](https://github.com/agentmorris/MegaDetector).
 
 
 ## Reasons you probably aren't looking for this package
 
 ### If you are an ecologist...
 
-If you are an ecologist looking to use MegaDetector to help you get through your camera trap images, you probably don't want this package.  We recommend starting with our "[Getting started with MegaDetector](https://github.com/agentmorris/MegaDetector/blob/main/collaborations.md)" page, then digging in to the [MegaDetector User Guide](https://github.com/agentmorris/MegaDetector/blob/main/megadetector.md), which will walk you through the process of using MegaDetector.  That journey will <i>not</i> involve this Python package.
+If you are an ecologist looking to use MegaDetector to help you get through your camera trap images, you probably don't want this package.  We recommend starting with our "[Getting started with MegaDetector](https://github.com/agentmorris/MegaDetector/blob/main/getting-started.md)" page, then digging in to the [MegaDetector User Guide](https://github.com/agentmorris/MegaDetector/blob/main/megadetector.md), which will walk you through the process of using MegaDetector.  That journey will <i>not</i> involve this Python package.
 
 ### If you are a computer-vision-y type...
 
 If you are a computer-vision-y person looking to run or fine-tune MegaDetector programmatically, you still probably don't want this package.  MegaDetector is just a fine-tuned version of [YOLOv5](https://github.com/ultralytics/yolov5), and the [ultralytics](https://github.com/ultralytics/ultralytics/) package (from the developers of YOLOv5) has a zillion bells and whistles for both inference and fine-tuning that this package doesn't.
 
 ## Reasons you might want to use this package
 
@@ -76,17 +77,22 @@
 
 To install:
 
 `pip install megadetector`
 
 MegaDetector model weights aren't downloaded at pip-install time, but they will be (optionally) automatically downloaded the first time you run the model.
 
-### Examples of things you can do with this package
+## Package reference
 
-#### Run MegaDetector on one image and count the number of detections
+See <megadetector.readthedocs.io>.
+
+
+## Examples of things you can do with this package
+
+### Run MegaDetector on one image and count the number of detections
 
 ```
 from md_utils import url_utils
 from md_visualization import visualization_utils as vis_utils
 from detection import run_detector
 
 # This is the image at the bottom of this page, it has one animal in it
@@ -100,15 +106,15 @@
 
 result = model.generate_detections_one_image(image)
 
 detections_above_threshold = [d for d in result['detections'] if d['conf'] > 0.2]
 print('Found {} detections above threshold'.format(len(detections_above_threshold)))
 ```
 
-#### Run MegaDetector on a folder of images
+### Run MegaDetector on a folder of images
 
 ```
 from detection.run_detector_batch import load_and_run_detector_batch,write_results_to_file
 from md_utils import path_utils
 import os
 
 # Pick a folder to run MD on recursively, and an output file
```

### Comparing `megadetector-5.0.8/megadetector.egg-info/SOURCES.txt` & `megadetector-5.0.9/megadetector.egg-info/SOURCES.txt`

 * *Files 3% similar despite different names*

```diff
@@ -1,28 +1,36 @@
 LICENSE
 README-package.md
 README.md
 pyproject.toml
+api/__init__.py
+api/batch_processing/__init__.py
+api/batch_processing/api_core/__init__.py
 api/batch_processing/api_core/server.py
 api/batch_processing/api_core/server_api_config.py
 api/batch_processing/api_core/server_app_config.py
 api/batch_processing/api_core/server_batch_job_manager.py
 api/batch_processing/api_core/server_job_status_table.py
 api/batch_processing/api_core/server_orchestration.py
 api/batch_processing/api_core/server_utils.py
+api/batch_processing/api_core/batch_service/__init__.py
 api/batch_processing/api_core/batch_service/score.py
+api/batch_processing/api_core_support/__init__.py
 api/batch_processing/api_core_support/aggregate_results_manually.py
+api/batch_processing/api_support/__init__.py
 api/batch_processing/api_support/summarize_daily_activity.py
+api/batch_processing/data_preparation/__init__.py
 api/batch_processing/data_preparation/manage_local_batch.py
 api/batch_processing/data_preparation/manage_video_batch.py
 api/batch_processing/integration/digiKam/setup.py
 api/batch_processing/integration/digiKam/xmp_integration.py
 api/batch_processing/integration/eMammal/test_scripts/config_template.py
 api/batch_processing/integration/eMammal/test_scripts/push_annotations_to_emammal.py
 api/batch_processing/integration/eMammal/test_scripts/select_images_for_testing.py
+api/batch_processing/postprocessing/__init__.py
 api/batch_processing/postprocessing/add_max_conf.py
 api/batch_processing/postprocessing/categorize_detections_by_size.py
 api/batch_processing/postprocessing/combine_api_outputs.py
 api/batch_processing/postprocessing/compare_batch_results.py
 api/batch_processing/postprocessing/convert_output_format.py
 api/batch_processing/postprocessing/load_api_results.py
 api/batch_processing/postprocessing/md_to_coco.py
@@ -33,14 +41,16 @@
 api/batch_processing/postprocessing/render_detection_confusion_matrix.py
 api/batch_processing/postprocessing/separate_detections_into_folders.py
 api/batch_processing/postprocessing/subset_json_detector_output.py
 api/batch_processing/postprocessing/top_folders_to_bottom.py
 api/batch_processing/postprocessing/repeat_detection_elimination/find_repeat_detections.py
 api/batch_processing/postprocessing/repeat_detection_elimination/remove_repeat_detections.py
 api/batch_processing/postprocessing/repeat_detection_elimination/repeat_detections_core.py
+api/synchronous/__init__.py
+api/synchronous/api_core/animal_detection_api/__init__.py
 api/synchronous/api_core/animal_detection_api/api_backend.py
 api/synchronous/api_core/animal_detection_api/api_frontend.py
 api/synchronous/api_core/animal_detection_api/config.py
 api/synchronous/api_core/animal_detection_api/data_management/annotations/annotation_constants.py
 api/synchronous/api_core/animal_detection_api/detection/process_video.py
 api/synchronous/api_core/animal_detection_api/detection/pytorch_detector.py
 api/synchronous/api_core/animal_detection_api/detection/run_detector.py
@@ -58,15 +68,17 @@
 api/synchronous/api_core/animal_detection_api/md_utils/path_utils.py
 api/synchronous/api_core/animal_detection_api/md_utils/process_utils.py
 api/synchronous/api_core/animal_detection_api/md_utils/sas_blob_utils.py
 api/synchronous/api_core/animal_detection_api/md_utils/string_utils.py
 api/synchronous/api_core/animal_detection_api/md_utils/url_utils.py
 api/synchronous/api_core/animal_detection_api/md_utils/write_html_image_list.py
 api/synchronous/api_core/animal_detection_api/md_visualization/visualization_utils.py
+api/synchronous/api_core/tests/__init__.py
 api/synchronous/api_core/tests/load_test.py
+classification/__init__.py
 classification/aggregate_classifier_probs.py
 classification/analyze_failed_images.py
 classification/cache_batchapi_outputs.py
 classification/create_classification_dataset.py
 classification/crop_detections.py
 classification/csv_to_json.py
 classification/detect_and_crop.py
@@ -82,17 +94,17 @@
 classification/save_mislabeled.py
 classification/train_classifier.py
 classification/train_classifier_tf.py
 classification/train_utils.py
 classification/efficientnet/__init__.py
 classification/efficientnet/model.py
 classification/efficientnet/utils.py
-data_management/cct_json_to_filename_json.py
+data_management/__init__.py
+data_management/camtrap_dp_to_coco.py
 data_management/cct_json_utils.py
-data_management/cct_to_csv.py
 data_management/cct_to_md.py
 data_management/cct_to_wi.py
 data_management/coco_to_labelme.py
 data_management/coco_to_yolo.py
 data_management/generate_crops_from_cct.py
 data_management/get_image_sizes.py
 data_management/labelme_to_coco.py
@@ -101,19 +113,20 @@
 data_management/read_exif.py
 data_management/remap_coco_categories.py
 data_management/remove_exif.py
 data_management/resize_coco_dataset.py
 data_management/wi_download_csv_to_coco.py
 data_management/yolo_output_to_md_output.py
 data_management/yolo_to_coco.py
+data_management/annotations/__init__.py
 data_management/annotations/annotation_constants.py
+data_management/databases/__init__.py
 data_management/databases/add_width_and_height_to_db.py
 data_management/databases/combine_coco_camera_traps_files.py
 data_management/databases/integrity_check_json_db.py
-data_management/databases/remove_corrupted_images_from_db.py
 data_management/databases/subset_json_db.py
 data_management/importers/add_nacti_sizes.py
 data_management/importers/add_timestamps_to_icct.py
 data_management/importers/animl_results_to_md_results.py
 data_management/importers/auckland_doc_test_to_json.py
 data_management/importers/auckland_doc_to_json.py
 data_management/importers/awc_to_json.py
@@ -151,56 +164,62 @@
 data_management/importers/wi_to_json.py
 data_management/importers/zamba_results_to_md_results.py
 data_management/importers/eMammal/copy_and_unzip_emammal.py
 data_management/importers/eMammal/eMammal_helpers.py
 data_management/importers/eMammal/make_eMammal_json.py
 data_management/importers/snapshotserengeti/make_full_SS_json.py
 data_management/importers/snapshotserengeti/make_per_season_SS_json.py
+data_management/lila/__init__.py
 data_management/lila/add_locations_to_island_camera_traps.py
 data_management/lila/add_locations_to_nacti.py
 data_management/lila/create_lila_blank_set.py
 data_management/lila/create_lila_test_set.py
 data_management/lila/create_links_to_md_results_files.py
 data_management/lila/download_lila_subset.py
 data_management/lila/generate_lila_per_image_labels.py
 data_management/lila/get_lila_annotation_counts.py
 data_management/lila/get_lila_image_counts.py
 data_management/lila/lila_common.py
 data_management/lila/test_lila_metadata_urls.py
+detection/__init__.py
 detection/process_video.py
 detection/pytorch_detector.py
 detection/run_detector.py
 detection/run_detector_batch.py
 detection/run_inference_with_yolov5_val.py
 detection/run_tiled_inference.py
 detection/tf_detector.py
 detection/video_utils.py
-detection/detector_training/copy_checkpoints.py
+detection/detector_training/__init__.py
 detection/detector_training/model_main_tf2.py
+docs/source/conf.py
+md_utils/__init__.py
 md_utils/azure_utils.py
 md_utils/ct_utils.py
 md_utils/directory_listing.py
 md_utils/md_tests.py
 md_utils/path_utils.py
 md_utils/process_utils.py
 md_utils/sas_blob_utils.py
 md_utils/split_locations_into_train_val.py
 md_utils/string_utils.py
 md_utils/url_utils.py
 md_utils/write_html_image_list.py
+md_visualization/__init__.py
 md_visualization/plot_utils.py
 md_visualization/render_images_with_thumbnails.py
 md_visualization/visualization_utils.py
 md_visualization/visualize_db.py
 md_visualization/visualize_detector_output.py
 megadetector.egg-info/PKG-INFO
 megadetector.egg-info/SOURCES.txt
 megadetector.egg-info/dependency_links.txt
 megadetector.egg-info/requires.txt
 megadetector.egg-info/top_level.txt
+taxonomy_mapping/__init__.py
 taxonomy_mapping/map_lila_taxonomy_to_wi_taxonomy.py
 taxonomy_mapping/map_new_lila_datasets.py
 taxonomy_mapping/prepare_lila_taxonomy_release.py
 taxonomy_mapping/preview_lila_taxonomy.py
 taxonomy_mapping/retrieve_sample_image.py
 taxonomy_mapping/simple_image_download.py
 taxonomy_mapping/species_lookup.py
```

### Comparing `megadetector-5.0.8/pyproject.toml` & `megadetector-5.0.9/pyproject.toml`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 [build-system]
 requires = [
   "setuptools>61"
 ]
 
 [project]
 name = "megadetector"
-version = "5.0.8"
+version = "5.0.9"
 description = "MegaDetector is an AI model that helps conservation folks spend less time doing boring things with camera trap images."
 readme = "README-package.md"
 # As of 2023.10.10, PyTorch isn't installable on Python 3.12
 requires-python = ">=3.9,<3.12"
 license = {file = "LICENSE"}
 keywords = ["camera traps", "conservation", "wildlife", "ai"]
 authors = [
@@ -43,14 +43,15 @@
   # "torch >= 2.0.1",
   # "torchvision >= 0.15.2",
   # "yolov5 == 7.0.13"
 ]
 
 [project.urls]
 "Homepage" = "https://github.com/agentmorris/MegaDetector"
+"Documentation" = "https://megadetector.readthedocs.io"
 "Bug Reports" = "https://github.com/agentmorris/MegaDetector/issues"
 "Source" = "https://github.com/agentmorris/MegaDetector"
 
 [tool.setuptools.packages.find]
 where = ["."]
 include = ["*"]
 exclude = [
```

### Comparing `megadetector-5.0.8/taxonomy_mapping/map_lila_taxonomy_to_wi_taxonomy.py` & `megadetector-5.0.9/taxonomy_mapping/map_lila_taxonomy_to_wi_taxonomy.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,484 +1,491 @@
-########
-#
-# map_lila_taxonomy_to_wi_taxonomy.py
-#
-# Loads the LILA category mapping (in which taxonomy information comes from an 
-# iNat taxonomy snapshot) and tries to map each class to the Wildlife Insights taxonomy.
-#
-########
+"""
+
+map_lila_taxonomy_to_wi_taxonomy.py
+
+Loads the LILA category mapping (in which taxonomy information comes from an 
+iNat taxonomy snapshot) and tries to map each class to the Wildlife Insights taxonomy.
+
+"""
 
 #%% Constants and imports
 
 import numpy as np
 import json
 import os
 
 from tqdm import tqdm
 
 from data_management.lila.lila_common import read_lila_taxonomy_mapping, \
     read_wildlife_insights_taxonomy_mapping
 
-lila_local_base = os.path.expanduser('~/lila')
-
-metadata_dir = os.path.join(lila_local_base, 'metadata')
-os.makedirs(metadata_dir, exist_ok=True)
-
-# Created by get_lila_category_list.py... contains counts for each category
-category_list_dir = os.path.join(lila_local_base, 'lila_categories_list')
-lila_dataset_to_categories_file = os.path.join(
-    category_list_dir, 'lila_dataset_to_categories.json')
-
-# This is a manually-curated file used to store mappings that had to be made manually
-lila_to_wi_supplementary_mapping_file = os.path.expanduser(
-    '~/git/MegaDetector/taxonomy_mapping/lila_to_wi_supplementary_mapping_file.csv')
-
-assert os.path.isfile(lila_dataset_to_categories_file)
 
-# This is the main output file from this whole process
-wi_mapping_table_file = os.path.join(lila_local_base,'lila_wi_mapping_table.csv')
+#%% Prevent execution during infrastructural imports
 
-id_column = 'uniqueIdentifier' # 'id'
+if False:
+    
+    #%%
+    
+    lila_local_base = os.path.expanduser('~/lila')
 
+    metadata_dir = os.path.join(lila_local_base, 'metadata')
+    os.makedirs(metadata_dir, exist_ok=True)
 
-#%% Load category and taxonomy files
+    # Created by get_lila_category_list.py... contains counts for each category
+    category_list_dir = os.path.join(lila_local_base, 'lila_categories_list')
+    lila_dataset_to_categories_file = os.path.join(
+        category_list_dir, 'lila_dataset_to_categories.json')
 
-with open(lila_dataset_to_categories_file, 'r') as f:
-    lila_dataset_to_categories = json.load(f)
+    # This is a manually-curated file used to store mappings that had to be made manually
+    lila_to_wi_supplementary_mapping_file = os.path.expanduser(
+        '~/git/MegaDetector/taxonomy_mapping/lila_to_wi_supplementary_mapping_file.csv')
 
-lila_taxonomy_df = read_lila_taxonomy_mapping(metadata_dir)
+    assert os.path.isfile(lila_dataset_to_categories_file)
 
-wi_taxonomy_df = read_wildlife_insights_taxonomy_mapping(metadata_dir)
+    # This is the main output file from this whole process
+    wi_mapping_table_file = os.path.join(lila_local_base,'lila_wi_mapping_table.csv')
 
+    id_column = 'uniqueIdentifier' # 'id'
 
-#%% Pull everything out of pandas
 
-lila_taxonomy = lila_taxonomy_df.to_dict('records')
-wi_taxonomy = wi_taxonomy_df.to_dict('records')
+    #%% Load category and taxonomy files
 
+    with open(lila_dataset_to_categories_file, 'r') as f:
+        lila_dataset_to_categories = json.load(f)
 
-#%% Cache WI taxonomy lookups
+    lila_taxonomy_df = read_lila_taxonomy_mapping(metadata_dir)
 
-def is_empty_wi_item(v):
-    if isinstance(v, str):
-        return len(v) == 0        
-    elif v is None:
-        return True
-    else:
-        assert isinstance(v, float) and np.isnan(v), 'Invalid item: {}'.format(str(v))
-        return True
+    wi_taxonomy_df = read_wildlife_insights_taxonomy_mapping(metadata_dir)
 
 
-def taxonomy_items_equal(a, b):
-    if isinstance(a, str) and (not isinstance(b, str)):
-        return False
-    if isinstance(b, str) and (not isinstance(a, str)):
-        return False
-    if (not isinstance(a, str)) or (not isinstance(b, str)):
-        assert (a is None and b is None) or (isinstance(a, float) and isinstance(b, float))
-        return True
-    return a == b
+    #%% Pull everything out of pandas
 
+    lila_taxonomy = lila_taxonomy_df.to_dict('records')
+    wi_taxonomy = wi_taxonomy_df.to_dict('records')
 
-for taxon in wi_taxonomy:
-    taxon['taxon_name'] = None
 
-from collections import defaultdict
-wi_taxon_name_to_taxa = defaultdict(list)
+    #%% Cache WI taxonomy lookups
 
-# This is just a handy lookup table that we'll use to debug mismatches
-wi_common_name_to_taxon = {}
-
-blank_taxon_name = 'blank'
-blank_taxon = None
+    def is_empty_wi_item(v):
+        if isinstance(v, str):
+            return len(v) == 0        
+        elif v is None:
+            return True
+        else:
+            assert isinstance(v, float) and np.isnan(v), 'Invalid item: {}'.format(str(v))
+            return True
 
-animal_taxon_name = 'animal'
-animal_taxon = None
 
-unknown_taxon_name = 'unknown'
-unknown_taxon = None
+    def taxonomy_items_equal(a, b):
+        if isinstance(a, str) and (not isinstance(b, str)):
+            return False
+        if isinstance(b, str) and (not isinstance(a, str)):
+            return False
+        if (not isinstance(a, str)) or (not isinstance(b, str)):
+            assert (a is None and b is None) or (isinstance(a, float) and isinstance(b, float))
+            return True
+        return a == b
 
-ignore_taxa = set(['No CV Result', 'CV Needed', 'CV Failed'])
 
-known_problematic_taxon_ids = ['f94e6d97-59cf-4d38-a05a-a75efdd2863b']
+    for taxon in wi_taxonomy:
+        taxon['taxon_name'] = None
 
-human_taxa = []
+    from collections import defaultdict
+    wi_taxon_name_to_taxa = defaultdict(list)
 
-# taxon = wi_taxonomy[21653]; print(taxon)
-for taxon in tqdm(wi_taxonomy):
+    # This is just a handy lookup table that we'll use to debug mismatches
+    wi_common_name_to_taxon = {}
 
-    taxon_name = None
+    blank_taxon_name = 'blank'
+    blank_taxon = None
 
-    assert taxon['taxonomyType'] == 'object' or taxon['taxonomyType'] == 'biological'
+    animal_taxon_name = 'animal'
+    animal_taxon = None
 
-    for k in taxon.keys():
-        v = taxon[k]
-        if isinstance(v,str):
-            taxon[k] = v.strip()
-            
-    if taxon['commonNameEnglish'] in ignore_taxa:
-        continue
+    unknown_taxon_name = 'unknown'
+    unknown_taxon = None
 
-    if isinstance(taxon['commonNameEnglish'], str):
+    ignore_taxa = set(['No CV Result', 'CV Needed', 'CV Failed'])
 
-        wi_common_name_to_taxon[taxon['commonNameEnglish'].strip(
-        ).lower()] = taxon
+    known_problematic_taxon_ids = ['f94e6d97-59cf-4d38-a05a-a75efdd2863b']
 
-        special_taxon = False
+    human_taxa = []
 
-        # Look for keywords that don't refer to specific taxa: blank/animal/unknown
-        if taxon['commonNameEnglish'].strip().lower() == blank_taxon_name:
-            blank_taxon = taxon
-            special_taxon = True
+    # taxon = wi_taxonomy[21653]; print(taxon)
+    for taxon in tqdm(wi_taxonomy):
 
-        elif taxon['commonNameEnglish'].strip().lower() == animal_taxon_name:
-            animal_taxon = taxon
-            special_taxon = True
+        taxon_name = None
 
-        elif taxon['commonNameEnglish'].strip().lower() == unknown_taxon_name:
-            unknown_taxon = taxon
-            special_taxon = True
+        assert taxon['taxonomyType'] == 'object' or taxon['taxonomyType'] == 'biological'
 
-        if special_taxon:
-            taxon_name = taxon['commonNameEnglish'].strip().lower()
-            taxon['taxon_name'] = taxon_name
-            wi_taxon_name_to_taxa[taxon_name].append(taxon)
+        for k in taxon.keys():
+            v = taxon[k]
+            if isinstance(v,str):
+                taxon[k] = v.strip()
+                
+        if taxon['commonNameEnglish'] in ignore_taxa:
             continue
 
-    # Do we have a species name?
-    if not is_empty_wi_item(taxon['species']):
+        if isinstance(taxon['commonNameEnglish'], str):
 
-        # If 'species' is populated, 'genus' should always be populated; one item currently breaks
-        # this rule.
-        assert not is_empty_wi_item(taxon['genus'])
-        
-        taxon_name = (taxon['genus'].strip() + ' ' +
-                      taxon['species'].strip()).strip().lower()
-        assert not is_empty_wi_item(taxon['class']) and \
-            not is_empty_wi_item(taxon['order']) and \
-            not is_empty_wi_item(taxon['family'])
+            wi_common_name_to_taxon[taxon['commonNameEnglish'].strip(
+            ).lower()] = taxon
 
-    elif not is_empty_wi_item(taxon['genus']):
+            special_taxon = False
 
-        assert not is_empty_wi_item(taxon['class']) and \
-            not is_empty_wi_item(taxon['order']) and \
-            not is_empty_wi_item(taxon['family'])
-        taxon_name = taxon['genus'].strip().lower()
+            # Look for keywords that don't refer to specific taxa: blank/animal/unknown
+            if taxon['commonNameEnglish'].strip().lower() == blank_taxon_name:
+                blank_taxon = taxon
+                special_taxon = True
+
+            elif taxon['commonNameEnglish'].strip().lower() == animal_taxon_name:
+                animal_taxon = taxon
+                special_taxon = True
+
+            elif taxon['commonNameEnglish'].strip().lower() == unknown_taxon_name:
+                unknown_taxon = taxon
+                special_taxon = True
+
+            if special_taxon:
+                taxon_name = taxon['commonNameEnglish'].strip().lower()
+                taxon['taxon_name'] = taxon_name
+                wi_taxon_name_to_taxa[taxon_name].append(taxon)
+                continue
 
-    elif not is_empty_wi_item(taxon['family']):
+        # Do we have a species name?
+        if not is_empty_wi_item(taxon['species']):
 
-        assert not is_empty_wi_item(taxon['class']) and \
-            not is_empty_wi_item(taxon['order'])
-        taxon_name = taxon['family'].strip().lower()
+            # If 'species' is populated, 'genus' should always be populated; one item currently breaks
+            # this rule.
+            assert not is_empty_wi_item(taxon['genus'])
+            
+            taxon_name = (taxon['genus'].strip() + ' ' +
+                        taxon['species'].strip()).strip().lower()
+            assert not is_empty_wi_item(taxon['class']) and \
+                not is_empty_wi_item(taxon['order']) and \
+                not is_empty_wi_item(taxon['family'])
 
-    elif not is_empty_wi_item(taxon['order']):
+        elif not is_empty_wi_item(taxon['genus']):
 
-        assert not is_empty_wi_item(taxon['class'])
-        taxon_name = taxon['order'].strip().lower()
+            assert not is_empty_wi_item(taxon['class']) and \
+                not is_empty_wi_item(taxon['order']) and \
+                not is_empty_wi_item(taxon['family'])
+            taxon_name = taxon['genus'].strip().lower()
 
-    elif not is_empty_wi_item(taxon['class']):
+        elif not is_empty_wi_item(taxon['family']):
 
-        taxon_name = taxon['class'].strip().lower()
+            assert not is_empty_wi_item(taxon['class']) and \
+                not is_empty_wi_item(taxon['order'])
+            taxon_name = taxon['family'].strip().lower()
 
-    if taxon_name is not None:
-        assert taxon['taxonomyType'] == 'biological'
-    else:
-        assert taxon['taxonomyType'] == 'object'
-        taxon_name = taxon['commonNameEnglish'].strip().lower()
+        elif not is_empty_wi_item(taxon['order']):
 
-    if taxon_name in wi_taxon_name_to_taxa:
-        if taxon[id_column] in known_problematic_taxon_ids:
-            print('Skipping problematic taxon ID {}'.format(taxon[id_column]))
-        else:
-            previous_taxa = wi_taxon_name_to_taxa[taxon_name]
-            for previous_taxon in previous_taxa:
-                for level in ['class', 'order', 'family', 'genus', 'species']:
-                    error_string = 'Error: taxon {} appeared previously in {} {} (as {}), now in {} {}'.format(
-                        taxon_name,
-                        level,previous_taxon[level],
-                        previous_taxon['taxon_name'],
-                        level,taxon[level])
-                    assert taxonomy_items_equal(previous_taxon[level], taxon[level]), error_string
-                
-    taxon['taxon_name'] = taxon_name
-    if taxon_name == 'homo sapiens':
-        human_taxa.append(taxon)
-    wi_taxon_name_to_taxa[taxon_name].append(taxon)
-
-# ...for each taxon
-
-assert unknown_taxon is not None
-assert animal_taxon is not None
-assert blank_taxon is not None
+            assert not is_empty_wi_item(taxon['class'])
+            taxon_name = taxon['order'].strip().lower()
 
+        elif not is_empty_wi_item(taxon['class']):
 
-#%% Find redundant taxa
+            taxon_name = taxon['class'].strip().lower()
 
-taxon_names_with_multiple_entries = []
-for wi_taxon_name in wi_taxon_name_to_taxa:
-    if len(wi_taxon_name_to_taxa[wi_taxon_name]) > 1:
-        taxon_names_with_multiple_entries.append(wi_taxon_name)
+        if taxon_name is not None:
+            assert taxon['taxonomyType'] == 'biological'
+        else:
+            assert taxon['taxonomyType'] == 'object'
+            taxon_name = taxon['commonNameEnglish'].strip().lower()
 
-print('{} names have multiple entries\n:'.format(len(taxon_names_with_multiple_entries)))
+        if taxon_name in wi_taxon_name_to_taxa:
+            if taxon[id_column] in known_problematic_taxon_ids:
+                print('Skipping problematic taxon ID {}'.format(taxon[id_column]))
+            else:
+                previous_taxa = wi_taxon_name_to_taxa[taxon_name]
+                for previous_taxon in previous_taxa:
+                    for level in ['class', 'order', 'family', 'genus', 'species']:
+                        error_string = 'Error: taxon {} appeared previously in {} {} (as {}), now in {} {}'.format(
+                            taxon_name,
+                            level,previous_taxon[level],
+                            previous_taxon['taxon_name'],
+                            level,taxon[level])
+                        assert taxonomy_items_equal(previous_taxon[level], taxon[level]), error_string
+                    
+        taxon['taxon_name'] = taxon_name
+        if taxon_name == 'homo sapiens':
+            human_taxa.append(taxon)
+        wi_taxon_name_to_taxa[taxon_name].append(taxon)
+
+    # ...for each taxon
+
+    assert unknown_taxon is not None
+    assert animal_taxon is not None
+    assert blank_taxon is not None
+
+
+    #%% Find redundant taxa
+
+    taxon_names_with_multiple_entries = []
+    for wi_taxon_name in wi_taxon_name_to_taxa:
+        if len(wi_taxon_name_to_taxa[wi_taxon_name]) > 1:
+            taxon_names_with_multiple_entries.append(wi_taxon_name)
+
+    print('{} names have multiple entries\n:'.format(len(taxon_names_with_multiple_entries)))
 
-for s in taxon_names_with_multiple_entries:
-    print(s)
+    for s in taxon_names_with_multiple_entries:
+        print(s)
 
-if False:
-    pass
+    if False:
+        pass
 
-    #%% Manual review of redundant taxa
-    
-    s = taxon_names_with_multiple_entries[15]
-    taxa = wi_taxon_name_to_taxa[s]
-    for t in taxa:
-        for k in t.keys():
-            print('{}: {}'.format(k,t[k]))
-        print()
-        # print(t,end='\n\n')
+        #%% Manual review of redundant taxa
+        
+        s = taxon_names_with_multiple_entries[15]
+        taxa = wi_taxon_name_to_taxa[s]
+        for t in taxa:
+            for k in t.keys():
+                print('{}: {}'.format(k,t[k]))
+            print()
+            # print(t,end='\n\n')
 
 
-#%% Clean up redundant taxa
+    #%% Clean up redundant taxa
 
-taxon_name_to_preferred_taxon_id = {}
+    taxon_name_to_preferred_taxon_id = {}
 
-# "helmeted guineafowl" vs "domestic guineafowl"
-taxon_name_to_preferred_taxon_id['numida meleagris'] = '83133617-8358-4910-82ee-4c23e40ba3dc' # 2005826 
+    # "helmeted guineafowl" vs "domestic guineafowl"
+    taxon_name_to_preferred_taxon_id['numida meleagris'] = '83133617-8358-4910-82ee-4c23e40ba3dc' # 2005826 
 
-# "domestic turkey" vs. "wild turkey"
-taxon_name_to_preferred_taxon_id['meleagris gallopavo'] = 'c10547c3-1748-48bf-a451-8066c820f22f' # 2021598 
+    # "domestic turkey" vs. "wild turkey"
+    taxon_name_to_preferred_taxon_id['meleagris gallopavo'] = 'c10547c3-1748-48bf-a451-8066c820f22f' # 2021598 
 
-# multiple sensible human entries
-taxon_name_to_preferred_taxon_id['homo sapiens'] = '990ae9dd-7a59-4344-afcb-1b7b21368000' # 2002045 
+    # multiple sensible human entries
+    taxon_name_to_preferred_taxon_id['homo sapiens'] = '990ae9dd-7a59-4344-afcb-1b7b21368000' # 2002045 
 
-# "domestic dog" and "dog-on-leash"
-taxon_name_to_preferred_taxon_id['canis familiaris'] = '3d80f1d6-b1df-4966-9ff4-94053c7a902a' # 2021548 
+    # "domestic dog" and "dog-on-leash"
+    taxon_name_to_preferred_taxon_id['canis familiaris'] = '3d80f1d6-b1df-4966-9ff4-94053c7a902a' # 2021548 
 
-# "small mammal" vs. "mammal"
-taxon_name_to_preferred_taxon_id['mammalia'] = 'f2d233e3-80e3-433d-9687-e29ecc7a467a' # 2021108 
+    # "small mammal" vs. "mammal"
+    taxon_name_to_preferred_taxon_id['mammalia'] = 'f2d233e3-80e3-433d-9687-e29ecc7a467a' # 2021108 
 
-# "Hispaniolan Mango" vs. NaN
-taxon_name_to_preferred_taxon_id['anthracothorax dominicus'] = 'f94e6d97-59cf-4d38-a05a-a75efdd2863b'
+    # "Hispaniolan Mango" vs. NaN
+    taxon_name_to_preferred_taxon_id['anthracothorax dominicus'] = 'f94e6d97-59cf-4d38-a05a-a75efdd2863b'
 
-# "millipedes" vs. "Millipede"
-taxon_name_to_preferred_taxon_id['diplopoda'] =  '065884eb-4e64-4233-84dc-de25bd06ffd2' # 2021760
+    # "millipedes" vs. "Millipede"
+    taxon_name_to_preferred_taxon_id['diplopoda'] =  '065884eb-4e64-4233-84dc-de25bd06ffd2' # 2021760
 
-# Different suborders: Squamata vs. Lacertilia
-taxon_name_to_preferred_taxon_id['squamata'] = '710c4066-bd5d-4313-bcf4-0217c4c84da7' # 2021703
+    # Different suborders: Squamata vs. Lacertilia
+    taxon_name_to_preferred_taxon_id['squamata'] = '710c4066-bd5d-4313-bcf4-0217c4c84da7' # 2021703
 
-# Redundancy (both "beautiful firetail")
-taxon_name_to_preferred_taxon_id['stagonopleura bella'] = '7fec8e7e-fd3b-4d7f-99fd-3ade6f3bbaa5' # 2021939
+    # Redundancy (both "beautiful firetail")
+    taxon_name_to_preferred_taxon_id['stagonopleura bella'] = '7fec8e7e-fd3b-4d7f-99fd-3ade6f3bbaa5' # 2021939
 
-# "yellow wagtail" vs. "yellow crowned-wagtail"
-taxon_name_to_preferred_taxon_id['motacilla flava'] = 'ac6669bc-9f9e-4473-b609-b9082f9bf50c' # 2016194 
+    # "yellow wagtail" vs. "yellow crowned-wagtail"
+    taxon_name_to_preferred_taxon_id['motacilla flava'] = 'ac6669bc-9f9e-4473-b609-b9082f9bf50c' # 2016194 
 
-# "dremomys species" vs. "dremomys genus"
-taxon_name_to_preferred_taxon_id['dremomys'] = '1507d153-af11-46f1-bfb8-77918d035ab3' # 2019370
+    # "dremomys species" vs. "dremomys genus"
+    taxon_name_to_preferred_taxon_id['dremomys'] = '1507d153-af11-46f1-bfb8-77918d035ab3' # 2019370
 
-# "elk" vs. "domestic elk"
-taxon_name_to_preferred_taxon_id['cervus canadensis'] = 'c5ce946f-8f0d-4379-992b-cc0982381f5e' 
+    # "elk" vs. "domestic elk"
+    taxon_name_to_preferred_taxon_id['cervus canadensis'] = 'c5ce946f-8f0d-4379-992b-cc0982381f5e' 
 
-# "American bison" vs. "domestic bison"
-taxon_name_to_preferred_taxon_id['bison bison'] = '539ebd55-081b-429a-9ae6-5a6a0f6999d4' # 2021593 
+    # "American bison" vs. "domestic bison"
+    taxon_name_to_preferred_taxon_id['bison bison'] = '539ebd55-081b-429a-9ae6-5a6a0f6999d4' # 2021593 
 
-# "woodrat or rat or mouse species" vs. "mouse species"
-taxon_name_to_preferred_taxon_id['muridae'] = 'e7503287-468c-45af-a1bd-a17821bb62f2' # 2021642 
+    # "woodrat or rat or mouse species" vs. "mouse species"
+    taxon_name_to_preferred_taxon_id['muridae'] = 'e7503287-468c-45af-a1bd-a17821bb62f2' # 2021642 
 
-# both "southern sand frog"
-taxon_name_to_preferred_taxon_id['tomopterna adiastola'] = 'a5dc63cb-41be-4090-84a7-b944b16dcee4' # 2021834
+    # both "southern sand frog"
+    taxon_name_to_preferred_taxon_id['tomopterna adiastola'] = 'a5dc63cb-41be-4090-84a7-b944b16dcee4' # 2021834
 
-# sericornis species vs. scrubwren species
-taxon_name_to_preferred_taxon_id['sericornis'] = 'ad82c0ac-df48-4028-bf71-d2b2f4bc4129' # 2021776
+    # sericornis species vs. scrubwren species
+    taxon_name_to_preferred_taxon_id['sericornis'] = 'ad82c0ac-df48-4028-bf71-d2b2f4bc4129' # 2021776
 
-    
-# taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0]
-for taxon_name in taxon_name_to_preferred_taxon_id.keys():
-    
-    candidate_taxa = wi_taxon_name_to_taxa[taxon_name]
-    
-    # If we've gotten this far, we should be choosing from multiple taxa.
-    #
-    # This will become untrue if any of these are resolved later, at which point we should
-    # remove them from taxon_name_to_preferred_id
-    assert len(candidate_taxa) > 1, 'Only one taxon available for {}'.format(taxon_name)
-    
-    # Choose the preferred taxa
-    selected_taxa = [t for t in candidate_taxa if t[id_column] == \
-                     taxon_name_to_preferred_taxon_id[taxon_name]]
-    assert len(selected_taxa) == 1
-    wi_taxon_name_to_taxa[taxon_name] = selected_taxa
+        
+    # taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0]
+    for taxon_name in taxon_name_to_preferred_taxon_id.keys():
+        
+        candidate_taxa = wi_taxon_name_to_taxa[taxon_name]
+        
+        # If we've gotten this far, we should be choosing from multiple taxa.
+        #
+        # This will become untrue if any of these are resolved later, at which point we should
+        # remove them from taxon_name_to_preferred_id
+        assert len(candidate_taxa) > 1, 'Only one taxon available for {}'.format(taxon_name)
+        
+        # Choose the preferred taxa
+        selected_taxa = [t for t in candidate_taxa if t[id_column] == \
+                        taxon_name_to_preferred_taxon_id[taxon_name]]
+        assert len(selected_taxa) == 1
+        wi_taxon_name_to_taxa[taxon_name] = selected_taxa
 
-wi_taxon_name_to_taxon = {}
+    wi_taxon_name_to_taxon = {}
 
-for taxon_name in wi_taxon_name_to_taxa.keys():
-    taxa = wi_taxon_name_to_taxa[taxon_name]
-    assert len(taxa) == 1
-    wi_taxon_name_to_taxon[taxon_name] = taxa[0]
+    for taxon_name in wi_taxon_name_to_taxa.keys():
+        taxa = wi_taxon_name_to_taxa[taxon_name]
+        assert len(taxa) == 1
+        wi_taxon_name_to_taxon[taxon_name] = taxa[0]
 
 
-#%% Read supplementary mappings
+    #%% Read supplementary mappings
 
-with open(lila_to_wi_supplementary_mapping_file, 'r') as f:
-    lines = f.readlines()
+    with open(lila_to_wi_supplementary_mapping_file, 'r') as f:
+        lines = f.readlines()
 
-supplementary_lila_query_to_wi_query = {}
+    supplementary_lila_query_to_wi_query = {}
 
-for line in lines:
-    # Each line is [lila query],[WI taxon name],[notes]
-    tokens = line.strip().split(',')
-    assert len(tokens) == 3
-    lila_query = tokens[0].strip().lower()
-    wi_taxon_name = tokens[1].strip().lower()
-    assert wi_taxon_name in wi_taxon_name_to_taxa
-    supplementary_lila_query_to_wi_query[lila_query] = wi_taxon_name
+    for line in lines:
+        # Each line is [lila query],[WI taxon name],[notes]
+        tokens = line.strip().split(',')
+        assert len(tokens) == 3
+        lila_query = tokens[0].strip().lower()
+        wi_taxon_name = tokens[1].strip().lower()
+        assert wi_taxon_name in wi_taxon_name_to_taxa
+        supplementary_lila_query_to_wi_query[lila_query] = wi_taxon_name
 
 
-#%% Map LILA categories to WI categories
+    #%% Map LILA categories to WI categories
 
-mismatches = set()
-mismatches_with_common_mappings = set()
-supplementary_mappings = set()
+    mismatches = set()
+    mismatches_with_common_mappings = set()
+    supplementary_mappings = set()
 
-all_searches = set()
+    all_searches = set()
 
-# Must be ordered from kingdom --> species
-lila_taxonomy_levels = ['kingdom', 'phylum', 'subphylum', 'superclass', 'class', 'subclass',
-                        'infraclass', 'superorder', 'order', 'suborder', 'infraorder',
-                        'superfamily', 'family', 'subfamily', 'tribe', 'genus', 'species']
+    # Must be ordered from kingdom --> species
+    lila_taxonomy_levels = ['kingdom', 'phylum', 'subphylum', 'superclass', 'class', 'subclass',
+                            'infraclass', 'superorder', 'order', 'suborder', 'infraorder',
+                            'superfamily', 'family', 'subfamily', 'tribe', 'genus', 'species']
 
-unknown_queries = set(
-    ['unidentifiable', 'other', 'unidentified', 'unknown', 'unclassifiable'])
-blank_queries = set(['empty'])
-animal_queries = set(['animalia'])
+    unknown_queries = set(
+        ['unidentifiable', 'other', 'unidentified', 'unknown', 'unclassifiable'])
+    blank_queries = set(['empty'])
+    animal_queries = set(['animalia'])
 
-lila_dataset_category_to_wi_taxon = {}
+    lila_dataset_category_to_wi_taxon = {}
 
-# i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon)
-for i_taxon, lila_taxon in enumerate(lila_taxonomy):
+    # i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon)
+    for i_taxon, lila_taxon in enumerate(lila_taxonomy):
 
-    query = None
+        query = None
 
-    lila_dataset_category = lila_taxon['dataset_name'] + ':' + lila_taxon['query']
-    
-    # Go from kingdom --> species, choosing the lowest-level description as the query
-    for level in lila_taxonomy_levels:
-        if isinstance(lila_taxon[level], str):
-            query = lila_taxon[level]
-            all_searches.add(query)
+        lila_dataset_category = lila_taxon['dataset_name'] + ':' + lila_taxon['query']
+        
+        # Go from kingdom --> species, choosing the lowest-level description as the query
+        for level in lila_taxonomy_levels:
+            if isinstance(lila_taxon[level], str):
+                query = lila_taxon[level]
+                all_searches.add(query)
 
-    if query is None:
-        # E.g., 'car'
-        query = lila_taxon['query']
+        if query is None:
+            # E.g., 'car'
+            query = lila_taxon['query']
 
-    wi_taxon = None
+        wi_taxon = None
 
-    if query in unknown_queries:
+        if query in unknown_queries:
 
-        wi_taxon = unknown_taxon
+            wi_taxon = unknown_taxon
 
-    elif query in blank_queries:
+        elif query in blank_queries:
 
-        wi_taxon = blank_taxon
+            wi_taxon = blank_taxon
 
-    elif query in animal_queries:
+        elif query in animal_queries:
 
-        wi_taxon = animal_taxon
+            wi_taxon = animal_taxon
 
-    elif query in wi_taxon_name_to_taxon:
+        elif query in wi_taxon_name_to_taxon:
 
-        wi_taxon = wi_taxon_name_to_taxon[query]
+            wi_taxon = wi_taxon_name_to_taxon[query]
 
-    elif query in supplementary_lila_query_to_wi_query:
+        elif query in supplementary_lila_query_to_wi_query:
 
-        wi_taxon = wi_taxon_name_to_taxon[supplementary_lila_query_to_wi_query[query]]
-        supplementary_mappings.add(query)
-        # print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))
+            wi_taxon = wi_taxon_name_to_taxon[supplementary_lila_query_to_wi_query[query]]
+            supplementary_mappings.add(query)
+            # print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))
 
-    else:
+        else:
 
-        # print('No match for {}'.format(query))
-        lila_common_name = lila_taxon['common_name']
+            # print('No match for {}'.format(query))
+            lila_common_name = lila_taxon['common_name']
 
-        if lila_common_name in wi_common_name_to_taxon:
-            wi_taxon = wi_common_name_to_taxon[lila_common_name]
-            wi_common_name = wi_taxon['commonNameEnglish']
-            wi_taxon_name = wi_taxon['taxon_name']
-            if False:
-                print('LILA common name {} maps to WI taxon {} ({})'.format(lila_common_name,
-                                                                            wi_taxon_name,
-                                                                            wi_common_name))
-            mismatches_with_common_mappings.add(query)
+            if lila_common_name in wi_common_name_to_taxon:
+                wi_taxon = wi_common_name_to_taxon[lila_common_name]
+                wi_common_name = wi_taxon['commonNameEnglish']
+                wi_taxon_name = wi_taxon['taxon_name']
+                if False:
+                    print('LILA common name {} maps to WI taxon {} ({})'.format(lila_common_name,
+                                                                                wi_taxon_name,
+                                                                                wi_common_name))
+                mismatches_with_common_mappings.add(query)
 
-        else:
+            else:
 
-            mismatches.add(query)
+                mismatches.add(query)
 
-    lila_dataset_category_to_wi_taxon[lila_dataset_category] = wi_taxon
+        lila_dataset_category_to_wi_taxon[lila_dataset_category] = wi_taxon
 
-# ...for each LILA taxon
+    # ...for each LILA taxon
 
-print('Of {} entities, there are {} mismatches ({} mapped by common name) ({} mapped by supplementary mapping file)'.format(
-    len(all_searches), len(mismatches), len(mismatches_with_common_mappings), len(supplementary_mappings)))
+    print('Of {} entities, there are {} mismatches ({} mapped by common name) ({} mapped by supplementary mapping file)'.format(
+        len(all_searches), len(mismatches), len(mismatches_with_common_mappings), len(supplementary_mappings)))
 
-assert len(mismatches) == 0
+    assert len(mismatches) == 0
 
 
-#%% Manual mapping
+    #%% Manual mapping
 
-if not os.path.isfile(lila_to_wi_supplementary_mapping_file):
-    print('Creating mapping file {}'.format(
-        lila_to_wi_supplementary_mapping_file))
-    with open(lila_to_wi_supplementary_mapping_file, 'w') as f:
-        for query in mismatches:
-            f.write(query + ',' + '\n')
-else:
-    print('{} exists, not re-writing'.format(lila_to_wi_supplementary_mapping_file))
+    if not os.path.isfile(lila_to_wi_supplementary_mapping_file):
+        print('Creating mapping file {}'.format(
+            lila_to_wi_supplementary_mapping_file))
+        with open(lila_to_wi_supplementary_mapping_file, 'w') as f:
+            for query in mismatches:
+                f.write(query + ',' + '\n')
+    else:
+        print('{} exists, not re-writing'.format(lila_to_wi_supplementary_mapping_file))
 
 
-#%% Build a dictionary from LILA dataset names and categories to LILA taxa
+    #%% Build a dictionary from LILA dataset names and categories to LILA taxa
 
-lila_dataset_category_to_lila_taxon = {}
+    lila_dataset_category_to_lila_taxon = {}
 
-# i_d = 0; d = lila_taxonomy[i_d]
-for i_d,d in enumerate(lila_taxonomy):
-    lila_dataset_category = d['dataset_name'] + ':' + d['query']
-    assert lila_dataset_category not in lila_dataset_category_to_lila_taxon
-    lila_dataset_category_to_lila_taxon[lila_dataset_category] = d
+    # i_d = 0; d = lila_taxonomy[i_d]
+    for i_d,d in enumerate(lila_taxonomy):
+        lila_dataset_category = d['dataset_name'] + ':' + d['query']
+        assert lila_dataset_category not in lila_dataset_category_to_lila_taxon
+        lila_dataset_category_to_lila_taxon[lila_dataset_category] = d
 
 
-#%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset
+    #%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset
 
-with open(wi_mapping_table_file,'w') as f:
-    
-    f.write('lila_dataset_name,lila_category_name,wi_guid,wi_taxon_name,wi_common,count\n')
-    
-    # dataset_name = list(lila_dataset_to_categories.keys())[0]
-    for dataset_name in lila_dataset_to_categories.keys():
+    with open(wi_mapping_table_file,'w') as f:
         
-        if '_bbox' in dataset_name:
-            continue
-        
-        dataset_categories = lila_dataset_to_categories[dataset_name]
+        f.write('lila_dataset_name,lila_category_name,wi_guid,wi_taxon_name,wi_common,count\n')
         
-        # dataset_category = dataset_categories[0]
-        for category in dataset_categories:
+        # dataset_name = list(lila_dataset_to_categories.keys())[0]
+        for dataset_name in lila_dataset_to_categories.keys():
             
-            lila_dataset_category = dataset_name + ':' + category['name'].strip().lower()
-            if '#' in lila_dataset_category:
+            if '_bbox' in dataset_name:
                 continue
-            assert lila_dataset_category in lila_dataset_category_to_lila_taxon
-            assert lila_dataset_category in lila_dataset_category_to_wi_taxon
-            assert 'count' in category
-    
-            wi_taxon = lila_dataset_category_to_wi_taxon[lila_dataset_category]
             
-            # Write out the dataset name, category name, WI GUID, WI scientific name, WI common name, 
-            # and count
-            s = f"{dataset_name},{category['name']},{wi_taxon['uniqueIdentifier']},"+\
-                f"{wi_taxon['taxon_name']},{wi_taxon['commonNameEnglish']},{category['count']}\n"
-            f.write(s)
+            dataset_categories = lila_dataset_to_categories[dataset_name]
             
-        # ...for each category in this dataset
-            
-    # ...for each dataset    
+            # dataset_category = dataset_categories[0]
+            for category in dataset_categories:
+                
+                lila_dataset_category = dataset_name + ':' + category['name'].strip().lower()
+                if '#' in lila_dataset_category:
+                    continue
+                assert lila_dataset_category in lila_dataset_category_to_lila_taxon
+                assert lila_dataset_category in lila_dataset_category_to_wi_taxon
+                assert 'count' in category
+        
+                wi_taxon = lila_dataset_category_to_wi_taxon[lila_dataset_category]
+                
+                # Write out the dataset name, category name, WI GUID, WI scientific name, WI common name, 
+                # and count
+                s = f"{dataset_name},{category['name']},{wi_taxon['uniqueIdentifier']},"+\
+                    f"{wi_taxon['taxon_name']},{wi_taxon['commonNameEnglish']},{category['count']}\n"
+                f.write(s)
+                
+            # ...for each category in this dataset
+                
+        # ...for each dataset    
 
-# ...with open()
+    # ...with open()
```

### Comparing `megadetector-5.0.8/taxonomy_mapping/map_new_lila_datasets.py` & `megadetector-5.0.9/taxonomy_mapping/map_new_lila_datasets.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,154 +1,154 @@
-########
-#
-# map_new_lila_datasets.py
-#
-# Given a subset of LILA datasets, find all the categories, and start the taxonomy
-# mapping process.
-#
-########
-
-#%% Constants and imports
-
-import os
-import json
-
-# Created by get_lila_category_list.py
-input_lila_category_list_file = os.path.expanduser('~/lila/lila_categories_list/lila_dataset_to_categories.json')
-
-output_file = os.path.expanduser('~/lila/lila_additions_2023.12.29.csv')
-
-datasets_to_map = [
-    'Trail Camera Images of New Zealand Animals'
-    ]
-
-
-#%% Initialize taxonomic lookup
-
-from taxonomy_mapping.species_lookup import (
-    initialize_taxonomy_lookup,
-    get_preferred_taxonomic_match)
-
-# from taxonomy_mapping.species_lookup import (
-#    get_taxonomic_info, print_taxonomy_matche)
-
-initialize_taxonomy_lookup(force_init=False)
-
-
-#%% Read the list of datasets
-
-with open(input_lila_category_list_file,'r') as f:
-    input_lila_categories = json.load(f)
-
-lila_datasets = set()
-
-for dataset_name in input_lila_categories.keys():
-    # The script that generates this dictionary creates a separate entry for bounding box
-    # metadata files, but those don't represent new dataset names
-    lila_datasets.add(dataset_name.replace('_bbox',''))
-    
-for s in datasets_to_map:
-    assert s in lila_datasets
-    
-    
-#%% Find all categories    
-
-category_mappings = []
-
-# dataset_name = datasets_to_map[0]
-for dataset_name in datasets_to_map:
-    
-    ds_categories = input_lila_categories[dataset_name]
-    for category in ds_categories:
-        category_name = category['name']
-        assert ':' not in category_name
-        mapping_name = dataset_name + ':' + category_name
-        category_mappings.append(mapping_name)
-        
-print('Need to create {} mappings'.format(len(category_mappings)))
-
-
-#%% Match every query against our taxonomies
-
-output_rows = []
-
-taxonomy_preference = 'inat'
-
-allow_non_preferred_matches = True
-
-# mapping_string = category_mappings[1]; print(mapping_string)
-for mapping_string in category_mappings:
-    
-    tokens = mapping_string.split(':')
-    assert len(tokens) == 2    
-
-    dataset_name = tokens[0]
-    query = tokens[1]
-
-    taxonomic_match = get_preferred_taxonomic_match(query,taxonomy_preference=taxonomy_preference)
-    
-    if (taxonomic_match.source == taxonomy_preference) or allow_non_preferred_matches:
-    
-        output_row = {
-            'dataset_name': dataset_name,
-            'query': query,
-            'source': taxonomic_match.source,
-            'taxonomy_level': taxonomic_match.taxonomic_level,
-            'scientific_name': taxonomic_match.scientific_name,
-            'common_name': taxonomic_match.common_name,
-            'taxonomy_string': taxonomic_match.taxonomy_string
-        }
-    
-    else:
-        
-        output_row = {
-            'dataset_name': dataset_name,
-            'query': query,
-            'source': '',
-            'taxonomy_level': '',
-            'scientific_name': '',
-            'common_name': '',
-            'taxonomy_string': ''
-        }
-        
-    output_rows.append(output_row)
-    
-# ...for each mapping    
-
-
-#%% Write output rows
-
-import os
-import pandas as pd
-
-assert not os.path.isfile(output_file), 'Delete the output file before re-generating'
-
-output_df = pd.DataFrame(data=output_rows, columns=[
-    'dataset_name', 'query', 'source', 'taxonomy_level',
-    'scientific_name', 'common_name', 'taxonomy_string'])
-output_df.to_csv(output_file, index=None, header=True)
-
-
-#%% Manual lookup
-
-if False:
-    
-    #%%
-    
-    # q = 'white-throated monkey'
-    # q = 'cingulata'
-    # q = 'notamacropus'
-    q = 'porzana'
-    taxonomy_preference = 'inat'
-    m = get_preferred_taxonomic_match(q,taxonomy_preference)
-    # print(m.scientific_name); import clipboard; clipboard.copy(m.scientific_name)
-    
-    if m is None:
-        print('No match')
-    else:
-        if m.source != taxonomy_preference:
-            print('\n*** non-preferred match ***\n')
-            # raise ValueError('')
-        print(m.source)
-        print(m.taxonomy_string)
-        # print(m.scientific_name); import clipboard; clipboard.copy(m.scientific_name)
-        import clipboard; clipboard.copy(m.taxonomy_string)
+"""
+
+map_new_lila_datasets.py
+
+Given a subset of LILA datasets, find all the categories, and start the taxonomy
+mapping process.
+
+"""
+
+#%% Constants and imports
+
+import os
+import json
+
+# Created by get_lila_category_list.py
+input_lila_category_list_file = os.path.expanduser('~/lila/lila_categories_list/lila_dataset_to_categories.json')
+
+output_file = os.path.expanduser('~/lila/lila_additions_2023.12.29.csv')
+
+datasets_to_map = [
+    'Trail Camera Images of New Zealand Animals'
+    ]
+
+
+#%% Initialize taxonomic lookup
+
+from taxonomy_mapping.species_lookup import (
+    initialize_taxonomy_lookup,
+    get_preferred_taxonomic_match)
+
+# from taxonomy_mapping.species_lookup import (
+#    get_taxonomic_info, print_taxonomy_matche)
+
+initialize_taxonomy_lookup(force_init=False)
+
+
+#%% Read the list of datasets
+
+with open(input_lila_category_list_file,'r') as f:
+    input_lila_categories = json.load(f)
+
+lila_datasets = set()
+
+for dataset_name in input_lila_categories.keys():
+    # The script that generates this dictionary creates a separate entry for bounding box
+    # metadata files, but those don't represent new dataset names
+    lila_datasets.add(dataset_name.replace('_bbox',''))
+    
+for s in datasets_to_map:
+    assert s in lila_datasets
+    
+    
+#%% Find all categories    
+
+category_mappings = []
+
+# dataset_name = datasets_to_map[0]
+for dataset_name in datasets_to_map:
+    
+    ds_categories = input_lila_categories[dataset_name]
+    for category in ds_categories:
+        category_name = category['name']
+        assert ':' not in category_name
+        mapping_name = dataset_name + ':' + category_name
+        category_mappings.append(mapping_name)
+        
+print('Need to create {} mappings'.format(len(category_mappings)))
+
+
+#%% Match every query against our taxonomies
+
+output_rows = []
+
+taxonomy_preference = 'inat'
+
+allow_non_preferred_matches = True
+
+# mapping_string = category_mappings[1]; print(mapping_string)
+for mapping_string in category_mappings:
+    
+    tokens = mapping_string.split(':')
+    assert len(tokens) == 2    
+
+    dataset_name = tokens[0]
+    query = tokens[1]
+
+    taxonomic_match = get_preferred_taxonomic_match(query,taxonomy_preference=taxonomy_preference)
+    
+    if (taxonomic_match.source == taxonomy_preference) or allow_non_preferred_matches:
+    
+        output_row = {
+            'dataset_name': dataset_name,
+            'query': query,
+            'source': taxonomic_match.source,
+            'taxonomy_level': taxonomic_match.taxonomic_level,
+            'scientific_name': taxonomic_match.scientific_name,
+            'common_name': taxonomic_match.common_name,
+            'taxonomy_string': taxonomic_match.taxonomy_string
+        }
+    
+    else:
+        
+        output_row = {
+            'dataset_name': dataset_name,
+            'query': query,
+            'source': '',
+            'taxonomy_level': '',
+            'scientific_name': '',
+            'common_name': '',
+            'taxonomy_string': ''
+        }
+        
+    output_rows.append(output_row)
+    
+# ...for each mapping    
+
+
+#%% Write output rows
+
+import os
+import pandas as pd
+
+assert not os.path.isfile(output_file), 'Delete the output file before re-generating'
+
+output_df = pd.DataFrame(data=output_rows, columns=[
+    'dataset_name', 'query', 'source', 'taxonomy_level',
+    'scientific_name', 'common_name', 'taxonomy_string'])
+output_df.to_csv(output_file, index=None, header=True)
+
+
+#%% Manual lookup
+
+if False:
+    
+    #%%
+    
+    # q = 'white-throated monkey'
+    # q = 'cingulata'
+    # q = 'notamacropus'
+    q = 'porzana'
+    taxonomy_preference = 'inat'
+    m = get_preferred_taxonomic_match(q,taxonomy_preference)
+    # print(m.scientific_name); import clipboard; clipboard.copy(m.scientific_name)
+    
+    if m is None:
+        print('No match')
+    else:
+        if m.source != taxonomy_preference:
+            print('\n*** non-preferred match ***\n')
+            # raise ValueError('')
+        print(m.source)
+        print(m.taxonomy_string)
+        # print(m.scientific_name); import clipboard; clipboard.copy(m.scientific_name)
+        import clipboard; clipboard.copy(m.taxonomy_string)
```

### Comparing `megadetector-5.0.8/taxonomy_mapping/prepare_lila_taxonomy_release.py` & `megadetector-5.0.9/taxonomy_mapping/prepare_lila_taxonomy_release.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,134 +1,142 @@
-########
-#
-# prepare_lila_taxonomy_release.py
-#
-# Given the private intermediate taxonomy mapping (produced by map_new_lila_datasets.py), 
-# prepare the public (release) taxonomy mapping file.
-#
-########
-
-#%% Imports and constants
-
-import os
-import json
-import pandas as pd
-
-lila_taxonomy_file = 'c:/git/agentmorrisprivate/lila-taxonomy/lila-taxonomy-mapping.csv'
-release_taxonomy_file = os.path.expanduser('~/lila/lila-taxonomy-mapping_release.csv')
-# import clipboard; clipboard.copy(release_taxonomy_file)
-
-# Created by get_lila_category_list.py... contains counts for each category
-lila_dataset_to_categories_file = os.path.expanduser('~/lila/lila_categories_list/lila_dataset_to_categories.json')
-
-assert os.path.isfile(lila_dataset_to_categories_file)
-assert os.path.isfile(lila_taxonomy_file)
-
-
-#%% Find out which categories are actually used
-
-df = pd.read_csv(lila_taxonomy_file)
-
-with open(lila_dataset_to_categories_file,'r') as f:
-    lila_dataset_to_categories = json.load(f)
-
-used_category_mappings = []
-
-# dataset_name = datasets_to_map[0]
-for dataset_name in lila_dataset_to_categories.keys():
-    
-    ds_categories = lila_dataset_to_categories[dataset_name]
-    for category in ds_categories:
-        category_name = category['name'].lower()
-        assert ':' not in category_name
-        mapping_name = dataset_name + ':' + category_name
-        used_category_mappings.append(mapping_name)
-
-df['used'] = False
-
-# i_row = 0; row = df.iloc[i_row]; row
-for i_row,row in df.iterrows():
-    ds_name = row['dataset_name']
-    query = row['query']
-    mapping_name = ds_name + ':' + query
-    if mapping_name in used_category_mappings:
-        df.loc[i_row,'used'] = True
-    else:
-        print('Dropping unused mapping {}'.format(mapping_name))
-
-df = df[df.used]
-df = df.drop('used',axis=1)
-
-
-#%% Generate the final output file
-
-assert not os.path.isfile(release_taxonomy_file)
-
-known_levels = ['stateofmatter',
-                     'kingdom',
-                     'phylum','subphylum',
-                     'superclass','class','subclass','infraclass',
-                     'superorder','order','parvorder','suborder','infraorder',
-                     'zoosection',
-                     'superfamily','family','subfamily','tribe',
-                     'genus',
-                     'species','subspecies','variety']
-
-levels_to_include = ['kingdom',
-                     'phylum','subphylum',
-                     'superclass','class','subclass','infraclass',
-                     'superorder','order','suborder','infraorder',
-                     'superfamily','family','subfamily','tribe',
-                     'genus',
-                     'species','subspecies','variety']
-
-levels_to_exclude = ['stateofmatter','zoosection','parvorder']
-
-for s in levels_to_exclude:
-    assert s not in levels_to_include
-    
-levels_used = set()
-
-# i_row = 0; row = df.iloc[i_row]; row
-for i_row,row in df.iterrows():
-    
-    if not isinstance(row['scientific_name'],str):
-        assert not isinstance(row['taxonomy_string'],str)
-        continue
-    
-    taxonomic_match = eval(row['taxonomy_string'])
-    
-    # match_at_level = taxonomic_match[0]
-    for match_at_level in taxonomic_match:
-        assert len(match_at_level) == 4
-        levels_used.add(match_at_level[1])
-        
-levels_used = [s for s in levels_used if isinstance(s,str)]
-
-for s in levels_used:
-    assert s in levels_to_exclude or s in levels_to_include, 'Unrecognized level {}'.format(s)
-
-for s in levels_to_include:
-    assert s in levels_used
-    
-for s in levels_to_include:
-    df[s] = ''
-    
-# i_row = 0; row = df.iloc[i_row]; row
-for i_row,row in df.iterrows():
-    
-    if not isinstance(row['scientific_name'],str):
-        assert not isinstance(row['taxonomy_string'],str)
-        continue
-    
-    # E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']
-    taxonomic_match = eval(row['taxonomy_string'])
-    
-    for match_at_level in taxonomic_match:
-        level = match_at_level[1]
-        if level in levels_to_include:
-            df.loc[i_row,level] = match_at_level[2]
-
-df = df.drop('source',axis=1)
-df.to_csv(release_taxonomy_file,header=True,index=False)
-
-print('Wrote final output to {}'.format(release_taxonomy_file))
+"""
+
+prepare_lila_taxonomy_release.py
+
+Given the private intermediate taxonomy mapping (produced by map_new_lila_datasets.py), 
+prepare the public (release) taxonomy mapping file.
+
+"""
+
+#%% Imports and constants
+
+import os
+import json
+import pandas as pd
+
+
+#%% Prevent execution during infrastructural imports
+
+if False:
+    
+    #%% Filenames
+    
+    lila_taxonomy_file = 'c:/git/agentmorrisprivate/lila-taxonomy/lila-taxonomy-mapping.csv'
+    release_taxonomy_file = os.path.expanduser('~/lila/lila-taxonomy-mapping_release.csv')
+    # import clipboard; clipboard.copy(release_taxonomy_file)
+
+    # Created by get_lila_category_list.py... contains counts for each category
+    lila_dataset_to_categories_file = os.path.expanduser('~/lila/lila_categories_list/lila_dataset_to_categories.json')
+
+    assert os.path.isfile(lila_dataset_to_categories_file)
+    assert os.path.isfile(lila_taxonomy_file)
+
+
+    #%% Find out which categories are actually used
+
+    df = pd.read_csv(lila_taxonomy_file)
+
+    with open(lila_dataset_to_categories_file,'r') as f:
+        lila_dataset_to_categories = json.load(f)
+
+    used_category_mappings = []
+
+    # dataset_name = datasets_to_map[0]
+    for dataset_name in lila_dataset_to_categories.keys():
+        
+        ds_categories = lila_dataset_to_categories[dataset_name]
+        for category in ds_categories:
+            category_name = category['name'].lower()
+            assert ':' not in category_name
+            mapping_name = dataset_name + ':' + category_name
+            used_category_mappings.append(mapping_name)
+
+    df['used'] = False
+
+    # i_row = 0; row = df.iloc[i_row]; row
+    for i_row,row in df.iterrows():
+        ds_name = row['dataset_name']
+        query = row['query']
+        mapping_name = ds_name + ':' + query
+        if mapping_name in used_category_mappings:
+            df.loc[i_row,'used'] = True
+        else:
+            print('Dropping unused mapping {}'.format(mapping_name))
+
+    df = df[df.used]
+    df = df.drop('used',axis=1)
+
+
+    #%% Generate the final output file
+
+    assert not os.path.isfile(release_taxonomy_file)
+
+    known_levels = ['stateofmatter', #noqa
+                        'kingdom',
+                        'phylum','subphylum',
+                        'superclass','class','subclass','infraclass',
+                        'superorder','order','parvorder','suborder','infraorder',
+                        'zoosection',
+                        'superfamily','family','subfamily','tribe',
+                        'genus',
+                        'species','subspecies','variety']
+
+    levels_to_include = ['kingdom',
+                        'phylum','subphylum',
+                        'superclass','class','subclass','infraclass',
+                        'superorder','order','suborder','infraorder',
+                        'superfamily','family','subfamily','tribe',
+                        'genus',
+                        'species','subspecies','variety']
+
+    levels_to_exclude = ['stateofmatter','zoosection','parvorder']
+
+    for s in levels_to_exclude:
+        assert s not in levels_to_include
+        
+    levels_used = set()
+
+    # i_row = 0; row = df.iloc[i_row]; row
+    for i_row,row in df.iterrows():
+        
+        if not isinstance(row['scientific_name'],str):
+            assert not isinstance(row['taxonomy_string'],str)
+            continue
+        
+        taxonomic_match = eval(row['taxonomy_string'])
+        
+        # match_at_level = taxonomic_match[0]
+        for match_at_level in taxonomic_match:
+            assert len(match_at_level) == 4
+            levels_used.add(match_at_level[1])
+            
+    levels_used = [s for s in levels_used if isinstance(s,str)]
+
+    for s in levels_used:
+        assert s in levels_to_exclude or s in levels_to_include, 'Unrecognized level {}'.format(s)
+
+    for s in levels_to_include:
+        assert s in levels_used
+        
+    for s in levels_to_include:
+        df[s] = ''
+        
+    # i_row = 0; row = df.iloc[i_row]; row
+    for i_row,row in df.iterrows():
+        
+        if not isinstance(row['scientific_name'],str):
+            assert not isinstance(row['taxonomy_string'],str)
+            continue
+        
+        # E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']
+        taxonomic_match = eval(row['taxonomy_string'])
+        
+        for match_at_level in taxonomic_match:
+            level = match_at_level[1]
+            if level in levels_to_include:
+                df.loc[i_row,level] = match_at_level[2]
+
+    df = df.drop('source',axis=1)
+    df.to_csv(release_taxonomy_file,header=True,index=False)
+
+    print('Wrote final output to {}'.format(release_taxonomy_file))
+
```

### Comparing `megadetector-5.0.8/taxonomy_mapping/preview_lila_taxonomy.py` & `megadetector-5.0.9/taxonomy_mapping/preview_lila_taxonomy.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,591 +1,591 @@
-########
-#
-# preview_lila_taxonomy.py
-#
-# Does some consistency-checking on the LILA taxonomy file, and generates
-# an HTML preview page that we can use to determine whether the mappings
-# make sense.
-#
-########
-
-#%% Imports and constants
-
-from tqdm import tqdm
-
-import os
-import pandas as pd
-
-# lila_taxonomy_file = r"c:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv"
-lila_taxonomy_file = os.path.expanduser('~/lila/lila_additions_2023.12.29.csv')
-
-preview_base = os.path.expanduser('~/lila/lila_taxonomy_preview')
-os.makedirs(preview_base,exist_ok=True)
-html_output_file = os.path.join(preview_base,'index.html')
-
-
-#%% Support functions
-
-def parse_taxonomy_string(taxonomy_string):
-
-    taxonomic_match = eval(taxonomy_string)        
-    matched_entity = taxonomic_match[0]
-    assert len(matched_entity) == 4
-    
-    level = matched_entity[1]
-    
-    scientific_name = matched_entity[2]
-    
-    common_names = matched_entity[3]
-    if len(common_names) == 1:
-        common_name = common_names[0]
-    else:
-        common_name = str(common_names)
-    
-    return scientific_name,common_name,level,taxonomic_match
-
-def taxonomy_string_to_common_name(taxonomy_string):
-    _,cn,_,_ = parse_taxonomy_string(taxonomy_string)
-    return cn
-
-def taxonomy_string_to_scientific(taxonomy_string):
-    sn,_,_,_ = parse_taxonomy_string(taxonomy_string)
-    return sn
-
-def taxonomy_string_to_level(taxonomy_string):
-    _,_,level,_ = parse_taxonomy_string(taxonomy_string)
-    return level
-
-
-#%% Read the taxonomy mapping file
-
-df = pd.read_csv(lila_taxonomy_file)
-
-
-#%% Prepare taxonomy lookup
-
-from taxonomy_mapping.species_lookup import (
-    initialize_taxonomy_lookup,
-    get_preferred_taxonomic_match)
-
-# from taxonomy_mapping.species_lookup import (
-#    get_taxonomic_info, print_taxonomy_matche)
-
-initialize_taxonomy_lookup()
-
-
-#%% Optionally remap all gbif-based mappings to inat (or vice-versa)
-
-if False:
-    
-    #%%
-    
-    source_mappings = ['gbif','manual']
-    target_mapping = 'inat'
-    valid_mappings = ['gbif','inat','manual']
-    
-    assert target_mapping in valid_mappings
-    for source_mapping in source_mappings:
-        assert source_mapping != target_mapping and \
-            source_mapping in valid_mappings
-    
-    n_remappings = 0
-    
-    # i_row = 1; row = df.iloc[i_row]; row
-    for i_row,row in df.iterrows():
-        
-        if row['source'] not in source_mappings:            
-            continue
-        
-        scientific_name = row['scientific_name']
-        old_common = taxonomy_string_to_common_name(row['taxonomy_string'])
-        
-        m = get_preferred_taxonomic_match(scientific_name,target_mapping)
-        
-        if m is None or m.source != target_mapping:
-            print('No mapping for {} ({}) ({})'.format(scientific_name,row['query'],old_common))
-            continue
-        
-        assert m.scientific_name == row['scientific_name']
-        
-        if m.taxonomic_level == 'variety' and row['taxonomy_level'] == 'subspecies':
-            pass
-        else:
-            assert m.taxonomic_level == row['taxonomy_level']
-        
-        new_common = taxonomy_string_to_common_name(m.taxonomy_string)
-        
-        if row['taxonomy_string'] != m.taxonomy_string:
-            print('Remapping {} ({} to {})'.format(scientific_name, old_common, new_common))
-            n_remappings += 1
-            df.loc[i_row,'taxonomy_string'] = m.taxonomy_string
-            
-        if row['source'] != 'manual':
-            df.loc[i_row,'source'] = m.source                        
-
-    # This should be zero for the release .csv
-    print('Made {} remappings'.format(n_remappings))
-    
-    #%%
-    
-    df.to_csv(lila_taxonomy_file.replace('.csv','_remapped.csv'),header=True,index=False)
-    
-
-#%% Check for mappings that disagree with the taxonomy string
-
-df = pd.read_csv(lila_taxonomy_file)
-
-n_taxonomy_changes = 0
-
-# Look for internal inconsistency
-for i_row,row in df.iterrows():
-    
-    sn = row['scientific_name']
-    if not isinstance(sn,str):
-        continue
-    
-    ts = row['taxonomy_string'] 
-    assert sn == taxonomy_string_to_scientific(ts)
-    
-    assert row['taxonomy_level'] == taxonomy_string_to_level(ts)
-
-# Look for outdated mappings
-taxonomy_preference = 'inat'
-
-# i_row = 0; row = df.iloc[i_row]
-for i_row,row in tqdm(df.iterrows(),total=len(df)):
-    
-    sn = row['scientific_name']
-    if not isinstance(sn,str):
-        continue
-    
-    m = get_preferred_taxonomic_match(sn,taxonomy_preference)
-    assert m.scientific_name == sn
-    
-    ts = row['taxonomy_string']
-    assert m.taxonomy_string[0:50] == ts[0:50], 'Mismatch for {}:\n\n{}\n\n{}\n'.format(
-        row['dataset_name'],ts,m.taxonomy_string)
-        
-    if ts != m.taxonomy_string:
-        n_taxonomy_changes += 1
-        df.loc[i_row,'taxonomy_string'] = m.taxonomy_string
-
-print('\nMade {} taxonomy changes'.format(n_taxonomy_changes))
-
-# Optionally re-write
-if False:
-    df.to_csv(lila_taxonomy_file,header=True,index=False)
-
-
-#%% List null mappings
-
-# These should all be things like "empty", "unidentified", "fire", "car", etc.
-
-# i_row = 0; row = df.iloc[i_row]
-for i_row,row in df.iterrows():
-    if (not isinstance(row['taxonomy_string'],str)) or (len(row['taxonomy_string']) == 0):
-        print('No mapping for {}:{}'.format(row['dataset_name'],row['query']))
-
-
-#%% List mappings with scientific names but no common names
-
-for i_row,row in df.iterrows():
-    cn = row['common_name']
-    sn = row['scientific_name']
-    ts = row['taxonomy_string']
-    if (isinstance(ts,str)) and (len(ts) >= 0):
-        if (not isinstance(cn,str)) or (len(cn) == 0):
-            print('No mapping for {}:{}:{}'.format(row['dataset_name'],row['query'],row['scientific_name']))
-
-
-#%% List mappings that map to different things in different data sets
-
-import numpy as np
-def isnan(x):
-    if not isinstance(x,float):
-        return False
-    return np.isnan(x)
-
-from collections import defaultdict
-query_to_rows = defaultdict(list)
-
-queries_with_multiple_mappings = set()
-
-n_suppressed = 0
-
-suppress_multiple_matches = [
-    ['porcupine','Snapshot Camdeboo','Idaho Camera Traps'],
-    ['porcupine','Snapshot Enonkishu','Idaho Camera Traps'],
-    ['porcupine','Snapshot Karoo','Idaho Camera Traps'],
-    ['porcupine','Snapshot Kgalagadi','Idaho Camera Traps'],
-    ['porcupine','Snapshot Kruger','Idaho Camera Traps'],
-    ['porcupine','Snapshot Mountain Zebra','Idaho Camera Traps'],
-    ['porcupine','Snapshot Serengeti','Idaho Camera Traps'],
-    
-    ['porcupine','Snapshot Serengeti','Snapshot Mountain Zebra'],
-    ['porcupine','Snapshot Serengeti','Snapshot Kruger'],
-    ['porcupine','Snapshot Serengeti','Snapshot Kgalagadi'],
-    ['porcupine','Snapshot Serengeti','Snapshot Karoo'],
-    ['porcupine','Snapshot Serengeti','Snapshot Camdeboo'],
-    
-    ['porcupine','Snapshot Enonkishu','Snapshot Camdeboo'],
-    ['porcupine','Snapshot Enonkishu','Snapshot Mountain Zebra'],
-    ['porcupine','Snapshot Enonkishu','Snapshot Kruger'],
-    ['porcupine','Snapshot Enonkishu','Snapshot Kgalagadi'],
-    ['porcupine','Snapshot Enonkishu','Snapshot Karoo'],
-    
-    ['kudu','Snapshot Serengeti','Snapshot Mountain Zebra'],
-    ['kudu','Snapshot Serengeti','Snapshot Kruger'],
-    ['kudu','Snapshot Serengeti','Snapshot Kgalagadi'],
-    ['kudu','Snapshot Serengeti','Snapshot Karoo'],
-    ['kudu','Snapshot Serengeti','Snapshot Camdeboo'],
-    
-    ['fox','Caltech Camera Traps','Channel Islands Camera Traps'],
-    ['fox','Idaho Camera Traps','Channel Islands Camera Traps'],
-    ['fox','Idaho Camera Traps','Caltech Camera Traps'],
-    
-    ['pangolin','Snapshot Serengeti','SWG Camera Traps'],
-    
-    ['deer', 'Wellington Camera Traps', 'Idaho Camera Traps'],
-    ['deer', 'Wellington Camera Traps', 'Caltech Camera Traps'],
-    
-    ['unknown cervid', 'WCS Camera Traps', 'Idaho Camera Traps']
-    
-]
-
-for i_row,row in df.iterrows():
-    
-    query = row['query']
-    taxonomy_string = row['taxonomy_string']
-    
-    for previous_i_row in query_to_rows[query]:
-        
-        previous_row = df.iloc[previous_i_row]
-        assert previous_row['query'] == query
-        query_match = False
-        if isnan(row['taxonomy_string']):
-            query_match = isnan(previous_row['taxonomy_string'])
-        elif isnan(previous_row['taxonomy_string']):
-            query_match = isnan(row['taxonomy_string'])
-        else:
-            query_match = previous_row['taxonomy_string'][0:10] == taxonomy_string[0:10]
-        
-        if not query_match:
-            
-            suppress = False
-            
-            # x = suppress_multiple_matches[-1]
-            for x in suppress_multiple_matches:
-                if x[0] == query and \
-                    ( \
-                    (x[1] == row['dataset_name'] and x[2] == previous_row['dataset_name']) \
-                    or \
-                    (x[2] == row['dataset_name'] and x[1] == previous_row['dataset_name']) \
-                    ):
-                    suppress = True
-                    n_suppressed += 1
-                    break
-                
-            if not suppress:
-                print('Query {} in {} and {}:\n\n{}\n\n{}\n'.format(
-                    query, row['dataset_name'], previous_row['dataset_name'],
-                    taxonomy_string, previous_row['taxonomy_string']))
-                
-            queries_with_multiple_mappings.add(query)
-                
-    # ...for each row where we saw this query
-    
-    query_to_rows[query].append(i_row)
-    
-# ...for each row
-
-print('Found {} queries with multiple mappings ({} occurrences suppressed)'.format(
-    len(queries_with_multiple_mappings),n_suppressed))
-
-
-#%% Verify that nothing "unidentified" maps to a species or subspecies
-
-# E.g., "unidentified skunk" should never map to a specific species of skunk
-
-allowable_unknown_species = [
-    'unknown_tayra' # AFAIK this is a unique species, I'm not sure what's implied here
-]
-
-unk_queries = ['skunk']
-for i_row,row in df.iterrows():
-
-    query = row['query']
-    level = row['taxonomy_level']
-
-    if not isinstance(level,str):
-        assert not isinstance(row['taxonomy_string'],str)
-        continue
-
-    if ( \
-        'unidentified' in query or \
-        ('unk' in query and ('skunk' not in query and 'chipmunk' not in query))\
-        ) \
-        and \
-        ('species' in level):
-        
-        if query not in allowable_unknown_species:
-            
-            print('Warning: query {}:{} maps to {} {}'.format(
-                row['dataset_name'],
-                row['query'],
-                row['taxonomy_level'],
-                row['scientific_name']
-                ))
-
-
-#%% Make sure there are valid source and level values for everything with a mapping
-
-for i_row,row in df.iterrows():
-    if isinstance(row['scientific_name'],str):
-        if 'source' in row:
-            assert isinstance(row['source'],str)
-        assert isinstance(row['taxonomy_level'],str)
-        
-
-#%% Find WCS mappings that aren't species or aren't the same as the input
-
-# WCS used scientific names, so these remappings are slightly more controversial
-# then the standard remappings.
-
-# row = df.iloc[-500]
-for i_row,row in df.iterrows():
-    
-    if not isinstance(row['scientific_name'],str):
-        continue
-    if 'WCS' not in row['dataset_name']:
-        continue
-    
-    query = row['query']
-    scientific_name = row['scientific_name']
-    common_name = row['common_name']
-    level = row['taxonomy_level']    
-    taxonomy_string = row['taxonomy_string']
-    
-    common_name_from_taxonomy = taxonomy_string_to_common_name(taxonomy_string)   
-    query_string = query.replace(' sp','')
-    query_string = query_string.replace('unknown ','')
-    
-    # Anything marked "species" or "unknown" by definition doesn't map to a species,
-    # so ignore these.
-    if (' sp' not in query) and ('unknown' not in query) and \
-        (level != 'species') and (level != 'subspecies'):
-        print('WCS query {} ({}) remapped to {} {} ({})'.format(
-            query,common_name,level,scientific_name,common_name_from_taxonomy))
-
-    if query_string != scientific_name:        
-        pass
-        # print('WCS query {} ({}) remapped to {} ({})'.format(
-        #     query,common_name,scientific_name,common_names_from_taxonomy))
-
-
-#%% Download sample images for all scientific names
-
-remapped_queries = {'papio':'papio+baboon',
-                    'damaliscus lunatus jimela':'damaliscus lunatus',
-                    'mazama':'genus+mazama',
-                    'mirafra':'genus+mirafra'}
-
-import os
-from taxonomy_mapping import retrieve_sample_image
-
-scientific_name_to_paths = {}
-image_base = os.path.join(preview_base,'images')
-images_per_query = 15
-min_valid_images_per_query = 3
-min_valid_image_size = 3000
-
-# TODO: trivially prallelizable
-#
-# i_row = 0; row = df.iloc[i_row]
-for i_row,row in df.iterrows():
-    
-    s = row['scientific_name']
-    
-    if (not isinstance(s,str)) or (len(s)==0):
-        continue
-    
-    query = s.replace(' ','+')
-    
-    if query in remapped_queries:
-        query = remapped_queries[query]
-        
-    query_folder = os.path.join(image_base,query)
-    os.makedirs(query_folder,exist_ok=True)
-    
-    # Check whether we already have enough images for this query
-    image_files = os.listdir(query_folder)
-    image_fullpaths = [os.path.join(query_folder,fn) for fn in image_files]
-    sizes = [os.path.getsize(p) for p in image_fullpaths]
-    sizes_above_threshold = [x for x in sizes if x > min_valid_image_size]
-    if len(sizes_above_threshold) > min_valid_images_per_query:
-        print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))
-        continue
-
-    # Check whether we've already run this query for a previous row
-    if query in scientific_name_to_paths:
-        continue
-    
-    print('Processing query {} of {} ({})'.format(i_row,len(df),query))
-    paths = retrieve_sample_image.download_images(query=query,
-                                             output_directory=image_base,
-                                             limit=images_per_query,
-                                             verbose=True)
-    print('Downloaded {} images for {}'.format(len(paths),query))
-    scientific_name_to_paths[query] = paths
-
-# ...for each row in the mapping table
-
-
-#%% Rename .jpeg to .jpg
-
-from md_utils import path_utils
-all_images = path_utils.recursive_file_list(image_base,False)
-
-for fn in tqdm(all_images):
-    if fn.lower().endswith('.jpeg'):
-        new_fn = fn[0:-5] + '.jpg'
-        os.rename(fn, new_fn)
-
-
-#%% Choose representative images for each scientific name
-
-# Specifically, sort by size, and take the largest unique sizes. Very small files tend
-# to be bogus thumbnails, etc.
-
-max_images_per_query = 4
-scientific_name_to_preferred_images = {}
-
-# s = list(scientific_name_to_paths.keys())[0]
-for s in list(df.scientific_name):
-    
-    if not isinstance(s,str):
-        continue
-    
-    query = s.replace(' ','+')
-    
-    if query in remapped_queries:
-        query = remapped_queries[query]
-    
-    query_folder = os.path.join(image_base,query)
-    assert os.path.isdir(query_folder)
-    image_files = os.listdir(query_folder)
-    image_fullpaths = [os.path.join(query_folder,fn) for fn in image_files]    
-    sizes = [os.path.getsize(p) for p in image_fullpaths]
-    path_to_size = {}
-    for i_fp,fp in enumerate(image_fullpaths):
-        path_to_size[fp] = sizes[i_fp]
-    paths_by_size = [x for _, x in sorted(zip(sizes, image_fullpaths),reverse=True)]
-    
-    # Be suspicious of duplicate sizes
-    b_duplicate_sizes = [False] * len(paths_by_size)
-    
-    for i_path,p in enumerate(paths_by_size):
-        if i_path == len(paths_by_size) - 1:
-            continue
-        if path_to_size[p] == path_to_size[paths_by_size[i_path+1]]:
-            b_duplicate_sizes[i_path] = True
-    
-    paths_by_size_non_dup = [i for (i, v) in zip(paths_by_size, b_duplicate_sizes) if not v]
-    
-    preferred_paths = paths_by_size_non_dup[:max_images_per_query]
-    scientific_name_to_preferred_images[s] = preferred_paths
-
-# ...for each scientific name    
-
-
-#%% Delete unused images
-
-used_images = []
-for images in scientific_name_to_preferred_images.values():
-    used_images.extend(images)
-    
-print('Using a total of {} images'.format(len(used_images)))
-used_images_set = set(used_images)
-
-from md_utils import path_utils
-all_images = path_utils.recursive_file_list(image_base,False)
-
-unused_images = []
-for fn in all_images:
-    if fn not in used_images_set:
-        unused_images.append(fn)
-
-print('{} of {} files unused (diff {})'.format(len(unused_images),len(all_images),
-                                               len(all_images) - len(unused_images)))
-
-for fn in tqdm(unused_images):
-    os.remove(fn)    
-
-
-#%% Produce HTML preview
-
-with open(html_output_file, 'w', encoding='utf-8') as f:
-    
-    f.write('<html><head></head><body>\n')
-
-    names = scientific_name_to_preferred_images.keys()
-    names = sorted(names)
-    
-    f.write('<p class="speciesinfo_p" style="font-weight:bold;font-size:130%">'
-            'dataset_name: <b><u>category</u></b> mapped to taxonomy_level scientific_name (taxonomic_common_name) (manual_common_name)</p>\n'
-            '</p>')
-
-    # i_row = 2; row = df.iloc[i_row]
-    for i_row, row in tqdm(df.iterrows(), total=len(df)):
-
-        s = row['scientific_name']
-        
-        taxonomy_string = row['taxonomy_string']
-        if isinstance(taxonomy_string,str):
-            taxonomic_match = eval(taxonomy_string)        
-            matched_entity = taxonomic_match[0]
-            assert len(matched_entity) == 4
-            common_names = matched_entity[3]
-            if len(common_names) == 1:
-                common_name_string = common_names[0]
-            else:
-                common_name_string = str(common_names)
-        else:
-            common_name_string = ''
-
-        f.write('<p class="speciesinfo_p" style="font-weight:bold;font-size:130%">')
-
-        if isinstance(row.scientific_name,str):
-            output_string = '{}: <b><u>{}</u></b> mapped to {} {} ({}) ({})</p>\n'.format(
-                row.dataset_name, row.query, 
-                row.taxonomy_level, row.scientific_name, common_name_string,
-                row.common_name)
-            f.write(output_string)
-        else:
-            f.write('{}: <b><u>{}</u></b> unmapped'.format(row.dataset_name,row.query))
-
-        if s is None or s not in names:
-            f.write('<p class="content_p">no images available</p>')
-        else:
-            image_paths = scientific_name_to_preferred_images[s]
-            basedir = os.path.dirname(html_output_file)
-            relative_paths = [os.path.relpath(p,basedir) for p in image_paths]
-            image_paths = [s.replace('\\','/') for s in relative_paths]
-            n_images = len(image_paths)
-            # image_paths = [os.path.relpath(p, output_base) for p in image_paths]
-            image_width_percent = round(100 / n_images)
-            f.write('<table class="image_table"><tr>\n')
-            for image_path in image_paths:
-                f.write('<td style="vertical-align:top;" width="{}%">'
-                        '<img src="{}" style="display:block; width:100%; vertical-align:top; height:auto;">'
-                        '</td>\n'.format(image_width_percent, image_path))
-            f.write('</tr></table>\n')
-
-    # ...for each row
-
-    f.write('</body></html>\n')
-
-
-#%% Open HTML preview
-
-from md_utils.path_utils import open_file
-open_file(html_output_file)
+"""
+
+preview_lila_taxonomy.py
+
+Does some consistency-checking on the LILA taxonomy file, and generates
+an HTML preview page that we can use to determine whether the mappings
+make sense.
+
+"""
+
+#%% Imports and constants
+
+from tqdm import tqdm
+
+import os
+import pandas as pd
+
+# lila_taxonomy_file = r"c:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv"
+lila_taxonomy_file = os.path.expanduser('~/lila/lila_additions_2023.12.29.csv')
+
+preview_base = os.path.expanduser('~/lila/lila_taxonomy_preview')
+os.makedirs(preview_base,exist_ok=True)
+html_output_file = os.path.join(preview_base,'index.html')
+
+
+#%% Support functions
+
+def parse_taxonomy_string(taxonomy_string):
+
+    taxonomic_match = eval(taxonomy_string)        
+    matched_entity = taxonomic_match[0]
+    assert len(matched_entity) == 4
+    
+    level = matched_entity[1]
+    
+    scientific_name = matched_entity[2]
+    
+    common_names = matched_entity[3]
+    if len(common_names) == 1:
+        common_name = common_names[0]
+    else:
+        common_name = str(common_names)
+    
+    return scientific_name,common_name,level,taxonomic_match
+
+def taxonomy_string_to_common_name(taxonomy_string):
+    _,cn,_,_ = parse_taxonomy_string(taxonomy_string)
+    return cn
+
+def taxonomy_string_to_scientific(taxonomy_string):
+    sn,_,_,_ = parse_taxonomy_string(taxonomy_string)
+    return sn
+
+def taxonomy_string_to_level(taxonomy_string):
+    _,_,level,_ = parse_taxonomy_string(taxonomy_string)
+    return level
+
+
+#%% Read the taxonomy mapping file
+
+df = pd.read_csv(lila_taxonomy_file)
+
+
+#%% Prepare taxonomy lookup
+
+from taxonomy_mapping.species_lookup import (
+    initialize_taxonomy_lookup,
+    get_preferred_taxonomic_match)
+
+# from taxonomy_mapping.species_lookup import (
+#    get_taxonomic_info, print_taxonomy_matche)
+
+initialize_taxonomy_lookup()
+
+
+#%% Optionally remap all gbif-based mappings to inat (or vice-versa)
+
+if False:
+    
+    #%%
+    
+    source_mappings = ['gbif','manual']
+    target_mapping = 'inat'
+    valid_mappings = ['gbif','inat','manual']
+    
+    assert target_mapping in valid_mappings
+    for source_mapping in source_mappings:
+        assert source_mapping != target_mapping and \
+            source_mapping in valid_mappings
+    
+    n_remappings = 0
+    
+    # i_row = 1; row = df.iloc[i_row]; row
+    for i_row,row in df.iterrows():
+        
+        if row['source'] not in source_mappings:            
+            continue
+        
+        scientific_name = row['scientific_name']
+        old_common = taxonomy_string_to_common_name(row['taxonomy_string'])
+        
+        m = get_preferred_taxonomic_match(scientific_name,target_mapping)
+        
+        if m is None or m.source != target_mapping:
+            print('No mapping for {} ({}) ({})'.format(scientific_name,row['query'],old_common))
+            continue
+        
+        assert m.scientific_name == row['scientific_name']
+        
+        if m.taxonomic_level == 'variety' and row['taxonomy_level'] == 'subspecies':
+            pass
+        else:
+            assert m.taxonomic_level == row['taxonomy_level']
+        
+        new_common = taxonomy_string_to_common_name(m.taxonomy_string)
+        
+        if row['taxonomy_string'] != m.taxonomy_string:
+            print('Remapping {} ({} to {})'.format(scientific_name, old_common, new_common))
+            n_remappings += 1
+            df.loc[i_row,'taxonomy_string'] = m.taxonomy_string
+            
+        if row['source'] != 'manual':
+            df.loc[i_row,'source'] = m.source                        
+
+    # This should be zero for the release .csv
+    print('Made {} remappings'.format(n_remappings))
+    
+    #%%
+    
+    df.to_csv(lila_taxonomy_file.replace('.csv','_remapped.csv'),header=True,index=False)
+    
+
+#%% Check for mappings that disagree with the taxonomy string
+
+df = pd.read_csv(lila_taxonomy_file)
+
+n_taxonomy_changes = 0
+
+# Look for internal inconsistency
+for i_row,row in df.iterrows():
+    
+    sn = row['scientific_name']
+    if not isinstance(sn,str):
+        continue
+    
+    ts = row['taxonomy_string'] 
+    assert sn == taxonomy_string_to_scientific(ts)
+    
+    assert row['taxonomy_level'] == taxonomy_string_to_level(ts)
+
+# Look for outdated mappings
+taxonomy_preference = 'inat'
+
+# i_row = 0; row = df.iloc[i_row]
+for i_row,row in tqdm(df.iterrows(),total=len(df)):
+    
+    sn = row['scientific_name']
+    if not isinstance(sn,str):
+        continue
+    
+    m = get_preferred_taxonomic_match(sn,taxonomy_preference)
+    assert m.scientific_name == sn
+    
+    ts = row['taxonomy_string']
+    assert m.taxonomy_string[0:50] == ts[0:50], 'Mismatch for {}:\n\n{}\n\n{}\n'.format(
+        row['dataset_name'],ts,m.taxonomy_string)
+        
+    if ts != m.taxonomy_string:
+        n_taxonomy_changes += 1
+        df.loc[i_row,'taxonomy_string'] = m.taxonomy_string
+
+print('\nMade {} taxonomy changes'.format(n_taxonomy_changes))
+
+# Optionally re-write
+if False:
+    df.to_csv(lila_taxonomy_file,header=True,index=False)
+
+
+#%% List null mappings
+
+# These should all be things like "empty", "unidentified", "fire", "car", etc.
+
+# i_row = 0; row = df.iloc[i_row]
+for i_row,row in df.iterrows():
+    if (not isinstance(row['taxonomy_string'],str)) or (len(row['taxonomy_string']) == 0):
+        print('No mapping for {}:{}'.format(row['dataset_name'],row['query']))
+
+
+#%% List mappings with scientific names but no common names
+
+for i_row,row in df.iterrows():
+    cn = row['common_name']
+    sn = row['scientific_name']
+    ts = row['taxonomy_string']
+    if (isinstance(ts,str)) and (len(ts) >= 0):
+        if (not isinstance(cn,str)) or (len(cn) == 0):
+            print('No mapping for {}:{}:{}'.format(row['dataset_name'],row['query'],row['scientific_name']))
+
+
+#%% List mappings that map to different things in different data sets
+
+import numpy as np
+def isnan(x):
+    if not isinstance(x,float):
+        return False
+    return np.isnan(x)
+
+from collections import defaultdict
+query_to_rows = defaultdict(list)
+
+queries_with_multiple_mappings = set()
+
+n_suppressed = 0
+
+suppress_multiple_matches = [
+    ['porcupine','Snapshot Camdeboo','Idaho Camera Traps'],
+    ['porcupine','Snapshot Enonkishu','Idaho Camera Traps'],
+    ['porcupine','Snapshot Karoo','Idaho Camera Traps'],
+    ['porcupine','Snapshot Kgalagadi','Idaho Camera Traps'],
+    ['porcupine','Snapshot Kruger','Idaho Camera Traps'],
+    ['porcupine','Snapshot Mountain Zebra','Idaho Camera Traps'],
+    ['porcupine','Snapshot Serengeti','Idaho Camera Traps'],
+    
+    ['porcupine','Snapshot Serengeti','Snapshot Mountain Zebra'],
+    ['porcupine','Snapshot Serengeti','Snapshot Kruger'],
+    ['porcupine','Snapshot Serengeti','Snapshot Kgalagadi'],
+    ['porcupine','Snapshot Serengeti','Snapshot Karoo'],
+    ['porcupine','Snapshot Serengeti','Snapshot Camdeboo'],
+    
+    ['porcupine','Snapshot Enonkishu','Snapshot Camdeboo'],
+    ['porcupine','Snapshot Enonkishu','Snapshot Mountain Zebra'],
+    ['porcupine','Snapshot Enonkishu','Snapshot Kruger'],
+    ['porcupine','Snapshot Enonkishu','Snapshot Kgalagadi'],
+    ['porcupine','Snapshot Enonkishu','Snapshot Karoo'],
+    
+    ['kudu','Snapshot Serengeti','Snapshot Mountain Zebra'],
+    ['kudu','Snapshot Serengeti','Snapshot Kruger'],
+    ['kudu','Snapshot Serengeti','Snapshot Kgalagadi'],
+    ['kudu','Snapshot Serengeti','Snapshot Karoo'],
+    ['kudu','Snapshot Serengeti','Snapshot Camdeboo'],
+    
+    ['fox','Caltech Camera Traps','Channel Islands Camera Traps'],
+    ['fox','Idaho Camera Traps','Channel Islands Camera Traps'],
+    ['fox','Idaho Camera Traps','Caltech Camera Traps'],
+    
+    ['pangolin','Snapshot Serengeti','SWG Camera Traps'],
+    
+    ['deer', 'Wellington Camera Traps', 'Idaho Camera Traps'],
+    ['deer', 'Wellington Camera Traps', 'Caltech Camera Traps'],
+    
+    ['unknown cervid', 'WCS Camera Traps', 'Idaho Camera Traps']
+    
+]
+
+for i_row,row in df.iterrows():
+    
+    query = row['query']
+    taxonomy_string = row['taxonomy_string']
+    
+    for previous_i_row in query_to_rows[query]:
+        
+        previous_row = df.iloc[previous_i_row]
+        assert previous_row['query'] == query
+        query_match = False
+        if isnan(row['taxonomy_string']):
+            query_match = isnan(previous_row['taxonomy_string'])
+        elif isnan(previous_row['taxonomy_string']):
+            query_match = isnan(row['taxonomy_string'])
+        else:
+            query_match = previous_row['taxonomy_string'][0:10] == taxonomy_string[0:10]
+        
+        if not query_match:
+            
+            suppress = False
+            
+            # x = suppress_multiple_matches[-1]
+            for x in suppress_multiple_matches:
+                if x[0] == query and \
+                    ( \
+                    (x[1] == row['dataset_name'] and x[2] == previous_row['dataset_name']) \
+                    or \
+                    (x[2] == row['dataset_name'] and x[1] == previous_row['dataset_name']) \
+                    ):
+                    suppress = True
+                    n_suppressed += 1
+                    break
+                
+            if not suppress:
+                print('Query {} in {} and {}:\n\n{}\n\n{}\n'.format(
+                    query, row['dataset_name'], previous_row['dataset_name'],
+                    taxonomy_string, previous_row['taxonomy_string']))
+                
+            queries_with_multiple_mappings.add(query)
+                
+    # ...for each row where we saw this query
+    
+    query_to_rows[query].append(i_row)
+    
+# ...for each row
+
+print('Found {} queries with multiple mappings ({} occurrences suppressed)'.format(
+    len(queries_with_multiple_mappings),n_suppressed))
+
+
+#%% Verify that nothing "unidentified" maps to a species or subspecies
+
+# E.g., "unidentified skunk" should never map to a specific species of skunk
+
+allowable_unknown_species = [
+    'unknown_tayra' # AFAIK this is a unique species, I'm not sure what's implied here
+]
+
+unk_queries = ['skunk']
+for i_row,row in df.iterrows():
+
+    query = row['query']
+    level = row['taxonomy_level']
+
+    if not isinstance(level,str):
+        assert not isinstance(row['taxonomy_string'],str)
+        continue
+
+    if ( \
+        'unidentified' in query or \
+        ('unk' in query and ('skunk' not in query and 'chipmunk' not in query))\
+        ) \
+        and \
+        ('species' in level):
+        
+        if query not in allowable_unknown_species:
+            
+            print('Warning: query {}:{} maps to {} {}'.format(
+                row['dataset_name'],
+                row['query'],
+                row['taxonomy_level'],
+                row['scientific_name']
+                ))
+
+
+#%% Make sure there are valid source and level values for everything with a mapping
+
+for i_row,row in df.iterrows():
+    if isinstance(row['scientific_name'],str):
+        if 'source' in row:
+            assert isinstance(row['source'],str)
+        assert isinstance(row['taxonomy_level'],str)
+        
+
+#%% Find WCS mappings that aren't species or aren't the same as the input
+
+# WCS used scientific names, so these remappings are slightly more controversial
+# then the standard remappings.
+
+# row = df.iloc[-500]
+for i_row,row in df.iterrows():
+    
+    if not isinstance(row['scientific_name'],str):
+        continue
+    if 'WCS' not in row['dataset_name']:
+        continue
+    
+    query = row['query']
+    scientific_name = row['scientific_name']
+    common_name = row['common_name']
+    level = row['taxonomy_level']    
+    taxonomy_string = row['taxonomy_string']
+    
+    common_name_from_taxonomy = taxonomy_string_to_common_name(taxonomy_string)   
+    query_string = query.replace(' sp','')
+    query_string = query_string.replace('unknown ','')
+    
+    # Anything marked "species" or "unknown" by definition doesn't map to a species,
+    # so ignore these.
+    if (' sp' not in query) and ('unknown' not in query) and \
+        (level != 'species') and (level != 'subspecies'):
+        print('WCS query {} ({}) remapped to {} {} ({})'.format(
+            query,common_name,level,scientific_name,common_name_from_taxonomy))
+
+    if query_string != scientific_name:        
+        pass
+        # print('WCS query {} ({}) remapped to {} ({})'.format(
+        #     query,common_name,scientific_name,common_names_from_taxonomy))
+
+
+#%% Download sample images for all scientific names
+
+remapped_queries = {'papio':'papio+baboon',
+                    'damaliscus lunatus jimela':'damaliscus lunatus',
+                    'mazama':'genus+mazama',
+                    'mirafra':'genus+mirafra'}
+
+import os
+from taxonomy_mapping import retrieve_sample_image
+
+scientific_name_to_paths = {}
+image_base = os.path.join(preview_base,'images')
+images_per_query = 15
+min_valid_images_per_query = 3
+min_valid_image_size = 3000
+
+# TODO: trivially prallelizable
+#
+# i_row = 0; row = df.iloc[i_row]
+for i_row,row in df.iterrows():
+    
+    s = row['scientific_name']
+    
+    if (not isinstance(s,str)) or (len(s)==0):
+        continue
+    
+    query = s.replace(' ','+')
+    
+    if query in remapped_queries:
+        query = remapped_queries[query]
+        
+    query_folder = os.path.join(image_base,query)
+    os.makedirs(query_folder,exist_ok=True)
+    
+    # Check whether we already have enough images for this query
+    image_files = os.listdir(query_folder)
+    image_fullpaths = [os.path.join(query_folder,fn) for fn in image_files]
+    sizes = [os.path.getsize(p) for p in image_fullpaths]
+    sizes_above_threshold = [x for x in sizes if x > min_valid_image_size]
+    if len(sizes_above_threshold) > min_valid_images_per_query:
+        print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))
+        continue
+
+    # Check whether we've already run this query for a previous row
+    if query in scientific_name_to_paths:
+        continue
+    
+    print('Processing query {} of {} ({})'.format(i_row,len(df),query))
+    paths = retrieve_sample_image.download_images(query=query,
+                                             output_directory=image_base,
+                                             limit=images_per_query,
+                                             verbose=True)
+    print('Downloaded {} images for {}'.format(len(paths),query))
+    scientific_name_to_paths[query] = paths
+
+# ...for each row in the mapping table
+
+
+#%% Rename .jpeg to .jpg
+
+from md_utils import path_utils
+all_images = path_utils.recursive_file_list(image_base,False)
+
+for fn in tqdm(all_images):
+    if fn.lower().endswith('.jpeg'):
+        new_fn = fn[0:-5] + '.jpg'
+        os.rename(fn, new_fn)
+
+
+#%% Choose representative images for each scientific name
+
+# Specifically, sort by size, and take the largest unique sizes. Very small files tend
+# to be bogus thumbnails, etc.
+
+max_images_per_query = 4
+scientific_name_to_preferred_images = {}
+
+# s = list(scientific_name_to_paths.keys())[0]
+for s in list(df.scientific_name):
+    
+    if not isinstance(s,str):
+        continue
+    
+    query = s.replace(' ','+')
+    
+    if query in remapped_queries:
+        query = remapped_queries[query]
+    
+    query_folder = os.path.join(image_base,query)
+    assert os.path.isdir(query_folder)
+    image_files = os.listdir(query_folder)
+    image_fullpaths = [os.path.join(query_folder,fn) for fn in image_files]    
+    sizes = [os.path.getsize(p) for p in image_fullpaths]
+    path_to_size = {}
+    for i_fp,fp in enumerate(image_fullpaths):
+        path_to_size[fp] = sizes[i_fp]
+    paths_by_size = [x for _, x in sorted(zip(sizes, image_fullpaths),reverse=True)]
+    
+    # Be suspicious of duplicate sizes
+    b_duplicate_sizes = [False] * len(paths_by_size)
+    
+    for i_path,p in enumerate(paths_by_size):
+        if i_path == len(paths_by_size) - 1:
+            continue
+        if path_to_size[p] == path_to_size[paths_by_size[i_path+1]]:
+            b_duplicate_sizes[i_path] = True
+    
+    paths_by_size_non_dup = [i for (i, v) in zip(paths_by_size, b_duplicate_sizes) if not v]
+    
+    preferred_paths = paths_by_size_non_dup[:max_images_per_query]
+    scientific_name_to_preferred_images[s] = preferred_paths
+
+# ...for each scientific name    
+
+
+#%% Delete unused images
+
+used_images = []
+for images in scientific_name_to_preferred_images.values():
+    used_images.extend(images)
+    
+print('Using a total of {} images'.format(len(used_images)))
+used_images_set = set(used_images)
+
+from md_utils import path_utils
+all_images = path_utils.recursive_file_list(image_base,False)
+
+unused_images = []
+for fn in all_images:
+    if fn not in used_images_set:
+        unused_images.append(fn)
+
+print('{} of {} files unused (diff {})'.format(len(unused_images),len(all_images),
+                                               len(all_images) - len(unused_images)))
+
+for fn in tqdm(unused_images):
+    os.remove(fn)    
+
+
+#%% Produce HTML preview
+
+with open(html_output_file, 'w', encoding='utf-8') as f:
+    
+    f.write('<html><head></head><body>\n')
+
+    names = scientific_name_to_preferred_images.keys()
+    names = sorted(names)
+    
+    f.write('<p class="speciesinfo_p" style="font-weight:bold;font-size:130%">'
+            'dataset_name: <b><u>category</u></b> mapped to taxonomy_level scientific_name (taxonomic_common_name) (manual_common_name)</p>\n'
+            '</p>')
+
+    # i_row = 2; row = df.iloc[i_row]
+    for i_row, row in tqdm(df.iterrows(), total=len(df)):
+
+        s = row['scientific_name']
+        
+        taxonomy_string = row['taxonomy_string']
+        if isinstance(taxonomy_string,str):
+            taxonomic_match = eval(taxonomy_string)        
+            matched_entity = taxonomic_match[0]
+            assert len(matched_entity) == 4
+            common_names = matched_entity[3]
+            if len(common_names) == 1:
+                common_name_string = common_names[0]
+            else:
+                common_name_string = str(common_names)
+        else:
+            common_name_string = ''
+
+        f.write('<p class="speciesinfo_p" style="font-weight:bold;font-size:130%">')
+
+        if isinstance(row.scientific_name,str):
+            output_string = '{}: <b><u>{}</u></b> mapped to {} {} ({}) ({})</p>\n'.format(
+                row.dataset_name, row.query, 
+                row.taxonomy_level, row.scientific_name, common_name_string,
+                row.common_name)
+            f.write(output_string)
+        else:
+            f.write('{}: <b><u>{}</u></b> unmapped'.format(row.dataset_name,row.query))
+
+        if s is None or s not in names:
+            f.write('<p class="content_p">no images available</p>')
+        else:
+            image_paths = scientific_name_to_preferred_images[s]
+            basedir = os.path.dirname(html_output_file)
+            relative_paths = [os.path.relpath(p,basedir) for p in image_paths]
+            image_paths = [s.replace('\\','/') for s in relative_paths]
+            n_images = len(image_paths)
+            # image_paths = [os.path.relpath(p, output_base) for p in image_paths]
+            image_width_percent = round(100 / n_images)
+            f.write('<table class="image_table"><tr>\n')
+            for image_path in image_paths:
+                f.write('<td style="vertical-align:top;" width="{}%">'
+                        '<img src="{}" style="display:block; width:100%; vertical-align:top; height:auto;">'
+                        '</td>\n'.format(image_width_percent, image_path))
+            f.write('</tr></table>\n')
+
+    # ...for each row
+
+    f.write('</body></html>\n')
+
+
+#%% Open HTML preview
+
+from md_utils.path_utils import open_file
+open_file(html_output_file)
```

### Comparing `megadetector-5.0.8/taxonomy_mapping/retrieve_sample_image.py` & `megadetector-5.0.9/taxonomy_mapping/retrieve_sample_image.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-########
-#
-# retrieve_sample_image.py
-#
-# Downloader that retrieves images from Google images, used for verifying taxonomy
-# lookups and looking for egregious mismappings (e.g., "snake" being mapped to a fish called
-# "snake").
-#
-# Simple wrapper around simple_image_download, but I've had to swap in and out the underlying
-# downloader a few times.
-#
-########
+"""
+
+retrieve_sample_image.py
+
+Downloader that retrieves images from Google images, used for verifying taxonomy
+lookups and looking for egregious mismappings (e.g., "snake" being mapped to a fish called
+"snake").
+
+Simple wrapper around simple_image_download, but I've had to swap in and out the underlying
+downloader a few times.
+
+"""
 
 #%% Imports and environment
 
 import os
 
 output_folder = os.path.expanduser('~/tmp/image-download-test')
 os.makedirs(output_folder,exist_ok=True)
```

### Comparing `megadetector-5.0.8/taxonomy_mapping/simple_image_download.py` & `megadetector-5.0.9/taxonomy_mapping/simple_image_download.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-########
-#
-# simple_image_download.py
-#
-# Web image downloader, used in preview_lila_taxonomy.py
-#
-# Slightly modified from:
-#
-# https://github.com/RiddlerQ/simple_image_download
-#
-########
+"""
+
+simple_image_download.py
+
+Web image downloader, used in preview_lila_taxonomy.py
+
+Slightly modified from:
+
+https://github.com/RiddlerQ/simple_image_download
+
+"""
 
 #%% Imports
 
 import os
 import urllib
 import requests
 import magic
```

### Comparing `megadetector-5.0.8/taxonomy_mapping/species_lookup.py` & `megadetector-5.0.9/taxonomy_mapping/species_lookup.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-########
-#
-# species_lookup.py
-#
-# Look up species names (common or scientific) in the GBIF and iNaturalist
-# taxonomies.
-#
-# Run initialize_taxonomy_lookup() before calling any other function.
-#
-########
+"""
+
+species_lookup.py
+
+Look up species names (common or scientific) in the GBIF and iNaturalist
+taxonomies.
+
+Run initialize_taxonomy_lookup() before calling any other function.
+
+"""
 
 #%% Constants and imports
 
 import argparse
 import pickle
 import shutil
 import zipfile
```

### Comparing `megadetector-5.0.8/taxonomy_mapping/taxonomy_csv_checker.py` & `megadetector-5.0.9/taxonomy_mapping/taxonomy_csv_checker.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,25 @@
-########
-#
-# taxonomy_csv_checker.py
-#
-# Checks the taxonomy CSV file to make sure that for each row:
-#
-# 1) The 'taxonomy_level' column matches the lowest-level taxon level in the
-#     'taxonomy_string' column.
-#
-# 2) The 'scientific_name' column matches the scientific name from the
-#     lowest-level taxon level in the 'taxonomy_string' column.
-# 
-# Prints out any mismatches.
-# 
-# Also prints out nodes that have 2 ambiguous parents. See "CASE 2" from the
-# module docstring of taxonomy_graph.py.
-#
-########
+"""
+
+taxonomy_csv_checker.py
+
+Checks the taxonomy CSV file to make sure that for each row:
+
+1) The 'taxonomy_level' column matches the lowest-level taxon level in the
+    'taxonomy_string' column.
+
+2) The 'scientific_name' column matches the scientific name from the
+    lowest-level taxon level in the 'taxonomy_string' column.
+
+Prints out any mismatches.
+
+Also prints out nodes that have 2 ambiguous parents. See "CASE 2" from the
+module docstring of taxonomy_graph.py.
+
+"""
 
 #%% Imports
 
 import sys
 import argparse
 
 import networkx as nx
```

### Comparing `megadetector-5.0.8/taxonomy_mapping/taxonomy_graph.py` & `megadetector-5.0.9/taxonomy_mapping/taxonomy_graph.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,54 +1,54 @@
-########
-# 
-# taxonomy_graph.py
-#
-# Methods for transforming taxonomy CSV into a graph structure backed by
-# NetworkX.
-# 
-# We treat each taxon in the taxonomy as a node in a graph, represented by the
-# TaxonNode class. We use a NetworkX directed graph (nx.DiGraph) to keep track of
-# the edges (parent-child relationships) between the nodes.
-# 
-# In theory, the true biological taxonomy graph should be a tree, where every
-# taxon node has exactly 1 parent. However, because we use both GBIF and INAT
-# taxonomies, there are 2 situations where a taxon node ends up with two parents.
-# Thus, the graph is actually a "directed acyclic graph" (DAG) instead of a tree.
-# 
-# The two situations are explained in detail below. This module includes a
-# function dag_to_tree() which converts a DAG to a tree by heuristically removing
-# edges from the DAG so that each node only has 1 parent.
-# 
-# CASE 1: INAT and GBIF have different granularity in their taxonomy levels
-# ======
-# An example is shown below. In dag_to_tree(), the lower parent is kept, while
-# the higher-up parent is discarded. In this example, the "sciurini -> sciurus"
-# edge would be kept, while "sciuridae -> sciurus" would be removed.
-# 
-#         "eastern gray squirrel" (inat)     "squirrel" (gbif)
-#         ------------------------------     -----------------
-#     family:                        sciuridae
-#                                   /          \
-#     subfamily:          sciurinae             |  # skips subfamily
-#                                 |             |
-#     tribe:               sciurini             |  # skips tribe
-#                                   \          /
-#     genus:                          sciurus
-# 
-# 
-# CASE 2: INAT and GBIF have different taxonomies
-# ======
-# An example is shown below. In dag_to_tree(), the resolution to these
-# discrepancies are hard-coded.
-# 
-#     order:    cathartiformes (inat)     accipitriformes (gbif)
-#                            \           /
-#     family:                 cathartidae
-#
-########
+"""
+
+taxonomy_graph.py
+
+Methods for transforming taxonomy CSV into a graph structure backed by
+NetworkX.
+
+We treat each taxon in the taxonomy as a node in a graph, represented by the
+TaxonNode class. We use a NetworkX directed graph (nx.DiGraph) to keep track of
+the edges (parent-child relationships) between the nodes.
+
+In theory, the true biological taxonomy graph should be a tree, where every
+taxon node has exactly 1 parent. However, because we use both GBIF and INAT
+taxonomies, there are 2 situations where a taxon node ends up with two parents.
+Thus, the graph is actually a "directed acyclic graph" (DAG) instead of a tree.
+
+The two situations are explained in detail below. This module includes a
+function dag_to_tree() which converts a DAG to a tree by heuristically removing
+edges from the DAG so that each node only has 1 parent.
+
+CASE 1: INAT and GBIF have different granularity in their taxonomy levels
+======
+An example is shown below. In dag_to_tree(), the lower parent is kept, while
+the higher-up parent is discarded. In this example, the "sciurini -> sciurus"
+edge would be kept, while "sciuridae -> sciurus" would be removed.
+
+        "eastern gray squirrel" (inat)     "squirrel" (gbif)
+        ------------------------------     -----------------
+    family:                        sciuridae
+                                  /          \
+    subfamily:          sciurinae             |  # skips subfamily
+                                |             |
+    tribe:               sciurini             |  # skips tribe
+                                  \          /
+    genus:                          sciurus
+
+
+CASE 2: INAT and GBIF have different taxonomies
+======
+An example is shown below. In dag_to_tree(), the resolution to these
+discrepancies are hard-coded.
+
+    order:    cathartiformes (inat)     accipitriformes (gbif)
+                           \           /
+    family:                 cathartidae
+
+"""
 
 #%% Imports and constants
 
 # allow forward references in typing annotations
 from __future__ import annotations
 
 from typing import (ClassVar, Container, Dict, Iterable, List, Optional, Set,
```

### Comparing `megadetector-5.0.8/taxonomy_mapping/validate_lila_category_mappings.py` & `megadetector-5.0.9/taxonomy_mapping/validate_lila_category_mappings.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,76 +1,83 @@
-########
-#
-# validate_lila_category_mappings.py
-#
-# Confirm that all category names on LILA have mappings in the taxonomy file.
-#
-########
-
-#%% Constants and imports
-
-import json
-import os
-
-from data_management.lila.lila_common import read_lila_taxonomy_mapping
-
-lila_local_base = os.path.expanduser('~/lila')
-
-metadata_dir = os.path.join(lila_local_base,'metadata')
-os.makedirs(metadata_dir,exist_ok=True)
-
-# Created by get_lila_category_list.py... contains counts for each category
-category_list_dir = os.path.join(lila_local_base,'lila_categories_list')
-lila_dataset_to_categories_file = os.path.join(category_list_dir,'lila_dataset_to_categories.json')
-
-assert os.path.isfile(lila_dataset_to_categories_file)
-    
-
-#%% Load category and taxonomy files
-
-with open(lila_dataset_to_categories_file,'r') as f:
-    lila_dataset_to_categories = json.load(f)
-
-taxonomy_df = read_lila_taxonomy_mapping(metadata_dir)
-
-
-#%% Map dataset names and category names to scientific names
-
-ds_query_to_scientific_name = {}
-
-unmapped_queries = set()
-
-# i_row = 1; row = taxonomy_df.iloc[i_row]; row
-for i_row,row in taxonomy_df.iterrows():
-    
-    ds_query = row['dataset_name'] + ':' + row['query']
-    ds_query = ds_query.lower()
-    
-    if not isinstance(row['scientific_name'],str):
-        unmapped_queries.add(ds_query)
-        ds_query_to_scientific_name[ds_query] = 'unmapped'
-        continue
-        
-    ds_query_to_scientific_name[ds_query] = row['scientific_name']
-    
-    
-#%% For each dataset, make sure we can map every category to the taxonomy
-
-# dataset_name = list(lila_dataset_to_categories.keys())[0]
-for _dataset_name in lila_dataset_to_categories.keys():
-    
-    if '_bbox' in _dataset_name:
-        dataset_name = _dataset_name.replace('_bbox','')
-    else:
-        dataset_name = _dataset_name
-    
-    categories = lila_dataset_to_categories[dataset_name]
-    
-    # c = categories[0]
-    for c in categories:
-        ds_query = dataset_name + ':' + c['name']
-        ds_query = ds_query.lower()
-        
-        if ds_query not in ds_query_to_scientific_name:
-            print('Could not find mapping for {}'.format(ds_query))            
-        else:
-            scientific_name = ds_query_to_scientific_name[ds_query]
+"""
+
+validate_lila_category_mappings.py
+
+Confirm that all category names on LILA have mappings in the taxonomy file.
+
+"""
+
+#%% Constants and imports
+
+import json
+import os
+
+from data_management.lila.lila_common import read_lila_taxonomy_mapping
+
+
+#%% Prevent execution during infrastructural imports
+
+if False:
+        
+    #%% Constants
+    
+    lila_local_base = os.path.expanduser('~/lila')
+
+    metadata_dir = os.path.join(lila_local_base,'metadata')
+    os.makedirs(metadata_dir,exist_ok=True)
+
+    # Created by get_lila_category_list.py... contains counts for each category
+    category_list_dir = os.path.join(lila_local_base,'lila_categories_list')
+    lila_dataset_to_categories_file = os.path.join(category_list_dir,'lila_dataset_to_categories.json')
+
+    assert os.path.isfile(lila_dataset_to_categories_file)
+        
+
+    #%% Load category and taxonomy files
+
+    with open(lila_dataset_to_categories_file,'r') as f:
+        lila_dataset_to_categories = json.load(f)
+
+    taxonomy_df = read_lila_taxonomy_mapping(metadata_dir)
+
+
+    #%% Map dataset names and category names to scientific names
+
+    ds_query_to_scientific_name = {}
+
+    unmapped_queries = set()
+
+    # i_row = 1; row = taxonomy_df.iloc[i_row]; row
+    for i_row,row in taxonomy_df.iterrows():
+        
+        ds_query = row['dataset_name'] + ':' + row['query']
+        ds_query = ds_query.lower()
+        
+        if not isinstance(row['scientific_name'],str):
+            unmapped_queries.add(ds_query)
+            ds_query_to_scientific_name[ds_query] = 'unmapped'
+            continue
+            
+        ds_query_to_scientific_name[ds_query] = row['scientific_name']
+        
+        
+    #%% For each dataset, make sure we can map every category to the taxonomy
+
+    # dataset_name = list(lila_dataset_to_categories.keys())[0]
+    for _dataset_name in lila_dataset_to_categories.keys():
+        
+        if '_bbox' in _dataset_name:
+            dataset_name = _dataset_name.replace('_bbox','')
+        else:
+            dataset_name = _dataset_name
+        
+        categories = lila_dataset_to_categories[dataset_name]
+        
+        # c = categories[0]
+        for c in categories:
+            ds_query = dataset_name + ':' + c['name']
+            ds_query = ds_query.lower()
+            
+            if ds_query not in ds_query_to_scientific_name:
+                print('Could not find mapping for {}'.format(ds_query))            
+            else:
+                scientific_name = ds_query_to_scientific_name[ds_query]
```

