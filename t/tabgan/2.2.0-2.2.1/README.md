# Comparing `tmp/tabgan-2.2.0-py2.py3-none-any.whl.zip` & `tmp/tabgan-2.2.1-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,27 +1,27 @@
-Zip file size: 39337 bytes, number of entries: 25
+Zip file size: 40103 bytes, number of entries: 25
 -rw-rw-rw-  2.0 fat      128 b- defN 24-May-25 12:23 _ForestDiffusion/README.MD
 -rw-rw-rw-  2.0 fat      124 b- defN 24-May-25 12:23 _ForestDiffusion/__init__.py
 -rw-rw-rw-  2.0 fat    18289 b- defN 24-May-25 12:23 _ForestDiffusion/diffusion_with_trees_class.py
 -rw-rw-rw-  2.0 fat      213 b- defN 24-May-25 12:23 _ForestDiffusion/utils/__init__.py
 -rw-rw-rw-  2.0 fat     6390 b- defN 24-May-25 12:23 _ForestDiffusion/utils/diffusion.py
 -rw-rw-rw-  2.0 fat     1215 b- defN 24-May-25 12:23 _ForestDiffusion/utils/utils_diffusion.py
 -rw-rw-rw-  2.0 fat       60 b- defN 21-Dec-25 19:24 _ctgan/README.MD
 -rw-rw-rw-  2.0 fat      202 b- defN 21-Dec-25 19:24 _ctgan/__init__.py
 -rw-rw-rw-  2.0 fat     4408 b- defN 23-May-04 17:50 _ctgan/conditional.py
 -rw-rw-rw-  2.0 fat     2466 b- defN 23-May-04 18:44 _ctgan/models.py
 -rw-rw-rw-  2.0 fat     1241 b- defN 23-Mar-29 20:14 _ctgan/sampler.py
 -rw-rw-rw-  2.0 fat    11533 b- defN 23-May-04 18:27 _ctgan/synthesizer.py
--rw-rw-rw-  2.0 fat     5959 b- defN 23-Oct-03 19:53 _ctgan/transformer.py
+-rw-rw-rw-  2.0 fat     5959 b- defN 24-Jun-01 12:51 _ctgan/transformer.py
 -rw-rw-rw-  2.0 fat      477 b- defN 24-May-26 12:32 tabgan/__init__.py
 -rw-rw-rw-  2.0 fat     4630 b- defN 24-May-25 12:23 tabgan/abc_sampler.py
 -rw-rw-rw-  2.0 fat     8458 b- defN 24-May-25 12:34 tabgan/adversarial_model.py
 -rw-rw-rw-  2.0 fat    10998 b- defN 23-Mar-29 20:14 tabgan/encoders.py
--rw-rw-rw-  2.0 fat    21364 b- defN 24-May-26 14:19 tabgan/sampler.py
--rw-rw-rw-  2.0 fat     5961 b- defN 24-May-26 14:58 tabgan/utils.py
--rw-rw-rw-  2.0 fat       84 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/AUTHORS.rst
--rw-rw-rw-  2.0 fat    11474 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    11332 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat      110 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       31 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2009 b- defN 24-May-26 15:03 tabgan-2.2.0.dist-info/RECORD
-25 files, 129156 bytes uncompressed, 36127 bytes compressed:  72.0%
+-rw-rw-rw-  2.0 fat    18422 b- defN 24-Jun-01 17:29 tabgan/sampler.py
+-rw-rw-rw-  2.0 fat     8863 b- defN 24-Jun-01 14:38 tabgan/utils.py
+-rw-rw-rw-  2.0 fat       84 b- defN 24-Jun-01 17:39 tabgan-2.2.1.dist-info/AUTHORS.rst
+-rw-rw-rw-  2.0 fat    11474 b- defN 24-Jun-01 17:39 tabgan-2.2.1.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    11332 b- defN 24-Jun-01 17:39 tabgan-2.2.1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      110 b- defN 24-Jun-01 17:39 tabgan-2.2.1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       31 b- defN 24-Jun-01 17:39 tabgan-2.2.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2009 b- defN 24-Jun-01 17:39 tabgan-2.2.1.dist-info/RECORD
+25 files, 129116 bytes uncompressed, 36893 bytes compressed:  71.4%
```

## zipnote {}

```diff
@@ -51,26 +51,26 @@
 
 Filename: tabgan/sampler.py
 Comment: 
 
 Filename: tabgan/utils.py
 Comment: 
 
-Filename: tabgan-2.2.0.dist-info/AUTHORS.rst
+Filename: tabgan-2.2.1.dist-info/AUTHORS.rst
 Comment: 
 
-Filename: tabgan-2.2.0.dist-info/LICENSE
+Filename: tabgan-2.2.1.dist-info/LICENSE
 Comment: 
 
-Filename: tabgan-2.2.0.dist-info/METADATA
+Filename: tabgan-2.2.1.dist-info/METADATA
 Comment: 
 
-Filename: tabgan-2.2.0.dist-info/WHEEL
+Filename: tabgan-2.2.1.dist-info/WHEEL
 Comment: 
 
-Filename: tabgan-2.2.0.dist-info/top_level.txt
+Filename: tabgan-2.2.1.dist-info/top_level.txt
 Comment: 
 
-Filename: tabgan-2.2.0.dist-info/RECORD
+Filename: tabgan-2.2.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tabgan/sampler.py

```diff
@@ -248,21 +248,51 @@
             if train_df.shape[0] != target.shape[0]:
                 raise ValueError(
                     "Something gone wrong: shape of train_df = {} is not equal to target = {} shape".format(
                         train_df.shape[0], target.shape[0]
                     )
                 )
 
+    def handle_generated_data(self, train_df, generated_df, only_generated_data):
+        generated_df = pd.DataFrame(generated_df)
+        generated_df.columns = train_df.columns
+        for i in range(len(generated_df.columns)):
+            generated_df[generated_df.columns[i]] = generated_df[
+                generated_df.columns[i]
+            ].astype(train_df.dtypes.values[i])
+        if not only_generated_data:
+            train_df = pd.concat([train_df, generated_df]).reset_index(drop=True)
+            logging.info(
+                "Generated shapes: {} plus target".format(
+                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
+                )
+            )
+            return (
+                _drop_col_if_exist(train_df, self.TEMP_TARGET),
+                get_columns_if_exists(train_df, self.TEMP_TARGET),
+            )
+        else:
+            logging.info(
+                "Generated shapes: {} plus target".format(
+                    _drop_col_if_exist(generated_df, self.TEMP_TARGET).shape
+                )
+            )
+            return (
+                _drop_col_if_exist(generated_df, self.TEMP_TARGET),
+                get_columns_if_exists(generated_df, self.TEMP_TARGET),
+            )
+
 
 class SamplerGAN(SamplerOriginal):
     def check_params(self):
-        if self.gen_params["batch_size"] % 2 != 0:
+        if self.gen_params["batch_size"] % 10 != 0:
             logging.warning(
-                "batch_size should even, but {} is provided. Increasing by 1".format(self.gen_params["batch_size"]))
-            self.gen_params["batch_size"] = self.gen_params["batch_size"] + 1
+                "Batch size should be divisible to 10, but provided {}. Fixing it".format(
+                    self.gen_params["batch_size"]))
+            self.gen_params["batch_size"] += 10 - (self.gen_params["batch_size"] % 10)
 
         if "patience" not in self.gen_params:
             logging.warning("patience param is not set for GAN params, so setting it to default ""25""")
             self.gen_params["patience"] = 25
 
         if "epochs" not in self.gen_params:
             logging.warning("patience param is not set for GAN params, so setting it to default ""50""")
@@ -281,47 +311,15 @@
             ctgan.fit(train_df, [], epochs=self.gen_params["epochs"])
         else:
             ctgan.fit(train_df, self.cat_cols, epochs=self.gen_params["epochs"])
         logging.info("Finished training GAN")
         generated_df = ctgan.sample(
             self.pregeneration_frac * self.get_generated_shape(train_df)
         )
-        data_dtype = train_df.dtypes.values
-
-        for i in range(len(generated_df.columns)):
-            generated_df[generated_df.columns[i]] = generated_df[
-                generated_df.columns[i]
-            ].astype(data_dtype[i])
-
-        if not only_generated_data:
-            train_df = pd.concat([train_df, generated_df]).reset_index(drop=True)
-            logging.info(
-                "Generated shapes: {} plus target".format(
-                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
-                )
-            )
-            return (
-                _drop_col_if_exist(train_df, self.TEMP_TARGET),
-                get_columns_if_exists(train_df, self.TEMP_TARGET),
-            )
-        else:
-            logging.info(
-                "Generated shapes: {} plus target".format(
-                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
-                )
-            )
-            return (
-                _drop_col_if_exist(generated_df, self.TEMP_TARGET),
-                get_columns_if_exists(generated_df, self.TEMP_TARGET),
-            )
-
-        return (
-            _drop_col_if_exist(train_df, self.TEMP_TARGET),
-            get_columns_if_exists(train_df, self.TEMP_TARGET),
-        )
+        return self.handle_generated_data(train_df, generated_df, only_generated_data)
 
 
 class SamplerDiffusion(SamplerOriginal):
     def generate_data(
             self, train_df, target, test_df, only_generated_data: bool
     ) -> Tuple[pd.DataFrame, pd.DataFrame]:
         self._validate_data(train_df, target, test_df)
@@ -336,48 +334,16 @@
             forest_model = ForestDiffusionModel(train_df.to_numpy(), label_y=None, n_t=50,
                                                 duplicate_K=100,
                                                 # todo fix bug with cat cols
                                                 # cat_indexes=self.get_column_indexes(train_df, self.cat_cols),
                                                 diffusion_type='flow', n_jobs=-1)
         logging.info("Finished training ForestDiffusionModel")
         generated_df = forest_model.generate(batch_size=int(self.gen_x_times * train_df.to_numpy().shape[0]))
-        data_dtype = train_df.dtypes.values
-        generated_df = pd.DataFrame(generated_df)
-        generated_df.columns = train_df.columns
-        for i in range(len(generated_df.columns)):
-            generated_df[generated_df.columns[i]] = generated_df[
-                generated_df.columns[i]
-            ].astype(data_dtype[i])
 
-        if not only_generated_data:
-            train_df = pd.concat([train_df, generated_df]).reset_index(drop=True)
-            logging.info(
-                "Generated shapes: {} plus target".format(
-                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
-                )
-            )
-            return (
-                _drop_col_if_exist(train_df, self.TEMP_TARGET),
-                get_columns_if_exists(train_df, self.TEMP_TARGET),
-            )
-        else:
-            logging.info(
-                "Generated shapes: {} plus target".format(
-                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
-                )
-            )
-            return (
-                _drop_col_if_exist(generated_df, self.TEMP_TARGET),
-                get_columns_if_exists(generated_df, self.TEMP_TARGET),
-            )
-
-        return (
-            _drop_col_if_exist(train_df, self.TEMP_TARGET),
-            get_columns_if_exists(train_df, self.TEMP_TARGET),
-        )
+        return self.handle_generated_data(train_df, generated_df, only_generated_data)
 
     @staticmethod
     def get_column_indexes(df, column_names):
         return [df.columns.get_loc(col) for col in column_names]
 
 
 class SamplerLLM(SamplerOriginal):
@@ -409,98 +375,43 @@
         model.fit(train_df)
 
         logging.info("Finished training ForestDiffusionModel")
         device = "cuda" if torch.cuda.is_available() else "cpu"
 
         generated_df = model.sample(int(self.gen_x_times * train_df.shape[0]), device=device,
                                     max_length=self.gen_params["max_length"])
-        data_dtype = train_df.dtypes.values
-        generated_df = pd.DataFrame(generated_df)
-        generated_df.columns = train_df.columns
-        for i in range(len(generated_df.columns)):
-            generated_df[generated_df.columns[i]] = generated_df[
-                generated_df.columns[i]
-            ].astype(data_dtype[i])
-
-        if not only_generated_data:
-            train_df = pd.concat([train_df, generated_df]).reset_index(drop=True)
-            logging.info(
-                "Generated shapes: {} plus target".format(
-                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
-                )
-            )
-            return (
-                _drop_col_if_exist(train_df, self.TEMP_TARGET),
-                get_columns_if_exists(train_df, self.TEMP_TARGET),
-            )
-        else:
-            logging.info(
-                "Generated shapes: {} plus target".format(
-                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
-                )
-            )
-            return (
-                _drop_col_if_exist(generated_df, self.TEMP_TARGET),
-                get_columns_if_exists(generated_df, self.TEMP_TARGET),
-            )
-
-        return (
-            _drop_col_if_exist(train_df, self.TEMP_TARGET),
-            get_columns_if_exists(train_df, self.TEMP_TARGET),
-        )
+        return self.handle_generated_data(train_df, generated_df, only_generated_data)
 
 
 if __name__ == "__main__":
     setup_logging(logging.DEBUG)
     train_size = 75
-    train = pd.DataFrame(
-        np.random.randint(-10, 150, size=(train_size, 4)), columns=list("ABCD")
-    )
-    logging.info(train)
+    train = pd.DataFrame(np.random.randint(-10, 150, size=(train_size, 4)), columns=list("ABCD"))
     target = pd.DataFrame(np.random.randint(0, 2, size=(train_size, 1)), columns=list("Y"))
     test = pd.DataFrame(np.random.randint(0, 100, size=(train_size, 4)), columns=list("ABCD"))
-    _sampler(OriginalGenerator(gen_x_times=15), train, target, test)
-    _sampler(
-        GANGenerator(gen_x_times=10, only_generated_data=False,
-                     gen_params={"batch_size": 500, "patience": 25, "epochs": 500, }), train, target, test
-    )
-    _sampler(
-        LLMGenerator(gen_params={"batch_size": 32, "epochs": 4, "llm": "distilgpt2",
-                                 "max_length": 500}).generate_data_pipe(train,
-                                                                        target,
-                                                                        test, )
-    )
+    logging.info(train)
 
-    _sampler(OriginalGenerator(gen_x_times=15), train, None, train)
-    _sampler(
+    generators = [
+        OriginalGenerator(gen_x_times=15),
+        GANGenerator(gen_x_times=10, only_generated_data=False,
+                     gen_params={"batch_size": 500, "patience": 25, "epochs": 500}),
+        LLMGenerator(gen_params={"batch_size": 32, "epochs": 4, "llm": "distilgpt2", "max_length": 500}),
+        OriginalGenerator(gen_x_times=15),
         GANGenerator(cat_cols=["A"], gen_x_times=20, only_generated_data=True),
-        train,
-        None,
-        train,
-    )
-    _sampler(
         ForestDiffusionGenerator(cat_cols=["A"], gen_x_times=1, only_generated_data=True),
-        train,
-        None,
-        train,
-    )
-    _sampler(
         ForestDiffusionGenerator(gen_x_times=10, only_generated_data=False,
-                                 gen_params={"batch_size": 500, "patience": 25, "epochs": 500, }),
-        train, target, test
-    )
-
-    min_date = pd.to_datetime('2019-01-01')
-    max_date = pd.to_datetime('2021-12-31')
+                                 gen_params={"batch_size": 500, "patience": 25, "epochs": 500})
+    ]
 
-    d = (max_date - min_date).days + 1
+    for gen in generators:
+        _sampler(gen, train, target if 'LLMGenerator' not in str(type(gen)) else None, test)
 
-    train['Date'] = min_date + pd.to_timedelta(np.random.randint(d, size=train_size), unit='d')
+    min_date, max_date = pd.to_datetime('2019-01-01'), pd.to_datetime('2021-12-31')
+    train['Date'] = min_date + pd.to_timedelta(np.random.randint((max_date - min_date).days + 1, size=train_size),
+                                               unit='d')
     train = get_year_mnth_dt_from_date(train, 'Date')
 
-    new_train, new_target = GANGenerator(gen_x_times=1.1, cat_cols=['year'], bot_filter_quantile=0.001,
-                                         top_filter_quantile=0.999,
-                                         is_post_process=True, pregeneration_frac=2, only_generated_data=False). \
-        generate_data_pipe(train.drop('Date', axis=1), None,
-                           train.drop('Date', axis=1)
-                           )
+    new_train, new_target = GANGenerator(
+        gen_x_times=1.1, cat_cols=['year'], bot_filter_quantile=0.001, top_filter_quantile=0.999,
+        is_post_process=True, pregeneration_frac=2, only_generated_data=False
+    ).generate_data_pipe(train.drop('Date', axis=1), None, train.drop('Date', axis=1))
     new_train = collect_dates(new_train)
```

## tabgan/utils.py

```diff
@@ -5,14 +5,15 @@
 
 import numpy as np
 import pandas as pd
 import torch
 from scipy.stats import entropy
 
 __all__ = ["compare_dataframes"]
+TEMP_TARGET = "_temp_target"
 
 
 def setup_logging(loglevel):
     """Setup basic logging
 
     Args:
       loglevel (int): minimum loglevel for emitting messages
@@ -23,46 +24,46 @@
     )
 
 
 def make_two_digit(num_as_str: str) -> pd.DataFrame:
     if len(num_as_str) == 2:
         return num_as_str
     else:
-        return '0' + num_as_str
+        return "0" + num_as_str
 
 
-def get_year_mnth_dt_from_date(df: pd.DataFrame, date_col='Date') -> pd.DataFrame:
+def get_year_mnth_dt_from_date(df: pd.DataFrame, date_col="Date") -> pd.DataFrame:
     """
     Extracts year, month, and day from a date column in a pandas DataFrame.
 
     Args:
         df (pd.DataFrame): Input DataFrame.
         date_col (str): Name of the date column.
 
     Returns:
         pd.DataFrame: DataFrame with year, month, and day columns added.
     """
     df[date_col] = pd.to_datetime(df[date_col])
-    df['year'] = df[date_col].dt.year
-    df['month'] = df[date_col].dt.month
-    df['day'] = df[date_col].dt.day
+    df["year"] = df[date_col].dt.year
+    df["month"] = df[date_col].dt.month
+    df["day"] = df[date_col].dt.day
     return df
 
 
 def collect_dates(df: pd.DataFrame) -> pd.DataFrame:
-    df["Date"] = df['year'].astype(str) + '-' \
-                 + df['month'].astype(str).apply(make_two_digit) + '-' \
-                 + df['day'].astype(str).apply(make_two_digit)
-    df.drop(['year', 'month', 'day'], axis=1, inplace=True)
+    df["Date"] = df["year"].astype(str) + "-" \
+                 + df["month"].astype(str).apply(make_two_digit) + "-" \
+                 + df["day"].astype(str).apply(make_two_digit)
+    df.drop(["year", "month", "day"], axis=1, inplace=True)
     return df
 
 
 def seed_everything(seed=1234):
     random.seed(seed)
-    os.environ['PYTHONHASHSEED'] = str(seed)
+    os.environ["PYTHONHASHSEED"] = str(seed)
     np.random.seed(seed)
     torch.manual_seed(seed)
     torch.cuda.manual_seed(seed)
     torch.backends.cudnn.deterministic = True
 
 
 def _sampler(creator, in_train, in_target, in_test) -> None:
@@ -85,14 +86,96 @@
 def get_columns_if_exists(df, col) -> pd.DataFrame:
     if col in df.columns:
         return df[col]
     else:
         return None
 
 
+def calculate_psi(expected, actual, buckettype="bins", buckets=10, axis=0):
+    '''Calculate the PSI (population stability index) across all variables
+
+    Args:
+       expected: numpy matrix of original values
+       actual: numpy matrix of new values
+       bucket type: type of strategy for creating buckets, bins splits into even splits, quantiles splits into quantile buckets
+       buckets: number of quantiles to use in bucketing variables
+       axis: axis by which variables are defined, 0 for vertical, 1 for horizontal
+
+    Returns:
+       psi_values: ndarray of psi values for each variable
+
+    Author:
+       Matthew Burke
+       github.com/mwburke
+       mwburke.github.io.com
+    '''
+
+    def psi(expected_array, actual_array, buckets):
+        '''Calculate the PSI for a single variable
+
+        Args:
+           expected_array: numpy array of original values
+           actual_array: numpy array of new values, same size as expected
+           buckets: number of percentile ranges to bucket the values into
+
+        Returns:
+           psi_value: calculated PSI value
+        '''
+
+        def scale_range(input, min, max):
+            input += -(np.min(input))
+            input /= np.max(input) / (max - min)
+            input += min
+            return input
+
+        breakpoints = np.arange(0, buckets + 1) / (buckets) * 100
+
+        if buckettype == "bins":
+            breakpoints = scale_range(breakpoints, np.min(expected_array), np.max(expected_array))
+        elif buckettype == "quantiles":
+            breakpoints = np.stack([np.percentile(expected_array, b) for b in breakpoints])
+
+        expected_fractions = np.histogram(expected_array, breakpoints)[0] / len(expected_array)
+        actual_fractions = np.histogram(actual_array, breakpoints)[0] / len(actual_array)
+
+        def sub_psi(e_perc, a_perc):
+            '''Calculate the actual PSI value from comparing the values.
+               Update the actual value to a very small number if equal to zero
+            '''
+            if a_perc == 0:
+                a_perc = 0.0001
+            if e_perc == 0:
+                e_perc = 0.0001
+
+            value = (e_perc - a_perc) * np.log(e_perc / a_perc)
+            return (value)
+
+        psi_value = sum(sub_psi(expected_fractions[i], actual_fractions[i]) for i in range(0, len(expected_fractions)))
+
+        return (psi_value)
+
+    if len(expected.shape) == 1:
+        psi_values = np.empty(len(expected.shape))
+    else:
+        psi_values = np.empty(expected.shape[1 - axis])
+
+    for i in range(0, len(psi_values)):
+        if len(psi_values) == 1:
+            try:
+                psi_values = psi(expected, actual, buckets)
+            except:
+                psi_values = 0.9
+        elif axis == 0:
+            psi_values[i] = psi(expected[:, i], actual[:, i], buckets)
+        elif axis == 1:
+            psi_values[i] = psi(expected[i, :], actual[i, :], buckets)
+
+    return psi_values
+
+
 def compare_dataframes(df_original, df_generated):
     """
     Compares two DataFrames for similarity in terms of uniqueness, data quality, and PSI.
 
     Args:
       df_original: The original DataFrame.
       df_generated: The DataFrame with generated numbers.
@@ -104,15 +187,18 @@
     # Example usage
     df1 = pd.DataFrame({"col1": [1, 2, 3, 4], "col2": ["a", "b", "a", "c"]})
     df2 = pd.DataFrame({"col1": [1, 2, 5, 6], "col2": ["a", "b", "x", "y"]})
 
     similarity_score = compare_dataframes(df1.copy(), df2.copy())
     print(similarity_score)
     """
-
+    # Handle DataFrames with different shapes
+    if len(df_original.columns) != len(df_generated.columns):
+        # Penalize if column names don't match
+        return 0.0
     # Handle potential differences in row count
     n_original = len(df_original)
     n_generated = len(df_generated)
     min_rows = min(n_original, n_generated)
 
     # Uniqueness: Ratio of non-null unique values in generated vs original (weighted by min rows)
     uniq_original = df_original.nunique().sum() / (len(df_original.columns) + 1e-6)
@@ -134,27 +220,18 @@
                 data_quality_scores.append(p_value)
     data_quality_score = sum(data_quality_scores) / len(data_quality_scores) if data_quality_scores else 1
 
     # PSI Similarity: Average PSI across all column pairs (capped at theoretical maximum)
     psi_scores = []
     for col_orig in df_original.columns:
         if col_orig in df_generated.columns:
-            p_orig = df_original[col_orig].value_counts(normalize=True)
-            p_gen = df_generated[col_orig].value_counts(normalize=True)
-            # Handle potential division by zero with entropy function (uses log2 internally)
-            h_orig = entropy(p_orig, base=2)
-            h_gen = entropy(p_gen, base=2)
-            h_joint = entropy(pd.concat([p_orig, p_gen], ignore_index=True), base=2)
-            psi = max(0, min(h_orig + h_gen - h_joint, 1))  # Ensure non-negative and cap at 1
-            psi_scores.append(psi)
+            psi_scores.append(calculate_psi(df_original[col_orig], df_generated[col_orig],
+                                            buckets=10))  # Assuming buckets=10 for PSI calculation
     psi_similarity = sum(psi_scores) / len(psi_scores) if psi_scores else 1
 
     # Combine uniqueness, data quality, and PSI scores (weighted)
-    similarity_score = 0.5 * uniqueness_score + 0.3 * data_quality_score + 0.2 * psi_similarity
-
+    similarity_score = 0.1 * uniqueness_score + 0.45 * data_quality_score + 0.45 * (1/psi_similarity)
+    print(uniqueness_score, data_quality_score, psi_similarity)
     # Ensure score is between 0 and 1
     similarity_score = min(max(similarity_score, 0), 1)
 
     return similarity_score
-
-
-TEMP_TARGET = "_temp_target"
```

## Comparing `tabgan-2.2.0.dist-info/LICENSE` & `tabgan-2.2.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tabgan-2.2.0.dist-info/METADATA` & `tabgan-2.2.1.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tabgan
-Version: 2.2.0
+Version: 2.2.1
 Summary: Applying GAN in tabular data generation for uneven distribution
 Home-page: https://github.com/Diyago/GAN-for-tabular-data
 Author: Insaf Ashrapov
 Author-email: iashrapov@gmail.com
 License: Apache License 2.0
 Project-URL: Documentation, https://github.com/Diyago/GAN-for-tabular-data
 Platform: any
```

## Comparing `tabgan-2.2.0.dist-info/RECORD` & `tabgan-2.2.1.dist-info/RECORD`

 * *Files 17% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 _ctgan/sampler.py,sha256=CV6qS6j3ak7b61blVXgg8U03h32uAJ-qmaz6aY5Ip7I,1241
 _ctgan/synthesizer.py,sha256=D-DXtEf1KWbzx_ZudRhrhJKeIxZmt9DVFDkr-pwJuFA,11533
 _ctgan/transformer.py,sha256=Q4SJ1RzhUxci9v03-0OzIoaTqh_qTahlVECSqIt3SbA,5959
 tabgan/__init__.py,sha256=whMF4h6WW0WfMDA4-i28t4vJ_W-wJDOwa0fxivqkKio,477
 tabgan/abc_sampler.py,sha256=o2NI8I5FjZzw5UGVpNmLpJ4SquGHeZKUIoBRF6djCwY,4630
 tabgan/adversarial_model.py,sha256=HvK2T4utz2kY4gYRYajcMYkREUkSwZtpkD3Bs3hlTrA,8458
 tabgan/encoders.py,sha256=h2_XF_0y0wzavWPirvrwGQsUEK_ip6OUCN5a6DH4MUE,10998
-tabgan/sampler.py,sha256=-mMDJJZ3FMfivdArgagL36dWMVvOy3jUv8CE4rb_9uk,21364
-tabgan/utils.py,sha256=lhArvfD5ROXA2J-QfbKfHkXC393LO-fs5LHhQqmRY1Y,5961
-tabgan-2.2.0.dist-info/AUTHORS.rst,sha256=W9n6ZnEnD4-CCJnQYfYD2TK2bAO50DlyZ26Iq0cLrys,84
-tabgan-2.2.0.dist-info/LICENSE,sha256=tyJzqT3CJebQ4J1RdlGJoIJobfuZyYz6LL-7XmWWtcI,11474
-tabgan-2.2.0.dist-info/METADATA,sha256=Abxl5nrfwBBv79n7f3ED5NokkBRQ3wd7lT04oIG1H6Q,11332
-tabgan-2.2.0.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-tabgan-2.2.0.dist-info/top_level.txt,sha256=jGz7lHjOpBcJOj1d2rrHpwuLyqmw32PmLfzpCEg2Eck,31
-tabgan-2.2.0.dist-info/RECORD,,
+tabgan/sampler.py,sha256=8PTa1mIAz6edkuXtu7Sm4iu1rZW777WlscHiaqoRhk0,18422
+tabgan/utils.py,sha256=70OiuDOuup1rCXGboEF3fKE2fyUDL9M8zqvQooGtU1M,8863
+tabgan-2.2.1.dist-info/AUTHORS.rst,sha256=W9n6ZnEnD4-CCJnQYfYD2TK2bAO50DlyZ26Iq0cLrys,84
+tabgan-2.2.1.dist-info/LICENSE,sha256=tyJzqT3CJebQ4J1RdlGJoIJobfuZyYz6LL-7XmWWtcI,11474
+tabgan-2.2.1.dist-info/METADATA,sha256=QbOWkWY9h7J_au7w4LnWpxi0kVJ4bWue4Zi4vzaNyYg,11332
+tabgan-2.2.1.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+tabgan-2.2.1.dist-info/top_level.txt,sha256=jGz7lHjOpBcJOj1d2rrHpwuLyqmw32PmLfzpCEg2Eck,31
+tabgan-2.2.1.dist-info/RECORD,,
```

