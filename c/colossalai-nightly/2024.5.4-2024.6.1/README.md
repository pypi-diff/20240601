# Comparing `tmp/colossalai-nightly-2024.5.4.tar.gz` & `tmp/colossalai-nightly-2024.6.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "colossalai-nightly-2024.5.4.tar", last modified: Sat May  4 00:14:38 2024, max compression
+gzip compressed data, was "colossalai-nightly-2024.6.1.tar", last modified: Sat Jun  1 00:16:37 2024, max compression
```

## Comparing `colossalai-nightly-2024.5.4.tar` & `colossalai-nightly-2024.6.1.tar`

### file list

```diff
@@ -1,1175 +1,1281 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/
--rw-r--r--   0 runner    (1001) docker     (127)    30134 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/LICENSE
--rw-r--r--   0 runner    (1001) docker     (127)      198 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (127)    36765 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    31195 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_C/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_C/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      597 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/
--rw-r--r--   0 runner    (1001) docker     (127)      165 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    20868 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_meta_registration.py
--rw-r--r--   0 runner    (1001) docker     (127)     2088 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_monkey_patch.py
--rw-r--r--   0 runner    (1001) docker     (127)    18677 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/flop_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     7589 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/meta_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/envs.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/
--rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18998 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/codegen.py
--rw-r--r--   0 runner    (1001) docker     (127)     9947 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/graph_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     8482 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/node_util.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/
--rw-r--r--   0 runner    (1001) docker     (127)      106 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/graph_profile.py
--rw-r--r--   0 runner    (1001) docker     (127)     9939 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/shape_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)      952 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/symbolic_profile.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/
--rw-r--r--   0 runner    (1001) docker     (127)       63 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5415 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/bias_addition.py
--rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/custom_leaf_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     3568 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/proxy.py
--rw-r--r--   0 runner    (1001) docker     (127)     5862 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/symbolic_trace.py
--rw-r--r--   0 runner    (1001) docker     (127)    15712 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/tracer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/accelerator/
--rw-r--r--   0 runner    (1001) docker     (127)      431 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/api.py
--rw-r--r--   0 runner    (1001) docker     (127)     9402 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/base_accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)    10154 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/cpu_accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)     9442 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/cuda_accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)     9453 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/npu_accelerator.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/
--rw-r--r--   0 runner    (1001) docker     (127)      222 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2119 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (127)      735 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4977 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/
--rw-r--r--   0 runner    (1001) docker     (127)      226 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2523 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/base.py
--rw-r--r--   0 runner    (1001) docker     (127)      504 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/bf16.py
--rw-r--r--   0 runner    (1001) docker     (127)     2695 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py
--rw-r--r--   0 runner    (1001) docker     (127)     7959 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/
--rw-r--r--   0 runner    (1001) docker     (127)      155 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      377 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/build_c_ext.py
--rw-r--r--   0 runner    (1001) docker     (127)     7711 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     3468 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py
--rw-r--r--   0 runner    (1001) docker     (127)    18538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4921 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/operation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.289213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/
--rw-r--r--   0 runner    (1001) docker     (127)       95 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      296 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.289213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/
--rw-r--r--   0 runner    (1001) docker     (127)      241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3940 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2840 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)     7571 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py
--rw-r--r--   0 runner    (1001) docker     (127)     2512 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    24482 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py
--rw-r--r--   0 runner    (1001) docker     (127)     9318 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py
--rw-r--r--   0 runner    (1001) docker     (127)     7392 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)     3258 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     2827 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/where.py
--rw-r--r--   0 runner    (1001) docker     (127)      761 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     4748 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/shard_metainfo.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.289213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6826 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/amp_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     3584 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/base_offload_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     2072 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/mem_optimize.py
--rw-r--r--   0 runner    (1001) docker     (127)     5167 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region.py
--rw-r--r--   0 runner    (1001) docker     (127)    20031 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     9909 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/runtime.py
--rw-r--r--   0 runner    (1001) docker     (127)    18503 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/solver.py
--rw-r--r--   0 runner    (1001) docker     (127)    17919 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/training_simulator.py
--rw-r--r--   0 runner    (1001) docker     (127)     2793 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/util.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.293213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5111 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/comm_metainfo_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)      417 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     6050 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/meta_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)    11328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_apply_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)    22439 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_preparation_pass.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.293213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/pipeline_shard/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/pipeline_shard/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.293213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2805 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)    16998 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.297213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/
--rw-r--r--   0 runner    (1001) docker     (127)     2062 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3731 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3070 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4988 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4805 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     5563 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    11414 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1316 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1734 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1901 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    13535 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    20347 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    16308 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1762 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2115 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2997 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1466 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)      744 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     1980 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/
--rw-r--r--   0 runner    (1001) docker     (127)     2100 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14274 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     5162 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    24078 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    12070 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3615 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     7361 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     9225 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    41664 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4597 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3636 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    18860 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4702 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    13053 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     2278 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3981 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4158 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3053 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1156 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2330 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1774 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1964 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3553 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/options.py
--rw-r--r--   0 runner    (1001) docker     (127)    10812 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/sharding_strategy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/
--rw-r--r--   0 runner    (1001) docker     (127)      238 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9987 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     5767 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py
--rw-r--r--   0 runner    (1001) docker     (127)    20206 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/solver.py
--rw-r--r--   0 runner    (1001) docker     (127)     8474 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/
--rw-r--r--   0 runner    (1001) docker     (127)     1210 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5899 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/broadcast.py
--rw-r--r--   0 runner    (1001) docker     (127)     8346 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/factory.py
--rw-r--r--   0 runner    (1001) docker     (127)     3841 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (127)     9066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/reshape.py
--rw-r--r--   0 runner    (1001) docker     (127)     4408 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/sharding.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/booster/
--rw-r--r--   0 runner    (1001) docker     (127)       93 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1481 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)    20278 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/booster.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (127)     1298 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      102 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/bf16.py
--rw-r--r--   0 runner    (1001) docker     (127)     3218 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_apex.py
--rw-r--r--   0 runner    (1001) docker     (127)      870 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_naive.py
--rw-r--r--   0 runner    (1001) docker     (127)     4824 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_torch.py
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp8.py
--rw-r--r--   0 runner    (1001) docker     (127)      565 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/mixed_precision_base.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/booster/plugin/
--rw-r--r--   0 runner    (1001) docker     (127)      529 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3098 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/dp_plugin_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    28131 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/gemini_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    63839 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/hybrid_parallel_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    19546 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/low_level_zero_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    20853 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)     2475 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/plugin_base.py
--rw-r--r--   0 runner    (1001) docker     (127)      559 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/pp_plugin_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     9770 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_ddp_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    15017 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_fsdp_plugin.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/
--rw-r--r--   0 runner    (1001) docker     (127)      318 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15804 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/checkpoint_io_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     8761 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/general_checkpoint_io.py
--rw-r--r--   0 runner    (1001) docker     (127)    43972 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py
--rw-r--r--   0 runner    (1001) docker     (127)     5758 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/index_file.py
--rw-r--r--   0 runner    (1001) docker     (127)    29632 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/cli/
--rw-r--r--   0 runner    (1001) docker     (127)       40 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/cli/check/
--rw-r--r--   0 runner    (1001) docker     (127)      396 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/check/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8454 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/check/check_installation.py
--rw-r--r--   0 runner    (1001) docker     (127)      310 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/cli.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/cli/launcher/
--rw-r--r--   0 runner    (1001) docker     (127)     3654 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/hostinfo.py
--rw-r--r--   0 runner    (1001) docker     (127)     4286 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/multinode_runner.py
--rw-r--r--   0 runner    (1001) docker     (127)    10530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/run.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/cluster/
--rw-r--r--   0 runner    (1001) docker     (127)      296 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/device_mesh_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     7233 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/dist_coordinator.py
--rw-r--r--   0 runner    (1001) docker     (127)     2356 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/process_group_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)    10848 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/process_group_mesh.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/context/
--rw-r--r--   0 runner    (1001) docker     (127)       96 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/context/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3153 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/context/config.py
--rw-r--r--   0 runner    (1001) docker     (127)      921 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/context/singleton_meta.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/device/
--rw-r--r--   0 runner    (1001) docker     (127)      139 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    17443 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/alpha_beta_profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     5634 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/calc_pipeline_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)    23616 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/device_mesh.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/fx/
--rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/_compatibility.py
--rw-r--r--   0 runner    (1001) docker     (127)    19328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_12.py
--rw-r--r--   0 runner    (1001) docker     (127)     1906 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_13.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/fx/codegen/
--rw-r--r--   0 runner    (1001) docker     (127)       45 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/codegen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    45231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/codegen/activation_checkpoint_codegen.py
--rw-r--r--   0 runner    (1001) docker     (127)     7438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/graph_module.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.313213 colossalai-nightly-2024.5.4/colossalai/fx/passes/
--rw-r--r--   0 runner    (1001) docker     (127)      266 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13496 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/adding_split_node_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)    12261 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/concrete_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)    14066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/meta_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)    15886 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/passes_for_gpt2_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     6888 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/shard_1d_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)    13714 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/split_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     6169 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.313213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/
--rw-r--r--   0 runner    (1001) docker     (127)      768 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      871 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     6285 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/dataflow.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.313213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/
--rw-r--r--   0 runner    (1001) docker     (127)      282 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      782 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     7062 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/
--rw-r--r--   0 runner    (1001) docker     (127)      211 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1304 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     3387 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py
--rw-r--r--   0 runner    (1001) docker     (127)      632 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     2027 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1188 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)      402 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/python_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)     2188 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/
--rw-r--r--   0 runner    (1001) docker     (127)      252 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     2037 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     6708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/convolution.py
--rw-r--r--   0 runner    (1001) docker     (127)      424 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/dropout.py
--rw-r--r--   0 runner    (1001) docker     (127)      431 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      495 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1582 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1013 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)     3020 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/rnn.py
--rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/torch_op.py
--rw-r--r--   0 runner    (1001) docker     (127)      603 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     1050 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/shard_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2232 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/memory_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    13214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/opcount.py
--rw-r--r--   0 runner    (1001) docker     (127)    15377 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4235 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/shard_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4990 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4010 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/proxy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/
--rw-r--r--   0 runner    (1001) docker     (127)      201 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4850 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/_meta_trace.py
--rw-r--r--   0 runner    (1001) docker     (127)     2197 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/_symbolic_trace.py
--rw-r--r--   0 runner    (1001) docker     (127)     1479 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/_tracer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/
--rw-r--r--   0 runner    (1001) docker     (127)       90 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/
--rw-r--r--   0 runner    (1001) docker     (127)      193 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py
--rw-r--r--   0 runner    (1001) docker     (127)     2171 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py
--rw-r--r--   0 runner    (1001) docker     (127)     4445 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py
--rw-r--r--   0 runner    (1001) docker     (127)      745 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/
--rw-r--r--   0 runner    (1001) docker     (127)       78 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4395 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     2370 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py
--rw-r--r--   0 runner    (1001) docker     (127)      503 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)    26431 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/experimental.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/
--rw-r--r--   0 runner    (1001) docker     (127)       62 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/
--rw-r--r--   0 runner    (1001) docker     (127)      167 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     3200 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py
--rw-r--r--   0 runner    (1001) docker     (127)     5707 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/convolution.py
--rw-r--r--   0 runner    (1001) docker     (127)      339 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      517 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     2047 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)     5622 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/
--rw-r--r--   0 runner    (1001) docker     (127)      180 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      428 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     4752 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/convolution.py
--rw-r--r--   0 runner    (1001) docker     (127)      254 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      400 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1129 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     6769 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)      644 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/rnn.py
--rw-r--r--   0 runner    (1001) docker     (127)      834 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)    24642 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/tracer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/inference/
--rw-r--r--   0 runner    (1001) docker     (127)      235 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/engine/
--rw-r--r--   0 runner    (1001) docker     (127)       67 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8315 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     8949 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/microbatch_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)      225 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    19778 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)    20627 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)    21828 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/llama.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/
--rw-r--r--   0 runner    (1001) docker     (127)      360 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5408 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)     3438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)     8485 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/llama.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/
--rw-r--r--   0 runner    (1001) docker     (127)       90 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/batch_infer_state.py
--rw-r--r--   0 runner    (1001) docker     (127)     4582 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/kvcache_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/
--rw-r--r--   0 runner    (1001) docker     (127)       61 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/
--rw-r--r--   0 runner    (1001) docker     (127)      155 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/
--rw-r--r--   0 runner    (1001) docker     (127)      372 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13893 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/cai_quant_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1629 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/gptq_op.py
--rw-r--r--   0 runner    (1001) docker     (127)     2423 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/gptq_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/
--rw-r--r--   0 runner    (1001) docker     (127)      297 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    20063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/base_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     6500 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)    35519 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    10276 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/parallel_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     6088 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/interface/
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      851 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/model.py
--rw-r--r--   0 runner    (1001) docker     (127)     4120 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/optimizer.py
--rw-r--r--   0 runner    (1001) docker     (127)      333 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/pretrained.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/
--rw-r--r--   0 runner    (1001) docker     (127)     1127 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2499 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/base_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     4730 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpp_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/
--rw-r--r--   0 runner    (1001) docker     (127)      150 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1025 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/cpu_adam_arm.py
--rw-r--r--   0 runner    (1001) docker     (127)     1624 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/cpu_adam_x86.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/
--rw-r--r--   0 runner    (1001) docker     (127)      350 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/
--rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/
--rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/colossal_C_frontend.cpp
--rw-r--r--   0 runner    (1001) docker     (127)      214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/compat.h
--rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/include/
--rw-r--r--   0 runner    (1001) docker     (127)     8585 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/include/block_reduce.h
--rw-r--r--   0 runner    (1001) docker     (127)     4878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    25829 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    25627 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5046 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_adam.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5200 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_apply.cuh
--rw-r--r--   0 runner    (1001) docker     (127)    13279 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    13115 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_lamb.cu
--rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_scale_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     6479 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     2684 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    21112 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.h
--rw-r--r--   0 runner    (1001) docker     (127)     3467 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
--rw-r--r--   0 runner    (1001) docker     (127)     2066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    23530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
--rw-r--r--   0 runner    (1001) docker     (127)     2818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
--rw-r--r--   0 runner    (1001) docker     (127)    14851 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/type_shim.h
--rw-r--r--   0 runner    (1001) docker     (127)     6588 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/scaled_softmax.py
--rw-r--r--   0 runner    (1001) docker     (127)     3878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cuda_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/
--rw-r--r--   0 runner    (1001) docker     (127)      470 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3760 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_dao_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     1922 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_npu.py
--rw-r--r--   0 runner    (1001) docker     (127)     1799 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_sdpa_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/
--rw-r--r--   0 runner    (1001) docker     (127)       89 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      822 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/layernorm_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/
--rw-r--r--   0 runner    (1001) docker     (127)       71 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      959 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/moe_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/optimizer/
--rw-r--r--   0 runner    (1001) docker     (127)      105 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1104 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/optimizer/fused_optimizer_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/
--rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)      589 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/triton_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/jit/
--rw-r--r--   0 runner    (1001) docker     (127)      317 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      670 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_dropout_add.py
--rw-r--r--   0 runner    (1001) docker     (127)     1358 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_gelu.py
--rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/option.py
--rw-r--r--   0 runner    (1001) docker     (127)     3649 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/kernel_loader.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/triton/
--rw-r--r--   0 runner    (1001) docker     (127)     1096 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14981 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/context_attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     2349 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/copy_kv_cache_dest.py
--rw-r--r--   0 runner    (1001) docker     (127)     7022 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/custom_autotune.py
--rw-r--r--   0 runner    (1001) docker     (127)     1804 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/flash_decoding.py
--rw-r--r--   0 runner    (1001) docker     (127)     2960 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/fused_layernorm.py
--rw-r--r--   0 runner    (1001) docker     (127)    18024 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/gptq_triton.py
--rw-r--r--   0 runner    (1001) docker     (127)     3280 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/int8_rotary_embedding_kernel.py
--rw-r--r--   0 runner    (1001) docker     (127)     6847 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/llama_act_combine_kernel.py
--rw-r--r--   0 runner    (1001) docker     (127)     4967 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/qkv_matmul_kernel.py
--rw-r--r--   0 runner    (1001) docker     (127)     5947 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/self_attention_nofusion.py
--rw-r--r--   0 runner    (1001) docker     (127)    21773 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/smooth_attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     3744 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/softmax.py
--rw-r--r--   0 runner    (1001) docker     (127)     7896 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/token_attention_kernel.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/lazy/
--rw-r--r--   0 runner    (1001) docker     (127)      107 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2187 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/construction.py
--rw-r--r--   0 runner    (1001) docker     (127)    25011 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/lazy_init.py
--rw-r--r--   0 runner    (1001) docker     (127)    13895 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/pretrained.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/
--rw-r--r--   0 runner    (1001) docker     (127)      301 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/
--rw-r--r--   0 runner    (1001) docker     (127)     2310 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      153 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/amp_type.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/
--rw-r--r--   0 runner    (1001) docker     (127)     1637 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1073 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/apex_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/
--rw-r--r--   0 runner    (1001) docker     (127)     2453 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13449 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     1741 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6081 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/naive_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/
--rw-r--r--   0 runner    (1001) docker     (127)     1538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    26771 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3368 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/torch_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/builder/
--rw-r--r--   0 runner    (1001) docker     (127)      166 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3025 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/builder/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/communication/
--rw-r--r--   0 runner    (1001) docker     (127)      868 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11387 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/collective.py
--rw-r--r--   0 runner    (1001) docker     (127)    17484 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p.py
--rw-r--r--   0 runner    (1001) docker     (127)     9047 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p_v2.py
--rw-r--r--   0 runner    (1001) docker     (127)     1945 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/ring.py
--rw-r--r--   0 runner    (1001) docker     (127)     5151 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      978 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/context/
--rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    24229 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_context.py
--rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_mode.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/
--rw-r--r--   0 runner    (1001) docker     (127)      763 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_1d.py
--rw-r--r--   0 runner    (1001) docker     (127)     6269 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2d.py
--rw-r--r--   0 runner    (1001) docker     (127)    12944 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (127)    13290 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_3d.py
--rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     2174 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     2652 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     4170 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_sequence.py
--rw-r--r--   0 runner    (1001) docker     (127)     2051 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/process_group_initializer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/
--rw-r--r--   0 runner    (1001) docker     (127)      420 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5204 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/_helper.py
--rw-r--r--   0 runner    (1001) docker     (127)     3407 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/seed_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/core.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/
--rw-r--r--   0 runner    (1001) docker     (127)       87 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7784 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/_base_engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/
--rw-r--r--   0 runner    (1001) docker     (127)     2558 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10265 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/
--rw-r--r--   0 runner    (1001) docker     (127)      537 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      750 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2138 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2488 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1148 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)      749 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1020 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/
--rw-r--r--   0 runner    (1001) docker     (127)      315 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5816 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_base_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     3808 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)    40085 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     7133 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py
--rw-r--r--   0 runner    (1001) docker     (127)     1993 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/global_variables.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     5935 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1345 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    13595 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/infer_batch.py
--rw-r--r--   0 runner    (1001) docker     (127)     5332 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/io_struct.py
--rw-r--r--   0 runner    (1001) docker     (127)     6304 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py
--rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_init_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     2810 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/req_queue.py
--rw-r--r--   0 runner    (1001) docker     (127)     3282 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/sampling_params.py
--rw-r--r--   0 runner    (1001) docker     (127)     1514 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/stats.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/
--rw-r--r--   0 runner    (1001) docker     (127)       65 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6972 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)       80 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    21591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/llama.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/
--rw-r--r--   0 runner    (1001) docker     (127)       78 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5477 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    11605 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/pipeline/
--rw-r--r--   0 runner    (1001) docker     (127)       83 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9019 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/pipeline/microbatch_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/
--rw-r--r--   0 runner    (1001) docker     (127)      123 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/batch_infer_state.py
--rw-r--r--   0 runner    (1001) docker     (127)    21286 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     4580 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/kvcache_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)      225 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    23642 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)    22757 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)    17826 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/llama.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/
--rw-r--r--   0 runner    (1001) docker     (127)      209 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)     2987 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    19842 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/
--rw-r--r--   0 runner    (1001) docker     (127)       63 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/
--rw-r--r--   0 runner    (1001) docker     (127)       22 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8669 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/
--rw-r--r--   0 runner    (1001) docker     (127)      242 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2817 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/base_layer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/
--rw-r--r--   0 runner    (1001) docker     (127)      300 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1404 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      999 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/dropout.py
--rw-r--r--   0 runner    (1001) docker     (127)     6156 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     5224 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/normalization.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/
--rw-r--r--   0 runner    (1001) docker     (127)      459 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3724 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)     5063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    44898 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/
--rw-r--r--   0 runner    (1001) docker     (127)      458 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    35857 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)      853 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    49324 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/
--rw-r--r--   0 runner    (1001) docker     (127)      494 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    39868 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)     1234 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    49527 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/
--rw-r--r--   0 runner    (1001) docker     (127)      498 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    22832 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)     3037 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    49624 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6392 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)      483 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    10693 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/
--rw-r--r--   0 runner    (1001) docker     (127)      445 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2411 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/common.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/
--rw-r--r--   0 runner    (1001) docker     (127)      345 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14693 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2170 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/
--rw-r--r--   0 runner    (1001) docker     (127)     1592 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_1d.py
--rw-r--r--   0 runner    (1001) docker     (127)     5732 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2d.py
--rw-r--r--   0 runner    (1001) docker     (127)     5540 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (127)     6436 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/
--rw-r--r--   0 runner    (1001) docker     (127)      686 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      148 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      787 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2d.py
--rw-r--r--   0 runner    (1001) docker     (127)      801 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (127)     1263 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/
--rw-r--r--   0 runner    (1001) docker     (127)       65 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6469 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/data_parallel.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/
--rw-r--r--   0 runner    (1001) docker     (127)      962 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/
--rw-r--r--   0 runner    (1001) docker     (127)      742 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1165 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    27958 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     8710 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py
--rw-r--r--   0 runner    (1001) docker     (127)      807 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     5690 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     9962 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py
--rw-r--r--   0 runner    (1001) docker     (127)     7138 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py
--rw-r--r--   0 runner    (1001) docker     (127)     1954 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/colo_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     1150 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     1136 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     4780 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/module_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3874 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/reducer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/
--rw-r--r--   0 runner    (1001) docker     (127)      163 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1558 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/layer_spec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/
--rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/
--rw-r--r--   0 runner    (1001) docker     (127)       79 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6151 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/fx.py
--rw-r--r--   0 runner    (1001) docker     (127)     6818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/topo.py
--rw-r--r--   0 runner    (1001) docker     (127)    11447 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipelinable.py
--rw-r--r--   0 runner    (1001) docker     (127)     5759 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipeline_process_group.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/
--rw-r--r--   0 runner    (1001) docker     (127)      237 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    59091 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    14979 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     5382 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     9017 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/registry/
--rw-r--r--   0 runner    (1001) docker     (127)      690 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/registry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3052 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/registry/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/
--rw-r--r--   0 runner    (1001) docker     (127)      418 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      779 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/compute_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/const.py
--rw-r--r--   0 runner    (1001) docker     (127)     8692 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/dist_spec_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     2714 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/distspec.py
--rw-r--r--   0 runner    (1001) docker     (127)     1678 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/op_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (127)    10505 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/process_group.py
--rw-r--r--   0 runner    (1001) docker     (127)      735 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/tensor_spec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/
--rw-r--r--   0 runner    (1001) docker     (127)       53 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14773 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/
--rw-r--r--   0 runner    (1001) docker     (127)      648 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2712 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_base_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     3226 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_checkpoint_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)      231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_commons_.py
--rw-r--r--   0 runner    (1001) docker     (127)    13085 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_log_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     2091 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)    16242 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_metric_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/
--rw-r--r--   0 runner    (1001) docker     (127)     1438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9872 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/activation_checkpoint.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/
--rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5297 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/module_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    11338 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpointing.py
--rw-r--r--   0 runner    (1001) docker     (127)    16595 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/common.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/
--rw-r--r--   0 runner    (1001) docker     (127)      177 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      339 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/base_sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     6704 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     6416 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/memory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/
--rw-r--r--   0 runner    (1001) docker     (127)       52 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      341 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/extention.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/
--rw-r--r--   0 runner    (1001) docker     (127)      266 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10461 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/comm_profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4861 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3776 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/prof_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     8459 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4036 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/
--rw-r--r--   0 runner    (1001) docker     (127)     1570 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/
--rw-r--r--   0 runner    (1001) docker     (127)      619 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7315 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/colo_init_context.py
--rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/gemini_context.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/
--rw-r--r--   0 runner    (1001) docker     (127)      118 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      774 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py
--rw-r--r--   0 runner    (1001) docker     (127)     1363 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py
--rw-r--r--   0 runner    (1001) docker     (127)     5081 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     4715 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/
--rw-r--r--   0 runner    (1001) docker     (127)       77 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1251 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     6905 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     3965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     6369 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_placement_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)     3807 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/
--rw-r--r--   0 runner    (1001) docker     (127)      171 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11099 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/init_context.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/
--rw-r--r--   0 runner    (1001) docker     (127)      259 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      635 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/base_shard_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)     2296 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)      706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/commons.py
--rw-r--r--   0 runner    (1001) docker     (127)     2751 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/
--rw-r--r--   0 runner    (1001) docker     (127)       75 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3003 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     8289 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/reduce_scatter.py
--rw-r--r--   0 runner    (1001) docker     (127)    28757 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/sharded_model_v2.py
--rw-r--r--   0 runner    (1001) docker     (127)      808 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4865 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/zero_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/
--rw-r--r--   0 runner    (1001) docker     (127)       83 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18982 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/
--rw-r--r--   0 runner    (1001) docker     (127)      131 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3859 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_param.py
--rw-r--r--   0 runner    (1001) docker     (127)     1144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/logging/
--rw-r--r--   0 runner    (1001) docker     (127)     1593 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/logging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6110 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/logging/logger.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/moe/
--rw-r--r--   0 runner    (1001) docker     (127)      531 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12432 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)    36076 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (127)     6249 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/experts.py
--rw-r--r--   0 runner    (1001) docker     (127)    16261 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/layers.py
--rw-r--r--   0 runner    (1001) docker     (127)    18267 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/load_balance.py
--rw-r--r--   0 runner    (1001) docker     (127)     3075 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/loss.py
--rw-r--r--   0 runner    (1001) docker     (127)     6138 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/manager.py
--rw-r--r--   0 runner    (1001) docker     (127)    20401 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/routers.py
--rw-r--r--   0 runner    (1001) docker     (127)     7403 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/
--rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9563 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/init.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/layer/
--rw-r--r--   0 runner    (1001) docker     (127)       21 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2497 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/layernorm.py
--rw-r--r--   0 runner    (1001) docker     (127)     6739 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/scaled_softmax.py
--rw-r--r--   0 runner    (1001) docker     (127)      449 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/loss/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/loss/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/
--rw-r--r--   0 runner    (1001) docker     (127)      673 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5867 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/cosine.py
--rw-r--r--   0 runner    (1001) docker     (127)     7717 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/delayed.py
--rw-r--r--   0 runner    (1001) docker     (127)     1178 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     2672 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/multistep.py
--rw-r--r--   0 runner    (1001) docker     (127)     5050 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/onecycle.py
--rw-r--r--   0 runner    (1001) docker     (127)     2458 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/poly.py
--rw-r--r--   0 runner    (1001) docker     (127)     3516 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/torch.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/
--rw-r--r--   0 runner    (1001) docker     (127)      303 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8611 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/cpu_adam.py
--rw-r--r--   0 runner    (1001) docker     (127)     6478 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_adam.py
--rw-r--r--   0 runner    (1001) docker     (127)     9056 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_lamb.py
--rw-r--r--   0 runner    (1001) docker     (127)     6012 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_sgd.py
--rw-r--r--   0 runner    (1001) docker     (127)     8004 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/hybrid_adam.py
--rw-r--r--   0 runner    (1001) docker     (127)     4423 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lamb.py
--rw-r--r--   0 runner    (1001) docker     (127)     3567 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lars.py
--rw-r--r--   0 runner    (1001) docker     (127)     6768 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/nvme_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/pipeline/
--rw-r--r--   0 runner    (1001) docker     (127)      344 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    26607 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/p2p.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/
--rw-r--r--   0 runner    (1001) docker     (127)      241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5194 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1478 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/base.py
--rw-r--r--   0 runner    (1001) docker     (127)    20009 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/generate.py
--rw-r--r--   0 runner    (1001) docker     (127)    26918 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/interleaved_pp.py
--rw-r--r--   0 runner    (1001) docker     (127)    19591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/one_f_one_b.py
--rw-r--r--   0 runner    (1001) docker     (127)     9552 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/stage_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13038 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/quantization/bnb.py
--rw-r--r--   0 runner    (1001) docker     (127)     4399 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/quantization/bnb_config.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/shardformer/
--rw-r--r--   0 runner    (1001) docker     (127)      118 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3327 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/
--rw-r--r--   0 runner    (1001) docker     (127)     1119 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    38285 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)    13198 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/attn.py
--rw-r--r--   0 runner    (1001) docker     (127)     3520 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/dropout.py
--rw-r--r--   0 runner    (1001) docker     (127)    15865 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    26710 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     5018 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/loss.py
--rw-r--r--   0 runner    (1001) docker     (127)    11530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)    17676 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/parallel_module.py
--rw-r--r--   0 runner    (1001) docker     (127)    31848 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/qkv_fused_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)    10572 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.389213 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    62664 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bert.py
--rw-r--r--   0 runner    (1001) docker     (127)     5064 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)    49669 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)    17629 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.389213 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2249 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py
--rw-r--r--   0 runner    (1001) docker     (127)    54608 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py
--rw-r--r--   0 runner    (1001) docker     (127)    34314 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)    59838 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gpt2.py
--rw-r--r--   0 runner    (1001) docker     (127)    44054 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)     1053 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/jit.py
--rw-r--r--   0 runner    (1001) docker     (127)    46388 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    27319 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)    36942 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/opt.py
--rw-r--r--   0 runner    (1001) docker     (127)     8328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/sam.py
--rw-r--r--   0 runner    (1001) docker     (127)    37484 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/t5.py
--rw-r--r--   0 runner    (1001) docker     (127)    16136 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/vit.py
--rw-r--r--   0 runner    (1001) docker     (127)    54363 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/whisper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.393213 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10821 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/auto_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)     8822 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/base_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)    27227 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bert.py
--rw-r--r--   0 runner    (1001) docker     (127)    15862 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)    17356 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)    12232 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)    16474 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)    22241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/gpt2.py
--rw-r--r--   0 runner    (1001) docker     (127)    14001 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)    18711 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    14549 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)    13829 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/opt.py
--rw-r--r--   0 runner    (1001) docker     (127)     9682 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/sam.py
--rw-r--r--   0 runner    (1001) docker     (127)    22174 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/t5.py
--rw-r--r--   0 runner    (1001) docker     (127)    10928 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/vit.py
--rw-r--r--   0 runner    (1001) docker     (127)    23410 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/whisper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.393213 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/
--rw-r--r--   0 runner    (1001) docker     (127)      320 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3763 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/grad_ckpt_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     6642 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/shard_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     9749 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/sharder.py
--rw-r--r--   0 runner    (1001) docker     (127)     1778 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/shardformer.py
--rw-r--r--   0 runner    (1001) docker     (127)      550 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.393213 colossalai-nightly-2024.5.4/colossalai/tensor/
--rw-r--r--   0 runner    (1001) docker     (127)      600 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3447 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/colo_parameter.py
--rw-r--r--   0 runner    (1001) docker     (127)     3349 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/colo_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)    21633 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/comm_spec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/
--rw-r--r--   0 runner    (1001) docker     (127)     1203 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18034 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/api.py
--rw-r--r--   0 runner    (1001) docker     (127)    11144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/comm_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)     2776 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout.py
--rw-r--r--   0 runner    (1001) docker     (127)    27564 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout_converter.py
--rw-r--r--   0 runner    (1001) docker     (127)      231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/misc.py
--rw-r--r--   0 runner    (1001) docker     (127)     9293 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/sharding_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)     3263 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3778 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/api.py
--rw-r--r--   0 runner    (1001) docker     (127)     1413 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/moe_info.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/tensor/padded_tensor/
--rw-r--r--   0 runner    (1001) docker     (127)      194 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/padded_tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3527 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/padded_tensor/api.py
--rw-r--r--   0 runner    (1001) docker     (127)     5169 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/param_op_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)    35882 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/shape_consistency.py
--rw-r--r--   0 runner    (1001) docker     (127)    11626 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/sharding_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)     8452 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/testing/
--rw-r--r--   0 runner    (1001) docker     (127)      865 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/comparison.py
--rw-r--r--   0 runner    (1001) docker     (127)     1406 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/pytest_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (127)      542 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/random.py
--rw-r--r--   0 runner    (1001) docker     (127)     9201 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/
--rw-r--r--   0 runner    (1001) docker     (127)      586 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1938 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/common.py
--rw-r--r--   0 runner    (1001) docker     (127)     2470 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/memory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/model/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3905 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/model/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/
--rw-r--r--   0 runner    (1001) docker     (127)       90 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5453 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/rank_recorder.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/
--rw-r--r--   0 runner    (1001) docker     (127)       44 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8392 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/tensor_detector.py
--rw-r--r--   0 runner    (1001) docker     (127)     4321 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/timer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/
--rw-r--r--   0 runner    (1001) docker     (127)      390 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/gemini/
--rw-r--r--   0 runner    (1001) docker     (127)      490 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/
--rw-r--r--   0 runner    (1001) docker     (127)      342 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    25030 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/chunk.py
--rw-r--r--   0 runner    (1001) docker     (127)    11791 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     6291 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/search_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1428 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    42911 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_ddp.py
--rw-r--r--   0 runner    (1001) docker     (127)     2458 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     6243 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)    37706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/
--rw-r--r--   0 runner    (1001) docker     (127)      511 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1281 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     4063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_monitor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_stats.py
--rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)      858 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/param_runtime_order.py
--rw-r--r--   0 runner    (1001) docker     (127)     3716 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py
--rw-r--r--   0 runner    (1001) docker     (127)     4126 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     1795 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     9569 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/placement_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)     4213 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/low_level/
--rw-r--r--   0 runner    (1001) docker     (127)       88 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7283 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/
--rw-r--r--   0 runner    (1001) docker     (127)      242 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      409 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/base_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     4545 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/bucket_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     4362 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/gradient_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     1502 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/parameter_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     1494 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/tensor_bucket.py
--rw-r--r--   0 runner    (1001) docker     (127)    43271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/low_level_optim.py
--rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)    36765 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    48382 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)       69 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (127)      205 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)       37 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.277213 colossalai-nightly-2024.5.4/examples/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/examples/language/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4227 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      713 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/model_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4385 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/performance_evaluator.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/extensions/
--rw-r--r--   0 runner    (1001) docker     (127)     1127 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2499 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/base_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     4730 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpp_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/extensions/cpu_adam/
--rw-r--r--   0 runner    (1001) docker     (127)      150 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpu_adam/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1025 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpu_adam/cpu_adam_arm.py
--rw-r--r--   0 runner    (1001) docker     (127)     1624 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpu_adam/cpu_adam_x86.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/extensions/csrc/
--rw-r--r--   0 runner    (1001) docker     (127)      350 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.409213 colossalai-nightly-2024.5.4/extensions/csrc/arm/
--rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.409213 colossalai-nightly-2024.5.4/extensions/csrc/cuda/
--rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/colossal_C_frontend.cpp
--rw-r--r--   0 runner    (1001) docker     (127)      214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/compat.h
--rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.409213 colossalai-nightly-2024.5.4/extensions/csrc/cuda/include/
--rw-r--r--   0 runner    (1001) docker     (127)     8585 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/include/block_reduce.h
--rw-r--r--   0 runner    (1001) docker     (127)     4878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    25829 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    25627 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5046 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_adam.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5200 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_apply.cuh
--rw-r--r--   0 runner    (1001) docker     (127)    13279 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    13115 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_lamb.cu
--rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_scale_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     6479 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     2684 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    21112 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax.h
--rw-r--r--   0 runner    (1001) docker     (127)     3467 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
--rw-r--r--   0 runner    (1001) docker     (127)     2066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    23530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
--rw-r--r--   0 runner    (1001) docker     (127)     2818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
--rw-r--r--   0 runner    (1001) docker     (127)    14851 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/type_shim.h
--rw-r--r--   0 runner    (1001) docker     (127)     6588 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/scaled_softmax.py
--rw-r--r--   0 runner    (1001) docker     (127)     3878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cuda_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/flash_attention/
--rw-r--r--   0 runner    (1001) docker     (127)      470 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3760 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_dao_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     1922 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_npu.py
--rw-r--r--   0 runner    (1001) docker     (127)     1799 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_sdpa_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/layernorm/
--rw-r--r--   0 runner    (1001) docker     (127)       89 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/layernorm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      822 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/layernorm/layernorm_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/moe/
--rw-r--r--   0 runner    (1001) docker     (127)       71 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      959 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/moe/moe_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/optimizer/
--rw-r--r--   0 runner    (1001) docker     (127)      105 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1104 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/optimizer/fused_optimizer_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/softmax/
--rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/softmax/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/softmax/scaled_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)      589 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/triton_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/requirements/
--rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/requirements/requirements-infer.txt
--rw-r--r--   0 runner    (1001) docker     (127)      557 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/requirements/requirements-test.txt
--rw-r--r--   0 runner    (1001) docker     (127)      205 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/requirements/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (127)     4379 2024-05-04 00:14:37.000000 colossalai-nightly-2024.5.4/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.277213 colossalai-nightly-2024.5.4/tests/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/
--rw-r--r--   0 runner    (1001) docker     (127)     1070 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/
--rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      832 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     1140 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/hanging_param_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/nested_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/repeated_computed_layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1259 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/simple_net.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/
--rw-r--r--   0 runner    (1001) docker     (127)       25 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2534 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/diffusers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1499 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/executor.py
--rw-r--r--   0 runner    (1001) docker     (127)     3413 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/
--rw-r--r--   0 runner    (1001) docker     (127)       20 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5975 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/timm.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/
--rw-r--r--   0 runner    (1001) docker     (127)       26 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4614 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/torchaudio.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/
--rw-r--r--   0 runner    (1001) docker     (127)       24 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3899 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/torchrec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/
--rw-r--r--   0 runner    (1001) docker     (127)       27 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4970 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/torchvision.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/
--rw-r--r--   0 runner    (1001) docker     (127)      408 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3768 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/albert.py
--rw-r--r--   0 runner    (1001) docker     (127)    12561 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bert.py
--rw-r--r--   0 runner    (1001) docker     (127)     2289 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)     4487 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)     2282 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)     4251 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)     6361 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/gpt.py
--rw-r--r--   0 runner    (1001) docker     (127)     3597 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)     3328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)     2837 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)     3063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/opt.py
--rw-r--r--   0 runner    (1001) docker     (127)     1833 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/sam.py
--rw-r--r--   0 runner    (1001) docker     (127)     3061 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/t5.py
--rw-r--r--   0 runner    (1001) docker     (127)     2214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/vit.py
--rw-r--r--   0 runner    (1001) docker     (127)     3717 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/whisper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/test_analyzer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3684 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_bias_addition.py
--rw-r--r--   0 runner    (1001) docker     (127)     2139 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_mod_dir.py
--rw-r--r--   0 runner    (1001) docker     (127)     1750 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_nested_ckpt.py
--rw-r--r--   0 runner    (1001) docker     (127)     2127 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_shape_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_symbolic_profile.py
--rw-r--r--   0 runner    (1001) docker     (127)     1271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/zoo.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3714 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_aten.py
--rw-r--r--   0 runner    (1001) docker     (127)     1893 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_flop_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_meta_mode.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1773 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_node_converting_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)     2250 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2702 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py
--rw-r--r--   0 runner    (1001) docker     (127)     2003 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py
--rw-r--r--   0 runner    (1001) docker     (127)     2446 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (127)     3383 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py
--rw-r--r--   0 runner    (1001) docker     (127)     3733 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py
--rw-r--r--   0 runner    (1001) docker     (127)     3674 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10992 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py
--rw-r--r--   0 runner    (1001) docker     (127)     7729 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py
--rw-r--r--   0 runner    (1001) docker     (127)     3818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     2052 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.425213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11120 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     7521 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4701 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     6548 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py
--rw-r--r--   0 runner    (1001) docker     (127)     6034 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py
--rw-r--r--   0 runner    (1001) docker     (127)    10708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     8717 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12574 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3595 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    11426 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2942 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     7708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12825 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     8338 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2552 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2556 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    18264 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3056 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4168 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py
--rw-r--r--   0 runner    (1001) docker     (127)     8698 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12216 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12137 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2781 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py
--rw-r--r--   0 runner    (1001) docker     (127)     3625 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    13847 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3556 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     8505 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5283 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/tests/test_shardformer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5683 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_flash_attention.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12502 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     8340 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_bert.py
--rw-r--r--   0 runner    (1001) docker     (127)     3196 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)     7723 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)     8331 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)     6763 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)     8635 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gpt2.py
--rw-r--r--   0 runner    (1001) docker     (127)     7743 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)    12103 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_llama.py
--rw-r--r--   0 runner    (1001) docker     (127)     5625 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)     7402 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_opt.py
--rw-r--r--   0 runner    (1001) docker     (127)     2557 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_sam.py
--rw-r--r--   0 runner    (1001) docker     (127)     7227 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_t5.py
--rw-r--r--   0 runner    (1001) docker     (127)     6528 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_vit.py
--rw-r--r--   0 runner    (1001) docker     (127)     7293 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_whisper.py
--rw-r--r--   0 runner    (1001) docker     (127)      833 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_shard_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2623 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_with_torch_ddp.py
--rw-r--r--   0 runner    (1001) docker     (127)        6 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/version.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.984568 colossalai-nightly-2024.6.1/
+-rw-r--r--   0 runner    (1001) docker     (127)    30134 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (127)      198 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (127)    36484 2024-06-01 00:16:37.984568 colossalai-nightly-2024.6.1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    30954 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.832567 colossalai-nightly-2024.6.1/colossalai/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.832567 colossalai-nightly-2024.6.1/colossalai/_C/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_C/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      597 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.832567 colossalai-nightly-2024.6.1/colossalai/_analyzer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.836567 colossalai-nightly-2024.6.1/colossalai/_analyzer/_subclasses/
+-rw-r--r--   0 runner    (1001) docker     (127)      165 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/_subclasses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20868 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/_subclasses/_meta_registration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2088 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/_subclasses/_monkey_patch.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18677 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/_subclasses/flop_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7589 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/_subclasses/meta_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/envs.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.836567 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/
+-rw-r--r--   0 runner    (1001) docker     (127)      129 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18998 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/codegen.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9947 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/graph_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8482 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/node_util.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.836567 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/passes/
+-rw-r--r--   0 runner    (1001) docker     (127)      106 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12708 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/passes/graph_profile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9939 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/passes/shape_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)      952 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/symbolic_profile.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.836567 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/
+-rw-r--r--   0 runner    (1001) docker     (127)       63 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5415 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/bias_addition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/custom_leaf_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3568 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/proxy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5862 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/symbolic_trace.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15712 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/tracer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.836567 colossalai-nightly-2024.6.1/colossalai/accelerator/
+-rw-r--r--   0 runner    (1001) docker     (127)      431 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/accelerator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/accelerator/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9402 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/accelerator/base_accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10154 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/accelerator/cpu_accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9442 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/accelerator/cuda_accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9453 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/accelerator/npu_accelerator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.836567 colossalai-nightly-2024.6.1/colossalai/amp/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/amp/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.836567 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.840567 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/grad_scaler/
+-rw-r--r--   0 runner    (1001) docker     (127)      222 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/grad_scaler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2119 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)      735 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4977 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.840567 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/mixed_precision_mixin/
+-rw-r--r--   0 runner    (1001) docker     (127)      226 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/mixed_precision_mixin/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2523 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/mixed_precision_mixin/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)      504 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/mixed_precision_mixin/bf16.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2695 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7959 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/mixed_precision_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.840567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.840567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/checkpoint/
+-rw-r--r--   0 runner    (1001) docker     (127)      155 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/checkpoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      377 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/checkpoint/build_c_ext.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7711 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3468 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18538 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4921 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/checkpoint/operation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.840567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/
+-rw-r--r--   0 runner    (1001) docker     (127)       95 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      296 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.840567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/
+-rw-r--r--   0 runner    (1001) docker     (127)      241 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3940 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2840 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7571 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2512 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24482 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9318 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7392 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3258 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2827 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/where.py
+-rw-r--r--   0 runner    (1001) docker     (127)      761 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4748 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/shard_metainfo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.844567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6826 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/amp_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3584 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/base_offload_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2072 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/mem_optimize.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5167 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/region.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20031 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/region_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9909 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/runtime.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18503 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/solver.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17919 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/training_simulator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2793 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/util.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.844567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/passes/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5111 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/passes/comm_metainfo_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)      417 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/passes/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6050 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/passes/meta_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11328 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/passes/runtime_apply_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22439 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/passes/runtime_preparation_pass.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.844567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/pipeline_shard/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/pipeline_shard/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.844567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2805 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16998 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.848567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/
+-rw-r--r--   0 runner    (1001) docker     (127)     2062 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3731 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3070 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4988 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4805 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5563 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11414 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1316 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1734 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1901 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13535 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20347 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16308 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1762 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2115 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2997 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1466 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)      744 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1980 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.852567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/
+-rw-r--r--   0 runner    (1001) docker     (127)     2100 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14274 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5162 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24078 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12070 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3615 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7361 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9225 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41664 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4597 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3636 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18860 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4702 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13053 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4965 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2278 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3981 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4158 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3053 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1156 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2330 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1774 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1964 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3553 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/options.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10812 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/sharding_strategy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.852567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/solver/
+-rw-r--r--   0 runner    (1001) docker     (127)      238 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/solver/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9987 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5767 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20206 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/solver/solver.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8474 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.852567 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     1210 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5899 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/broadcast.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8346 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3841 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9066 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/reshape.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4408 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/sharding.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.852567 colossalai-nightly-2024.6.1/colossalai/booster/
+-rw-r--r--   0 runner    (1001) docker     (127)       93 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1481 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20278 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/booster.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.852567 colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (127)     1298 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      102 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/bf16.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3218 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/fp16_apex.py
+-rw-r--r--   0 runner    (1001) docker     (127)      870 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/fp16_naive.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4824 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/fp16_torch.py
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/fp8.py
+-rw-r--r--   0 runner    (1001) docker     (127)      565 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/mixed_precision_base.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.856567 colossalai-nightly-2024.6.1/colossalai/booster/plugin/
+-rw-r--r--   0 runner    (1001) docker     (127)      529 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/plugin/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3098 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/plugin/dp_plugin_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28296 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/plugin/gemini_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    65054 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/plugin/hybrid_parallel_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20867 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/plugin/low_level_zero_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20853 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2475 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/plugin/plugin_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)      559 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/plugin/pp_plugin_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9770 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/plugin/torch_ddp_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15017 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/booster/plugin/torch_fsdp_plugin.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.856567 colossalai-nightly-2024.6.1/colossalai/checkpoint_io/
+-rw-r--r--   0 runner    (1001) docker     (127)      318 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/checkpoint_io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15804 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/checkpoint_io/checkpoint_io_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8761 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/checkpoint_io/general_checkpoint_io.py
+-rw-r--r--   0 runner    (1001) docker     (127)    43972 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5758 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/checkpoint_io/index_file.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29632 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/checkpoint_io/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.856567 colossalai-nightly-2024.6.1/colossalai/cli/
+-rw-r--r--   0 runner    (1001) docker     (127)       40 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cli/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.856567 colossalai-nightly-2024.6.1/colossalai/cli/check/
+-rw-r--r--   0 runner    (1001) docker     (127)      396 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cli/check/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8454 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cli/check/check_installation.py
+-rw-r--r--   0 runner    (1001) docker     (127)      310 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cli/cli.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.856567 colossalai-nightly-2024.6.1/colossalai/cli/launcher/
+-rw-r--r--   0 runner    (1001) docker     (127)     3654 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cli/launcher/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3214 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cli/launcher/hostinfo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4286 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cli/launcher/multinode_runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10530 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cli/launcher/run.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.860567 colossalai-nightly-2024.6.1/colossalai/cluster/
+-rw-r--r--   0 runner    (1001) docker     (127)      296 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cluster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4241 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cluster/device_mesh_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7233 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cluster/dist_coordinator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2356 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cluster/process_group_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10963 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/cluster/process_group_mesh.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.860567 colossalai-nightly-2024.6.1/colossalai/context/
+-rw-r--r--   0 runner    (1001) docker     (127)       96 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/context/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3153 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/context/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)      921 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/context/singleton_meta.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.860567 colossalai-nightly-2024.6.1/colossalai/device/
+-rw-r--r--   0 runner    (1001) docker     (127)      139 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/device/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17443 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/device/alpha_beta_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5634 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/device/calc_pipeline_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23559 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/device/device_mesh.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.860567 colossalai-nightly-2024.6.1/colossalai/fx/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/_compatibility.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19328 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/_meta_regist_12.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1906 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/_meta_regist_13.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.860567 colossalai-nightly-2024.6.1/colossalai/fx/codegen/
+-rw-r--r--   0 runner    (1001) docker     (127)       45 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/codegen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45231 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/codegen/activation_checkpoint_codegen.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7438 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/graph_module.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.860567 colossalai-nightly-2024.6.1/colossalai/fx/passes/
+-rw-r--r--   0 runner    (1001) docker     (127)      266 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13496 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/passes/adding_split_node_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12261 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/passes/concrete_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14066 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/passes/meta_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15886 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/passes/passes_for_gpt2_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6888 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/passes/shard_1d_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13714 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/passes/split_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6169 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/passes/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.864567 colossalai-nightly-2024.6.1/colossalai/fx/profiler/
+-rw-r--r--   0 runner    (1001) docker     (127)      768 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      871 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6285 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/dataflow.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.864567 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/
+-rw-r--r--   0 runner    (1001) docker     (127)      282 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      782 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7062 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.864567 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/
+-rw-r--r--   0 runner    (1001) docker     (127)      211 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1304 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3387 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py
+-rw-r--r--   0 runner    (1001) docker     (127)      632 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      438 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2027 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1188 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)      402 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/python_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2188 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.864567 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/
+-rw-r--r--   0 runner    (1001) docker     (127)      252 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2037 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6708 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (127)      424 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/dropout.py
+-rw-r--r--   0 runner    (1001) docker     (127)      431 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      495 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1582 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1013 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3020 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/rnn.py
+-rw-r--r--   0 runner    (1001) docker     (127)      271 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/torch_op.py
+-rw-r--r--   0 runner    (1001) docker     (127)      603 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1050 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/shard_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2232 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/memory_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13214 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/opcount.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15377 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4235 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/shard_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4990 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/profiler/tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4010 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/proxy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.868567 colossalai-nightly-2024.6.1/colossalai/fx/tracer/
+-rw-r--r--   0 runner    (1001) docker     (127)      201 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4850 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/_meta_trace.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2197 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/_symbolic_trace.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1479 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/_tracer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.868567 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/
+-rw-r--r--   0 runner    (1001) docker     (127)       90 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.868567 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/
+-rw-r--r--   0 runner    (1001) docker     (127)      193 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2171 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4445 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)      745 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.868567 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/
+-rw-r--r--   0 runner    (1001) docker     (127)       78 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4395 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2370 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)      503 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26431 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/experimental.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.868567 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/
+-rw-r--r--   0 runner    (1001) docker     (127)       62 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.868567 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/
+-rw-r--r--   0 runner    (1001) docker     (127)      167 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3200 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5707 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (127)      339 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      517 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2047 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5622 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.872567 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/
+-rw-r--r--   0 runner    (1001) docker     (127)      180 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      428 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4752 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (127)      254 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      400 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1129 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6769 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)      644 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/rnn.py
+-rw-r--r--   0 runner    (1001) docker     (127)      834 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24642 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/fx/tracer/tracer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.872567 colossalai-nightly-2024.6.1/colossalai/inference/
+-rw-r--r--   0 runner    (1001) docker     (127)      120 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24213 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/batch_bucket.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16668 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/config.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.872567 colossalai-nightly-2024.6.1/colossalai/inference/core/
+-rw-r--r--   0 runner    (1001) docker     (127)      129 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12749 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/core/async_engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33976 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/core/engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5543 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/core/plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16508 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/core/request_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11954 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/core/rpc_engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3098 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/flash_decoding_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3802 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/graph_runner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.872567 colossalai-nightly-2024.6.1/colossalai/inference/kv_cache/
+-rw-r--r--   0 runner    (1001) docker     (127)      164 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/kv_cache/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2081 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/kv_cache/block_cache.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27863 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/kv_cache/kvcache_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6685 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/logit_processors.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.872567 colossalai-nightly-2024.6.1/colossalai/inference/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.872567 colossalai-nightly-2024.6.1/colossalai/inference/modeling/layers/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13669 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/layers/attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1395 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/layers/baichuan_tp_linear.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.876567 colossalai-nightly-2024.6.1/colossalai/inference/modeling/models/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19467 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/models/glide_llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17728 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/models/nopadding_baichuan.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30542 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/models/nopadding_llama.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.876567 colossalai-nightly-2024.6.1/colossalai/inference/modeling/policy/
+-rw-r--r--   0 runner    (1001) docker     (127)      501 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/policy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1575 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/policy/glide_llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4583 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/policy/nopadding_baichuan.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4556 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/modeling/policy/nopadding_llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4346 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/sampler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.876567 colossalai-nightly-2024.6.1/colossalai/inference/server/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/server/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8381 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/server/api_server.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5876 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/server/chat_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1072 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/server/completion_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)      713 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/server/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.876567 colossalai-nightly-2024.6.1/colossalai/inference/spec/
+-rw-r--r--   0 runner    (1001) docker     (127)      129 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/spec/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4578 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/spec/drafter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2150 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/spec/struct.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5269 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/struct.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4136 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/inference/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6088 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.876567 colossalai-nightly-2024.6.1/colossalai/interface/
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/interface/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      851 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/interface/model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5156 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/interface/optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      333 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/interface/pretrained.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.876567 colossalai-nightly-2024.6.1/colossalai/kernel/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.876567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/
+-rw-r--r--   0 runner    (1001) docker     (127)     1306 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2499 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/base_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4879 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/cpp_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.876567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.880567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      694 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/common/data_type.h
+-rw-r--r--   0 runner    (1001) docker     (127)    14986 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/common/micros.h
+-rw-r--r--   0 runner    (1001) docker     (127)      901 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/common/mp_type_traits.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2886 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/common/target.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2797 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/common/vec_type_traits.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.880567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/funcs/
+-rw-r--r--   0 runner    (1001) docker     (127)    10622 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/funcs/binary_functor.h
+-rw-r--r--   0 runner    (1001) docker     (127)    21630 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/funcs/cast_functor.h
+-rw-r--r--   0 runner    (1001) docker     (127)     3932 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/funcs/reduce_function.h
+-rw-r--r--   0 runner    (1001) docker     (127)     8393 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/funcs/ternary_functor.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2460 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/funcs/unary_functor.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.820567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.880567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/arm/
+-rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.884567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/
+-rw-r--r--   0 runner    (1001) docker     (127)     2614 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/activation_kernel.cu
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.884567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/attention/
+-rw-r--r--   0 runner    (1001) docker     (127)     6820 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/attention/attention_utils.h
+-rw-r--r--   0 runner    (1001) docker     (127)    10773 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     3931 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/convert_fp8_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    10013 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    41480 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    21861 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     9781 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    25833 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/layer_norm_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    25880 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/moe_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5050 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5207 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_apply.cuh
+-rw-r--r--   0 runner    (1001) docker     (127)    15325 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    13119 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     4444 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     6486 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    14092 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/rms_layernorm_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    22113 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    23452 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.884567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     2538 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/utils/gpu_launch_config.h
+-rw-r--r--   0 runner    (1001) docker     (127)      545 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/utils/micros.h
+-rw-r--r--   0 runner    (1001) docker     (127)     1287 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/utils/nvgpu_dev_info.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2063 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/utils/vec_copy.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.884567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/x86/
+-rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.h
+-rw-r--r--   0 runner    (1001) docker     (127)     4131 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/cuda_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.884567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.884567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/cpu_adam/
+-rw-r--r--   0 runner    (1001) docker     (127)      150 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/cpu_adam/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1090 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/cpu_adam/cpu_adam_arm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/cpu_adam/cpu_adam_x86.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.884567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/flash_attention/
+-rw-r--r--   0 runner    (1001) docker     (127)      470 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/flash_attention/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3761 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_dao_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1923 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_npu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1800 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.884567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/inference/
+-rw-r--r--   0 runner    (1001) docker     (127)       99 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/inference/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5125 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/inference/inference.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     1272 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/inference/inference_ops_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.884567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/layernorm/
+-rw-r--r--   0 runner    (1001) docker     (127)       89 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/layernorm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4885 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/layernorm/layer_norm.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)      923 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/layernorm/layernorm_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.884567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/moe/
+-rw-r--r--   0 runner    (1001) docker     (127)       71 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/moe/moe.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)      898 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/moe/moe_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.888567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      105 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1089 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/optimizer/fused_optimizer_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/optimizer/optimizer.cpp
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.888567 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/softmax/
+-rw-r--r--   0 runner    (1001) docker     (127)      271 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/softmax/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2055 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     1002 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1690 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     1069 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)      589 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/triton_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/extensions/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.888567 colossalai-nightly-2024.6.1/colossalai/kernel/jit/
+-rw-r--r--   0 runner    (1001) docker     (127)      317 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/jit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      670 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/jit/bias_dropout_add.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1358 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/jit/bias_gelu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/jit/option.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3791 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/kernel_loader.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.888567 colossalai-nightly-2024.6.1/colossalai/kernel/triton/
+-rw-r--r--   0 runner    (1001) docker     (127)     1078 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/triton/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26677 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/triton/context_attn_unpad.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20403 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/triton/flash_decoding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4857 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/triton/fused_rotary_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10761 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/triton/kvcache_copy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6847 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/triton/llama_act_combine_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20790 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/triton/no_pad_rotary_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4967 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/triton/qkv_matmul_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4555 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/triton/rms_layernorm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4897 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/triton/rotary_cache_copy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3744 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/kernel/triton/softmax.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.888567 colossalai-nightly-2024.6.1/colossalai/lazy/
+-rw-r--r--   0 runner    (1001) docker     (127)      107 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/lazy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2187 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/lazy/construction.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25011 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/lazy/lazy_init.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14790 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/lazy/pretrained.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.892567 colossalai-nightly-2024.6.1/colossalai/legacy/
+-rw-r--r--   0 runner    (1001) docker     (127)      301 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.892567 colossalai-nightly-2024.6.1/colossalai/legacy/amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     2310 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      153 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/amp/amp_type.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.892567 colossalai-nightly-2024.6.1/colossalai/legacy/amp/apex_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     1637 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/amp/apex_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1073 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/amp/apex_amp/apex_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.892567 colossalai-nightly-2024.6.1/colossalai/legacy/amp/naive_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     2453 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/amp/naive_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13449 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1741 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/amp/naive_amp/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6081 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/amp/naive_amp/naive_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.892567 colossalai-nightly-2024.6.1/colossalai/legacy/amp/torch_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     1538 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/amp/torch_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26771 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/amp/torch_amp/_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3368 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/amp/torch_amp/torch_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.892567 colossalai-nightly-2024.6.1/colossalai/legacy/builder/
+-rw-r--r--   0 runner    (1001) docker     (127)      166 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3025 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/builder/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.892567 colossalai-nightly-2024.6.1/colossalai/legacy/communication/
+-rw-r--r--   0 runner    (1001) docker     (127)      868 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/communication/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11387 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/communication/collective.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17484 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/communication/p2p.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9047 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/communication/p2p_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1945 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/communication/ring.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5151 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/communication/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      978 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.892567 colossalai-nightly-2024.6.1/colossalai/legacy/context/
+-rw-r--r--   0 runner    (1001) docker     (127)      149 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24229 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/parallel_context.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/parallel_mode.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.896567 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/
+-rw-r--r--   0 runner    (1001) docker     (127)      763 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2144 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_1d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6269 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_2d.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12944 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13290 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_3d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2174 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2652 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4170 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_sequence.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2051 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/process_group_initializer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.896567 colossalai-nightly-2024.6.1/colossalai/legacy/context/random/
+-rw-r--r--   0 runner    (1001) docker     (127)      420 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/random/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5204 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/random/_helper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3407 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/context/random/seed_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)      149 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/core.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.896567 colossalai-nightly-2024.6.1/colossalai/legacy/engine/
+-rw-r--r--   0 runner    (1001) docker     (127)       87 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7784 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/_base_engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.896567 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_accumulation/
+-rw-r--r--   0 runner    (1001) docker     (127)     2558 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_accumulation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10265 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.896567 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/
+-rw-r--r--   0 runner    (1001) docker     (127)      537 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      750 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1149 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2138 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2488 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1148 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)      749 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1020 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.896567 colossalai-nightly-2024.6.1/colossalai/legacy/engine/schedule/
+-rw-r--r--   0 runner    (1001) docker     (127)      315 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/schedule/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5816 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/schedule/_base_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3808 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)    40085 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/schedule/_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7133 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1993 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/global_variables.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.900567 colossalai-nightly-2024.6.1/colossalai/legacy/inference/
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/async_engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5938 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/async_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.900567 colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1345 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13595 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/infer_batch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5332 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/io_struct.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6304 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/ray_init_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2810 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/req_queue.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3282 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/sampling_params.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1514 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/stats.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.900567 colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/
+-rw-r--r--   0 runner    (1001) docker     (127)       65 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6972 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.900567 colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)       80 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/modeling/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21591 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/modeling/llama.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.900567 colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/polices/
+-rw-r--r--   0 runner    (1001) docker     (127)       78 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/polices/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5477 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/polices/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11609 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.900567 colossalai-nightly-2024.6.1/colossalai/legacy/inference/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)       83 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9019 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/pipeline/microbatch_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.900567 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)      123 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/batch_infer_state.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21286 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4580 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/kvcache_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.900567 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)      225 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/modeling/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23642 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/modeling/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22757 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17826 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/modeling/llama.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.904568 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/policies/
+-rw-r--r--   0 runner    (1001) docker     (127)      209 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/policies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/policies/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2987 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/policies/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/policies/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19842 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.904568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)       63 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.904568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/_ops/
+-rw-r--r--   0 runner    (1001) docker     (127)       22 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/_ops/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8669 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/_ops/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.904568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/
+-rw-r--r--   0 runner    (1001) docker     (127)      242 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2817 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/base_layer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.904568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/
+-rw-r--r--   0 runner    (1001) docker     (127)      300 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1404 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      999 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/dropout.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6156 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5224 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/normalization.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.904568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_1d/
+-rw-r--r--   0 runner    (1001) docker     (127)      459 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_1d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3724 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_1d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5063 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_1d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44898 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_1d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.904568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2d/
+-rw-r--r--   0 runner    (1001) docker     (127)      458 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35857 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)      853 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49324 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.904568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2p5d/
+-rw-r--r--   0 runner    (1001) docker     (127)      494 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2p5d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    39868 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1234 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49527 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2p5d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.908568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_3d/
+-rw-r--r--   0 runner    (1001) docker     (127)      498 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_3d/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    22832 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_3d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3037 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_3d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49624 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_3d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.908568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_sequence/
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_sequence/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6392 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_sequence/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)      483 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_sequence/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10693 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_sequence/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.908568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      445 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2411 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/utils/common.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.908568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/vanilla/
+-rw-r--r--   0 runner    (1001) docker     (127)      345 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/vanilla/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14693 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/vanilla/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.908568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/wrapper/
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/wrapper/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2170 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.908568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/loss/
+-rw-r--r--   0 runner    (1001) docker     (127)     1592 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/loss/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/loss/loss_1d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5732 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/loss/loss_2d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5540 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/loss/loss_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6436 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/loss/loss_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.908568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/metric/
+-rw-r--r--   0 runner    (1001) docker     (127)      686 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/metric/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      148 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/metric/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      787 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/metric/accuracy_2d.py
+-rw-r--r--   0 runner    (1001) docker     (127)      801 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/metric/accuracy_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1263 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/metric/accuracy_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.908568 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)       65 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6469 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/data_parallel.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.912567 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/
+-rw-r--r--   0 runner    (1001) docker     (127)      962 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.912567 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/
+-rw-r--r--   0 runner    (1001) docker     (127)      742 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1165 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27958 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8710 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      807 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5690 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9962 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7138 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1954 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/colo_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1150 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1136 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4780 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/module_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3874 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/reducer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.912567 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)      163 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1558 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/layer_spec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.912567 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/middleware/
+-rw-r--r--   0 runner    (1001) docker     (127)      149 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/middleware/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.912567 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/middleware/adaptor/
+-rw-r--r--   0 runner    (1001) docker     (127)       79 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/middleware/adaptor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6151 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/middleware/adaptor/fx.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6818 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/middleware/topo.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11447 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/pipelinable.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5759 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/pipeline_process_group.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.912567 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/rpc/
+-rw-r--r--   0 runner    (1001) docker     (127)      237 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/rpc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    59091 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/rpc/_pipeline_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14979 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5382 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/rpc/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9017 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.912567 colossalai-nightly-2024.6.1/colossalai/legacy/registry/
+-rw-r--r--   0 runner    (1001) docker     (127)      690 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/registry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3052 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/registry/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.916568 colossalai-nightly-2024.6.1/colossalai/legacy/tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)      418 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      779 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/tensor/compute_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/tensor/const.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8692 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/tensor/dist_spec_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2714 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/tensor/distspec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1678 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/tensor/op_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10505 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/tensor/process_group.py
+-rw-r--r--   0 runner    (1001) docker     (127)      735 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/tensor/tensor_spec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.916568 colossalai-nightly-2024.6.1/colossalai/legacy/trainer/
+-rw-r--r--   0 runner    (1001) docker     (127)       53 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/trainer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14773 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/trainer/_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.916568 colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/
+-rw-r--r--   0 runner    (1001) docker     (127)      648 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2712 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/_base_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3226 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/_checkpoint_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)      231 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/_commons_.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13085 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/_log_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2091 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16242 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/_metric_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.916568 colossalai-nightly-2024.6.1/colossalai/legacy/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     1438 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9872 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/activation_checkpoint.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.916568 colossalai-nightly-2024.6.1/colossalai/legacy/utils/checkpoint/
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/checkpoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5297 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/checkpoint/module_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/checkpoint/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11338 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/checkpointing.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16595 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/common.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.916568 colossalai-nightly-2024.6.1/colossalai/legacy/utils/data_sampler/
+-rw-r--r--   0 runner    (1001) docker     (127)      177 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/data_sampler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      339 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/data_sampler/base_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6704 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6416 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/memory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.916568 colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/
+-rw-r--r--   0 runner    (1001) docker     (127)       52 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      341 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/extention.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.920568 colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/legacy/
+-rw-r--r--   0 runner    (1001) docker     (127)      266 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/legacy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10461 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/legacy/comm_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4861 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3776 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/legacy/prof_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8459 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4036 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.920568 colossalai-nightly-2024.6.1/colossalai/legacy/zero/
+-rw-r--r--   0 runner    (1001) docker     (127)     1570 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.920568 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/
+-rw-r--r--   0 runner    (1001) docker     (127)      619 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7315 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/colo_init_context.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/gemini_context.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.920568 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/ophooks/
+-rw-r--r--   0 runner    (1001) docker     (127)      118 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/ophooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      774 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1363 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5081 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4715 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/ophooks/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.920568 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/paramhooks/
+-rw-r--r--   0 runner    (1001) docker     (127)       77 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/paramhooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1251 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6905 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/stateful_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3965 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6369 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/tensor_placement_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3807 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/tensor_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.920568 colossalai-nightly-2024.6.1/colossalai/legacy/zero/init_ctx/
+-rw-r--r--   0 runner    (1001) docker     (127)      171 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/init_ctx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11099 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/init_ctx/init_context.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.920568 colossalai-nightly-2024.6.1/colossalai/legacy/zero/shard_utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      259 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/shard_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      635 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/shard_utils/base_shard_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2296 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)      706 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/shard_utils/commons.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2751 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.924568 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/
+-rw-r--r--   0 runner    (1001) docker     (127)       75 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3003 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8289 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/reduce_scatter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28757 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/sharded_model_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)      808 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4865 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/zero_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.924568 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_optim/
+-rw-r--r--   0 runner    (1001) docker     (127)       83 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_optim/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18982 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.924568 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_param/
+-rw-r--r--   0 runner    (1001) docker     (127)      131 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_param/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3859 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_param/sharded_param.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1144 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_param/sharded_tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.924568 colossalai-nightly-2024.6.1/colossalai/logging/
+-rw-r--r--   0 runner    (1001) docker     (127)     1593 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/logging/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6110 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/logging/logger.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.924568 colossalai-nightly-2024.6.1/colossalai/moe/
+-rw-r--r--   0 runner    (1001) docker     (127)      531 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12432 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/moe/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36076 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/moe/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6249 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/moe/experts.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16261 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/moe/layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18267 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/moe/load_balance.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3075 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/moe/loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6138 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/moe/manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20401 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/moe/routers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7403 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/moe/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.924568 colossalai-nightly-2024.6.1/colossalai/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9563 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/init.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.924568 colossalai-nightly-2024.6.1/colossalai/nn/layer/
+-rw-r--r--   0 runner    (1001) docker     (127)       21 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2497 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/layer/layernorm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6739 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/layer/scaled_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (127)      449 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/layer/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.924568 colossalai-nightly-2024.6.1/colossalai/nn/loss/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/loss/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.928568 colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/
+-rw-r--r--   0 runner    (1001) docker     (127)      673 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5867 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/cosine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7717 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/delayed.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1178 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2672 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/multistep.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5050 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/onecycle.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2458 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/poly.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3516 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/torch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.928568 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (127)     1517 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7195 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/adafactor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5955 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/came.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8611 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/cpu_adam.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21082 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/distributed_adafactor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28254 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/distributed_came.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12580 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/distributed_galore.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7665 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/distributed_lamb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6478 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/fused_adam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9056 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/fused_lamb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6012 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/fused_sgd.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12125 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/galore.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8004 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/hybrid_adam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4526 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/lamb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3567 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/lars.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6768 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/nn/optimizer/nvme_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.928568 colossalai-nightly-2024.6.1/colossalai/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)      344 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26607 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/pipeline/p2p.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.932568 colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/
+-rw-r--r--   0 runner    (1001) docker     (127)      241 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5194 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1478 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20009 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/generate.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26918 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/interleaved_pp.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19591 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/one_f_one_b.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9552 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/pipeline/stage_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.932568 colossalai-nightly-2024.6.1/colossalai/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      144 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13038 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/quantization/bnb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4399 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/quantization/bnb_config.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.932568 colossalai-nightly-2024.6.1/colossalai/shardformer/
+-rw-r--r--   0 runner    (1001) docker     (127)      118 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3327 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.932568 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/
+-rw-r--r--   0 runner    (1001) docker     (127)     1119 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38285 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13198 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/attn.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3520 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/dropout.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15864 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26710 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5174 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11530 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17676 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/parallel_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31848 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/qkv_fused_linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10572 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/layer/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.936568 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    62664 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5064 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    53779 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17629 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/chatglm2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.936568 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/chatglm2_6b/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/chatglm2_6b/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2249 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    54608 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38507 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)    59934 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/gpt2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44054 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1053 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/jit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    46476 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32013 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45316 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33866 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/qwen2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8328 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37484 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16136 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    54363 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/whisper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.940568 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11302 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/auto_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8822 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/base_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27227 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15862 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17787 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12232 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16924 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22241 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/gpt2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14001 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18920 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14853 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14258 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14648 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/qwen2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9682 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22174 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10928 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23410 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/policies/whisper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.940568 colossalai-nightly-2024.6.1/colossalai/shardformer/shard/
+-rw-r--r--   0 runner    (1001) docker     (127)      320 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/shard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3763 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/shard/grad_ckpt_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6438 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/shard/shard_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9749 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/shard/sharder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1949 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/shard/shardformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      550 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/shardformer/shard/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.940568 colossalai-nightly-2024.6.1/colossalai/tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)      600 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3447 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/colo_parameter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3349 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/colo_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21633 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/comm_spec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.940568 colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)     1249 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18592 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11144 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/comm_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2776 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/layout.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27564 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/layout_converter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      231 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/misc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9490 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/sharding_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3263 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.940568 colossalai-nightly-2024.6.1/colossalai/tensor/moe_tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/moe_tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3778 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/moe_tensor/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1413 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/moe_tensor/moe_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.944568 colossalai-nightly-2024.6.1/colossalai/tensor/padded_tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)      194 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/padded_tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3527 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/padded_tensor/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5169 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/param_op_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35882 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/shape_consistency.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11626 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/sharding_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8452 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/tensor/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.944568 colossalai-nightly-2024.6.1/colossalai/testing/
+-rw-r--r--   0 runner    (1001) docker     (127)      865 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/testing/comparison.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1406 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/testing/pytest_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)      542 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/testing/random.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9201 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/testing/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.944568 colossalai-nightly-2024.6.1/colossalai/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      586 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1938 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2470 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/memory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.944568 colossalai-nightly-2024.6.1/colossalai/utils/model/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3905 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/model/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.944568 colossalai-nightly-2024.6.1/colossalai/utils/multi_tensor_apply/
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/multi_tensor_apply/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.944568 colossalai-nightly-2024.6.1/colossalai/utils/rank_recorder/
+-rw-r--r--   0 runner    (1001) docker     (127)       90 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/rank_recorder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5453 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/rank_recorder/rank_recorder.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.944568 colossalai-nightly-2024.6.1/colossalai/utils/tensor_detector/
+-rw-r--r--   0 runner    (1001) docker     (127)       44 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/tensor_detector/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8392 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/tensor_detector/tensor_detector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4321 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/utils/timer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.944568 colossalai-nightly-2024.6.1/colossalai/zero/
+-rw-r--r--   0 runner    (1001) docker     (127)      390 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.944568 colossalai-nightly-2024.6.1/colossalai/zero/gemini/
+-rw-r--r--   0 runner    (1001) docker     (127)      490 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.948568 colossalai-nightly-2024.6.1/colossalai/zero/gemini/chunk/
+-rw-r--r--   0 runner    (1001) docker     (127)      342 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/chunk/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26051 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/chunk/chunk.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12384 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/chunk/manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6291 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/chunk/search_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1529 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/chunk/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45520 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/gemini_ddp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3519 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/gemini_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7632 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/gemini_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37815 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/gemini_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.948568 colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/
+-rw-r--r--   0 runner    (1001) docker     (127)      511 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1281 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4063 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/memory_monitor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4101 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/memory_stats.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)      858 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/param_runtime_order.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3716 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4126 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1795 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11966 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/placement_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4213 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/gemini/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.948568 colossalai-nightly-2024.6.1/colossalai/zero/low_level/
+-rw-r--r--   0 runner    (1001) docker     (127)       88 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/low_level/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7283 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/low_level/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.948568 colossalai-nightly-2024.6.1/colossalai/zero/low_level/bookkeeping/
+-rw-r--r--   0 runner    (1001) docker     (127)      242 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/low_level/bookkeeping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      442 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/low_level/bookkeeping/base_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5939 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/low_level/bookkeeping/bucket_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4537 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/low_level/bookkeeping/gradient_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1725 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/low_level/bookkeeping/parameter_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1494 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/low_level/bookkeeping/tensor_bucket.py
+-rw-r--r--   0 runner    (1001) docker     (127)    46468 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/low_level/low_level_optim.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/colossalai/zero/wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.948568 colossalai-nightly-2024.6.1/colossalai_nightly.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    36484 2024-06-01 00:16:37.000000 colossalai-nightly-2024.6.1/colossalai_nightly.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    53244 2024-06-01 00:16:37.000000 colossalai-nightly-2024.6.1/colossalai_nightly.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-06-01 00:16:37.000000 colossalai-nightly-2024.6.1/colossalai_nightly.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       69 2024-06-01 00:16:37.000000 colossalai-nightly-2024.6.1/colossalai_nightly.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      254 2024-06-01 00:16:37.000000 colossalai-nightly-2024.6.1/colossalai_nightly.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       37 2024-06-01 00:16:37.000000 colossalai-nightly-2024.6.1/colossalai_nightly.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.828567 colossalai-nightly-2024.6.1/examples/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.952568 colossalai-nightly-2024.6.1/examples/language/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/examples/language/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4227 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/examples/language/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      713 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/examples/language/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5235 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/examples/language/performance_evaluator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.952568 colossalai-nightly-2024.6.1/extensions/
+-rw-r--r--   0 runner    (1001) docker     (127)     1306 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2499 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/base_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4879 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/cpp_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.952568 colossalai-nightly-2024.6.1/extensions/csrc/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.952568 colossalai-nightly-2024.6.1/extensions/csrc/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      694 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/common/data_type.h
+-rw-r--r--   0 runner    (1001) docker     (127)    14986 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/common/micros.h
+-rw-r--r--   0 runner    (1001) docker     (127)      901 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/common/mp_type_traits.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2886 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/common/target.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2797 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/common/vec_type_traits.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.952568 colossalai-nightly-2024.6.1/extensions/csrc/funcs/
+-rw-r--r--   0 runner    (1001) docker     (127)    10622 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/funcs/binary_functor.h
+-rw-r--r--   0 runner    (1001) docker     (127)    21630 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/funcs/cast_functor.h
+-rw-r--r--   0 runner    (1001) docker     (127)     3932 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/funcs/reduce_function.h
+-rw-r--r--   0 runner    (1001) docker     (127)     8393 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/funcs/ternary_functor.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2460 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/funcs/unary_functor.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.828567 colossalai-nightly-2024.6.1/extensions/csrc/kernel/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.952568 colossalai-nightly-2024.6.1/extensions/csrc/kernel/arm/
+-rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/arm/cpu_adam_arm.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/arm/cpu_adam_arm.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.956568 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/
+-rw-r--r--   0 runner    (1001) docker     (127)     2614 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/activation_kernel.cu
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.956568 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/attention/
+-rw-r--r--   0 runner    (1001) docker     (127)     6820 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/attention/attention_utils.h
+-rw-r--r--   0 runner    (1001) docker     (127)    10773 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     3931 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/convert_fp8_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    10013 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    41480 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    21861 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     9781 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    25833 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/layer_norm_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    25880 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/moe_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5050 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5207 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_apply.cuh
+-rw-r--r--   0 runner    (1001) docker     (127)    15325 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    13119 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     4444 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     6486 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    14092 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/rms_layernorm_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    22113 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    23452 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.956568 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     2538 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/utils/gpu_launch_config.h
+-rw-r--r--   0 runner    (1001) docker     (127)      545 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/utils/micros.h
+-rw-r--r--   0 runner    (1001) docker     (127)     1287 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/utils/nvgpu_dev_info.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2063 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/utils/vec_copy.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.956568 colossalai-nightly-2024.6.1/extensions/csrc/kernel/x86/
+-rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/x86/cpu_adam.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/csrc/kernel/x86/cpu_adam.h
+-rw-r--r--   0 runner    (1001) docker     (127)     4131 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/cuda_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.956568 colossalai-nightly-2024.6.1/extensions/pybind/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.956568 colossalai-nightly-2024.6.1/extensions/pybind/cpu_adam/
+-rw-r--r--   0 runner    (1001) docker     (127)      150 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/cpu_adam/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1090 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/cpu_adam/cpu_adam_arm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/cpu_adam/cpu_adam_x86.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.956568 colossalai-nightly-2024.6.1/extensions/pybind/flash_attention/
+-rw-r--r--   0 runner    (1001) docker     (127)      470 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/flash_attention/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3761 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/flash_attention/flash_attention_dao_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1923 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/flash_attention/flash_attention_npu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1800 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.960568 colossalai-nightly-2024.6.1/extensions/pybind/inference/
+-rw-r--r--   0 runner    (1001) docker     (127)       99 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/inference/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5125 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/inference/inference.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     1272 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/inference/inference_ops_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.960568 colossalai-nightly-2024.6.1/extensions/pybind/layernorm/
+-rw-r--r--   0 runner    (1001) docker     (127)       89 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/layernorm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4885 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/layernorm/layer_norm.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)      923 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/layernorm/layernorm_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.960568 colossalai-nightly-2024.6.1/extensions/pybind/moe/
+-rw-r--r--   0 runner    (1001) docker     (127)       71 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/moe/moe.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)      898 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/moe/moe_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.960568 colossalai-nightly-2024.6.1/extensions/pybind/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      105 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1089 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/optimizer/fused_optimizer_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/optimizer/optimizer.cpp
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.960568 colossalai-nightly-2024.6.1/extensions/pybind/softmax/
+-rw-r--r--   0 runner    (1001) docker     (127)      271 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/softmax/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2055 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/softmax/scaled_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     1002 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/softmax/scaled_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1690 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     1069 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)      589 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/triton_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/extensions/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.960568 colossalai-nightly-2024.6.1/requirements/
+-rw-r--r--   0 runner    (1001) docker     (127)      520 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/requirements/requirements-test.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      254 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/requirements/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-06-01 00:16:37.984568 colossalai-nightly-2024.6.1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     4353 2024-06-01 00:16:37.000000 colossalai-nightly-2024.6.1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.832567 colossalai-nightly-2024.6.1/tests/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.960568 colossalai-nightly-2024.6.1/tests/kit/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.960568 colossalai-nightly-2024.6.1/tests/kit/model_zoo/
+-rw-r--r--   0 runner    (1001) docker     (127)     1070 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.964568 colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/
+-rw-r--r--   0 runner    (1001) docker     (127)      155 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      832 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1140 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/hanging_param_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/nested_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/repeated_computed_layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1707 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/simple_mlp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1259 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/simple_net.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.964568 colossalai-nightly-2024.6.1/tests/kit/model_zoo/diffusers/
+-rw-r--r--   0 runner    (1001) docker     (127)       25 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/diffusers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2534 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/diffusers/diffusers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1499 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/executor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3413 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.964568 colossalai-nightly-2024.6.1/tests/kit/model_zoo/timm/
+-rw-r--r--   0 runner    (1001) docker     (127)       20 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/timm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5975 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/timm/timm.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.964568 colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchaudio/
+-rw-r--r--   0 runner    (1001) docker     (127)       26 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchaudio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4614 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchaudio/torchaudio.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.964568 colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchrec/
+-rw-r--r--   0 runner    (1001) docker     (127)       24 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchrec/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3899 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchrec/torchrec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.964568 colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchvision/
+-rw-r--r--   0 runner    (1001) docker     (127)       27 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchvision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4970 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchvision/torchvision.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.968568 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/
+-rw-r--r--   0 runner    (1001) docker     (127)      524 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3768 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/albert.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12561 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2289 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4487 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2282 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4251 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6361 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/gpt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3597 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3328 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2837 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3063 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3015 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/qwen2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1833 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3061 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2214 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3717 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/whisper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.968568 colossalai-nightly-2024.6.1/tests/test_analyzer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.968568 colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3684 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/test_bias_addition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2139 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/test_mod_dir.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1750 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/test_nested_ckpt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2127 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/test_shape_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/test_symbolic_profile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1271 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/zoo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.968568 colossalai-nightly-2024.6.1/tests/test_analyzer/test_subclasses/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/test_subclasses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3714 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/test_subclasses/test_aten.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1893 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/test_subclasses/test_flop_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_analyzer/test_subclasses/test_meta_mode.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.968568 colossalai-nightly-2024.6.1/tests/test_auto_parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.968568 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_pass/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_pass/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1773 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_pass/test_node_converting_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2250 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.968568 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2702 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2003 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2446 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3383 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3733 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3674 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.972568 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_gpt/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_gpt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10992 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7729 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3818 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2052 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.976568 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11120 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7521 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4701 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6548 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6034 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10708 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8717 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12574 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3595 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11426 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2942 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7708 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4231 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12825 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8338 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2552 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2556 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18264 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3056 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4168 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8698 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12216 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12137 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2781 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3625 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13847 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3556 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8505 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5283 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.976568 colossalai-nightly-2024.6.1/tests/test_infer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1210 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5233 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_batch_bucket.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1195 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_config_and_struct.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2347 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_continuous_batching.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2942 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_cuda_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2607 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_drafter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7403 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_inference_engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.976568 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.976568 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/cuda/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/cuda/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1765 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/cuda/test_convert_fp8.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11612 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1792 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5290 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1586 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/cuda/test_rms_layernorm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5544 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1099 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/cuda/test_silu_and_mul.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.980568 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/triton/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/triton/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15777 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/triton/kernel_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6808 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/triton/test_context_attn_unpad.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7581 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/triton/test_decoding_attn.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1647 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/triton/test_fused_rotary_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6411 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/triton/test_kvcache_copy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1692 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4011 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2562 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kernels/triton/test_xine_copy.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6796 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_kvcache_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2964 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_request_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4177 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_infer/test_rpc_engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.980568 colossalai-nightly-2024.6.1/tests/test_shardformer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5683 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_flash_attention.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:37.984568 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15543 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8340 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3196 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7723 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8331 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6763 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8635 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_gpt2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7705 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12165 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5625 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7402 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7600 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_qwen2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2557 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7227 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6528 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7293 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_whisper.py
+-rw-r--r--   0 runner    (1001) docker     (127)      833 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_shard_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2623 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/tests/test_shardformer/test_with_torch_ddp.py
+-rw-r--r--   0 runner    (1001) docker     (127)        6 2024-06-01 00:16:28.000000 colossalai-nightly-2024.6.1/version.txt
```

### Comparing `colossalai-nightly-2024.5.4/LICENSE` & `colossalai-nightly-2024.6.1/LICENSE`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/PKG-INFO` & `colossalai-nightly-2024.6.1/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: colossalai-nightly
-Version: 2024.5.4
+Version: 2024.6.1
 Summary: An integrated large-scale model training system with efficient parallelization techniques
 Home-page: https://www.colossalai.org
 License: Apache Software License 2.0
 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/discussions
 Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/issues
 Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io
@@ -32,14 +32,15 @@
         
         
            | [English](README.md) | [](docs/README-zh-Hans.md) |
         
         </div>
         
         ## Latest News
+        * [2024/05] [Large AI Models Inference Speed Doubled, Colossal-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference)
         * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
         * [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)
         * [2024/03] [314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
         * [2024/03] [Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models](https://hpc-ai.com/blog/open-sora-v1.0)
         * [2024/03] [Open-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/open-sora)
         * [2024/01] [Inference Performance Improved by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer)
         * [2024/01] [Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b)
@@ -82,19 +83,17 @@
              <li><a href="#GPT-2-Single">GPT-2</a></li>
              <li><a href="#PaLM-Single">PaLM</a></li>
            </ul>
          </li>
          <li>
            <a href="#Inference">Inference</a>
            <ul>
+             <li><a href="#Colossal-Inference">Colossal-Inference: Large AI  Models Inference Speed Doubled</a></li>
              <li><a href="#Grok-1">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>
              <li><a href="#SwiftInfer">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>
-             <li><a href="#GPT-3-Inference">GPT-3</a></li>
-             <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
-             <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
            </ul>
          </li>
          <li>
            <a href="#Installation">Installation</a>
            <ul>
              <li><a href="#PyPI">PyPI</a></li>
              <li><a href="#Install-From-Source">Install From Source</a></li>
@@ -384,56 +383,52 @@
         
         - 34x larger model size on the same hardware
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         
         ## Inference
+        ### Colossal-Inference
+        <p align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-1.png" width=1000/>
+        </p>
+        
+        <p align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-2.png" width=1000/>
+        </p>
+        
+         - Large AI models inference speed doubled, compared to the offline inference performance of vLLM in some cases.
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/inference)
+        [[blog]](https://hpc-ai.com/blog/colossal-inference)
+        
         ### Grok-1
         <p id="Grok-1" align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg" width=600/>
         </p>
         
          - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.
         
         [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)
         [[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
         [[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)
         [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)
         
+        ### SwiftInfer
         <p id="SwiftInfer" align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg" width=800/>
         </p>
         
         - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations
         
-        <p id="GPT-3-Inference" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference_GPT-3.jpg" width=800/>
-        </p>
-        
-        - [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference acceleration on the same hardware
-        
-        <p id="OPT-Serving" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20serving.png" width=600/>
-        </p>
-        
-        - [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service): Try 175-billion-parameter OPT online services
-        
-        <p id="BLOOM-Inference" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20Inference.PNG" width=800/>
-        </p>
-        
-        - [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom): Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10 times.
-        
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         ## Installation
         
         Requirements:
-        - PyTorch >= 1.11 and PyTorch <= 2.1
+        - PyTorch >= 2.1
         - Python >= 3.7
         - CUDA >= 11.0
         - [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)
         - Linux OS
         
         If you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.5.4 Summary: An
+Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.6.1 Summary: An
 integrated large-scale model training system with efficient parallelization
 techniques Home-page: https://www.colossalai.org License: Apache Software
 License 2.0 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/
 discussions Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/
 issues Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io Project-URL:
 Github, https://github.com/hpcaitech/ColossalAI Description: # Colossal-AI
@@ -22,46 +22,47 @@
  (https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://
 huggingface.co/hpcai-tech) [![slack badge](https://img.shields.io/badge/Slack-
 join-blueviolet?logo=slack&)](https://github.com/hpcaitech/public_assets/tree/
  main/colossalai/contact/slack) [![WeChat badge](https://img.shields.io/badge/
 --green?logo=wechat&)](https://raw.githubusercontent.com/hpcaitech/
 public_assets/main/colossalai/img/WeChat.png) | [English](README.md) | []
                           (docs/README-zh-Hans.md) |
-## Latest News * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open
-Source with Single-Shot 16-Second Video Generation and 720p Resolution](https:/
-/hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-
-video-generation-and-720p-resolution-in-open-source) * [2024/04] [Most cost-
-effective solutions for inference, fine-tuning and pretraining, tailored to
-LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-
-inference-fine-tuning-and-pretraining-tailored-to-llama3-series) * [2024/03]
-[314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and
-Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-
-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-
-use-pytorchhuggingface-version-is-here) * [2024/03] [Open-Sora: Revealing
-Complete Model Parameters, Training Details, and Everything for Sora-like Video
-Generation Models](https://hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-
-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to
-Nearly a Million](https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference
-Performance Improved by 46%, Open Source Solution Breaks the Length Limit of
-LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-
-SwiftInfer) * [2024/01] [Construct Refined 13B Private Model With Just $5000
-USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/
-colossal-llama-2-13b) * [2023/11] [Enhanced MoE Parallelism, Open-source MoE
-Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/
-enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-
-efficient) * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars
-Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-
-Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-
-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-
-large-models-open-source-and-commercial-free-domain-specific-llm-solution) *
-[2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%]
-(https://www.hpc-ai.tech/blog/70b-llama2-training) * [2023/07] [HPC-AI Tech
-Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-
-tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-
-business-growth) ## Table of Contents
+## Latest News * [2024/05] [Large AI Models Inference Speed Doubled, Colossal-
+Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference) *
+[2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-
+Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/
+open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-
+and-720p-resolution-in-open-source) * [2024/04] [Most cost-effective solutions
+for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://
+hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-
+pretraining-tailored-to-llama3-series) * [2024/03] [314 Billion Parameter Grok-
+1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace
+version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-
+inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-
+version-is-here) * [2024/03] [Open-Sora: Revealing Complete Model Parameters,
+Training Details, and Everything for Sora-like Video Generation Models](https:/
+/hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-SoraSora Replication
+Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million]
+(https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference Performance Improved
+by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round
+Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer) * [2024/01]
+[Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI
+Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b) * [2023/11]
+[Enhanced MoE Parallelism, Open-source MoE Model Training Can Be 9 Times More
+Efficient](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-
+moe-model-training-can-be-9-times-more-efficient) * [2023/09] [One Half-Day of
+Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large
+Models, Open-Source and Commercial-Free Domain-Specific LLM Solution](https://
+www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-
+yields-similar-results-to-mainstream-large-models-open-source-and-commercial-
+free-domain-specific-llm-solution) * [2023/09] [70 Billion Parameter LLaMA2
+Model Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-
+training) * [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding]
+(https://www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-
+funding-to-fuel-team-expansion-and-business-growth) ## Table of Contents
     * _W_h_y_ _C_o_l_o_s_s_a_l_-_A_I
     * _F_e_a_t_u_r_e_s
     * _C_o_l_o_s_s_a_l_-_A_I_ _f_o_r_ _R_e_a_l_ _W_o_r_l_d_ _A_p_p_l_i_c_a_t_i_o_n_s
           o _O_p_e_n_-_S_o_r_a_:_ _R_e_v_e_a_l_i_n_g_ _C_o_m_p_l_e_t_e_ _M_o_d_e_l_ _P_a_r_a_m_e_t_e_r_s_,_ _T_r_a_i_n_i_n_g_ _D_e_t_a_i_l_s_,
             _a_n_d_ _E_v_e_r_y_t_h_i_n_g_ _f_o_r_ _S_o_r_a_-_l_i_k_e_ _V_i_d_e_o_ _G_e_n_e_r_a_t_i_o_n_ _M_o_d_e_l_s
           o _C_o_l_o_s_s_a_l_-_L_L_a_M_A_-_2_:_ _O_n_e_ _H_a_l_f_-_D_a_y_ _o_f_ _T_r_a_i_n_i_n_g_ _U_s_i_n_g_ _a_ _F_e_w_ _H_u_n_d_r_e_d
             _D_o_l_l_a_r_s_ _Y_i_e_l_d_s_ _S_i_m_i_l_a_r_ _R_e_s_u_l_t_s_ _t_o_ _M_a_i_n_s_t_r_e_a_m_ _L_a_r_g_e_ _M_o_d_e_l_s_,_ _O_p_e_n_-
@@ -80,20 +81,18 @@
           o _O_P_T
           o _V_i_T
           o _R_e_c_o_m_m_e_n_d_a_t_i_o_n_ _S_y_s_t_e_m_ _M_o_d_e_l_s
     * _S_i_n_g_l_e_ _G_P_U_ _T_r_a_i_n_i_n_g_ _D_e_m_o
           o _G_P_T_-_2
           o _P_a_L_M
     * _I_n_f_e_r_e_n_c_e
+          o _C_o_l_o_s_s_a_l_-_I_n_f_e_r_e_n_c_e_:_ _L_a_r_g_e_ _A_I_ _M_o_d_e_l_s_ _I_n_f_e_r_e_n_c_e_ _S_p_e_e_d_ _D_o_u_b_l_e_d
           o _G_r_o_k_-_1_:_ _3_1_4_B_ _m_o_d_e_l_ _o_f_ _P_y_T_o_r_c_h_ _+_ _H_u_g_g_i_n_g_F_a_c_e_ _I_n_f_e_r_e_n_c_e
           o _S_w_i_f_t_I_n_f_e_r_:_B_r_e_a_k_s_ _t_h_e_ _L_e_n_g_t_h_ _L_i_m_i_t_ _o_f_ _L_L_M_ _f_o_r_ _M_u_l_t_i_-_R_o_u_n_d
             _C_o_n_v_e_r_s_a_t_i_o_n_s_ _w_i_t_h_ _4_6_%_ _A_c_c_e_l_e_r_a_t_i_o_n
-          o _G_P_T_-_3
-          o _O_P_T_-_1_7_5_B_ _O_n_l_i_n_e_ _S_e_r_v_i_n_g_ _f_o_r_ _T_e_x_t_ _G_e_n_e_r_a_t_i_o_n
-          o _1_7_6_B_ _B_L_O_O_M
     * _I_n_s_t_a_l_l_a_t_i_o_n
           o _P_y_P_I
           o _I_n_s_t_a_l_l_ _F_r_o_m_ _S_o_u_r_c_e
     * _U_s_e_ _D_o_c_k_e_r
     * _C_o_m_m_u_n_i_t_y
     * _C_o_n_t_r_i_b_u_t_i_n_g
     * _C_i_t_e_ _U_s
@@ -283,72 +282,66 @@
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 GPT2-NVME.png]
 - 120x larger model size on the same hardware (RTX 3080) ### PaLM
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 PaLM-GPU1.png]
 - 34x larger model size on the same hardware
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Inference ### Grok-1
+## Inference ### Colossal-Inference
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                    inference/colossal-inference-v1-1.png]
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                    inference/colossal-inference-v1-2.png]
+- Large AI models inference speed doubled, compared to the offline inference
+performance of vLLM in some cases. [[code]](https://github.com/hpcaitech/
+ColossalAI/tree/main/colossalai/inference) [[blog]](https://hpc-ai.com/blog/
+colossal-inference) ### Grok-1
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                          images/grok-1-inference.jpg]
 - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use
 Python + PyTorch + HuggingFace version for Inference. [[code]](https://
 github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1) [[blog]]
 (https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-
 3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here) [
 [HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/
 grok-1) [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/
-models/colossalai/grok-1-pytorch/summary)
+models/colossalai/grok-1-pytorch/summary) ### SwiftInfer
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 SwiftInfer.jpg]
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance
 improved by 46%, open source solution breaks the length limit of LLM for multi-
 round conversations
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             inference_GPT-3.jpg]
-- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
-acceleration on the same hardware
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             BLOOM%20serving.png]
-- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
-Try 175-billion-parameter OPT online services
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                            BLOOM%20Inference.PNG]
-- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
-Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
-times.
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Installation Requirements: - PyTorch >= 1.11 and PyTorch <= 2.1 - Python >=
-3.7 - CUDA >= 11.0 - [NVIDIA GPU Compute Capability](https://
-developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher) - Linux OS If
-you encounter any problem with installation, you may want to raise an [issue]
-(https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
-### Install from PyPI You can easily install Colossal-AI with the following
-command. **By default, we do not build PyTorch extensions during
-installation.** ```bash pip install colossalai ``` **Note: only Linux is
-supported for now.** However, if you want to build the PyTorch extensions
-during installation, you can set `BUILD_EXT=1`. ```bash BUILD_EXT=1 pip install
-colossalai ``` **Otherwise, CUDA kernels will be built during runtime when you
-actually need them.** We also keep releasing the nightly version to PyPI every
-week. This allows you to access the unreleased features and bug fixes in the
-main branch. Installation can be made via ```bash pip install colossalai-
-nightly ``` ### Download From Source > The version of Colossal-AI will be in
-line with the main branch of the repository. Feel free to raise an issue if you
-encounter any problems. :) ```shell git clone https://github.com/hpcaitech/
-ColossalAI.git cd ColossalAI # install colossalai pip install . ``` By default,
-we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
-If you want to install and enable CUDA kernel fusion (compulsory installation
-when using fused optimizer): ```shell BUILD_EXT=1 pip install . ``` For Users
-with CUDA 10.2, you can still build ColossalAI from source. However, you need
-to manually download the cub library and copy it to the corresponding
-directory. ```bash # clone the repository git clone https://github.com/
-hpcaitech/ColossalAI.git cd ColossalAI # download the cub library wget https://
-github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip cp -r cub-
-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ # install
-BUILD_EXT=1 pip install . ```
+## Installation Requirements: - PyTorch >= 2.1 - Python >= 3.7 - CUDA >= 11.0 -
+[NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0
+(V100/RTX20 and higher) - Linux OS If you encounter any problem with
+installation, you may want to raise an [issue](https://github.com/hpcaitech/
+ColossalAI/issues/new/choose) in this repository. ### Install from PyPI You can
+easily install Colossal-AI with the following command. **By default, we do not
+build PyTorch extensions during installation.** ```bash pip install colossalai
+``` **Note: only Linux is supported for now.** However, if you want to build
+the PyTorch extensions during installation, you can set `BUILD_EXT=1`. ```bash
+BUILD_EXT=1 pip install colossalai ``` **Otherwise, CUDA kernels will be built
+during runtime when you actually need them.** We also keep releasing the
+nightly version to PyPI every week. This allows you to access the unreleased
+features and bug fixes in the main branch. Installation can be made via ```bash
+pip install colossalai-nightly ``` ### Download From Source > The version of
+Colossal-AI will be in line with the main branch of the repository. Feel free
+to raise an issue if you encounter any problems. :) ```shell git clone https://
+github.com/hpcaitech/ColossalAI.git cd ColossalAI # install colossalai pip
+install . ``` By default, we do not compile CUDA/C++ kernels. ColossalAI will
+build them during runtime. If you want to install and enable CUDA kernel fusion
+(compulsory installation when using fused optimizer): ```shell BUILD_EXT=1 pip
+install . ``` For Users with CUDA 10.2, you can still build ColossalAI from
+source. However, you need to manually download the cub library and copy it to
+the corresponding directory. ```bash # clone the repository git clone https://
+github.com/hpcaitech/ColossalAI.git cd ColossalAI # download the cub library
+wget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip
+cp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ #
+install BUILD_EXT=1 pip install . ```
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
 ## Use Docker ### Pull from DockerHub You can directly pull the docker image
 from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The
 image is automatically uploaded upon release. ### Build On Your Own Run the
 following command to build a docker image from Dockerfile provided. > Building
 Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker
 Runtime as the default when doing `docker build`. More details can be found
```

### Comparing `colossalai-nightly-2024.5.4/README.md` & `colossalai-nightly-2024.6.1/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -21,14 +21,15 @@
 
 
    | [English](README.md) | [](docs/README-zh-Hans.md) |
 
 </div>
 
 ## Latest News
+* [2024/05] [Large AI Models Inference Speed Doubled, Colossal-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference)
 * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
 * [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)
 * [2024/03] [314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
 * [2024/03] [Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models](https://hpc-ai.com/blog/open-sora-v1.0)
 * [2024/03] [Open-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/open-sora)
 * [2024/01] [Inference Performance Improved by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer)
 * [2024/01] [Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b)
@@ -71,19 +72,17 @@
      <li><a href="#GPT-2-Single">GPT-2</a></li>
      <li><a href="#PaLM-Single">PaLM</a></li>
    </ul>
  </li>
  <li>
    <a href="#Inference">Inference</a>
    <ul>
+     <li><a href="#Colossal-Inference">Colossal-Inference: Large AI  Models Inference Speed Doubled</a></li>
      <li><a href="#Grok-1">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>
      <li><a href="#SwiftInfer">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>
-     <li><a href="#GPT-3-Inference">GPT-3</a></li>
-     <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
-     <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
    </ul>
  </li>
  <li>
    <a href="#Installation">Installation</a>
    <ul>
      <li><a href="#PyPI">PyPI</a></li>
      <li><a href="#Install-From-Source">Install From Source</a></li>
@@ -373,56 +372,52 @@
 
 - 34x larger model size on the same hardware
 
 <p align="right">(<a href="#top">back to top</a>)</p>
 
 
 ## Inference
+### Colossal-Inference
+<p align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-1.png" width=1000/>
+</p>
+
+<p align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-2.png" width=1000/>
+</p>
+
+ - Large AI models inference speed doubled, compared to the offline inference performance of vLLM in some cases.
+[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/inference)
+[[blog]](https://hpc-ai.com/blog/colossal-inference)
+
 ### Grok-1
 <p id="Grok-1" align="center">
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg" width=600/>
 </p>
 
  - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.
 
 [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)
 [[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
 [[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)
 [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)
 
+### SwiftInfer
 <p id="SwiftInfer" align="center">
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg" width=800/>
 </p>
 
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations
 
-<p id="GPT-3-Inference" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference_GPT-3.jpg" width=800/>
-</p>
-
-- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference acceleration on the same hardware
-
-<p id="OPT-Serving" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20serving.png" width=600/>
-</p>
-
-- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service): Try 175-billion-parameter OPT online services
-
-<p id="BLOOM-Inference" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20Inference.PNG" width=800/>
-</p>
-
-- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom): Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10 times.
-
 <p align="right">(<a href="#top">back to top</a>)</p>
 
 ## Installation
 
 Requirements:
-- PyTorch >= 1.11 and PyTorch <= 2.1
+- PyTorch >= 2.1
 - Python >= 3.7
 - CUDA >= 11.0
 - [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)
 - Linux OS
 
 If you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
```

#### html2text {}

```diff
@@ -15,46 +15,47 @@
  (https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://
 huggingface.co/hpcai-tech) [![slack badge](https://img.shields.io/badge/Slack-
 join-blueviolet?logo=slack&)](https://github.com/hpcaitech/public_assets/tree/
  main/colossalai/contact/slack) [![WeChat badge](https://img.shields.io/badge/
 --green?logo=wechat&)](https://raw.githubusercontent.com/hpcaitech/
 public_assets/main/colossalai/img/WeChat.png) | [English](README.md) | []
                           (docs/README-zh-Hans.md) |
-## Latest News * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open
-Source with Single-Shot 16-Second Video Generation and 720p Resolution](https:/
-/hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-
-video-generation-and-720p-resolution-in-open-source) * [2024/04] [Most cost-
-effective solutions for inference, fine-tuning and pretraining, tailored to
-LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-
-inference-fine-tuning-and-pretraining-tailored-to-llama3-series) * [2024/03]
-[314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and
-Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-
-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-
-use-pytorchhuggingface-version-is-here) * [2024/03] [Open-Sora: Revealing
-Complete Model Parameters, Training Details, and Everything for Sora-like Video
-Generation Models](https://hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-
-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to
-Nearly a Million](https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference
-Performance Improved by 46%, Open Source Solution Breaks the Length Limit of
-LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-
-SwiftInfer) * [2024/01] [Construct Refined 13B Private Model With Just $5000
-USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/
-colossal-llama-2-13b) * [2023/11] [Enhanced MoE Parallelism, Open-source MoE
-Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/
-enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-
-efficient) * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars
-Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-
-Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-
-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-
-large-models-open-source-and-commercial-free-domain-specific-llm-solution) *
-[2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%]
-(https://www.hpc-ai.tech/blog/70b-llama2-training) * [2023/07] [HPC-AI Tech
-Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-
-tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-
-business-growth) ## Table of Contents
+## Latest News * [2024/05] [Large AI Models Inference Speed Doubled, Colossal-
+Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference) *
+[2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-
+Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/
+open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-
+and-720p-resolution-in-open-source) * [2024/04] [Most cost-effective solutions
+for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://
+hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-
+pretraining-tailored-to-llama3-series) * [2024/03] [314 Billion Parameter Grok-
+1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace
+version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-
+inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-
+version-is-here) * [2024/03] [Open-Sora: Revealing Complete Model Parameters,
+Training Details, and Everything for Sora-like Video Generation Models](https:/
+/hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-SoraSora Replication
+Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million]
+(https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference Performance Improved
+by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round
+Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer) * [2024/01]
+[Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI
+Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b) * [2023/11]
+[Enhanced MoE Parallelism, Open-source MoE Model Training Can Be 9 Times More
+Efficient](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-
+moe-model-training-can-be-9-times-more-efficient) * [2023/09] [One Half-Day of
+Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large
+Models, Open-Source and Commercial-Free Domain-Specific LLM Solution](https://
+www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-
+yields-similar-results-to-mainstream-large-models-open-source-and-commercial-
+free-domain-specific-llm-solution) * [2023/09] [70 Billion Parameter LLaMA2
+Model Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-
+training) * [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding]
+(https://www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-
+funding-to-fuel-team-expansion-and-business-growth) ## Table of Contents
     * _W_h_y_ _C_o_l_o_s_s_a_l_-_A_I
     * _F_e_a_t_u_r_e_s
     * _C_o_l_o_s_s_a_l_-_A_I_ _f_o_r_ _R_e_a_l_ _W_o_r_l_d_ _A_p_p_l_i_c_a_t_i_o_n_s
           o _O_p_e_n_-_S_o_r_a_:_ _R_e_v_e_a_l_i_n_g_ _C_o_m_p_l_e_t_e_ _M_o_d_e_l_ _P_a_r_a_m_e_t_e_r_s_,_ _T_r_a_i_n_i_n_g_ _D_e_t_a_i_l_s_,
             _a_n_d_ _E_v_e_r_y_t_h_i_n_g_ _f_o_r_ _S_o_r_a_-_l_i_k_e_ _V_i_d_e_o_ _G_e_n_e_r_a_t_i_o_n_ _M_o_d_e_l_s
           o _C_o_l_o_s_s_a_l_-_L_L_a_M_A_-_2_:_ _O_n_e_ _H_a_l_f_-_D_a_y_ _o_f_ _T_r_a_i_n_i_n_g_ _U_s_i_n_g_ _a_ _F_e_w_ _H_u_n_d_r_e_d
             _D_o_l_l_a_r_s_ _Y_i_e_l_d_s_ _S_i_m_i_l_a_r_ _R_e_s_u_l_t_s_ _t_o_ _M_a_i_n_s_t_r_e_a_m_ _L_a_r_g_e_ _M_o_d_e_l_s_,_ _O_p_e_n_-
@@ -73,20 +74,18 @@
           o _O_P_T
           o _V_i_T
           o _R_e_c_o_m_m_e_n_d_a_t_i_o_n_ _S_y_s_t_e_m_ _M_o_d_e_l_s
     * _S_i_n_g_l_e_ _G_P_U_ _T_r_a_i_n_i_n_g_ _D_e_m_o
           o _G_P_T_-_2
           o _P_a_L_M
     * _I_n_f_e_r_e_n_c_e
+          o _C_o_l_o_s_s_a_l_-_I_n_f_e_r_e_n_c_e_:_ _L_a_r_g_e_ _A_I_ _M_o_d_e_l_s_ _I_n_f_e_r_e_n_c_e_ _S_p_e_e_d_ _D_o_u_b_l_e_d
           o _G_r_o_k_-_1_:_ _3_1_4_B_ _m_o_d_e_l_ _o_f_ _P_y_T_o_r_c_h_ _+_ _H_u_g_g_i_n_g_F_a_c_e_ _I_n_f_e_r_e_n_c_e
           o _S_w_i_f_t_I_n_f_e_r_:_B_r_e_a_k_s_ _t_h_e_ _L_e_n_g_t_h_ _L_i_m_i_t_ _o_f_ _L_L_M_ _f_o_r_ _M_u_l_t_i_-_R_o_u_n_d
             _C_o_n_v_e_r_s_a_t_i_o_n_s_ _w_i_t_h_ _4_6_%_ _A_c_c_e_l_e_r_a_t_i_o_n
-          o _G_P_T_-_3
-          o _O_P_T_-_1_7_5_B_ _O_n_l_i_n_e_ _S_e_r_v_i_n_g_ _f_o_r_ _T_e_x_t_ _G_e_n_e_r_a_t_i_o_n
-          o _1_7_6_B_ _B_L_O_O_M
     * _I_n_s_t_a_l_l_a_t_i_o_n
           o _P_y_P_I
           o _I_n_s_t_a_l_l_ _F_r_o_m_ _S_o_u_r_c_e
     * _U_s_e_ _D_o_c_k_e_r
     * _C_o_m_m_u_n_i_t_y
     * _C_o_n_t_r_i_b_u_t_i_n_g
     * _C_i_t_e_ _U_s
@@ -276,72 +275,66 @@
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 GPT2-NVME.png]
 - 120x larger model size on the same hardware (RTX 3080) ### PaLM
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 PaLM-GPU1.png]
 - 34x larger model size on the same hardware
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Inference ### Grok-1
+## Inference ### Colossal-Inference
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                    inference/colossal-inference-v1-1.png]
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                    inference/colossal-inference-v1-2.png]
+- Large AI models inference speed doubled, compared to the offline inference
+performance of vLLM in some cases. [[code]](https://github.com/hpcaitech/
+ColossalAI/tree/main/colossalai/inference) [[blog]](https://hpc-ai.com/blog/
+colossal-inference) ### Grok-1
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                          images/grok-1-inference.jpg]
 - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use
 Python + PyTorch + HuggingFace version for Inference. [[code]](https://
 github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1) [[blog]]
 (https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-
 3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here) [
 [HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/
 grok-1) [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/
-models/colossalai/grok-1-pytorch/summary)
+models/colossalai/grok-1-pytorch/summary) ### SwiftInfer
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 SwiftInfer.jpg]
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance
 improved by 46%, open source solution breaks the length limit of LLM for multi-
 round conversations
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             inference_GPT-3.jpg]
-- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
-acceleration on the same hardware
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             BLOOM%20serving.png]
-- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
-Try 175-billion-parameter OPT online services
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                            BLOOM%20Inference.PNG]
-- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
-Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
-times.
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Installation Requirements: - PyTorch >= 1.11 and PyTorch <= 2.1 - Python >=
-3.7 - CUDA >= 11.0 - [NVIDIA GPU Compute Capability](https://
-developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher) - Linux OS If
-you encounter any problem with installation, you may want to raise an [issue]
-(https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
-### Install from PyPI You can easily install Colossal-AI with the following
-command. **By default, we do not build PyTorch extensions during
-installation.** ```bash pip install colossalai ``` **Note: only Linux is
-supported for now.** However, if you want to build the PyTorch extensions
-during installation, you can set `BUILD_EXT=1`. ```bash BUILD_EXT=1 pip install
-colossalai ``` **Otherwise, CUDA kernels will be built during runtime when you
-actually need them.** We also keep releasing the nightly version to PyPI every
-week. This allows you to access the unreleased features and bug fixes in the
-main branch. Installation can be made via ```bash pip install colossalai-
-nightly ``` ### Download From Source > The version of Colossal-AI will be in
-line with the main branch of the repository. Feel free to raise an issue if you
-encounter any problems. :) ```shell git clone https://github.com/hpcaitech/
-ColossalAI.git cd ColossalAI # install colossalai pip install . ``` By default,
-we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
-If you want to install and enable CUDA kernel fusion (compulsory installation
-when using fused optimizer): ```shell BUILD_EXT=1 pip install . ``` For Users
-with CUDA 10.2, you can still build ColossalAI from source. However, you need
-to manually download the cub library and copy it to the corresponding
-directory. ```bash # clone the repository git clone https://github.com/
-hpcaitech/ColossalAI.git cd ColossalAI # download the cub library wget https://
-github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip cp -r cub-
-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ # install
-BUILD_EXT=1 pip install . ```
+## Installation Requirements: - PyTorch >= 2.1 - Python >= 3.7 - CUDA >= 11.0 -
+[NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0
+(V100/RTX20 and higher) - Linux OS If you encounter any problem with
+installation, you may want to raise an [issue](https://github.com/hpcaitech/
+ColossalAI/issues/new/choose) in this repository. ### Install from PyPI You can
+easily install Colossal-AI with the following command. **By default, we do not
+build PyTorch extensions during installation.** ```bash pip install colossalai
+``` **Note: only Linux is supported for now.** However, if you want to build
+the PyTorch extensions during installation, you can set `BUILD_EXT=1`. ```bash
+BUILD_EXT=1 pip install colossalai ``` **Otherwise, CUDA kernels will be built
+during runtime when you actually need them.** We also keep releasing the
+nightly version to PyPI every week. This allows you to access the unreleased
+features and bug fixes in the main branch. Installation can be made via ```bash
+pip install colossalai-nightly ``` ### Download From Source > The version of
+Colossal-AI will be in line with the main branch of the repository. Feel free
+to raise an issue if you encounter any problems. :) ```shell git clone https://
+github.com/hpcaitech/ColossalAI.git cd ColossalAI # install colossalai pip
+install . ``` By default, we do not compile CUDA/C++ kernels. ColossalAI will
+build them during runtime. If you want to install and enable CUDA kernel fusion
+(compulsory installation when using fused optimizer): ```shell BUILD_EXT=1 pip
+install . ``` For Users with CUDA 10.2, you can still build ColossalAI from
+source. However, you need to manually download the cub library and copy it to
+the corresponding directory. ```bash # clone the repository git clone https://
+github.com/hpcaitech/ColossalAI.git cd ColossalAI # download the cub library
+wget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip
+cp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ #
+install BUILD_EXT=1 pip install . ```
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
 ## Use Docker ### Pull from DockerHub You can directly pull the docker image
 from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The
 image is automatically uploaded upon release. ### Build On Your Own Run the
 following command to build a docker image from Dockerfile provided. > Building
 Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker
 Runtime as the default when doing `docker build`. More details can be found
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_meta_registration.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/_subclasses/_meta_registration.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_monkey_patch.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/_subclasses/_monkey_patch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/flop_tensor.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/_subclasses/flop_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/meta_tensor.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/_subclasses/meta_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/codegen.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/codegen.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/graph_module.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/graph_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/node_util.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/node_util.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/graph_profile.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/passes/graph_profile.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/shape_prop.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/passes/shape_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/symbolic_profile.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/symbolic_profile.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/bias_addition.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/bias_addition.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/custom_leaf_module.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/custom_leaf_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/proxy.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/proxy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/symbolic_trace.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/symbolic_trace.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/tracer.py` & `colossalai-nightly-2024.6.1/colossalai/_analyzer/fx/tracer/tracer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/accelerator/api.py` & `colossalai-nightly-2024.6.1/colossalai/accelerator/api.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/accelerator/base_accelerator.py` & `colossalai-nightly-2024.6.1/colossalai/accelerator/base_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/accelerator/cpu_accelerator.py` & `colossalai-nightly-2024.6.1/colossalai/accelerator/cpu_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/accelerator/cuda_accelerator.py` & `colossalai-nightly-2024.6.1/colossalai/accelerator/cuda_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/accelerator/npu_accelerator.py` & `colossalai-nightly-2024.6.1/colossalai/accelerator/npu_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py` & `colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py` & `colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py` & `colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/base.py` & `colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/mixed_precision_mixin/base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py` & `colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_optimizer.py` & `colossalai-nightly-2024.6.1/colossalai/amp/naive_amp/mixed_precision_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/operation.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/checkpoint/operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/where.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/meta_registry/where.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/registry.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/shard_metainfo.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/meta_profiler/shard_metainfo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/amp_optimizer.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/amp_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/base_offload_module.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/base_offload_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/mem_optimize.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/mem_optimize.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/region.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region_manager.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/region_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/runtime.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/runtime.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/solver.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/solver.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/training_simulator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/training_simulator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/util.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/offload/util.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/comm_metainfo_pass.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/passes/comm_metainfo_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/meta_info_prop.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/passes/meta_info_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_apply_pass.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/passes/runtime_apply_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_preparation_pass.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/passes/runtime_preparation_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/constants.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/initialize.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/initialize.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/registry.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/options.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/options.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/sharding_strategy.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/sharding_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/solver.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/solver/solver.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/broadcast.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/broadcast.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/factory.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/factory.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/misc.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/misc.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/reshape.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/reshape.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/sharding.py` & `colossalai-nightly-2024.6.1/colossalai/auto_parallel/tensor_shard/utils/sharding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/accelerator.py` & `colossalai-nightly-2024.6.1/colossalai/booster/accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/booster.py` & `colossalai-nightly-2024.6.1/colossalai/booster/booster.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_apex.py` & `colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/fp16_apex.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_naive.py` & `colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/fp16_naive.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_torch.py` & `colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/fp16_torch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/mixed_precision_base.py` & `colossalai-nightly-2024.6.1/colossalai/booster/mixed_precision/mixed_precision_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/plugin/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/booster/plugin/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/plugin/dp_plugin_base.py` & `colossalai-nightly-2024.6.1/colossalai/booster/plugin/dp_plugin_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/plugin/gemini_plugin.py` & `colossalai-nightly-2024.6.1/colossalai/booster/plugin/gemini_plugin.py`

 * *Files 2% similar despite different names*

```diff
@@ -325,14 +325,15 @@
 
     def __init__(
         self,
         chunk_config_dict: Optional[dict] = None,
         chunk_init_device: Optional[torch.device] = None,
         placement_policy: str = "static",
         enable_gradient_accumulation: bool = False,
+        max_prefetch: int = 0,
         shard_param_frac: float = 1.0,  # only for static placement
         offload_optim_frac: float = 0.0,  # only for static placement
         offload_param_frac: float = 0.0,  # only for static placement
         warmup_non_model_data_ratio: float = 0.8,  # only for auto placement
         steady_cuda_cap_ratio: float = 0.9,  # only for auto placement
         precision: str = "fp16",
         master_weights: bool = True,
@@ -357,14 +358,15 @@
         extra_dp_size: int = 1,
         enable_all_optimization: bool = False,
         enable_fused_normalization: bool = False,
         enable_flash_attention: bool = False,
         enable_sequence_parallelism: bool = False,
         enable_jit_fused: bool = False,
         enable_sequence_overlap: bool = False,
+        enable_async_reduce: bool = True,
         verbose: bool = False,
     ) -> None:
         super().__init__()
         assert precision in SUPPORTED_PRECISION, f"precision {precision} is not supported"
         if get_accelerator().name == "npu":
             assert placement_policy == "static", "NPU only supports static placement policy"
         self.gemini_config = dict(
@@ -382,14 +384,16 @@
             strict_ddp_mode=strict_ddp_mode,
             search_range_m=search_range_m,
             hidden_dim=hidden_dim,
             min_chunk_size_m=min_chunk_size_m,
             memstats=memstats,
             mixed_precision=PRECISION_STR_TO_DTYPE[precision],
             master_weights=master_weights,
+            max_prefetch=max_prefetch,
+            enable_async_reduce=enable_async_reduce,
         )
         self.zero_optim_config = dict(
             gpu_margin_mem_ratio=gpu_margin_mem_ratio,
         )
         self.optim_kwargs = dict(
             initial_scale=initial_scale,
             growth_factor=growth_factor,
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/plugin/hybrid_parallel_plugin.py` & `colossalai-nightly-2024.6.1/colossalai/booster/plugin/hybrid_parallel_plugin.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 import ctypes
 import random
 import warnings
+from collections import defaultdict
 from contextlib import contextmanager
+from copy import deepcopy
 from functools import partial
 from types import MethodType
 from typing import Any, Callable, Dict, Iterator, List, Optional, OrderedDict, Tuple, Union
 
 import numpy as np
 import torch
 import torch.distributed as dist
@@ -20,14 +22,16 @@
 from torch.utils.data.distributed import DistributedSampler
 
 from colossalai.accelerator import get_accelerator
 from colossalai.amp.naive_amp.mixed_precision_optimizer import MixedPrecisionOptimizer
 from colossalai.checkpoint_io import CheckpointIO, HybridParallelCheckpointIO
 from colossalai.cluster import ProcessGroupMesh
 from colossalai.interface import AMPModelMixin, ModelWrapper, OptimizerWrapper
+from colossalai.interface.optimizer import DistributedOptim
+from colossalai.nn.optimizer import DistGaloreAwamW, cast_to_distributed
 from colossalai.pipeline.schedule import InterleavedSchedule, OneForwardOneBackwardSchedule
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer import GradientCheckpointConfig, ShardConfig, ShardFormer
 from colossalai.shardformer.layer.utils import SeqParallelUtils
 from colossalai.shardformer.policies.base_policy import Policy
 from colossalai.tensor.d_tensor.api import is_distributed_tensor
 from colossalai.zero.low_level import LowLevelZeroOptimizer
@@ -731,15 +735,15 @@
                 return grads_to_sync
             else:
                 return None
 
         # Get all working gradients and gradients to be synchronized.
         all_working_grads = _get_all_working_grads()
         grads_to_sync = _get_grads_to_sync(all_working_grads)
-        if self.require_grad_sync and grads_to_sync is not None:
+        if self._grad_store.require_grad_sync and grads_to_sync is not None:
             # Synchronize sequence parallelism gradients if required.
             SeqParallelUtils.allreduce_partial_data_grad(process_group=self.tp_pg, grads=grads_to_sync)
         else:
             return
 
     def backward(self, loss, retain_graph=False):
         """
@@ -755,15 +759,15 @@
 
         Returns:
             None
         """
         # Call the superclass backward method to compute gradients.
         super().backward(loss, retain_graph)
 
-        if self.require_grad_sync and self.model.shard_config.enable_sequence_parallelism:
+        if self._grad_store.require_grad_sync and self.model.shard_config.enable_sequence_parallelism:
             # If gradient synchronization is required, sync sequence parallelism gradients.
             self._sync_sp_grads()
         else:
             # If gradient synchronization is is not required, return.
             return
 
     def backward_by_grad(self, tensor, grad):
@@ -780,15 +784,15 @@
 
         Returns:
             None
         """
         # Call the superclass backward_by_grad method to compute gradients.
         super().backward_by_grad(tensor, grad)
 
-        if self.require_grad_sync and self.model.shard_config.enable_sequence_parallelism:
+        if self._grad_store.require_grad_sync and self.model.shard_config.enable_sequence_parallelism:
             # If gradient synchronization is required, sync sequence parallelism gradients.
             self._sync_sp_grads()
         else:
             # If gradient synchronization is is not required, return.
             return
 
     def _compute_grad_norm(self, gradients: List[Tensor], norm_type: int = 2) -> float:
@@ -1167,14 +1171,27 @@
         model: Module,
         optimizer: Optional[Optimizer] = None,
         criterion: Optional[Callable] = None,
         dataloader: Optional[DataLoader] = None,
         lr_scheduler: Optional[LRScheduler] = None,
     ) -> Tuple[Module, OptimizerWrapper, Callable, DataLoader, LRScheduler]:
         param_info = get_param_info(optimizer)
+
+        # TODO: Support Galore + ZeRO
+        zero_stage = self.zero_stage
+        zero_config = deepcopy(self.zero_config)
+
+        # Replace with distributed implementation if exists
+        optimizer = cast_to_distributed(optimizer)
+
+        if isinstance(optimizer, DistGaloreAwamW) and zero_stage > 0 and self.dp_size > 0:
+            warnings.warn("Galore is only supported for Tensor Parallel and vanilla Data Parallel yet. Disabling ZeRO.")
+            zero_config["partition_grad"] = False
+            zero_stage = 0
+
         if not isinstance(model, ModelWrapper):
             use_ddp = (self.dp_size > 1 and self.pp_size == 1 and self.zero_stage == 0) or (
                 self.dp_size == 1
                 and self.pp_size == 1
                 and self.enable_sequence_parallelism
                 and self.sequence_parallelism_mode == "all_to_all"
             )
@@ -1190,15 +1207,16 @@
                 tp_group=self.tp_group,
                 sp_group=self.sp_group,
                 use_ddp=use_ddp,
                 ddp_config=self.ddp_config,
                 custom_policy=self.custom_policy,
             )
         if optimizer is not None and not isinstance(optimizer, OptimizerWrapper):
-            if self.zero_stage == 0:
+            if zero_stage == 0:
+                is_zero = False
                 if self.precision in ["fp16", "bf16"]:
                     optimizer = HybridParallelAMPOptimizer(
                         optimizer,
                         model,
                         use_pipeline=self.enable_pipeline_parallelism,
                         param_info=param_info,
                         precision=self.precision,
@@ -1214,37 +1232,45 @@
                         use_pipeline=self.enable_pipeline_parallelism,
                         param_info=param_info,
                         max_norm=self.max_norm,
                         pp_process_group=self.pp_group,
                         tp_process_group=self.tp_group,
                     )
             else:
-                zero_dp_size = dist.get_world_size(dp_group)
-                if zero_dp_size == 1:
+                is_zero = self.dp_size > 1
+                if self.dp_size == 1:
                     warnings.warn(
                         "Use Zero Optimizer when data parallel size is 1 may introduce unnecessary overhead. "
-                        "If you are not intended to use cpu_offload, please consider set zero_stage=0."
+                        "If you do not intend to use cpu_offload, please consider set zero_stage=0."
                     )
 
                 assert self.precision != "fp32", "Please set precision to 'fp16' or 'bf16' when using ZeRO."
                 optimizer = HybridParallelZeroOptimizer(
                     optimizer,
                     model,
                     use_pipeline=self.enable_pipeline_parallelism,
                     param_info=param_info,
                     dp_process_group=dp_group,
                     tp_process_group=self.tp_group,
                     pp_process_group=self.pp_group,
                     verbose=True,
                     clip_grad_norm=self.max_norm,
-                    **self.zero_config,
+                    **zero_config,
                     **self.amp_config,
                 )
             # inject update_master_params
             model.update_master_params = MethodType(optimizer.update_master_params, model)
+
+            # Setup optimizers that require global states
+            optim = optimizer.optim
+            if isinstance(optim, DistributedOptim):
+                shard_to_param = optimizer.get_master_to_working_map() if is_zero else {}
+                padding_map = optimizer.get_param_padding_map() if is_zero else defaultdict(int)
+                optim.setup_distributed(self.tp_group, self.dp_group, shard_to_param, padding_map, is_zero)
+
         return model, optimizer, criterion, dataloader, lr_scheduler
 
     def execute_pipeline(
         self,
         data_iter: Iterator,
         model: HybridParallelModule,
         criterion: Callable[[Any, Any], torch.Tensor],
@@ -1268,15 +1294,15 @@
         with ctx:
             outputs = self.schedule.forward_backward_step(
                 model, data_iter, criterion, optimizer, return_loss, return_outputs
             )
 
         # run with gradients accumulation
         if model.require_grad_sync == False or (
-            isinstance(optimizer, HybridParallelZeroOptimizer) and optimizer.require_grad_sync == False
+            isinstance(optimizer, HybridParallelZeroOptimizer) and optimizer._grad_store.require_grad_sync == False
         ):
             return outputs
 
         # Synchronize the grads of shared parameters of the model.
         model.sync_shared_params()
         # Synchronize sequence parallelism gradients of the model.
         model.sync_sp_grads()
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/plugin/low_level_zero_plugin.py` & `colossalai-nightly-2024.6.1/colossalai/booster/plugin/low_level_zero_plugin.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,15 +4,18 @@
 import warnings
 from functools import partial
 from pathlib import Path
 from types import MethodType
 from typing import Callable, Dict, Iterator, List, Optional, Tuple
 
 import torch
+import torch.distributed
+import torch.distributed as dist
 import torch.nn as nn
+from torch.distributed.distributed_c10d import _get_default_group
 from torch.nn import Parameter
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
 from torch.utils._pytree import tree_map
 from torch.utils.data import DataLoader
 
 from colossalai.accelerator import get_accelerator
@@ -24,14 +27,16 @@
     load_shard_state_dict,
     load_states_into_optimizer,
     save_param_groups,
     save_state_dict,
     sharded_optimizer_loading_epilogue,
 )
 from colossalai.interface import AMPModelMixin, ModelWrapper, OptimizerWrapper
+from colossalai.interface.optimizer import DistributedOptim
+from colossalai.nn.optimizer import DistGaloreAwamW, cast_to_distributed
 from colossalai.quantization import BnbQuantizationConfig, quantize_model
 from colossalai.zero import LowLevelZeroOptimizer
 
 from .dp_plugin_base import DPPluginBase
 from .torch_ddp_plugin import TorchDDPCheckpointIO
 
 __all__ = ["LowLevelZeroPlugin"]
@@ -424,21 +429,43 @@
             ), "The model should have been wrapped as a PeftModel when self.lora_enabled is True"
             if optimizer is not None:
                 self.add_lora_params_to_optimizer(model, optimizer)
 
         if not isinstance(model, ModelWrapper):
             model = LowLevelZeroModel(model, self.precision)
 
+        # TODO: Support Galore + ZeRO
+        zero_stage = self.stage
+        zero_optim_kwargs = {**self.zero_optim_kwargs}
+        dp_size = dist.get_world_size()
+
+        # Replace with the distributed implementation if exists
+        optimizer = cast_to_distributed(optimizer)
+
+        if isinstance(optimizer, DistGaloreAwamW) and zero_stage > 0 and dp_size > 0:
+            warnings.warn("Galore is only supported for Tensor Parallel and vanilla Data Parallel yet. Disabling ZeRO.")
+            zero_optim_kwargs["partition_grad"] = False
+            zero_stage = 0
+
         if optimizer is not None and not isinstance(optimizer, OptimizerWrapper):
             optimizer: LowLevelZeroOptimizer = LowLevelZeroOptimizer(
-                optimizer, **self.zero_optim_kwargs, verbose=self.verbose
+                optimizer, **zero_optim_kwargs, verbose=self.verbose
             )
             # inject update_master_params
             model.update_master_params = MethodType(optimizer.update_master_params, model)
 
+            # Setup optimizers that require global states
+            optim = optimizer.optim
+            is_zero = dp_size > 1 and zero_stage > 0
+            dp_group = _get_default_group()  # Use the whole world
+            if isinstance(optim, DistributedOptim):
+                shard_to_param = optimizer.get_master_to_working_map()
+                padding_map = optimizer.get_param_padding_map()
+                optim.setup_distributed(None, dp_group, shard_to_param, padding_map, is_zero)
+
         return model, optimizer, criterion, dataloader, lr_scheduler
 
     def control_checkpoint_io(self) -> bool:
         return True
 
     def get_checkpoint_io(self) -> CheckpointIO:
         return LowLevelZeroCheckpointIO()
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py` & `colossalai-nightly-2024.6.1/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/plugin/plugin_base.py` & `colossalai-nightly-2024.6.1/colossalai/booster/plugin/plugin_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/plugin/pp_plugin_base.py` & `colossalai-nightly-2024.6.1/colossalai/booster/plugin/pp_plugin_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_ddp_plugin.py` & `colossalai-nightly-2024.6.1/colossalai/booster/plugin/torch_ddp_plugin.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_fsdp_plugin.py` & `colossalai-nightly-2024.6.1/colossalai/booster/plugin/torch_fsdp_plugin.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/checkpoint_io_base.py` & `colossalai-nightly-2024.6.1/colossalai/checkpoint_io/checkpoint_io_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/general_checkpoint_io.py` & `colossalai-nightly-2024.6.1/colossalai/checkpoint_io/general_checkpoint_io.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py` & `colossalai-nightly-2024.6.1/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/index_file.py` & `colossalai-nightly-2024.6.1/colossalai/checkpoint_io/index_file.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/utils.py` & `colossalai-nightly-2024.6.1/colossalai/checkpoint_io/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/cli/check/check_installation.py` & `colossalai-nightly-2024.6.1/colossalai/cli/check/check_installation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/cli/launcher/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/cli/launcher/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/cli/launcher/hostinfo.py` & `colossalai-nightly-2024.6.1/colossalai/cli/launcher/hostinfo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/cli/launcher/multinode_runner.py` & `colossalai-nightly-2024.6.1/colossalai/cli/launcher/multinode_runner.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/cli/launcher/run.py` & `colossalai-nightly-2024.6.1/colossalai/cli/launcher/run.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/cluster/device_mesh_manager.py` & `colossalai-nightly-2024.6.1/colossalai/cluster/device_mesh_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/cluster/dist_coordinator.py` & `colossalai-nightly-2024.6.1/colossalai/cluster/dist_coordinator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/cluster/process_group_manager.py` & `colossalai-nightly-2024.6.1/colossalai/cluster/process_group_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/cluster/process_group_mesh.py` & `colossalai-nightly-2024.6.1/colossalai/cluster/process_group_mesh.py`

 * *Files 2% similar despite different names*

```diff
@@ -34,15 +34,20 @@
     Attributes:
         shape (Tuple[int, ...]): The shape of the process group mesh.
         rank (int): The rank of the current process.
     """
 
     def __init__(self, *size: int) -> None:
         assert dist.is_initialized(), "Please initialize torch.distributed first."
-        assert prod(size) == dist.get_world_size(), "The product of the size must be equal to the world size."
+        world_size = dist.get_world_size()
+        prod_size = prod(size)
+        assert (
+            prod_size == world_size
+        ), f"The product of the size({prod_size}) must be equal to the world size({world_size})."
+
         self._shape = size
         self._rank = dist.get_rank()
         self._coord = ProcessGroupMesh.unravel(self._rank, self._shape)
         self._ranks_to_group: Dict[Tuple[int, ...], ProcessGroup] = {}
         self._group_to_ranks: Dict[ProcessGroup, Tuple[int, ...]] = {}
 
     def destroy_mesh_process_groups(self):
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/context/config.py` & `colossalai-nightly-2024.6.1/colossalai/context/config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/context/singleton_meta.py` & `colossalai-nightly-2024.6.1/colossalai/context/singleton_meta.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/device/alpha_beta_profiler.py` & `colossalai-nightly-2024.6.1/colossalai/device/alpha_beta_profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/device/calc_pipeline_strategy.py` & `colossalai-nightly-2024.6.1/colossalai/device/calc_pipeline_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/device/device_mesh.py` & `colossalai-nightly-2024.6.1/colossalai/device/device_mesh.py`

 * *Files 0% similar despite different names*

```diff
@@ -302,17 +302,16 @@
             mapping (Dict): a dictionary that maps the global rank to the local rank in the logical device mesh.
                 The value is a list of integers and each integer represents the local rank in the indexed axis.
         """
         for index, inner_tensor in enumerate(tensor):
             # index means the local rank in the current axis
             # inner_tensor refers to the processes with the same local rank
 
-            if inner_tensor.numel() == 1:
-                # if the inner_tensor only has one element, it means that
-                # it already reaches the last axis
+            if inner_tensor.dim() == 0:
+                # if the inner_tensor already reaches the last axis,
                 # we append its local_rank in the last axis to the index_list
                 # and assign to the mapping
                 # the value of the mapping is the the local rank at the indexed axis of the device mesh
                 mapping[int(inner_tensor)] = index_list + [index]
             else:
                 # we recursively go into the function until we reach the last axis
                 # meanwhile, we should add the local rank in the current axis in the index_list
@@ -455,14 +454,15 @@
                     processes_in_the_same_process_group[dim] = []
 
                 # get the local rank corresponding to the global rank
                 process_coordinates = self._global_to_local_rank_mapping[global_rank].copy()
 
                 # replace the local rank in the given dimension with the
                 # local rank of the current process iterated
+
                 process_coordinates[dim] = _local_rank
                 processes_in_the_same_process_group[dim].append(process_coordinates)
 
         # =================================================================
         # Step 2
         # Use local rank combination to find its corresponding global rank
         # =================================================================
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/_compatibility.py` & `colossalai-nightly-2024.6.1/colossalai/fx/_compatibility.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_12.py` & `colossalai-nightly-2024.6.1/colossalai/fx/_meta_regist_12.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_13.py` & `colossalai-nightly-2024.6.1/colossalai/fx/_meta_regist_13.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/codegen/activation_checkpoint_codegen.py` & `colossalai-nightly-2024.6.1/colossalai/fx/codegen/activation_checkpoint_codegen.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/graph_module.py` & `colossalai-nightly-2024.6.1/colossalai/fx/graph_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/passes/adding_split_node_pass.py` & `colossalai-nightly-2024.6.1/colossalai/fx/passes/adding_split_node_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/passes/concrete_info_prop.py` & `colossalai-nightly-2024.6.1/colossalai/fx/passes/concrete_info_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/passes/meta_info_prop.py` & `colossalai-nightly-2024.6.1/colossalai/fx/passes/meta_info_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/passes/passes_for_gpt2_test.py` & `colossalai-nightly-2024.6.1/colossalai/fx/passes/passes_for_gpt2_test.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/passes/shard_1d_pass.py` & `colossalai-nightly-2024.6.1/colossalai/fx/passes/shard_1d_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/passes/split_module.py` & `colossalai-nightly-2024.6.1/colossalai/fx/passes/split_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/passes/utils.py` & `colossalai-nightly-2024.6.1/colossalai/fx/passes/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/constants.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/dataflow.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/dataflow.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/constants.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/activation_function.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/activation_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/embedding.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/normalization.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/pooling.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/activation_function.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/activation_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/attention.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/attention.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/convolution.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/normalization.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/pooling.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/rnn.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/profiler_module/rnn.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/registry.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/shard_utils.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/experimental/shard_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/memory_utils.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/memory_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/opcount.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/opcount.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/profiler.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/shard_utils.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/shard_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/profiler/tensor.py` & `colossalai-nightly-2024.6.1/colossalai/fx/profiler/tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/proxy.py` & `colossalai-nightly-2024.6.1/colossalai/fx/proxy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/_meta_trace.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/_meta_trace.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/_symbolic_trace.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/_symbolic_trace.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/_tracer_utils.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/_tracer_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/experimental.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/experimental.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/convolution.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/normalization.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/convolution.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/normalization.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/pooling.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/rnn.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/meta_patch/patched_module/rnn.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/registry.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/fx/tracer/tracer.py` & `colossalai-nightly-2024.6.1/colossalai/fx/tracer/tracer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/inference/engine/engine.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/engine.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,52 +1,63 @@
-from typing import Union
-
 import torch
 import torch.distributed as dist
 import torch.nn as nn
-from transformers.utils import logging
+from transformers.tokenization_utils_base import BatchEncoding
 
 from colossalai.cluster import ProcessGroupMesh
 from colossalai.pipeline.schedule.generate import GenerateSchedule
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer import ShardConfig, ShardFormer
 from colossalai.shardformer.policies.base_policy import Policy
 
-from ..kv_cache import MemoryManager
-from .microbatch_manager import MicroBatchManager
-from .policies import model_policy_map
+from ..pipeline.microbatch_manager import MicroBatchManager
+from ..tensor_parallel.kvcache_manager import MemoryManager
 
 PP_AXIS, TP_AXIS = 0, 1
 
 _supported_models = [
     "LlamaForCausalLM",
-    "BloomForCausalLM",
-    "LlamaGPTQForCausalLM",
-    "SmoothLlamaForCausalLM",
-    "ChatGLMForConditionalGeneration",
 ]
 
 
-class InferenceEngine:
+class CaiInferEngine:
     """
-    InferenceEngine is a class that handles the pipeline parallel inference.
+    CaiInferEngine is a class that handles the pipeline parallel inference.
 
     Args:
         tp_size (int): the size of tensor parallelism.
         pp_size (int): the size of pipeline parallelism.
-        dtype (str): the data type of the model, should be one of 'fp16', 'fp32', 'bf16'.
         model (`nn.Module`): the model not in pipeline style, and will be modified with `ShardFormer`.
-        model_policy (`colossalai.shardformer.policies.base_policy.Policy`): the policy to shardformer model. It will be determined by the model type if not provided.
-        micro_batch_size (int): the micro batch size. Only useful when `pp_size` > 1.
+        model_policy (`colossalai.shardformer.policies.base_policy.Policy`): the policy to shardformer model.
+        micro_batch_size (int): the micro batch size.
         micro_batch_buffer_size (int): the buffer size for micro batch. Normally, it should be the same as the number of pipeline stages.
         max_batch_size (int): the maximum batch size.
         max_input_len (int): the maximum input length.
         max_output_len (int): the maximum output length.
-        quant (str): the quantization method, should be one of 'smoothquant', 'gptq', None.
-        verbose (bool): whether to return the time cost of each step.
+
+    Example:
+
+    ```python
+    from colossalai.inference import InferEngine
+    from colossalai.inference.pipeline.policies import LlamaModelInferPolicy
+    import colossalai
+    from transformers import LlamaForCausalLM, LlamaTokenizer
+
+    colossalai.launch_from_torch()
+
+    model = LlamaForCausalLM.from_pretrained("your_path_to_model")
+    tokenizer = LlamaTokenizer.from_pretrained("/home/lczyh/share/models/llama-7b-hf")
+    # assume the model is inferred with 2 pipeline stages
+    inferengine = CaiInferEngine(pp_size=2, model=model, model_policy=LlamaModelInferPolicy())
+
+    input = ["Introduce a landmark in China ","Introduce a landmark in China "]
+    data = tokenizer(input, return_tensors='pt')
+    output = inferengine.inference([data.to('cuda').data])
+
+    ```
 
     """
 
     def __init__(
         self,
         tp_size: int = 1,
         pp_size: int = 1,
@@ -54,142 +65,106 @@
         model: nn.Module = None,
         model_policy: Policy = None,
         micro_batch_size: int = 1,
         micro_batch_buffer_size: int = None,
         max_batch_size: int = 4,
         max_input_len: int = 32,
         max_output_len: int = 32,
-        quant: str = None,
         verbose: bool = False,
-        # TODO: implement early_stopping, and various gerneration options
+        # TODO: implement early_stopping, and various generation options
         early_stopping: bool = False,
         do_sample: bool = False,
         num_beams: int = 1,
     ) -> None:
-        if quant == "gptq":
-            from ..quant.gptq import GPTQManager
-
-            self.gptq_manager = GPTQManager(model.quantize_config, max_input_len=max_input_len)
-            model = model.model
-        elif quant == "smoothquant":
-            model = model.model
-
         assert model.__class__.__name__ in _supported_models, f"Model {model.__class__.__name__} is not supported."
         assert (
             tp_size * pp_size == dist.get_world_size()
         ), f"TP size({tp_size}) * PP size({pp_size}) should be equal to the global world size ({dist.get_world_size()})"
-        assert model, "Model should be provided."
+        assert model and model_policy, "Model with model_policy should be provided."
         assert dtype in ["fp16", "fp32", "bf16"], "dtype should be one of 'fp16', 'fp32', 'bf16'"
 
         assert max_batch_size <= 64, "Max batch size exceeds the constraint"
         assert max_input_len + max_output_len <= 4096, "Max length exceeds the constraint"
-        assert quant in ["smoothquant", "gptq", None], "quant should be one of 'smoothquant', 'gptq'"
+
+        # TODO: support only tensor parallel inference
+        assert pp_size > 1, "Not support only tensor parallel inference."
         self.pp_size = pp_size
         self.tp_size = tp_size
-        self.quant = quant
-
-        logger = logging.get_logger(__name__)
-        if quant == "smoothquant" and dtype != "fp32":
-            dtype = "fp32"
-            logger.warning_once("Warning: smoothquant only support fp32 and int8 mix precision. set dtype to fp32")
 
         if dtype == "fp16":
             self.dtype = torch.float16
             model.half()
         elif dtype == "bf16":
             self.dtype = torch.bfloat16
             model.to(torch.bfloat16)
         else:
             self.dtype = torch.float32
 
-        if model_policy is None:
-            model_policy = model_policy_map[model.config.model_type]()
-
         # Init pg mesh
         pg_mesh = ProcessGroupMesh(pp_size, tp_size)
 
-        stage_manager = PipelineStageManager(pg_mesh, PP_AXIS, True if pp_size * tp_size > 1 else False)
-        self.cache_manager_list = [
-            self._init_manager(model, max_batch_size, max_input_len, max_output_len)
-            for _ in range(micro_batch_buffer_size or pp_size)
-        ]
-        self.mb_manager = MicroBatchManager(
-            stage_manager.stage,
-            micro_batch_size,
-            micro_batch_buffer_size or pp_size,
-            max_input_len,
-            max_output_len,
-            self.cache_manager_list,
-        )
-        self.verbose = verbose
-        self.schedule = GenerateSchedule(stage_manager, self.mb_manager, verbose)
+        stage_manager = None
+        if pp_size > 1:
+            stage_manager = PipelineStageManager(pg_mesh, PP_AXIS, True)
+            self.cache_manager_list = [
+                self._init_manager(model, max_batch_size, max_input_len, max_output_len)
+                for _ in range(micro_batch_buffer_size or pp_size)
+            ]
+            self.mb_manager = MicroBatchManager(
+                stage_manager.stage,
+                micro_batch_size,
+                micro_batch_buffer_size or pp_size,
+                max_input_len,
+                max_output_len,
+                self.cache_manager_list,
+            )
+            self.verbose = verbose
+            self.schedule = GenerateSchedule(stage_manager, self.mb_manager, verbose)
 
-        self.model = self._shardformer(
-            model, model_policy, stage_manager, pg_mesh.get_group_along_axis(TP_AXIS) if pp_size * tp_size > 1 else None
-        )
-        if quant == "gptq":
-            self.gptq_manager.post_init_gptq_buffer(self.model)
+        self.model = self._shardformer(model, model_policy, stage_manager, pg_mesh.get_group_along_axis(TP_AXIS))
 
-    def generate(self, input_list: Union[list, dict]):
+    def inference(self, input_list):
         """
         Args:
             input_list (list): a list of input data, each element is a `BatchEncoding` or `dict`.
 
         Returns:
             out (list): a list of output data, each element is a list of token.
             timestamp (float): the time cost of the inference, only return when verbose is `True`.
         """
-
+        assert isinstance(
+            input_list, (BatchEncoding, dict)
+        ), f"Only accept BatchEncoding or dict as input, but got {input_list.__class__.__name__}."
+        if isinstance(input_list, BatchEncoding):
+            input_list = input_list.data
         out, timestamp = self.schedule.generate_step(self.model, iter([input_list]))
         if self.verbose:
             return out, timestamp
         else:
             return out
 
     def _shardformer(self, model, model_policy, stage_manager, tp_group):
         shardconfig = ShardConfig(
             tensor_parallel_process_group=tp_group,
             pipeline_stage_manager=stage_manager,
-            enable_tensor_parallelism=(self.tp_size > 1),
+            enable_tensor_parallelism=False,
             enable_fused_normalization=False,
             enable_all_optimization=False,
             enable_flash_attention=False,
             enable_jit_fused=False,
             enable_sequence_parallelism=False,
-            extra_kwargs={"quant": self.quant},
         )
         shardformer = ShardFormer(shard_config=shardconfig)
         shard_model, _ = shardformer.optimize(model, model_policy)
         return shard_model.cuda()
 
     def _init_manager(self, model, max_batch_size: int, max_input_len: int, max_output_len: int) -> None:
         max_total_token_num = max_batch_size * (max_input_len + max_output_len)
-        if model.config.model_type == "llama":
-            head_dim = model.config.hidden_size // model.config.num_attention_heads
-            head_num = model.config.num_key_value_heads // self.tp_size
-            num_hidden_layers = (
-                model.config.num_hidden_layers
-                if hasattr(model.config, "num_hidden_layers")
-                else model.config.num_layers
-            )
-            layer_num = num_hidden_layers // self.pp_size
-        elif model.config.model_type == "bloom":
-            head_dim = model.config.hidden_size // model.config.n_head
-            head_num = model.config.n_head // self.tp_size
-            num_hidden_layers = model.config.n_layer
-            layer_num = num_hidden_layers // self.pp_size
-        elif model.config.model_type == "chatglm":
-            head_dim = model.config.hidden_size // model.config.num_attention_heads
-            if model.config.multi_query_attention:
-                head_num = model.config.multi_query_group_num // self.tp_size
-            else:
-                head_num = model.config.num_attention_heads // self.tp_size
-            num_hidden_layers = model.config.num_layers
-            layer_num = num_hidden_layers // self.pp_size
-        else:
-            raise NotImplementedError("Only support llama, bloom and chatglm model.")
+        head_dim = model.config.hidden_size // model.config.num_attention_heads
+        head_num = model.config.num_attention_heads
+        num_hidden_layers = (
+            model.config.num_hidden_layers if hasattr(model.config, "num_hidden_layers") else model.config.num_layers
+        )
+        layer_num = num_hidden_layers // self.pp_size
 
-        if self.quant == "smoothquant":
-            cache_manager = MemoryManager(max_total_token_num, torch.int8, head_num, head_dim, layer_num)
-        else:
-            cache_manager = MemoryManager(max_total_token_num, self.dtype, head_num, head_dim, layer_num)
+        cache_manager = MemoryManager(max_total_token_num, self.dtype, head_num, head_dim, layer_num)
         return cache_manager
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/inference/engine/microbatch_manager.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/pipeline/microbatch_manager.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 from enum import Enum
 from typing import Dict
 
 import torch
 
-from ..kv_cache import BatchInferState, MemoryManager
+from ..tensor_parallel.batch_infer_state import BatchInferState
+from ..tensor_parallel.kvcache_manager import MemoryManager
 
 __all__ = "MicroBatchManager"
 
 
 class Status(Enum):
     PREFILL = 1
     GENERATE = 2
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/modeling/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/bloom.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/modeling/llama.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,452 +1,489 @@
+# This code is adapted from huggingface transformers: https://github.com/huggingface/transformers/blob/v4.34.1/src/transformers/models/llama/modeling_llama.py
 import math
-import warnings
-from typing import List, Optional, Tuple, Union
+from typing import List, Optional, Tuple
 
 import torch
-import torch.distributed as dist
-from torch.nn import functional as F
-from transformers.models.bloom.modeling_bloom import (
-    BaseModelOutputWithPastAndCrossAttentions,
-    BloomAttention,
-    BloomBlock,
-    BloomForCausalLM,
-    BloomModel,
-)
+from transformers.models.llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaForCausalLM, LlamaModel
 from transformers.utils import logging
 
-from colossalai.inference.kv_cache.batch_infer_state import BatchInferState
-from colossalai.kernel.triton import bloom_context_attn_fwd, copy_kv_cache_to_dest, token_attention_fwd
+from colossalai.inference.tensor_parallel.batch_infer_state import BatchInferState
+from colossalai.kernel.triton import llama_context_attn_fwd, token_attention_fwd
+from colossalai.kernel.triton.token_attention_kernel import Llama2TokenAttentionForwards
 from colossalai.pipeline.stage_manager import PipelineStageManager
 
+from ._utils import copy_kv_to_mem_cache
+
 try:
-    from lightllm.models.bloom.triton_kernel.context_flashattention_nopad import (
-        context_attention_fwd as lightllm_bloom_context_attention_fwd,
+    from lightllm.models.llama2.triton_kernel.context_flashattention_nopad import (
+        context_attention_fwd as lightllm_llama2_context_attention_fwd,
+    )
+    from lightllm.models.llama.triton_kernel.context_flashattention_nopad import (
+        context_attention_fwd as lightllm_context_attention_fwd,
     )
+    from lightllm.models.llama.triton_kernel.rotary_emb import rotary_emb_fwd as llama_rotary_embedding_fwd
 
     HAS_LIGHTLLM_KERNEL = True
 except:
+    print("please install lightllm from source to run inference: https://github.com/ModelTC/lightllm")
     HAS_LIGHTLLM_KERNEL = False
 
+try:
+    from flash_attn import flash_attn_with_kvcache
 
-def generate_alibi(n_head, dtype=torch.float16):
-    """
-    This method is adapted from `_generate_alibi` function
-    in `lightllm/models/bloom/layer_weights/transformer_layer_weight.py`
-    of the ModelTC/lightllm GitHub repository.
-    This method is originally the `build_alibi_tensor` function
-    in `transformers/models/bloom/modeling_bloom.py`
-    of the huggingface/transformers GitHub repository.
-    """
+    HAS_FLASH_KERNEL = True
+except:
+    HAS_FLASH_KERNEL = False
+    print("please install flash attentiom from https://github.com/Dao-AILab/flash-attention")
 
-    def get_slopes_power_of_2(n):
-        start = 2 ** (-(2 ** -(math.log2(n) - 3)))
-        return [start * start**i for i in range(n)]
-
-    def get_slopes(n):
-        if math.log2(n).is_integer():
-            return get_slopes_power_of_2(n)
+
+def rotate_half(x):
+    """Rotates half the hidden dims of the input."""
+    x1 = x[..., : x.shape[-1] // 2]
+    x2 = x[..., x.shape[-1] // 2 :]
+    return torch.cat((-x2, x1), dim=-1)
+
+
+def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
+    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.
+    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]
+    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]
+    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
+    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
+
+    q_embed = (q * cos) + (rotate_half(q) * sin)
+    k_embed = (k * cos) + (rotate_half(k) * sin)
+    return q_embed, k_embed
+
+
+def llama_triton_context_attention(
+    query_states, key_states, value_states, attn_output, infer_state, num_key_value_groups=1
+):
+    if num_key_value_groups == 1:
+        if HAS_LIGHTLLM_KERNEL is False:
+            llama_context_attn_fwd(
+                query_states,
+                key_states,
+                value_states,
+                attn_output,
+                infer_state.start_loc,
+                infer_state.seq_len,
+                # infer_state.cache_manager.past_key_values_length,
+                infer_state.max_len_in_batch,
+            )
         else:
-            closest_power_of_2 = 2 ** math.floor(math.log2(n))
-            slopes_power_of_2 = get_slopes_power_of_2(closest_power_of_2)
-            slopes_double = get_slopes(2 * closest_power_of_2)
-            slopes_combined = slopes_power_of_2 + slopes_double[0::2][: n - closest_power_of_2]
-            return slopes_combined
+            lightllm_context_attention_fwd(
+                query_states,
+                key_states,
+                value_states,
+                attn_output,
+                infer_state.start_loc,
+                infer_state.seq_len,
+                # infer_state.cache_manager.past_key_values_length,
+                infer_state.max_len_in_batch,
+            )
+    else:
+        assert HAS_LIGHTLLM_KERNEL is True, "You have to install lightllm kernels to run llama2 model"
+        lightllm_llama2_context_attention_fwd(
+            query_states,
+            key_states,
+            value_states,
+            attn_output,
+            infer_state.start_loc,
+            infer_state.seq_len,
+            # infer_state.cache_manager.past_key_values_length,
+            infer_state.max_len_in_batch,
+        )
 
-    slopes = get_slopes(n_head)
-    return torch.tensor(slopes, dtype=dtype)
+
+def llama_triton_token_attention(query_states, attn_output, infer_state, num_key_value_groups=1):
+    assert HAS_LIGHTLLM_KERNEL is True, "You have to install lightllm kernel to run token attention for llama models"
+    if num_key_value_groups == 1:
+        token_attention_fwd(
+            query_states,
+            infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
+            infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
+            attn_output,
+            infer_state.block_loc,
+            infer_state.start_loc,
+            infer_state.seq_len,
+            # infer_state.cache_manager.past_key_values_length,
+            infer_state.max_len_in_batch,
+        )
+    else:
+        Llama2TokenAttentionForwards.token_attn(
+            query_states,
+            infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
+            infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
+            attn_output,
+            infer_state.block_loc,
+            infer_state.start_loc,
+            infer_state.seq_len,
+            # infer_state.cache_manager.past_key_values_length,
+            infer_state.max_len_in_batch,
+            infer_state.other_kv_index,
+        )
 
 
-class BloomInferenceForwards:
+class LlamaInferenceForwards:
     """
-    This class serves a micro library for bloom inference forwards.
-    We intend to replace the forward methods for BloomForCausalLM, BloomModel, BloomBlock, and BloomAttention,
-    as well as prepare_inputs_for_generation method for BloomForCausalLM.
-    For future improvement, we might want to skip replacing methods for BloomForCausalLM,
-    and call BloomModel.forward iteratively in TpInferEngine
+    This class holds forwards for llama inference.
+    We intend to replace the forward methods for LlamaModel, LlamaDecoderLayer, and LlamaAttention for LlamaForCausalLM.
     """
 
     @staticmethod
-    def bloom_for_causal_lm_forward(
-        self: BloomForCausalLM,
-        input_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
+    def llama_causal_lm_forward(
+        self: LlamaForCausalLM,
+        input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
-        head_mask: Optional[torch.Tensor] = None,
-        inputs_embeds: Optional[torch.Tensor] = None,
-        labels: Optional[torch.Tensor] = None,
-        use_cache: Optional[bool] = False,
-        output_attentions: Optional[bool] = False,
-        output_hidden_states: Optional[bool] = False,
-        return_dict: Optional[bool] = False,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[List[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        labels: Optional[torch.LongTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
         infer_state: BatchInferState = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
-        tp_group: Optional[dist.ProcessGroup] = None,
-        **deprecated_arguments,
     ):
         r"""
-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
-            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
-            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
+        Args:
+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
+
         """
         logger = logging.get_logger(__name__)
 
-        if deprecated_arguments.pop("position_ids", False) is not False:
-            # `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`
-            warnings.warn(
-                "`position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore"
-                " passing `position_ids`.",
-                FutureWarning,
-            )
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        # TODO(jianghai): left the recording kv-value tensors as () or None type, this feature may be added in the future.
         if output_attentions:
             logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
             output_attentions = False
         if output_hidden_states:
             logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
             output_hidden_states = False
 
-        # If is first stage and hidden_states is not None, go throught lm_head first
+        # If is first stage and after warmup, go throught lm_head first
         if stage_manager.is_first_stage() and hidden_states is not None:
             lm_logits = self.lm_head(hidden_states)
             return {"logits": lm_logits}
 
-        outputs = BloomInferenceForwards.bloom_model_forward(
-            self.transformer,
-            input_ids,
-            past_key_values=past_key_values,
+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+        outputs = LlamaInferenceForwards.llama_model_forward(
+            self.model,
+            input_ids=input_ids,
             attention_mask=attention_mask,
-            head_mask=head_mask,
+            position_ids=position_ids,
+            past_key_values=past_key_values,
             inputs_embeds=inputs_embeds,
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             infer_state=infer_state,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
             stage_index=stage_index,
-            tp_group=tp_group,
         )
 
         return outputs
 
     @staticmethod
-    def bloom_model_forward(
-        self: BloomModel,
-        input_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
+    def llama_model_forward(
+        self: LlamaModel,
+        input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
-        head_mask: Optional[torch.LongTensor] = None,
-        inputs_embeds: Optional[torch.LongTensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[List[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = False,
+        output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         infer_state: BatchInferState = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
-        tp_group: Optional[dist.ProcessGroup] = None,
-        **deprecated_arguments,
-    ) -> Union[Tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:
-        logger = logging.get_logger(__name__)
-
-        # add warnings here
-        if output_attentions:
-            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
-            output_attentions = False
-        if output_hidden_states:
-            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
-            output_hidden_states = False
-        if use_cache:
-            logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
-            use_cache = False
-
-        if deprecated_arguments.pop("position_ids", False) is not False:
-            # `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`
-            warnings.warn(
-                "`position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore"
-                " passing `position_ids`.",
-                FutureWarning,
-            )
-        if len(deprecated_arguments) > 0:
-            raise ValueError(f"Got unexpected arguments: {deprecated_arguments}")
+    ):
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        # Prepare head mask if needed
-        # 1.0 in head_mask indicate we keep the head
-        # attention_probs has shape batch_size x num_heads x N x N
-        # head_mask has shape n_layer x batch x num_heads x N x N
-        head_mask = self.get_head_mask(head_mask, self.config.n_layer)
-
-        # first stage
-        if stage_manager.is_first_stage():
-            # check inputs and inputs embeds
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        # retrieve input_ids and inputs_embeds
+        if stage_manager is None or stage_manager.is_first_stage():
             if input_ids is not None and inputs_embeds is not None:
-                raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
+                raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
             elif input_ids is not None:
                 batch_size, seq_length = input_ids.shape
             elif inputs_embeds is not None:
                 batch_size, seq_length, _ = inputs_embeds.shape
             else:
-                raise ValueError("You have to specify either input_ids or inputs_embeds")
-
+                raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
+            device = input_ids.device if input_ids is not None else inputs_embeds.device
             if inputs_embeds is None:
-                inputs_embeds = self.word_embeddings(input_ids)
-
-            hidden_states = self.word_embeddings_layernorm(inputs_embeds)
-        # other stage
+                inputs_embeds = self.embed_tokens(input_ids)
+            hidden_states = inputs_embeds
         else:
+            assert stage_manager is not None
+            assert hidden_states is not None, f"hidden_state should not be none in stage {stage_manager.stage}"
             input_shape = hidden_states.shape[:-1]
             batch_size, seq_length = input_shape
+            device = hidden_states.device
 
         if infer_state.is_context_stage:
             past_key_values_length = 0
         else:
             past_key_values_length = infer_state.max_len_in_batch - 1
 
-        if seq_length != 1:
-            # prefill stage
+        # NOTE: differentiate with prefill stage
+        #       block_loc require different value-assigning method for two different stage
+        if use_cache and seq_length != 1:
+            # NOTE assume prefill stage
+            # allocate memory block
             infer_state.is_context_stage = True  # set prefill stage, notify attention layer
             infer_state.context_mem_index = infer_state.cache_manager.alloc(infer_state.total_token_num)
-            BatchInferState.init_block_loc(
+            infer_state.init_block_loc(
                 infer_state.block_loc, infer_state.seq_len, seq_length, infer_state.context_mem_index
             )
         else:
             infer_state.is_context_stage = False
             alloc_mem = infer_state.cache_manager.alloc_contiguous(batch_size)
             if alloc_mem is not None:
                 infer_state.decode_is_contiguous = True
                 infer_state.decode_mem_index = alloc_mem[0]
                 infer_state.decode_mem_start = alloc_mem[1]
                 infer_state.decode_mem_end = alloc_mem[2]
                 infer_state.block_loc[:, infer_state.max_len_in_batch - 1] = infer_state.decode_mem_index
             else:
-                print(f" *** Encountered allocation non-contiguous")
-                print(f"    infer_state.max_len_in_batch : {infer_state.max_len_in_batch}")
                 infer_state.decode_is_contiguous = False
                 alloc_mem = infer_state.cache_manager.alloc(batch_size)
                 infer_state.decode_mem_index = alloc_mem
                 infer_state.block_loc[:, infer_state.max_len_in_batch - 1] = infer_state.decode_mem_index
 
-        if attention_mask is None:
-            attention_mask = torch.ones((batch_size, infer_state.max_len_in_batch), device=hidden_states.device)
+        if position_ids is None:
+            position_ids = torch.arange(
+                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
+            )
+            position_ids = position_ids.repeat(batch_size, 1)
+            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
         else:
-            attention_mask = attention_mask.to(hidden_states.device)
+            position_ids = position_ids.view(-1, seq_length).long()
 
-        # NOTE revise: we might want to store a single 1D alibi(length is #heads) in model,
-        #      or store to BatchInferState to prevent re-calculating
-        #      When we have multiple process group (e.g. dp together with tp), we need to pass the pg to here
-        tp_size = dist.get_world_size(tp_group) if tp_group is not None else 1
-        curr_tp_rank = dist.get_rank(tp_group) if tp_group is not None else 0
-        alibi = (
-            generate_alibi(self.num_heads * tp_size)
-            .contiguous()[curr_tp_rank * self.num_heads : (curr_tp_rank + 1) * self.num_heads]
-            .cuda()
-        )
-        causal_mask = self._prepare_attn_mask(
-            attention_mask,
-            input_shape=(batch_size, seq_length),
-            past_key_values_length=past_key_values_length,
+        if infer_state.is_context_stage:
+            infer_state.position_cos = torch.index_select(self._cos_cached, 0, position_ids.view(-1)).view(
+                position_ids.view(-1).shape[0], -1
+            )
+            infer_state.position_sin = torch.index_select(self._sin_cached, 0, position_ids.view(-1)).view(
+                position_ids.view(-1).shape[0], -1
+            )
+
+        else:
+            seq_len = infer_state.seq_len
+            infer_state.position_cos = torch.index_select(self._cos_cached, 0, seq_len - 1).view(seq_len.shape[0], -1)
+            infer_state.position_sin = torch.index_select(self._sin_cached, 0, seq_len - 1).view(seq_len.shape[0], -1)
+            infer_state.other_kv_index = infer_state.block_loc[0, infer_state.max_len_in_batch - 1].item()
+
+        # embed positions
+        if attention_mask is None:
+            attention_mask = torch.ones(
+                (batch_size, infer_state.max_len_in_batch), dtype=torch.bool, device=hidden_states.device
+            )
+
+        attention_mask = self._prepare_decoder_attention_mask(
+            attention_mask, (batch_size, seq_length), hidden_states, past_key_values_length
         )
 
+        # decoder layers
         infer_state.decode_layer_id = 0
 
         start_idx, end_idx = stage_index[0], stage_index[1]
         if past_key_values is None:
             past_key_values = tuple([None] * (end_idx - start_idx + 1))
 
         for idx, past_key_value in zip(range(start_idx, end_idx), past_key_values):
-            block = self.h[idx]
-            outputs = block(
+            decoder_layer = self.layers[idx]
+            # NOTE: modify here for passing args to decoder layer
+            layer_outputs = decoder_layer(
                 hidden_states,
-                layer_past=past_key_value,
-                attention_mask=causal_mask,
-                head_mask=head_mask[idx],
-                use_cache=use_cache,
+                attention_mask=attention_mask,
+                position_ids=position_ids,
+                past_key_value=past_key_value,
                 output_attentions=output_attentions,
-                alibi=alibi,
+                use_cache=use_cache,
                 infer_state=infer_state,
             )
-
             infer_state.decode_layer_id += 1
-            hidden_states = outputs[0]
+            hidden_states = layer_outputs[0]
 
         if stage_manager.is_last_stage() or stage_manager.num_stages == 1:
-            hidden_states = self.ln_f(hidden_states)
+            hidden_states = self.norm(hidden_states)
 
         # update indices
-        infer_state.start_loc = infer_state.start_loc + torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
+        # infer_state.block_loc[:, infer_state.max_len_in_batch-1] = infer_state.total_token_num + torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
+        infer_state.start_loc += torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
         infer_state.seq_len += 1
         infer_state.max_len_in_batch += 1
 
-        # always return dict for imediate stage
+        # if not return_dict:
+        #     return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+
+        # return BaseModelOutputWithPast(
+        #     last_hidden_state=hidden_states,
+        #     past_key_values=next_cache,
+        #     hidden_states=all_hidden_states,
+        #     attentions=all_self_attns,
+        # )
         return {"hidden_states": hidden_states}
 
     @staticmethod
-    def bloom_block_forward(
-        self: BloomBlock,
+    def llama_decoder_layer_forward(
+        self: LlamaDecoderLayer,
         hidden_states: torch.Tensor,
-        alibi: torch.Tensor,
-        attention_mask: torch.Tensor,
-        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
-        head_mask: Optional[torch.Tensor] = None,
-        use_cache: bool = False,
-        output_attentions: bool = False,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        output_attentions: Optional[bool] = False,
+        use_cache: Optional[bool] = False,
         infer_state: Optional[BatchInferState] = None,
-    ):
-        # hidden_states: [batch_size, seq_length, hidden_size]
-
-        # Layer norm at the beginning of the transformer layer.
-        layernorm_output = self.input_layernorm(hidden_states)
-
-        # Layer norm post the self attention.
-        if self.apply_residual_connection_post_layernorm:
-            residual = layernorm_output
-        else:
-            residual = hidden_states
+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
+        residual = hidden_states
 
-        # Self attention.
-        attn_outputs = self.self_attention(
-            layernorm_output,
-            residual,
-            layer_past=layer_past,
+        hidden_states = self.input_layernorm(hidden_states)
+        # Self Attention
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
             attention_mask=attention_mask,
-            alibi=alibi,
-            head_mask=head_mask,
-            use_cache=use_cache,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
             output_attentions=output_attentions,
+            use_cache=use_cache,
             infer_state=infer_state,
         )
 
-        attention_output = attn_outputs[0]
-
-        outputs = attn_outputs[1:]
+        hidden_states = residual + hidden_states
 
-        layernorm_output = self.post_attention_layernorm(attention_output)
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = self.post_attention_layernorm(hidden_states)
+        hidden_states = self.mlp(hidden_states)
+        hidden_states = residual + hidden_states
 
-        # Get residual
-        if self.apply_residual_connection_post_layernorm:
-            residual = layernorm_output
-        else:
-            residual = attention_output
+        outputs = (hidden_states,)
 
-        # MLP.
-        output = self.mlp(layernorm_output, residual)
+        if output_attentions:
+            outputs += (self_attn_weights,)
 
         if use_cache:
-            outputs = (output,) + outputs
-        else:
-            outputs = (output,) + outputs[1:]
+            outputs += (present_key_value,)
 
-        return outputs  # hidden_states, present, attentions
+        return outputs
 
     @staticmethod
-    def bloom_attention_forward(
-        self: BloomAttention,
+    def llama_flash_attn_kvcache_forward(
+        self: LlamaAttention,
         hidden_states: torch.Tensor,
-        residual: torch.Tensor,
-        alibi: torch.Tensor,
-        attention_mask: torch.Tensor,
-        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
-        head_mask: Optional[torch.Tensor] = None,
-        use_cache: bool = False,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_value: Optional[Tuple[torch.Tensor]] = None,
         output_attentions: bool = False,
+        use_cache: bool = False,
         infer_state: Optional[BatchInferState] = None,
-    ):
-        fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+        assert use_cache is True, "use_cache should be set to True using this llama attention"
 
-        # 3 x [batch_size, seq_length, num_heads, head_dim]
-        (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
-        batch_size, q_length, H, D_HEAD = query_layer.shape
-        k = key_layer.reshape(-1, H, D_HEAD)  # batch_size * q_length, H, D_HEAD, q_lenth == 1
-        v = value_layer.reshape(-1, H, D_HEAD)  # batch_size * q_length, H, D_HEAD, q_lenth == 1
+        bsz, q_len, _ = hidden_states.size()
 
-        mem_manager = infer_state.cache_manager
-        layer_id = infer_state.decode_layer_id
+        # NOTE might think about better way to handle transposed k and v
+        # key_states            [bs, seq_len, num_heads, head_dim/embed_size_per_head]
+        # key_states_transposed [bs, num_heads, seq_len, head_dim/embed_size_per_head]
 
-        if infer_state.is_context_stage:
-            # context process
-            max_input_len = q_length
-            b_start_loc = infer_state.start_loc
-            b_seq_len = infer_state.seq_len[:batch_size]
-            q = query_layer.reshape(-1, H, D_HEAD)
+        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim)
+        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim)
+        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim)
 
-            copy_kv_cache_to_dest(k, infer_state.context_mem_index, mem_manager.key_buffer[layer_id])
-            copy_kv_cache_to_dest(v, infer_state.context_mem_index, mem_manager.value_buffer[layer_id])
+        # NOTE might want to revise
+        #   need some way to record the length of past key values cache
+        #   since we won't return past_key_value_cache right now
 
-            # output = self.output[:batch_size*q_length, :, :]
-            output = torch.empty_like(q)
+        cos, sin = infer_state.position_cos, infer_state.position_sin
 
-            if HAS_LIGHTLLM_KERNEL:
-                lightllm_bloom_context_attention_fwd(q, k, v, output, alibi, b_start_loc, b_seq_len, max_input_len)
-            else:
-                bloom_context_attn_fwd(q, k, v, output, b_start_loc, b_seq_len, max_input_len, alibi)
+        llama_rotary_embedding_fwd(query_states.view(-1, self.num_heads, self.head_dim), cos, sin)
+        llama_rotary_embedding_fwd(key_states.view(-1, self.num_key_value_heads, self.head_dim), cos, sin)
 
-            context_layer = output.view(batch_size, q_length, H * D_HEAD)
-        else:
-            # query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, q_length, self.head_dim)
-            # need shape: batch_size, H, D_HEAD (q_length == 1), input q shape : (batch_size, q_length(1), H, D_HEAD)
-            assert q_length == 1, "for non-context process, we only support q_length == 1"
-            q = query_layer.reshape(-1, H, D_HEAD)
+        query_states = query_states.reshape(-1, self.num_heads, self.head_dim)
+        key_states = key_states.reshape(-1, self.num_key_value_heads, self.head_dim)
+        value_states = value_states.reshape(-1, self.num_key_value_heads, self.head_dim)
+
+        if infer_state.is_context_stage:
+            # first token generation
+            # copy key and value calculated in current step to memory manager
+            copy_kv_to_mem_cache(
+                infer_state.decode_layer_id,
+                key_states,
+                value_states,
+                infer_state.context_mem_index,
+                infer_state.cache_manager,
+            )
+            attn_output = torch.empty_like(query_states)
 
+            llama_triton_context_attention(
+                query_states,
+                key_states,
+                value_states,
+                attn_output,
+                infer_state,
+                num_key_value_groups=self.num_key_value_groups,
+            )
+        else:
             if infer_state.decode_is_contiguous:
                 # if decode is contiguous, then we copy to key cache and value cache in cache manager directly
-                cache_k = infer_state.cache_manager.key_buffer[layer_id][
+                cache_k = infer_state.cache_manager.key_buffer[infer_state.decode_layer_id][
                     infer_state.decode_mem_start : infer_state.decode_mem_end, :, :
                 ]
-                cache_v = infer_state.cache_manager.value_buffer[layer_id][
+                cache_v = infer_state.cache_manager.value_buffer[infer_state.decode_layer_id][
                     infer_state.decode_mem_start : infer_state.decode_mem_end, :, :
                 ]
-                cache_k.copy_(k)
-                cache_v.copy_(v)
+                cache_k.copy_(key_states)
+                cache_v.copy_(value_states)
             else:
                 # if decode is not contiguous, use triton kernel to copy key and value cache
-                # k, v shape: [batch_size, num_heads, head_dim/embed_size_per_head]
-                copy_kv_cache_to_dest(k, infer_state.decode_mem_index, mem_manager.key_buffer[layer_id])
-                copy_kv_cache_to_dest(v, infer_state.decode_mem_index, mem_manager.value_buffer[layer_id])
-
-            b_start_loc = infer_state.start_loc
-            b_loc = infer_state.block_loc
-            b_seq_len = infer_state.seq_len
-            output = torch.empty_like(q)
-            token_attention_fwd(
-                q,
-                mem_manager.key_buffer[layer_id],
-                mem_manager.value_buffer[layer_id],
-                output,
-                b_loc,
-                b_start_loc,
-                b_seq_len,
-                infer_state.max_len_in_batch,
-                alibi,
-            )
-
-            context_layer = output.view(batch_size, q_length, H * D_HEAD)
+                # k, v shape: [batch_size, num_heads, head_dim/embed_size_per_head
+                copy_kv_to_mem_cache(
+                    infer_state.decode_layer_id,
+                    key_states,
+                    value_states,
+                    infer_state.decode_mem_index,
+                    infer_state.cache_manager,
+                )
 
-        # NOTE: always set present as none for now, instead of returning past key value to the next decoding,
-        #       we create the past key value pair from the cache manager
-        present = None
-
-        # aggregate results across tp ranks. See here: https://github.com/pytorch/pytorch/issues/76232
-        if self.pretraining_tp > 1 and self.slow_but_exact:
-            slices = self.hidden_size / self.pretraining_tp
-            output_tensor = torch.zeros_like(context_layer)
-            for i in range(self.pretraining_tp):
-                output_tensor = output_tensor + F.linear(
-                    context_layer[:, :, int(i * slices) : int((i + 1) * slices)],
-                    self.dense.weight[:, int(i * slices) : int((i + 1) * slices)],
+            if HAS_LIGHTLLM_KERNEL:
+                attn_output = torch.empty_like(query_states)
+                llama_triton_token_attention(
+                    query_states, attn_output, infer_state, num_key_value_groups=self.num_key_value_groups
+                )
+            else:
+                self.num_heads // self.num_key_value_heads
+                cache_k = infer_state.cache_manager.key_buffer[infer_state.decode_layer_id]
+                cache_v = infer_state.cache_manager.value_buffer[infer_state.decode_layer_id]
+
+                query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim)
+                copy_cache_k = cache_k.view(bsz, -1, self.num_key_value_heads, self.head_dim)
+                copy_cache_v = cache_v.view(bsz, -1, self.num_key_value_heads, self.head_dim)
+
+                attn_output = flash_attn_with_kvcache(
+                    q=query_states,
+                    k_cache=copy_cache_k,
+                    v_cache=copy_cache_v,
+                    softmax_scale=1 / math.sqrt(self.head_dim),
+                    causal=True,
                 )
-        else:
-            output_tensor = self.dense(context_layer)
 
-        # dropout is not required here during inference
-        output_tensor = residual + output_tensor
+        attn_output = attn_output.view(bsz, q_len, self.hidden_size)
 
-        outputs = (output_tensor, present)
-        assert output_attentions is False, "we do not support output_attentions at this time"
+        attn_output = self.o_proj(attn_output)
 
-        return outputs
+        # return past_key_value as None
+        return attn_output, None, None
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/chatglm2.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-from typing import List, Optional, Tuple
+import os
+from typing import Optional, Tuple
 
 import torch
-from transformers.utils import logging
+from torch.nn import CrossEntropyLoss
+from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
 
-from colossalai.inference.kv_cache import BatchInferState
+from colossalai.inference.tensor_parallel.batch_infer_state import BatchInferState
 from colossalai.kernel.triton.token_attention_kernel import Llama2TokenAttentionForwards
-from colossalai.pipeline.stage_manager import PipelineStageManager
-from colossalai.shardformer import ShardConfig
 from colossalai.shardformer.modeling.chatglm2_6b.modeling_chatglm import (
     ChatGLMForConditionalGeneration,
     ChatGLMModel,
     GLMBlock,
     GLMTransformer,
     SelfAttention,
     split_tensor_along_last_dim,
@@ -26,14 +26,49 @@
 
     HAS_LIGHTLLM_KERNEL = True
 except:
     print("please install lightllm from source to run inference: https://github.com/ModelTC/lightllm")
     HAS_LIGHTLLM_KERNEL = False
 
 
+# This func is same as Llama model init_to_get_rotary, we should move them into _utils.py
+def _init_to_get_rotary(self, base=10000):
+    self.config.head_dim_ = self.config.hidden_size // self.config.num_attention_heads
+    if not hasattr(self.config, "rope_scaling"):
+        rope_scaling_factor = 1.0
+    else:
+        rope_scaling_factor = self.config.rope_scaling.factor if self.config.rope_scaling is not None else 1.0
+    if hasattr(self.config, "max_sequence_length"):
+        max_seq_len = self.config.max_sequence_length
+    elif hasattr(self.config, "max_position_embeddings"):
+        max_seq_len = self.config.max_position_embeddings * rope_scaling_factor
+    else:
+        max_seq_len = 2048 * rope_scaling_factor
+    base = float(base)
+
+    # NTK  ref: https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/
+    try:
+        ntk_alpha = float(os.environ.get("INFER_NTK_ALPHA", 1))
+        assert ntk_alpha >= 1
+        if ntk_alpha > 1:
+            print(f"Note: NTK enabled, alpha set to {ntk_alpha}")
+        max_seq_len *= ntk_alpha
+        base = base * (ntk_alpha ** (self.head_dim_ / (self.head_dim_ - 2)))  # Base change formula
+    except:
+        pass
+    n_elem = self.config.head_dim_ // 2
+    inv_freq = 1.0 / (base ** (torch.arange(0, n_elem, 2, device="cpu", dtype=torch.float32) / n_elem))
+    t = torch.arange(max_seq_len + 1024 * 64, device="cpu", dtype=torch.float32) / rope_scaling_factor
+    freqs = torch.outer(t, inv_freq)
+
+    self._cos_cached = torch.cos(freqs).to(torch.float16).cuda()
+    self._sin_cached = torch.sin(freqs).to(torch.float16).cuda()
+    return
+
+
 def get_masks(self, input_ids, past_length, padding_mask=None):
     batch_size, seq_length = input_ids.shape
     full_attention_mask = torch.ones(batch_size, seq_length, seq_length, device=input_ids.device)
     full_attention_mask.tril_()
     if past_length:
         full_attention_mask = torch.cat(
             (
@@ -48,19 +83,14 @@
     if not past_length and padding_mask is not None:
         full_attention_mask -= padding_mask.unsqueeze(-1) - 1
     full_attention_mask = (full_attention_mask < 0.5).bool()
     full_attention_mask.unsqueeze_(1)
     return full_attention_mask
 
 
-def get_position_ids(batch_size, seq_length, device):
-    position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).repeat(batch_size, 1)
-    return position_ids
-
-
 class ChatGLM2InferenceForwards:
     """
     This class holds forwards for Chatglm2 inference.
     We intend to replace the forward methods for ChatGLMModel, ChatGLMEecoderLayer, and ChatGLMAttention.
     """
 
     @staticmethod
@@ -68,111 +98,42 @@
         self: ChatGLMForConditionalGeneration,
         input_ids: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.Tensor] = None,
         attention_mask: Optional[torch.Tensor] = None,
         past_key_values: Optional[Tuple[torch.FloatTensor]] = None,
         inputs_embeds: Optional[torch.Tensor] = None,
         labels: Optional[torch.Tensor] = None,
-        use_cache: Optional[bool] = True,
+        use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         return_last_logit: Optional[bool] = False,
-        infer_state: Optional[BatchInferState] = None,
-        stage_manager: Optional[PipelineStageManager] = None,
-        hidden_states: Optional[torch.FloatTensor] = None,
-        stage_index: Optional[List[int]] = None,
-        shard_config: ShardConfig = None,
     ):
-        logger = logging.get_logger(__name__)
-
-        if output_attentions:
-            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
-            output_attentions = False
-        if output_hidden_states:
-            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
-            output_hidden_states = False
-
-        # If is first stage and hidden_states is not None, go throught lm_head first
-        if stage_manager.is_first_stage() and hidden_states is not None:
-            if return_last_logit:
-                hidden_states = hidden_states[-1:]
-            lm_logits = self.transformer.output_layer(hidden_states)
-            lm_logits = lm_logits.transpose(0, 1).contiguous()
-            return {"logits": lm_logits}
-
-        outputs = self.transformer(
-            input_ids=input_ids,
-            position_ids=position_ids,
-            attention_mask=attention_mask,
-            past_key_values=past_key_values,
-            inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
-            infer_state=infer_state,
-            stage_manager=stage_manager,
-            hidden_states=hidden_states,
-            stage_index=stage_index,
-            shard_config=shard_config,
-        )
-
-        return outputs
-
-    @staticmethod
-    def chatglm_model_forward(
-        self: ChatGLMModel,
-        input_ids: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.Tensor] = None,
-        attention_mask: Optional[torch.BoolTensor] = None,
-        full_attention_mask: Optional[torch.BoolTensor] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
-        inputs_embeds: Optional[torch.Tensor] = None,
-        use_cache: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-        infer_state: BatchInferState = None,
-        stage_manager: Optional[PipelineStageManager] = None,
-        hidden_states: Optional[torch.FloatTensor] = None,
-        stage_index: Optional[List[int]] = None,
-        shard_config: ShardConfig = None,
-    ):
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-        )
-
-        if stage_manager.is_first_stage():
-            if input_ids is not None and inputs_embeds is not None:
-                raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
-            elif input_ids is not None:
-                batch_size, seq_length = input_ids.shape
-            elif inputs_embeds is not None:
-                batch_size, seq_length, _ = inputs_embeds.shape
-            else:
-                raise ValueError("You have to specify either input_ids or inputs_embeds")
-            if inputs_embeds is None:
-                inputs_embeds = self.embedding(input_ids)
-            if position_ids is None:
-                position_ids = get_position_ids(batch_size, seq_length, input_ids.device)
-            hidden_states = inputs_embeds
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+        infer_state = self.infer_state
+
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
+        elif input_ids is not None:
+            batch_size, seq_length = input_ids.shape
+        elif inputs_embeds is not None:
+            batch_size, seq_length, _ = inputs_embeds.shape
         else:
-            assert hidden_states is not None, "hidden_states should not be None in non-first stage"
-            seq_length, batch_size, _ = hidden_states.shape
-            if position_ids is None:
-                position_ids = get_position_ids(batch_size, seq_length, hidden_states.device)
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
 
         if infer_state.is_context_stage:
             past_key_values_length = 0
         else:
             past_key_values_length = infer_state.max_len_in_batch - 1
 
         seq_length_with_past = seq_length + past_key_values_length
 
         # prefill stage at first
-        if seq_length != 1:
+        if use_cache and seq_length != 1:
             infer_state.is_context_stage = True
             infer_state.context_mem_index = infer_state.cache_manager.alloc(infer_state.total_token_num)
             infer_state.init_block_loc(
                 infer_state.block_loc, infer_state.seq_len, seq_length, infer_state.context_mem_index
             )
         else:
             infer_state.is_context_stage = False
@@ -187,14 +148,16 @@
                 print(f" *** Encountered allocation non-contiguous")
                 print(
                     f"    infer_state.cache_manager.past_key_values_length: {infer_state.cache_manager.past_key_values_length}"
                 )
                 infer_state.decode_is_contiguous = False
                 alloc_mem = infer_state.cache_manager.alloc(batch_size)
                 infer_state.decode_mem_index = alloc_mem
+                # infer_state.decode_key_buffer = torch.empty((batch_size, self.tp_head_num_, self.head_dim_), dtype=torch.float16, device="cuda")
+                # infer_state.decode_value_buffer = torch.empty((batch_size, self.tp_head_num_, self.head_dim_), dtype=torch.float16, device="cuda")
                 infer_state.block_loc[:, seq_length_with_past - 1] = infer_state.decode_mem_index
 
         # related to rotary embedding
         if infer_state.is_context_stage:
             infer_state.position_cos = torch.index_select(self._cos_cached, 0, position_ids.view(-1)).view(
                 position_ids.view(-1).shape[0], -1
             )
@@ -203,14 +166,82 @@
             )
         else:
             seq_len = infer_state.seq_len
             infer_state.position_cos = torch.index_select(self._cos_cached, 0, seq_len - 1).view(seq_len.shape[0], -1)
             infer_state.position_sin = torch.index_select(self._sin_cached, 0, seq_len - 1).view(seq_len.shape[0], -1)
             infer_state.other_kv_index = infer_state.block_loc[0, infer_state.max_len_in_batch - 1].item()
 
+        transformer_outputs = self.transformer(
+            input_ids=input_ids,
+            position_ids=position_ids,
+            attention_mask=attention_mask,
+            past_key_values=past_key_values,
+            inputs_embeds=inputs_embeds,
+            use_cache=use_cache,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+            infer_state=infer_state,
+        )
+
+        hidden_states = transformer_outputs[0]
+        if return_last_logit:
+            hidden_states = hidden_states[-1:]
+        lm_logits = self.transformer.output_layer(hidden_states)
+        lm_logits = lm_logits.transpose(0, 1).contiguous()
+
+        loss = None
+        if labels is not None:
+            lm_logits = lm_logits.to(torch.float32)
+
+            # Shift so that tokens < n predict n
+            shift_logits = lm_logits[..., :-1, :].contiguous()
+            shift_labels = labels[..., 1:].contiguous()
+            # Flatten the tokens
+            loss_fct = CrossEntropyLoss(ignore_index=-100)
+            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
+
+            lm_logits = lm_logits.to(hidden_states.dtype)
+            loss = loss.to(hidden_states.dtype)
+
+        if not return_dict:
+            output = (lm_logits,) + transformer_outputs[1:]
+            return ((loss,) + output) if loss is not None else output
+
+        return CausalLMOutputWithPast(
+            loss=loss,
+            logits=lm_logits,
+            past_key_values=transformer_outputs.past_key_values,
+            hidden_states=transformer_outputs.hidden_states,
+            attentions=transformer_outputs.attentions,
+        )
+
+    @staticmethod
+    def chatglm_model_forward(
+        self: ChatGLMModel,
+        input_ids,
+        position_ids: Optional[torch.Tensor] = None,
+        attention_mask: Optional[torch.BoolTensor] = None,
+        full_attention_mask: Optional[torch.BoolTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        use_cache: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        infer_state: BatchInferState = None,
+    ):
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+        batch_size, seq_length = input_ids.shape
+
+        if inputs_embeds is None:
+            inputs_embeds = self.embedding(input_ids)
+
         if self.pre_seq_len is not None:
             if past_key_values is None:
                 past_key_values = self.get_prompt(
                     batch_size=batch_size,
                     device=input_ids.device,
                     dtype=inputs_embeds.dtype,
                 )
@@ -225,73 +256,93 @@
         if full_attention_mask is None:
             if (attention_mask is not None and not attention_mask.all()) or (past_key_values and seq_length != 1):
                 full_attention_mask = get_masks(
                     self, input_ids, infer_state.cache_manager.past_key_values_length, padding_mask=attention_mask
                 )
 
         # Run encoder.
-        hidden_states = self.encoder(
-            hidden_states,
+        hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(
+            inputs_embeds,
             full_attention_mask,
             kv_caches=past_key_values,
             use_cache=use_cache,
             output_hidden_states=output_hidden_states,
             infer_state=infer_state,
-            stage_manager=stage_manager,
-            stage_index=stage_index,
-            shard_config=shard_config,
         )
 
         # update indices
+        # infer_state.block_loc[:, infer_state.max_len_in_batch-1] = infer_state.total_token_num + torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
         infer_state.start_loc = infer_state.start_loc + torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
         infer_state.seq_len += 1
         infer_state.max_len_in_batch += 1
 
-        return {"hidden_states": hidden_states}
+        if not return_dict:
+            return tuple(
+                v
+                for v in [
+                    hidden_states,
+                    presents,
+                    all_hidden_states,
+                    all_self_attentions,
+                ]
+                if v is not None
+            )
+
+        return BaseModelOutputWithPast(
+            last_hidden_state=hidden_states,
+            past_key_values=presents,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attentions,
+        )
 
     @staticmethod
     def chatglm_encoder_forward(
         self: GLMTransformer,
         hidden_states,
         attention_mask,
         kv_caches=None,
         use_cache: Optional[bool] = True,
         output_hidden_states: Optional[bool] = False,
         infer_state: Optional[BatchInferState] = None,
-        stage_manager: Optional[PipelineStageManager] = None,
-        stage_index: Optional[List[int]] = None,
-        shard_config: ShardConfig = None,
     ):
         hidden_states = hidden_states.transpose(0, 1).contiguous()
+        if not kv_caches:
+            kv_caches = [None for _ in range(self.num_layers)]
+        presents = () if use_cache else None
+        all_self_attentions = None
+        all_hidden_states = () if output_hidden_states else None
 
         infer_state.decode_layer_id = 0
-        start_idx, end_idx = stage_index[0], stage_index[1]
-        if kv_caches is None:
-            kv_caches = tuple([None] * (end_idx - start_idx + 1))
+        for index in range(self.num_layers):
+            layer = self.layers[index]
 
-        for idx, kv_cache in zip(range(start_idx, end_idx), kv_caches):
-            layer = self.layers[idx]
             layer_ret = layer(
                 hidden_states,
                 attention_mask,
-                kv_cache=kv_cache,
+                kv_cache=kv_caches[index],
                 use_cache=use_cache,
                 infer_state=infer_state,
             )
+
             infer_state.decode_layer_id += 1
 
-            hidden_states, _ = layer_ret
+            hidden_states, kv_cache = layer_ret
+            if use_cache:
+                presents = presents + (kv_cache,)
 
+        if output_hidden_states:
+            all_hidden_states = all_hidden_states + (hidden_states,)
+
+        # Final layer norm.
         hidden_states = hidden_states.transpose(0, 1).contiguous()
 
-        if self.post_layer_norm and (stage_manager.is_last_stage() or stage_manager.num_stages == 1):
-            # Final layer norm.
+        if self.post_layer_norm:
             hidden_states = self.final_layernorm(hidden_states)
 
-        return hidden_states
+        return hidden_states, presents, all_hidden_states, all_self_attentions
 
     @staticmethod
     def chatglm_glmblock_forward(
         self: GLMBlock,
         hidden_states,
         attention_mask,
         kv_cache=None,
@@ -480,13 +531,15 @@
                 infer_state.block_loc,
                 infer_state.start_loc,
                 infer_state.seq_len,
                 infer_state.max_len_in_batch,
                 infer_state.other_kv_index,
             )
 
+            # print('after attention',torch.isnan(attn_output).any())
+
         # =================
         # Output:[b,sq, h]
         # =================
         output = self.dense(attn_output).reshape(batch_size, -1, hidden_size)
 
         return output, kv_cache
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/llama.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/modeling/llama.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,47 +1,32 @@
-# This code is adapted from huggingface transformers: https://github.com/huggingface/transformers/blob/v4.34.1/src/transformers/models/llama/modeling_llama.py
 import math
 from typing import List, Optional, Tuple
 
 import torch
-from transformers.models.llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaForCausalLM, LlamaModel
-from transformers.utils import logging
+from transformers.modeling_outputs import BaseModelOutputWithPast
+from transformers.models.llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaModel
 
-from colossalai.inference.kv_cache.batch_infer_state import BatchInferState
+from colossalai.inference.tensor_parallel.batch_infer_state import BatchInferState
 from colossalai.kernel.triton import llama_context_attn_fwd, token_attention_fwd
 from colossalai.kernel.triton.token_attention_kernel import Llama2TokenAttentionForwards
-from colossalai.pipeline.stage_manager import PipelineStageManager
 
 from ._utils import copy_kv_to_mem_cache
 
 try:
-    from lightllm.models.llama2.triton_kernel.context_flashattention_nopad import (
-        context_attention_fwd as lightllm_llama2_context_attention_fwd,
-    )
     from lightllm.models.llama.triton_kernel.context_flashattention_nopad import (
-        context_attention_fwd as lightllm_context_attention_fwd,
+        context_attention_fwd as lightllm_llama_context_attention_fwd,
     )
     from lightllm.models.llama.triton_kernel.rotary_emb import rotary_emb_fwd as llama_rotary_embedding_fwd
 
     HAS_LIGHTLLM_KERNEL = True
 except:
     print("please install lightllm from source to run inference: https://github.com/ModelTC/lightllm")
     HAS_LIGHTLLM_KERNEL = False
 
 try:
-    from colossalai.kernel.triton.flash_decoding import token_flash_decoding
-
-    HAS_TRITON_FLASH_DECODING_KERNEL = True
-except:
-    print(
-        "no triton flash decoding support, please install lightllm from https://github.com/ModelTC/lightllm/blob/ece7b43f8a6dfa74027adc77c2c176cff28c76c8"
-    )
-    HAS_TRITON_FLASH_DECODING_KERNEL = False
-
-try:
     from flash_attn import flash_attn_with_kvcache
 
     HAS_FLASH_KERNEL = True
 except:
     HAS_FLASH_KERNEL = False
     print("please install flash attentiom from https://github.com/Dao-AILab/flash-attention")
 
@@ -64,196 +49,102 @@
     k_embed = (k * cos) + (rotate_half(k) * sin)
     return q_embed, k_embed
 
 
 def llama_triton_context_attention(
     query_states, key_states, value_states, attn_output, infer_state, num_key_value_groups=1
 ):
-    if num_key_value_groups == 1:
-        if HAS_LIGHTLLM_KERNEL is False:
-            llama_context_attn_fwd(
-                query_states,
-                key_states,
-                value_states,
-                attn_output,
-                infer_state.start_loc,
-                infer_state.seq_len,
-                infer_state.max_len_in_batch,
-            )
-        else:
-            lightllm_context_attention_fwd(
-                query_states,
-                key_states,
-                value_states,
-                attn_output,
-                infer_state.start_loc,
-                infer_state.seq_len,
-                infer_state.max_len_in_batch,
-            )
+    # if num_key_value_groups == 1:
+    if HAS_LIGHTLLM_KERNEL is False:
+        llama_context_attn_fwd(
+            query_states,
+            key_states,
+            value_states,
+            attn_output,
+            infer_state.start_loc,
+            infer_state.seq_len,
+            # infer_state.cache_manager.past_key_values_length,
+            infer_state.max_len_in_batch,
+        )
     else:
-        assert HAS_LIGHTLLM_KERNEL is True, "You have to install lightllm kernels to run llama2 model"
-        lightllm_llama2_context_attention_fwd(
+        lightllm_llama_context_attention_fwd(
             query_states,
             key_states,
             value_states,
             attn_output,
             infer_state.start_loc,
             infer_state.seq_len,
+            # infer_state.cache_manager.past_key_values_length,
             infer_state.max_len_in_batch,
         )
 
 
-def llama_triton_token_attention(
-    query_states, attn_output, infer_state, num_key_value_groups=1, q_head_num=-1, head_dim=-1
-):
-    if HAS_TRITON_FLASH_DECODING_KERNEL and q_head_num != -1 and head_dim != -1:
-        token_flash_decoding(
-            q=query_states,
-            o_tensor=attn_output,
-            infer_state=infer_state,
-            q_head_num=q_head_num,
-            head_dim=head_dim,
-            cache_k=infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
-            cache_v=infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
-        )
-        return
-
+def llama_triton_token_attention(query_states, attn_output, infer_state, num_key_value_groups=1):
+    assert HAS_LIGHTLLM_KERNEL is True, "You have to install lightllm kernel to run token attention for llama models"
     if num_key_value_groups == 1:
         token_attention_fwd(
             query_states,
             infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
             infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
             attn_output,
             infer_state.block_loc,
             infer_state.start_loc,
             infer_state.seq_len,
+            # infer_state.cache_manager.past_key_values_length,
             infer_state.max_len_in_batch,
         )
+
     else:
         Llama2TokenAttentionForwards.token_attn(
             query_states,
             infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
             infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
             attn_output,
             infer_state.block_loc,
             infer_state.start_loc,
             infer_state.seq_len,
+            # infer_state.cache_manager.past_key_values_length,
             infer_state.max_len_in_batch,
             infer_state.other_kv_index,
         )
 
 
 class LlamaInferenceForwards:
     """
     This class holds forwards for llama inference.
     We intend to replace the forward methods for LlamaModel, LlamaDecoderLayer, and LlamaAttention for LlamaForCausalLM.
     """
 
     @staticmethod
-    def llama_causal_lm_forward(
-        self: LlamaForCausalLM,
-        input_ids: torch.LongTensor = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[List[torch.FloatTensor]] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
-        labels: Optional[torch.LongTensor] = None,
-        use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-        infer_state: BatchInferState = None,
-        stage_manager: Optional[PipelineStageManager] = None,
-        hidden_states: Optional[torch.FloatTensor] = None,
-        stage_index: Optional[List[int]] = None,
-    ):
-        r"""
-        Args:
-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
-
-        """
-        logger = logging.get_logger(__name__)
-
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-        if output_attentions:
-            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
-            output_attentions = False
-        if output_hidden_states:
-            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
-            output_hidden_states = False
-
-        # If is first stage and hidden_states is None, go throught lm_head first
-        if stage_manager.is_first_stage() and hidden_states is not None:
-            lm_logits = self.lm_head(hidden_states)
-            return {"logits": lm_logits}
-
-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-        outputs = LlamaInferenceForwards.llama_model_forward(
-            self.model,
-            input_ids=input_ids,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_values=past_key_values,
-            inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
-            output_attentions=output_attentions,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
-            infer_state=infer_state,
-            stage_manager=stage_manager,
-            hidden_states=hidden_states,
-            stage_index=stage_index,
-        )
-
-        return outputs
-
-    @staticmethod
     def llama_model_forward(
         self: LlamaModel,
         input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
-        infer_state: BatchInferState = None,
-        stage_manager: Optional[PipelineStageManager] = None,
-        hidden_states: Optional[torch.FloatTensor] = None,
-        stage_index: Optional[List[int]] = None,
     ):
+        infer_state = self.infer_state
+
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
         use_cache = use_cache if use_cache is not None else self.config.use_cache
         # retrieve input_ids and inputs_embeds
-        if stage_manager is None or stage_manager.is_first_stage():
-            if input_ids is not None and inputs_embeds is not None:
-                raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
-            elif input_ids is not None:
-                batch_size, seq_length = input_ids.shape
-            elif inputs_embeds is not None:
-                batch_size, seq_length, _ = inputs_embeds.shape
-            else:
-                raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
-            device = input_ids.device if input_ids is not None else inputs_embeds.device
-            if inputs_embeds is None:
-                inputs_embeds = self.embed_tokens(input_ids)
-            hidden_states = inputs_embeds
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
+        elif input_ids is not None:
+            batch_size, seq_length = input_ids.shape
+        elif inputs_embeds is not None:
+            batch_size, seq_length, _ = inputs_embeds.shape
         else:
-            assert stage_manager is not None
-            assert hidden_states is not None, f"hidden_state should not be none in stage {stage_manager.stage}"
-            input_shape = hidden_states.shape[:-1]
-            batch_size, seq_length = input_shape
-            device = hidden_states.device
+            raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
 
         if infer_state.is_context_stage:
             past_key_values_length = 0
         else:
             past_key_values_length = infer_state.max_len_in_batch - 1
 
         # NOTE: differentiate with prefill stage
@@ -272,20 +163,25 @@
             if alloc_mem is not None:
                 infer_state.decode_is_contiguous = True
                 infer_state.decode_mem_index = alloc_mem[0]
                 infer_state.decode_mem_start = alloc_mem[1]
                 infer_state.decode_mem_end = alloc_mem[2]
                 infer_state.block_loc[:, infer_state.max_len_in_batch - 1] = infer_state.decode_mem_index
             else:
+                print(f" *** Encountered allocation non-contiguous")
+                print(f"    infer_state.max_len_in_batch : {infer_state.max_len_in_batch}")
                 infer_state.decode_is_contiguous = False
                 alloc_mem = infer_state.cache_manager.alloc(batch_size)
                 infer_state.decode_mem_index = alloc_mem
+                # infer_state.decode_key_buffer = torch.empty((batch_size, self.tp_head_num_, self.head_dim_), dtype=torch.float16, device="cuda")
+                # infer_state.decode_value_buffer = torch.empty((batch_size, self.tp_head_num_, self.head_dim_), dtype=torch.float16, device="cuda")
                 infer_state.block_loc[:, infer_state.max_len_in_batch - 1] = infer_state.decode_mem_index
 
         if position_ids is None:
+            device = input_ids.device if input_ids is not None else inputs_embeds.device
             position_ids = torch.arange(
                 past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
             )
             position_ids = position_ids.repeat(batch_size, 1)
             position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
         else:
             position_ids = position_ids.view(-1, seq_length).long()
@@ -300,56 +196,71 @@
 
         else:
             seq_len = infer_state.seq_len
             infer_state.position_cos = torch.index_select(self._cos_cached, 0, seq_len - 1).view(seq_len.shape[0], -1)
             infer_state.position_sin = torch.index_select(self._sin_cached, 0, seq_len - 1).view(seq_len.shape[0], -1)
             infer_state.other_kv_index = infer_state.block_loc[0, infer_state.max_len_in_batch - 1].item()
 
+        if inputs_embeds is None:
+            inputs_embeds = self.embed_tokens(input_ids)
+
         # embed positions
         if attention_mask is None:
             attention_mask = torch.ones(
-                (batch_size, infer_state.max_len_in_batch), dtype=torch.bool, device=hidden_states.device
+                (batch_size, infer_state.max_len_in_batch), dtype=torch.bool, device=inputs_embeds.device
             )
 
         attention_mask = self._prepare_decoder_attention_mask(
-            attention_mask, (batch_size, seq_length), hidden_states, past_key_values_length
+            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
         )
 
-        # decoder layers
-        infer_state.decode_layer_id = 0
+        hidden_states = inputs_embeds
 
-        start_idx, end_idx = stage_index[0], stage_index[1]
-        if past_key_values is None:
-            past_key_values = tuple([None] * (end_idx - start_idx + 1))
+        # decoder layers
+        all_hidden_states = () if output_hidden_states else None
+        all_self_attns = () if output_attentions else None
+        next_decoder_cache = () if use_cache else None
 
-        for idx, past_key_value in zip(range(start_idx, end_idx), past_key_values):
-            decoder_layer = self.layers[idx]
+        infer_state.decode_layer_id = 0
+        for idx, decoder_layer in enumerate(self.layers):
+            past_key_value = past_key_values[idx] if past_key_values is not None else None
             # NOTE: modify here for passing args to decoder layer
             layer_outputs = decoder_layer(
                 hidden_states,
                 attention_mask=attention_mask,
                 position_ids=position_ids,
                 past_key_value=past_key_value,
                 output_attentions=output_attentions,
                 use_cache=use_cache,
                 infer_state=infer_state,
             )
             infer_state.decode_layer_id += 1
             hidden_states = layer_outputs[0]
 
-        if stage_manager.is_last_stage() or stage_manager.num_stages == 1:
-            hidden_states = self.norm(hidden_states)
+            if use_cache:
+                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
+
+        hidden_states = self.norm(hidden_states)
+        next_cache = next_decoder_cache if use_cache else None
 
         # update indices
         # infer_state.block_loc[:, infer_state.max_len_in_batch-1] = infer_state.total_token_num + torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
         infer_state.start_loc += torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
         infer_state.seq_len += 1
         infer_state.max_len_in_batch += 1
 
-        return {"hidden_states": hidden_states}
+        if not return_dict:
+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+
+        return BaseModelOutputWithPast(
+            last_hidden_state=hidden_states,
+            past_key_values=next_cache,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attns,
+        )
 
     @staticmethod
     def llama_decoder_layer_forward(
         self: LlamaDecoderLayer,
         hidden_states: torch.Tensor,
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
@@ -467,20 +378,15 @@
                     infer_state.decode_mem_index,
                     infer_state.cache_manager,
                 )
 
             if HAS_LIGHTLLM_KERNEL:
                 attn_output = torch.empty_like(query_states)
                 llama_triton_token_attention(
-                    query_states=query_states,
-                    attn_output=attn_output,
-                    infer_state=infer_state,
-                    num_key_value_groups=self.num_key_value_groups,
-                    q_head_num=q_len * self.num_heads,
-                    head_dim=self.head_dim,
+                    query_states, attn_output, infer_state, num_key_value_groups=self.num_key_value_groups
                 )
             else:
                 self.num_heads // self.num_key_value_heads
                 cache_k = infer_state.cache_manager.key_buffer[infer_state.decode_layer_id]
                 cache_v = infer_state.cache_manager.value_buffer[infer_state.decode_layer_id]
 
                 query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/chatglm2.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/policies/chatglm2.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,10 +1,8 @@
-from typing import List
-
-import torch.nn as nn
+from functools import partial
 
 from colossalai.shardformer.modeling.chatglm2_6b.modeling_chatglm import (
     ChatGLMForConditionalGeneration,
     ChatGLMModel,
     GLMBlock,
     GLMTransformer,
     SelfAttention,
@@ -51,39 +49,29 @@
             description=method_replacement, policy=policy, target_key=SelfAttention
         )
         if self.shard_config.enable_tensor_parallelism:
             policy[GLMBlock].attribute_replacement["self_attention.num_multi_query_groups_per_partition"] = (
                 self.model.config.multi_query_group_num // self.shard_config.tensor_parallel_size
             )
         # for rmsnorm and others, we need to check the shape
-
-        self.set_pipeline_forward(
-            model_cls=ChatGLMForConditionalGeneration,
-            new_forward=ChatGLM2InferenceForwards.chatglm_for_conditional_generation_forward,
-            policy=policy,
-        )
-
         return policy
 
-    def get_held_layers(self) -> List[nn.Module]:
-        module = self.model.transformer
-        stage_manager = self.pipeline_stage_manager
-
-        held_layers = []
-        layers_per_stage = stage_manager.distribute_layers(module.num_layers)
-        if stage_manager.is_first_stage():
-            held_layers.append(module.embedding)
-            held_layers.append(module.output_layer)
-        start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
-        held_layers.extend(module.encoder.layers[start_idx:end_idx])
-        if stage_manager.is_last_stage():
-            if module.encoder.post_layer_norm:
-                held_layers.append(module.encoder.final_layernorm)
+    def postprocess(self):
+        init_to_get_rotary(self.model)
+        return self.model
 
-        # rotary_pos_emb is needed for all stages
-        held_layers.append(module.rotary_pos_emb)
 
-        return held_layers
+class ChatGLM2ForConditionalGenerationInferPolicy(ChatGLM2InferPolicy):
+    def __init__(self) -> None:
+        super().__init__()
+
+    def module_policy(self):
+        policy = super().module_policy()
+        model_infer_forward = ChatGLM2InferenceForwards.chatglm_for_conditional_generation_forward
+        method_replacement = {"forward": partial(model_infer_forward)}
+        self.append_or_create_method_replacement(
+            description=method_replacement, policy=policy, target_key=ChatGLMForConditionalGeneration
+        )
+        return policy
 
     def postprocess(self):
-        init_to_get_rotary(self.model.transformer)
-        return self.model
+        return super().postprocess()
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/llama.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/base_policy.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,206 +1,208 @@
-from functools import partial
-from typing import List
+# part of code modified from https://github.com/tunib-ai/parallelformers
 
-import torch
-from torch.nn import Module
-from transformers.models.llama.modeling_llama import (
-    LlamaAttention,
-    LlamaDecoderLayer,
-    LlamaForCausalLM,
-    LlamaModel,
-    LlamaRMSNorm,
-)
-
-from colossalai.shardformer.policies.base_policy import ModulePolicyDescription, SubModuleReplacementDescription
-
-# import colossalai
-from colossalai.shardformer.policies.llama import LlamaForCausalLMPolicy
-
-from ..modeling._utils import init_to_get_rotary
-from ..modeling.llama import LlamaInferenceForwards
-
-try:
-    from colossalai.kernel.triton import rmsnorm_forward
-
-    HAS_TRITON_RMSNORM = True
-except:
-    print("you should install triton from https://github.com/openai/triton")
-    HAS_TRITON_RMSNORM = False
+from abc import ABC, abstractmethod
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Union
 
+import torch.nn as nn
+from torch import Tensor
+from torch.nn import Module
 
-def get_triton_rmsnorm_forward():
-    if HAS_TRITON_RMSNORM:
-
-        def _triton_rmsnorm_forward(self: LlamaRMSNorm, hidden_states: torch.Tensor):
-            return rmsnorm_forward(hidden_states, self.weight.data, self.variance_epsilon)
+from colossalai.pipeline.stage_manager import PipelineStageManager
 
-        return _triton_rmsnorm_forward
-    else:
-        return None
+from ..layer.normalization import BaseLayerNorm
+from ..layer.parallel_module import ParallelModule
+from ..shard.shard_config import ShardConfig
+
+__all__ = ["ParallelModule", "SubModuleReplacementDescription", "ModulePolicyDescription", "Policy"]
+
+
+@dataclass
+class SubModuleReplacementDescription:
+    r"""
+    Describe how a submodule will be replaced
+
+    Args:
+        suffix (str): used to get the submodule object
+        target_module (ParallelModule): specifies the module class used to replace to submodule
+        kwargs (Dict[str, Any]): the dictionary used to pass extra arguments to the `ParallelModule.from_native_module` method.
+        ignore_if_not_exist (bool): if the submodule does not exist, ignore it or raise an exception
+    """
+
+    suffix: str
+    target_module: Union[ParallelModule, BaseLayerNorm]
+    kwargs: Dict[str, Any] = None
+    ignore_if_not_exist: bool = False
+
+
+@dataclass
+class ModulePolicyDescription:
+    r"""
+    Describe how the attributes and parameters will be transformed in a policy.
+
+    Args:
+        attribute_replacement (Dict[str, Any]): key is the attribute name, value is the attribute value after sharding
+        param_replacement (List[Callable]): a list of functions to perform in-place param replacement. The function
+                    must receive only one arguments: module. One example is
+
+                    ```python
+                    def example_replace_weight(module: torch.nn.Module):
+                        weight = module.weight
+                        new_weight = shard_rowwise(weight, process_group)
+                        module.weight = torch.nn.Parameter(new_weight)
+                    ```
+        sub_module_replacement (List[SubModuleReplacementDescription]): each element in the list is a SubModuleReplacementDescription
+                    object which specifies the module to be replaced and the target module used to replacement.
+        method_replace (Dict[str, Callable]): key is the method name, value is the method for replacement
+    """
+
+    attribute_replacement: Dict[str, Any] = None
+    param_replacement: List[Callable] = None
+    sub_module_replacement: List[SubModuleReplacementDescription] = None
+    method_replacement: Dict[str, Callable] = None
+
+
+class Policy(ABC):
+    r"""
+    The base class for all the policies. For each different model, it should have a different policy class,
+    like BertPolicy for Bert Model or OPTPolicy for OPT model.
+
+    Shardformer has provided many built-in sharding policies for the mainstream models. You can use the
+    built-in policies by setting `policy = None`, which is already the default argument for `Shardformer.optimize`.
+    If you want to define your own policy, you can inherit from this class and overwrite the methods you want to modify.
+    """
 
-
-class LlamaModelInferPolicy(LlamaForCausalLMPolicy):
     def __init__(self) -> None:
-        super().__init__()
+        self.shard_config: Optional[ShardConfig] = None
+        self.model: Optional[Module] = None
 
-    def module_policy(self):
-        policy = super().module_policy()
-        decoder_attribute_replacement = {
-            "self_attn.hidden_size": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
-            "self_attn.num_heads": self.model.config.num_attention_heads // self.shard_config.tensor_parallel_size,
-            "self_attn.num_key_value_heads": self.model.config.num_key_value_heads
-            // self.shard_config.tensor_parallel_size,
-        }
-        if self.shard_config.extra_kwargs.get("quant", None) == "gptq":
-            from colossalai.inference.quant.gptq.cai_gptq import ColCaiQuantLinear, RowCaiQuantLinear
-
-            policy[LlamaDecoderLayer] = ModulePolicyDescription(
-                attribute_replacement=decoder_attribute_replacement,
-                sub_module_replacement=[
-                    SubModuleReplacementDescription(
-                        suffix="self_attn.q_proj",
-                        target_module=ColCaiQuantLinear,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="self_attn.k_proj",
-                        target_module=ColCaiQuantLinear,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="self_attn.v_proj",
-                        target_module=ColCaiQuantLinear,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="self_attn.o_proj",
-                        target_module=RowCaiQuantLinear,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="mlp.gate_proj",
-                        target_module=ColCaiQuantLinear,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="mlp.up_proj",
-                        target_module=ColCaiQuantLinear,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="mlp.down_proj",
-                        target_module=RowCaiQuantLinear,
-                        kwargs={"split_num": 1},
-                    ),
-                ],
-            )
-
-        elif self.shard_config.extra_kwargs.get("quant", None) == "smoothquant":
-            from colossalai.inference.quant.smoothquant.models.llama import LlamaSmoothquantDecoderLayer
-            from colossalai.inference.quant.smoothquant.models.parallel_linear import (
-                ColW8A8BFP32OFP32Linear,
-                RowW8A8B8O8Linear,
-                RowW8A8BFP32O32LinearSiLU,
-                RowW8A8BFP32OFP32Linear,
-            )
-
-            policy[LlamaSmoothquantDecoderLayer] = ModulePolicyDescription(
-                attribute_replacement=decoder_attribute_replacement,
-                sub_module_replacement=[
-                    SubModuleReplacementDescription(
-                        suffix="self_attn.q_proj",
-                        target_module=RowW8A8B8O8Linear,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="self_attn.k_proj",
-                        target_module=RowW8A8B8O8Linear,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="self_attn.v_proj",
-                        target_module=RowW8A8B8O8Linear,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="self_attn.o_proj",
-                        target_module=ColW8A8BFP32OFP32Linear,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="mlp.gate_proj",
-                        target_module=RowW8A8BFP32O32LinearSiLU,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="mlp.up_proj",
-                        target_module=RowW8A8BFP32OFP32Linear,
-                        kwargs={"split_num": 1},
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="mlp.down_proj",
-                        target_module=ColW8A8BFP32OFP32Linear,
-                        kwargs={"split_num": 1},
-                    ),
-                ],
-            )
-        self.shard_config._infer()
-
-        infer_forward = LlamaInferenceForwards.llama_model_forward
-        method_replacement = {"forward": partial(infer_forward)}
-        self.append_or_create_method_replacement(description=method_replacement, policy=policy, target_key=LlamaModel)
-
-        infer_forward = LlamaInferenceForwards.llama_decoder_layer_forward
-        method_replacement = {"forward": partial(infer_forward)}
-        self.append_or_create_method_replacement(
-            description=method_replacement, policy=policy, target_key=LlamaDecoderLayer
-        )
+    def set_model(self, model: nn.Module) -> None:
+        r"""
+        Set model as an attribute of the Policy object so that we can access the model's attributes.
+        Args:
+            model (:class:`nn.Module`): The model to be perform
+        """
+        self.model = model
+
+    def set_shard_config(self, shard_config: ShardConfig) -> None:
+        r"""
+        Set shard config as an attribute of the Policy object.
+        Args:
+            shard_config (:class:`ShardConfig`): The shard config to be perform
+        """
+        self.shard_config = shard_config
+
+        self.config_sanity_check()
+
+    @property
+    def pipeline_stage_manager(self) -> Optional[PipelineStageManager]:
+        if self.shard_config is not None:
+            return self.shard_config.pipeline_stage_manager
+        return None
 
-        infer_forward = LlamaInferenceForwards.llama_flash_attn_kvcache_forward
-        method_replacement = {"forward": partial(infer_forward)}
-        self.append_or_create_method_replacement(
-            description=method_replacement, policy=policy, target_key=LlamaAttention
-        )
+    @abstractmethod
+    def config_sanity_check(self):
+        """
+        Check if the shard config is valid for the model. Raise an exception if the config is invalid.
+        This method is made abstractmethod with no default implementation because we want to the policy writer
+        to take note of the feature supported by his/her model and policy.
+        """
+
+    @abstractmethod
+    def preprocess(self) -> nn.Module:
+        r"""
+        Perform some preprocessing of the model, like reshaping the embedding layer.
+        """
+
+    @abstractmethod
+    def module_policy(self) -> Dict[Union[str, nn.Module], ModulePolicyDescription]:
+        r"""
+        This method returns the module policy, which is a dictionary. The key is the module name or the module object,
+        and the value is the ModulePolicyDescription object. The ModulePolicyDescription object describes how the module
+        will be transformed.
+        """
+
+    @abstractmethod
+    def postprocess(self) -> nn.Module:
+        r"""
+        Perform some postprocessing of the model, like binding the weight of embedding layer with
+        the classifier layer
+        """
+
+    def append_or_create_submodule_replacement(
+        self,
+        description: Union[SubModuleReplacementDescription, List[SubModuleReplacementDescription]],
+        policy: Dict[Union[str, nn.Module], ModulePolicyDescription],
+        target_key: Union[str, nn.Module],
+    ) -> Dict[Union[str, nn.Module], ModulePolicyDescription]:
+        r"""
+        Append or create a new submodule replacement description to the policy for the given key.
+
+        Args:
+            submodule_replace_desc (Union[SubModuleReplacementDescription, List[SubModuleReplacementDescription]]): the submodule replacement description to be appended
+            policy (Dict[Union[str, nn.Module], ModulePolicyDescription]): the policy to be updated
+            target_key (Union[str, nn.Module]): the key of the policy to be updated
+        """
+        # convert to list
+        if isinstance(description, SubModuleReplacementDescription):
+            description = [description]
+
+        # append or create a new description
+        if target_key in policy:
+            if policy[target_key].sub_module_replacement is None:
+                policy[target_key].sub_module_replacement = description
+            else:
+                policy[target_key].sub_module_replacement.extend(description)
+        else:
+            policy[target_key] = ModulePolicyDescription(sub_module_replacement=description)
 
-        # set as default, in inference we also use pipeline style forward, just setting stage as 1
-        self.set_pipeline_forward(
-            model_cls=LlamaForCausalLM, new_forward=LlamaInferenceForwards.llama_causal_lm_forward, policy=policy
-        )
+        return policy
 
-        infer_forward = None
-        if HAS_TRITON_RMSNORM:
-            infer_forward = get_triton_rmsnorm_forward()
-
-        if infer_forward is not None:
-            method_replacement = {"forward": partial(infer_forward)}
-            self.append_or_create_method_replacement(
-                description=method_replacement, policy=policy, target_key=LlamaRMSNorm
-            )
+    def append_or_create_method_replacement(
+        self,
+        description: Dict[str, Callable],
+        policy: Dict[Union[str, nn.Module], ModulePolicyDescription],
+        target_key: Union[str, nn.Module],
+    ) -> Dict[Union[str, nn.Module], ModulePolicyDescription]:
+        r"""
+        Append or create a new method replacement description to the policy for the given key.
+
+        Args:
+            description (Union[SubModuleReplacementDescription, List[SubModuleReplacementDescription]]): the submodule replacement description to be appended
+            policy (Dict[Union[str, nn.Module], ModulePolicyDescription]): the policy to be updated
+            target_key (Union[str, nn.Module]): the key of the policy to be updated
+        """
+        if target_key in policy:
+            if policy[target_key].method_replacement is None:
+                policy[target_key].method_replacement = description
+            else:
+                policy[target_key].method_replacement.update(description)
+        else:
+            policy[target_key] = ModulePolicyDescription(method_replacement=description)
 
         return policy
 
-    def postprocess(self):
-        init_to_get_rotary(self.model.model)
-        return self.model
-
     def get_held_layers(self) -> List[Module]:
-        """Get pipeline layers for current stage."""
-        assert self.pipeline_stage_manager is not None
-
-        if self.model.__class__.__name__ == "LlamaModel":
-            module = self.model
-        else:
-            module = self.model.model
-        stage_manager = self.pipeline_stage_manager
+        """Get layers that should be held in current stage. This method should be implemented by subclass.
 
-        held_layers = []
-        layers_per_stage = stage_manager.distribute_layers(len(module.layers))
-        if stage_manager.is_first_stage():
-            held_layers.append(module.embed_tokens)
-            held_layers.append(self.model.lm_head)
-        start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
-        held_layers.extend(module.layers[start_idx:end_idx])
-        if stage_manager.is_last_stage():
-            held_layers.append(module.norm)
-
-        return held_layers
+        Returns:
+            List[Module]: List of layers that should be hold in current stage
+        """
+        raise NotImplementedError
+
+    def get_shared_params(self) -> List[Dict[int, Tensor]]:
+        """Get parameters that should be shared across stages. This method should be implemented by subclass.
+
+        Returns:
+            List[Dict[int, Tensor]]: List of parameters that should be shared across stages. E.g. [{0: module.model.embed_tokens.weight, 3: module.lm_head.weight}]
+        """
+        return []
+
+    def tie_weight_check(self):
+        input_embedding = self.model.get_input_embeddings()
+        output_embedding = self.model.get_output_embeddings()
+        return (
+            input_embedding is not None
+            and output_embedding is not None
+            and id(input_embedding.weight) == id(output_embedding.weight)
+        )
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/batch_infer_state.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/batch_infer_state.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/kvcache_manager.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/kvcache_manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -99,8 +99,8 @@
 
     @torch.no_grad()
     def free_all(self):
         """free all memory by updating memory states"""
         self.available_size = len(self.mem_state)
         self.mem_state[:] = 1
         self.max_len_in_batch = 0
-        # self.logger.info("freed all space of memory manager")
+        self.logger.info("freed all space of memory manager")
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/initialize.py` & `colossalai-nightly-2024.6.1/colossalai/initialize.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/interface/model.py` & `colossalai-nightly-2024.6.1/colossalai/interface/model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,32 +1,39 @@
-from .cpu_adam import CpuAdamArmExtension, CpuAdamX86Extension
-from .flash_attention import FlashAttentionDaoCudaExtension, FlashAttentionNpuExtension, FlashAttentionSdpaCudaExtension
-from .layernorm import LayerNormCudaExtension
-from .moe import MoeCudaExtension
-from .optimizer import FusedOptimizerCudaExtension
-from .softmax import ScaledMaskedSoftmaxCudaExtension, ScaledUpperTriangleMaskedSoftmaxCudaExtension
+from .pybind.cpu_adam import CpuAdamArmExtension, CpuAdamX86Extension
+from .pybind.flash_attention import (
+    FlashAttentionDaoCudaExtension,
+    FlashAttentionNpuExtension,
+    FlashAttentionSdpaCudaExtension,
+)
+from .pybind.inference import InferenceOpsCudaExtension
+from .pybind.layernorm import LayerNormCudaExtension
+from .pybind.moe import MoeCudaExtension
+from .pybind.optimizer import FusedOptimizerCudaExtension
+from .pybind.softmax import ScaledMaskedSoftmaxCudaExtension, ScaledUpperTriangleMaskedSoftmaxCudaExtension
 
 ALL_EXTENSIONS = [
     CpuAdamArmExtension,
     CpuAdamX86Extension,
     LayerNormCudaExtension,
     MoeCudaExtension,
     FusedOptimizerCudaExtension,
+    InferenceOpsCudaExtension,
     ScaledMaskedSoftmaxCudaExtension,
     ScaledUpperTriangleMaskedSoftmaxCudaExtension,
     FlashAttentionDaoCudaExtension,
     FlashAttentionSdpaCudaExtension,
     FlashAttentionNpuExtension,
 ]
 
 __all__ = [
     "CpuAdamArmExtension",
     "CpuAdamX86Extension",
     "LayerNormCudaExtension",
     "MoeCudaExtension",
     "FusedOptimizerCudaExtension",
+    "InferenceOpsCudaExtension",
     "ScaledMaskedSoftmaxCudaExtension",
     "ScaledUpperTriangleMaskedSoftmaxCudaExtension",
     "FlashAttentionDaoCudaExtension",
     "FlashAttentionSdpaCudaExtension",
     "FlashAttentionNpuExtension",
 ]
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/base_extension.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/base_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpp_extension.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/cpp_extension.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,14 +21,17 @@
         self.prebuilt_module_path = "colossalai._C"
         self.prebuilt_import_path = f"{self.prebuilt_module_path}.{self.name}"
         self.version_dependent_macros = ["-DVERSION_GE_1_1", "-DVERSION_GE_1_3", "-DVERSION_GE_1_5"]
 
     def csrc_abs_path(self, path):
         return os.path.join(self.relative_to_abs_path("csrc"), path)
 
+    def pybind_abs_path(self, path):
+        return os.path.join(self.relative_to_abs_path("pybind"), path)
+
     def relative_to_abs_path(self, code_path: str) -> str:
         """
         This function takes in a path relative to the colossalai root directory and return the absolute path.
         """
 
         # get the current file path
         # iteratively check the parent directory
@@ -112,14 +115,15 @@
         """
 
     @abstractmethod
     def include_dirs(self) -> List[str]:
         """
         This function should return a list of include files for extensions.
         """
+        return [self.csrc_abs_path("")]
 
     @abstractmethod
     def cxx_flags(self) -> List[str]:
         """
         This function should return a list of cxx compilation flags for extensions.
         """
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/cpu_adam_x86.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/cpu_adam/cpu_adam_x86.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import platform
 
-from ..cuda_extension import _CudaExtension
-from ..utils import append_nvcc_threads
+from ...cuda_extension import _CudaExtension
+from ...utils import append_nvcc_threads
 
 
 class CpuAdamX86Extension(_CudaExtension):
     def __init__(self):
         super().__init__(name="cpu_adam_x86")
 
     def is_available(self) -> bool:
@@ -17,21 +17,18 @@
             arch == "x86_64"
         ), f"[extension] The {self.name} kernel requires the CPU architecture to be x86_64 but got {arch}"
         super().assert_compatible()
 
     # necessary 4 functions
     def sources_files(self):
         ret = [
-            self.csrc_abs_path("cuda/cpu_adam.cpp"),
+            self.csrc_abs_path("kernel/x86/cpu_adam.cpp"),
         ]
         return ret
 
-    def include_dirs(self):
-        return [self.csrc_abs_path("includes"), self.get_cuda_home_include()]
-
     def cxx_flags(self):
         extra_cxx_flags = [
             "-std=c++14",
             "-std=c++17",
             "-lcudart",
             "-lcublas",
             "-g",
@@ -46,9 +43,9 @@
             "-std=c++14",
             "-std=c++17",
             "-U__CUDA_NO_HALF_OPERATORS__",
             "-U__CUDA_NO_HALF_CONVERSIONS__",
             "-U__CUDA_NO_HALF2_OPERATORS__",
             "-DTHRUST_IGNORE_CUB_VERSION_CHECK",
         ]
-        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags
+        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags + super().nvcc_flags()
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.cpp` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.h` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/colossal_C_frontend.cpp` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/optimizer/optimizer.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.cpp` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.h` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda.cpp` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/layernorm/layer_norm.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
  *     with minor changes. */
 
 #include <torch/extension.h>
 
 #include <cassert>
 #include <vector>
 
-#include "compat.h"
+#include "common/micros.h"
 
 namespace {
 
 void compute_n1_n2(at::Tensor input, at::IntArrayRef normalized_shape, int &n1,
                    int &n2) {
   int idiff = input.ndimension() - normalized_shape.size();
   n2 = 1;
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda_kernel.cu` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/layer_norm_kernel.cu`

 * *Files 6% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 #include <cuda.h>
 #include <cuda_runtime.h>
 
 #include "ATen/ATen.h"
 #include "ATen/AccumulateType.h"
 #include "ATen/cuda/CUDAContext.h"
 #include "ATen/cuda/DeviceUtils.cuh"
-#include "type_shim.h"
+#include "common/micros.h"
 
 template <typename U>
 __device__ void cuWelfordOnlineSum(const U curr, U& mu, U& sigma2, U& count) {
   count = count + U(1);
   U delta = curr - mu;
   U lmean = mu + delta / count;
   mu = lmean;
@@ -602,19 +602,19 @@
 #else
                      at::IntList normalized_shape,
 #endif
                      at::Tensor* gamma, at::Tensor* beta, double epsilon) {
   using namespace at;
   DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
       input->scalar_type(), output->scalar_type(), "cuda_layer_norm_kernel",
-      HostApplyLayerNorm(output->DATA_PTR<scalar_t_out>(),
-                         mean->DATA_PTR<float>(), invvar->DATA_PTR<float>(),
-                         input->DATA_PTR<scalar_t_in>(), n1, n2, epsilon,
-                         gamma != NULL ? gamma->DATA_PTR<scalar_t_out>() : NULL,
-                         beta != NULL ? beta->DATA_PTR<scalar_t_out>() : NULL);)
+      HostApplyLayerNorm(output->data_ptr<scalar_t_out>(),
+                         mean->data_ptr<float>(), invvar->data_ptr<float>(),
+                         input->data_ptr<scalar_t_in>(), n1, n2, epsilon,
+                         gamma != NULL ? gamma->data_ptr<scalar_t_out>() : NULL,
+                         beta != NULL ? beta->data_ptr<scalar_t_out>() : NULL);)
 }
 
 template <typename T, typename U, typename V>
 void HostLayerNormGradient(const V* dout, const U* mean, const U* invvar,
                            at::Tensor* input, int n1, int n2, const V* gamma,
                            const V* beta, double epsilon, T* grad_input,
                            V* grad_gamma, V* grad_beta) {
@@ -629,33 +629,33 @@
         2 * sizeof(U) * threads2.y * threads2.y * (threads2.x + 1);
     const int nshared2_b = threads2.x * threads2.y * sizeof(U);
     const int nshared2 = nshared2_a > nshared2_b ? nshared2_a : nshared2_b;
     at::Tensor part_grad_gamma = at::empty(
         {part_size, n2}, input->options().dtype(at::ScalarType::Float));
     at::Tensor part_grad_beta = at::empty_like(part_grad_gamma);
     cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
-        dout, input->DATA_PTR<T>(), n1, n2, mean, invvar, U(epsilon),
-        part_grad_gamma.DATA_PTR<U>(), part_grad_beta.DATA_PTR<U>());
+        dout, input->data_ptr<T>(), n1, n2, mean, invvar, U(epsilon),
+        part_grad_gamma.data_ptr<U>(), part_grad_beta.data_ptr<U>());
 
     const dim3 threads3(32, 8, 1);
     const dim3 blocks3((n2 + threads2.x - 1) / threads2.x, 1, 1);
     const int nshared3 = threads3.x * threads3.y * sizeof(U);
     cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
-        part_grad_gamma.DATA_PTR<U>(), part_grad_beta.DATA_PTR<U>(), part_size,
+        part_grad_gamma.data_ptr<U>(), part_grad_beta.data_ptr<U>(), part_size,
         n1, n2, grad_gamma, grad_beta);
   }
 
   // compute grad_input
   const uint64_t maxGridY =
       at::cuda::getCurrentDeviceProperties()->maxGridSize[1];
   const dim3 blocks1(1, std::min((uint64_t)n1, maxGridY), 1);
   const dim3 threads1(32, 4, 1);
   int nshared = threads1.y > 1 ? threads1.y * threads1.x * sizeof(U) : 0;
   cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
-      dout, input->DATA_PTR<T>(), n1, n2, mean, invvar, U(epsilon), gamma,
+      dout, input->data_ptr<T>(), n1, n2, mean, invvar, U(epsilon), gamma,
       grad_input);
 }
 
 void cuda_layer_norm_gradient(at::Tensor* dout, at::Tensor* mean,
                               at::Tensor* invvar, at::Tensor* input, int n1,
                               int n2,
 #ifdef VERSION_GE_1_1
@@ -667,17 +667,17 @@
                               double epsilon, at::Tensor* grad_input,
                               at::Tensor* grad_gamma, at::Tensor* grad_beta) {
   using namespace at;
   DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
       input->scalar_type(), gamma->scalar_type(),
       "cuda_layer_norm_gradient_kernel",
       HostLayerNormGradient(
-          dout->DATA_PTR<scalar_t_out>(), mean->DATA_PTR<float>(),
-          invvar->DATA_PTR<float>(), input, n1, n2,
+          dout->data_ptr<scalar_t_out>(), mean->data_ptr<float>(),
+          invvar->data_ptr<float>(), input, n1, n2,
           // TMJ pass NULL argument for gamma, beta, grad_gamma and grad_beta
           // if gamma Tensor is NULL on input.
-          gamma != NULL ? gamma->DATA_PTR<scalar_t_out>() : NULL,
-          gamma != NULL ? beta->DATA_PTR<scalar_t_out>() : NULL, epsilon,
-          grad_input->DATA_PTR<scalar_t_in>(),
-          gamma != NULL ? grad_gamma->DATA_PTR<scalar_t_out>() : NULL,
-          gamma != NULL ? grad_beta->DATA_PTR<scalar_t_out>() : NULL);)
+          gamma != NULL ? gamma->data_ptr<scalar_t_out>() : NULL,
+          gamma != NULL ? beta->data_ptr<scalar_t_out>() : NULL, epsilon,
+          grad_input->data_ptr<scalar_t_in>(),
+          gamma != NULL ? grad_gamma->data_ptr<scalar_t_out>() : NULL,
+          gamma != NULL ? grad_beta->data_ptr<scalar_t_out>() : NULL);)
 }
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda.cpp` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/moe/moe.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda_kernel.cu` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/moe_kernel.cu`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,17 @@
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <torch/extension.h>
 
 #include <cub/cub.cuh>
 
-#include "block_reduce.h"
+#include "funcs/reduce_function.h"
+
+using colossalAI::funcs::block_reduce;
+using colossalAI::funcs::ReduceType;
 
 template <typename T, int block_size, int pack_size>
 __device__ void moe_dpch_one_fwd(T *src_row, T *dst_row, const int cols) {
   assert(cols % pack_size == 0);
   const int bpack_size = block_size * pack_size;
 
   typedef cub::BlockLoad<T, block_size, pack_size, cub::BLOCK_LOAD_VECTORIZE>
@@ -153,16 +156,15 @@
     for (int i = 0; i < pack_size; ++i) {
       thread_sum += grad[i] * tokens[i];
       grad[i] *= weight;
     }
 
     BlockStore(ts_store).Store(src_row + idx, grad);
   }
-
-  blockReduce<ReduceType::kSum, 1>(&thread_sum);
+  block_reduce<float, ReduceType::kSum, 1>(&thread_sum);
 
   if (threadIdx.x == 0) *weight_grad = static_cast<T>(thread_sum);
 }
 
 template <typename T, int block_size, int pack_size>
 __device__ void moe_cb_two_fwd(T *src_row1, T *src_row2, T *dst_row,
                                const T weight1, const T weight2,
@@ -226,15 +228,15 @@
       sgrad2[i] = weight2 * grad[i];
     }
 
     BlockStore(ts_store).Store(src_row1 + idx, sgrad1);
     BlockStore(ts_store).Store(src_row2 + idx, sgrad2);
   }
 
-  blockReduce<ReduceType::kSum, 2>(thread_sum);
+  block_reduce<float, ReduceType::kSum, 2>(thread_sum);
 
   if (threadIdx.x == 0)
     *weight_grad1 = static_cast<T>(thread_sum[0]);
   else if (threadIdx.x == 1)
     *weight_grad2 = static_cast<T>(thread_sum[1]);
 }
 
@@ -533,15 +535,15 @@
     cumsum_kernel<1024, 2><<<e, 1024>>>(inputs, outputs, s, e);
   else
     cumsum_kernel<1024, 4><<<e, 1024>>>(inputs, outputs, s, e);
 }
 
 // API FUNCTIONS --------------------------------
 
-#define DISPATCH_FLOAT_AND_HALF(TYPE, NAME, ...)                       \
+#define DISPATCH_FLOAT_AND_HALF_MOE(TYPE, NAME, ...)                   \
   switch (TYPE) {                                                      \
     case at::ScalarType::Float: {                                      \
       using scalar_t = float;                                          \
       __VA_ARGS__;                                                     \
       break;                                                           \
     }                                                                  \
     case at::ScalarType::Half: {                                       \
@@ -559,41 +561,41 @@
                                         torch::Tensor dest_idx) {
   assert(h % 16 == 0);
   auto res = torch::zeros(
       {ec, h},
       torch::dtype(batch_tokens.dtype()).device(batch_tokens.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF(
+  DISPATCH_FLOAT_AND_HALF_MOE(
       batch_tokens.scalar_type(), "moe dispatch forward",
       moe_dpch_fwd_launch<scalar_t>(
-          batch_tokens.data<scalar_t>(), res.data<scalar_t>(),
-          mask[0].data<int>(), k == 1 ? nullptr : mask[1].data<int>(),
-          dest_idx[0].data<int>(),
-          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, h));
+          batch_tokens.data_ptr<scalar_t>(), res.data_ptr<scalar_t>(),
+          mask[0].data_ptr<int>(), k == 1 ? nullptr : mask[1].data_ptr<int>(),
+          dest_idx[0].data_ptr<int>(),
+          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, h));
 
   return res;
 }
 
 torch::Tensor moe_dispatch_cuda_backward(int s, int ec, int h,
                                          torch::Tensor expert_grad,
                                          torch::Tensor mask,
                                          torch::Tensor dest_idx) {
   assert(h % 16 == 0);
   auto res = torch::zeros(
       {s, h}, torch::dtype(expert_grad.dtype()).device(expert_grad.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF(
+  DISPATCH_FLOAT_AND_HALF_MOE(
       expert_grad.scalar_type(), "moe dispatch backward",
       moe_dpch_bwd_launch<scalar_t>(
-          res.data<scalar_t>(), expert_grad.data<scalar_t>(),
-          mask[0].data<int>(), k == 1 ? nullptr : mask[1].data<int>(),
-          dest_idx[0].data<int>(),
-          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, h));
+          res.data_ptr<scalar_t>(), expert_grad.data_ptr<scalar_t>(),
+          mask[0].data_ptr<int>(), k == 1 ? nullptr : mask[1].data_ptr<int>(),
+          dest_idx[0].data_ptr<int>(),
+          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, h));
 
   return res;
 }
 
 torch::Tensor moe_combine_cuda_forward(int s, int e, int c, int h,
                                        torch::Tensor expert_tokens,
                                        torch::Tensor logits, torch::Tensor mask,
@@ -602,21 +604,21 @@
   assert(expert_tokens.dtype() == logits.dtype());
 
   auto res = torch::zeros(
       {s, h},
       torch::dtype(expert_tokens.dtype()).device(expert_tokens.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF(
+  DISPATCH_FLOAT_AND_HALF_MOE(
       expert_tokens.scalar_type(), "moe combine forward",
       moe_cb_fwd_launch<scalar_t>(
-          expert_tokens.data<scalar_t>(), res.data<scalar_t>(),
-          logits.data<scalar_t>(), mask[0].data<int>(),
-          k == 1 ? nullptr : mask[1].data<int>(), dest_idx[0].data<int>(),
-          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, e, c,
+          expert_tokens.data_ptr<scalar_t>(), res.data_ptr<scalar_t>(),
+          logits.data_ptr<scalar_t>(), mask[0].data_ptr<int>(),
+          k == 1 ? nullptr : mask[1].data_ptr<int>(), dest_idx[0].data_ptr<int>(),
+          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, e, c,
           h));
 
   return res;
 }
 
 std::vector<torch::Tensor> moe_combine_cuda_backward(
     int s, int e, int c, int h, torch::Tensor tokens_grad,
@@ -629,31 +631,31 @@
   auto egrad = torch::zeros(
            {e * c, h},
            torch::dtype(tokens_grad.dtype()).device(tokens_grad.device())),
        wgrad = torch::zeros(
            {s, e}, torch::dtype(logits.dtype()).device(logits.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF(
+  DISPATCH_FLOAT_AND_HALF_MOE(
       tokens_grad.scalar_type(), "moe combine backward",
       moe_cb_bwd_launch<scalar_t>(
-          tokens_grad.data<scalar_t>(), egrad.data<scalar_t>(),
-          expert_tokens.data<scalar_t>(), logits.data<scalar_t>(),
-          wgrad.data<scalar_t>(), mask[0].data<int>(),
-          k == 1 ? nullptr : mask[1].data<int>(), dest_idx[0].data<int>(),
-          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, e, c,
+          tokens_grad.data_ptr<scalar_t>(), egrad.data_ptr<scalar_t>(),
+          expert_tokens.data_ptr<scalar_t>(), logits.data_ptr<scalar_t>(),
+          wgrad.data_ptr<scalar_t>(), mask[0].data_ptr<int>(),
+          k == 1 ? nullptr : mask[1].data_ptr<int>(), dest_idx[0].data_ptr<int>(),
+          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, e, c,
           h));
 
   return {egrad, wgrad};
 }
 
 torch::Tensor cumsum_sub_one_in_dim0(torch::Tensor mask) {
   assert(mask.dim() == 2);
   assert(mask.dtype() == torch::kInt32);
 
   const int s = mask.size(0), e = mask.size(1);
   auto res =
       torch::empty({s, e}, torch::dtype(torch::kInt32).device(mask.device()));
-  cumsum_launch(mask.data<int>(), res.data<int>(), s, e);
+  cumsum_launch(mask.data_ptr<int>(), res.data_ptr<int>(), s, e);
 
   return res;
 }
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_adam.cu` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 #include <ATen/cuda/Exceptions.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "type_shim.h"
+#include "common/micros.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 typedef enum {
   ADAM_MODE_0 = 0,  // L2 regularization mode
   ADAM_MODE_1 = 1   // Decoupled weight decay mode(AdamW)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_apply.cuh` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_apply.cuh`

 * *Files 1% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 #include <assert.h>
 #include <c10/cuda/CUDAGuard.h>
 
-#include "compat.h"
+#include "common/micros.h"
 
 // #include <iostream>
 
 // This header is the one-stop shop for all your multi-tensor apply needs.
 
 // TODO:  Kernel arg size limit may be <4KB for some other cards (ie Jetson)
 constexpr int depth_to_max_tensors[5] = {110, 64, 48, 36, 30};
@@ -100,15 +100,15 @@
       bool tensors_full = (loc_tensor_info == depth_to_max_tensors[depth - 1] &&
                            chunk == chunks_this_tensor - 1);
       bool blocks_full = (loc_block_info == depth_to_max_blocks[depth - 1]);
       bool last_chunk = (t == ntensors - 1 && chunk == chunks_this_tensor - 1);
       if (tensors_full || blocks_full || last_chunk) {
         // using accscalar_t = acc_type<scalar_t, true>;
         multi_tensor_apply_kernel<<<loc_block_info, block_size, 0, stream>>>(
-            chunk_size, noop_flag.DATA_PTR<int>(), tl, callable, args...);
+            chunk_size, noop_flag.data_ptr<int>(), tl, callable, args...);
 
         AT_CUDA_CHECK(cudaGetLastError());
 
         // Reset.  The control flow possibilities here make my brain hurt.
         loc_block_info = 0;
         if (chunk == chunks_this_tensor - 1) {
           // std::cout << "Hit case 1 " << cond1 << " " << cond2 << " " << cond3
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu`

 * *Files 15% similar despite different names*

```diff
@@ -7,19 +7,107 @@
 #include <c10/cuda/CUDAGuard.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "type_shim.h"
+#include "common/micros.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
+
+template <typename T>
+__device__ __forceinline__ T reduce_block_into_lanes(
+    T* x, T val, int lanes = 1,
+    bool share_result = false)  // lanes is intended to be <= 32.
+{
+  int tid = threadIdx.x + threadIdx.y * blockDim.x;
+  int blockSize =
+      blockDim.x * blockDim.y;  // blockSize is intended to be a multiple of 32.
+
+  if (blockSize >= 64) {
+    x[tid] = val;
+    __syncthreads();
+  }
+
+#pragma unroll
+  for (int i = (blockSize >> 1); i >= 64; i >>= 1) {
+    if (tid < i) x[tid] = x[tid] + x[tid + i];
+    __syncthreads();
+  }
+
+  T final;
+
+  if (tid < 32) {
+    if (blockSize >= 64)
+      final = x[tid] + x[tid + 32];
+    else
+      final = val;
+      // __SYNCWARP();
+
+#pragma unroll
+    for (int i = 16; i >= lanes; i >>= 1)
+      final = final + __shfl_down_sync(0xffffffff, final, i);
+  }
+
+  if (share_result) {
+    if (tid < lanes) x[tid] = final;  // EpilogueOp
+    // Make sure the smem result is visible to all warps.
+    __syncthreads();
+  }
+
+  return final;
+}
+
+template <typename T>
+__device__ __forceinline__ T reduce_block_into_lanes_max_op(
+    T* x, T val, int lanes = 1,
+    bool share_result = false)  // lanes is intended to be <= 32.
+{
+  int tid = threadIdx.x + threadIdx.y * blockDim.x;
+  int blockSize =
+      blockDim.x * blockDim.y;  // blockSize is intended to be a multiple of 32.
+
+  if (blockSize >= 64) {
+    x[tid] = val;
+    __syncthreads();
+  }
+
+#pragma unroll
+  for (int i = (blockSize >> 1); i >= 64; i >>= 1) {
+    if (tid < i) x[tid] = fmaxf(fabsf(x[tid]), fabsf(x[tid + i]));
+    __syncthreads();
+  }
+
+  T final;
+
+  if (tid < 32) {
+    if (blockSize >= 64)
+      final = fmaxf(fabsf(x[tid]), fabsf(x[tid + 32]));
+    else
+      final = val;
+      // __SYNCWARP();
+
+#pragma unroll
+    for (int i = 16; i >= lanes; i >>= 1)
+      final =
+          fmaxf(fabsf(final), fabsf(__shfl_down_sync(0xffffffff, final, i)));
+  }
+
+  if (share_result) {
+    if (tid < lanes) x[tid] = final;  // EpilogueOp
+    // Make sure the smem result is visible to all warps.
+    __syncthreads();
+  }
+
+  return final;
+}
+
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
 }
 
 template <typename T>
 __device__ __forceinline__ void load_store(T *dst, T *src, int dst_offset,
@@ -285,32 +373,32 @@
     ret_per_tensor = at::empty({0}, float_options);
   }
 
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "multi_tensor_l2norm_cuda",
       multi_tensor_apply<1>(
           BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-          L2NormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
-          per_tensor ? output_per_tensor.DATA_PTR<float>() : nullptr,
+          L2NormFunctor<scalar_t_0>(), output.data_ptr<float>(),
+          per_tensor ? output_per_tensor.data_ptr<float>() : nullptr,
           per_tensor, max_chunks_per_tensor);)
 
   AT_CUDA_CHECK(cudaGetLastError());
   // AT_CUDA_CHECK(cudaDeviceSynchronize());
 
   // This involves one more small kernel launches, but will be negligible end to
   // end. I could get rid of these by hacking the functor + multi tensor harness
   // with persistence logic, but keeping it simple for now
   auto ret = at::empty({1}, output.options());
   const at::cuda::OptionalCUDAGuard device_guard(device_of(output));
   auto stream = at::cuda::getCurrentCUDAStream();
   cleanup<<<per_tensor ? ntensors : 1, 512, 0, stream>>>(
-      output.DATA_PTR<float>(),
-      per_tensor ? output_per_tensor.DATA_PTR<float>() : nullptr,
-      ret.DATA_PTR<float>(),
-      per_tensor ? ret_per_tensor.DATA_PTR<float>() : nullptr, per_tensor,
+      output.data_ptr<float>(),
+      per_tensor ? output_per_tensor.data_ptr<float>() : nullptr,
+      ret.data_ptr<float>(),
+      per_tensor ? ret_per_tensor.data_ptr<float>() : nullptr, per_tensor,
       max_chunks_per_tensor);
 
   return std::tuple<at::Tensor, at::Tensor>(ret, ret_per_tensor);
 }
 
 // Compute and update grad norm
 // Here use a per tensor norm, and blend new norm(n) and old norm(gn) by
@@ -345,23 +433,23 @@
       at::zeros({ntensors * max_chunks_per_tensor}, float_options);
 
   if (norm_type == 0) {
     DISPATCH_FLOAT_AND_HALF(
         tensor_lists[0][0].scalar_type(), 0, "multi_tensor_maxnorm_cuda",
         multi_tensor_apply<1>(
             BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-            MaxNormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
-            output_per_tensor.DATA_PTR<float>(), true, max_chunks_per_tensor);)
+            MaxNormFunctor<scalar_t_0>(), output.data_ptr<float>(),
+            output_per_tensor.data_ptr<float>(), true, max_chunks_per_tensor);)
   } else {
     DISPATCH_FLOAT_AND_HALF(
         tensor_lists[0][0].scalar_type(), 0, "multi_tensor_l2norm_cuda",
         multi_tensor_apply<1>(
             BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-            L2NormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
-            output_per_tensor.DATA_PTR<float>(), true, max_chunks_per_tensor);)
+            L2NormFunctor<scalar_t_0>(), output.data_ptr<float>(),
+            output_per_tensor.data_ptr<float>(), true, max_chunks_per_tensor);)
   }
   AT_CUDA_CHECK(cudaGetLastError());
 
   // AT_CUDA_CHECK(cudaDeviceSynchronize());
 
   // This involves one more small kernel launches, but will be negligible end to
   // end. I could get rid of these by hacking the functor + multi tensor harness
@@ -370,13 +458,13 @@
 
   // Adding the following device guard since it happens sometimes that the
   // tensors are on one device and the cuda stream is on another device which
   // results in ILLEGAL MEM ACCESS error.
   const at::cuda::OptionalCUDAGuard device_guard(device_of(output));
   auto stream = at::cuda::getCurrentCUDAStream();
   cleanup_v2<<<ntensors, 512, 0, stream>>>(
-      output.DATA_PTR<float>(), output_per_tensor.DATA_PTR<float>(),
-      ret.DATA_PTR<float>(), out.DATA_PTR<float>(), true, max_chunks_per_tensor,
+      output.data_ptr<float>(), output_per_tensor.data_ptr<float>(),
+      ret.data_ptr<float>(), out.data_ptr<float>(), true, max_chunks_per_tensor,
       norm_type, alpha, beta);
 
   return;
 }
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_lamb.cu` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 #include <ATen/cuda/Exceptions.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "type_shim.h"
+#include "common/micros.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
@@ -329,26 +329,26 @@
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "lamb_stage_1",
       multi_tensor_apply<4>(BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
                             LAMBStage1Functor<scalar_t_0>(), beta1, beta2,
                             beta3,  // 1-beta1 or 1 depends on averaging mode
                             bias_correction1, bias_correction2, epsilon,
                             (adamMode_t)mode, weight_decay,
-                            global_grad_norm.DATA_PTR<float>(), max_grad_norm);)
+                            global_grad_norm.data_ptr<float>(), max_grad_norm);)
 
   // Compute update norms
   auto update_norm_tuple =
       multi_tensor_l2norm_cuda(chunk_size, noop_flag, grad_list, true);
 
   std::vector<std::vector<at::Tensor>> grad_param_list(
       tensor_lists.begin(), tensor_lists.begin() + 2);
 
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "lamb_stage_2",
       multi_tensor_apply<2>(BLOCK_SIZE, chunk_size, noop_flag, grad_param_list,
                             LAMBStage2Functor<scalar_t_0>(),
-                            std::get<1>(param_norm_tuple).DATA_PTR<float>(),
-                            std::get<1>(update_norm_tuple).DATA_PTR<float>(),
+                            std::get<1>(param_norm_tuple).data_ptr<float>(),
+                            std::get<1>(update_norm_tuple).data_ptr<float>(),
                             lr, weight_decay, use_nvlamb);)
 
   AT_CUDA_CHECK(cudaGetLastError());
 }
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_scale_kernel.cu` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 // #include <torch/all.h>
 
 #include <assert.h>
 // Stringstream is a big hammer, but I want to rely on operator<< for dtype.
 #include <sstream>
 
 #include "multi_tensor_apply.cuh"
-#include "type_shim.h"
+#include "common/micros.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 #include <assert.h>
 #include <cuda_runtime.h>
 
-#include "compat.h"
+#include "common/micros.h"
 #include "multi_tensor_apply.cuh"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 /**
  * Perform fused SGD on multiple buffers
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.cpp` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax.cpp`

 * *Files 18% similar despite different names*

```diff
@@ -2,27 +2,23 @@
  *     with minor changes. */
 
 #include <cuda_fp16.h>
 #include <torch/extension.h>
 
 #include <vector>
 
-namespace multihead_attn {
-namespace fused_softmax {
-namespace scaled_masked_softmax {
-
 torch::Tensor fwd_cuda(torch::Tensor const& input, torch::Tensor const& mask,
                        float scale_factor);
 
 torch::Tensor bwd_cuda(torch::Tensor const& output_grads,
                        torch::Tensor const& softmax_results,
                        float scale_factor);
 
-int get_batch_per_block_cuda(int query_seq_len, int key_seq_len, int batches,
-                             int attn_heads);
+int get_batch_per_block(int query_seq_len, int key_seq_len, int batches,
+                        int attn_heads);
 
 torch::Tensor fwd(torch::Tensor const& input, torch::Tensor const& mask,
                   float scale_factor) {
   AT_ASSERTM(input.dim() == 4, "expected 4D tensor");
   AT_ASSERTM((input.scalar_type() == at::ScalarType::Half) ||
                  (input.scalar_type() == at::ScalarType::BFloat16),
              "Only fp16 and bf16 are supported");
@@ -42,29 +38,17 @@
   AT_ASSERTM((softmax_results.scalar_type() == at::ScalarType::Half) ||
                  (softmax_results.scalar_type() == at::ScalarType::BFloat16),
              "Only fp16 and bf16 are supported");
 
   return bwd_cuda(output_grads, softmax_results, scale_factor);
 }
 
-int get_batch_per_block(int query_seq_len, int key_seq_len, int batches,
-                        int attn_heads) {
-  return get_batch_per_block_cuda(query_seq_len, key_seq_len, batches,
-                                  attn_heads);
-}
-
-}  // end namespace scaled_masked_softmax
-}  // end namespace fused_softmax
-}  // end namespace multihead_attn
-
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-  m.def("forward", &multihead_attn::fused_softmax::scaled_masked_softmax::fwd,
+  m.def("forward", &fwd,
         "Self Multihead Attention scaled, time masked softmax -- Forward.");
 
-  m.def("backward", &multihead_attn::fused_softmax::scaled_masked_softmax::bwd,
+  m.def("backward", &bwd,
         "Self Multihead Attention scaled, time masked softmax -- Backward.");
 
-  m.def("get_batch_per_block",
-        &multihead_attn::fused_softmax::scaled_masked_softmax::
-            get_batch_per_block,
+  m.def("get_batch_per_block", &get_batch_per_block,
         "Return Batch per block size.");
 }
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.h` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu`

 * *Files 9% similar despite different names*

```diff
@@ -1,103 +1,34 @@
 /*This code from NVIDIA Megatron:
  *     with minor changes. */
 
-#pragma once
+#include <ATen/ATen.h>
+#include <ATen/cuda/CUDAContext.h>
+#include <cuda.h>
+#include <cuda_fp16.h>
+#include <cuda_profiler_api.h>
+#include <cuda_runtime.h>
+#include <torch/extension.h>
 
 #include <assert.h>
 #include <c10/macros/Macros.h>
-#include <cuda_fp16.h>
-#include <stdint.h>
-
 #include <cfloat>
 #include <limits>
 
-namespace {
-
-template <typename Datatype, int ELEMENTS_PER_LDG>
-__device__ __inline__ void copy_vector(Datatype *dst, const Datatype *src);
-
-template <>
-__device__ __inline__ void copy_vector<c10::BFloat16, 1>(
-    c10::BFloat16 *dst, const c10::BFloat16 *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<c10::BFloat16, 4>(
-    c10::BFloat16 *dst, const c10::BFloat16 *src) {
-  *((float2 *)dst) = *((float2 *)src);
-}
+#include "common/micros.h"
+#include "utils/vec_copy.h"
+#include "funcs/reduce_function.h"
+#include "funcs/unary_functor.h"
+
+using colossalAI::funcs::UnaryOpFunctor;
+using colossalAI::funcs::UnaryOpType;
+using colossalAI::funcs::warp_reduce;
+using colossalAI::funcs::ReduceType;
+using colossalAI::cuda::utils::copy;
 
-template <>
-__device__ __inline__ void copy_vector<c10::Half, 1>(c10::Half *dst,
-                                                     const c10::Half *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<c10::Half, 4>(c10::Half *dst,
-                                                     const c10::Half *src) {
-  *((float2 *)dst) = *((float2 *)src);
-}
-
-template <>
-__device__ __inline__ void copy_vector<uint8_t, 1>(uint8_t *dst,
-                                                   const uint8_t *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<uint8_t, 4>(uint8_t *dst,
-                                                   const uint8_t *src) {
-  *((half2 *)dst) = *((half2 *)src);
-}
-
-int log2_ceil(int value) {
-  int log2_value = 0;
-  while ((1 << log2_value) < value) ++log2_value;
-  return log2_value;
-}
-
-template <typename T>
-struct Add {
-  __device__ __forceinline__ T operator()(T a, T b) const { return a + b; }
-};
-
-template <typename T>
-struct Max {
-  __device__ __forceinline__ T operator()(T a, T b) const {
-    return a < b ? b : a;
-  }
-};
-
-template <typename T>
-__device__ __forceinline__ T
-WARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,
-                     unsigned int mask = 0xffffffff) {
-#if CUDA_VERSION >= 9000
-  return __shfl_xor_sync(mask, value, laneMask, width);
-#else
-  return __shfl_xor(value, laneMask, width);
-#endif
-}
-
-template <typename acc_t, int WARP_BATCH, int WARP_SIZE,
-          template <typename> class ReduceOp>
-__device__ __forceinline__ void warp_reduce(acc_t *sum) {
-  ReduceOp<acc_t> r;
-#pragma unroll
-  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
-#pragma unroll
-    for (int i = 0; i < WARP_BATCH; ++i) {
-      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);
-      sum[i] = r(sum[i], b);
-    }
-  }
-}
 
 /*
  * Extended softmax (from native aten pytorch) with following additional
  * features 1) input scaling 2) Explicit masking
  */
 template <typename input_t, typename output_t, typename acc_t,
           int log2_elements>
@@ -152,16 +83,16 @@
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
 
       if (element_index < batch_element_count) {
         int itr_idx = i * element_count + it * WARP_SIZE;
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(temp_data, src + itr_idx);
-        copy_vector<uint8_t, ELEMENTS_PER_LDG_STG>(temp_mask, mask + itr_idx);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(src + itr_idx, temp_data);
+        copy<uint8_t, ELEMENTS_PER_LDG_STG>(mask + itr_idx, temp_mask);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (temp_mask[element] != 1) {
             elements[i][it + element] = (acc_t)temp_data[element] * scale;
           } else {
             elements[i][it + element] = -10000.0;
@@ -183,42 +114,42 @@
     max_value[i] = elements[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       max_value[i] =
           (max_value[i] > elements[i][it]) ? max_value[i] : elements[i][it];
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Max>(max_value);
+  warp_reduce<acc_t,ReduceType::kMax,WARP_BATCH,WARP_SIZE>(max_value);
 
   acc_t sum[WARP_BATCH]{0.0f};
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; ++it) {
       elements[i][it] = std::exp((elements[i][it] - max_value[i]));
       sum[i] += elements[i][it];
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
+  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
 
   // store result
   output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < element_count) {
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] = elements[i][it + element] / sum[i];
         }
-        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
-            dst + i * element_count + it * WARP_SIZE, out);
+        copy<output_t, ELEMENTS_PER_LDG_STG>(
+          out,  dst + i * element_count + it * WARP_SIZE);
       } else {
         break;
       }
     }
   }
 }
 
@@ -265,18 +196,18 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     int batch_element_count = (i >= local_batches) ? 0 : element_count;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < batch_element_count) {
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
-            temp_grad, grad + i * element_count + it * WARP_SIZE);
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
-            temp_output, output + i * element_count + it * WARP_SIZE);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(
+            grad + i * element_count + it * WARP_SIZE, temp_grad);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(
+            output + i * element_count + it * WARP_SIZE, temp_output);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           output_reg[i][it + element] = (acc_t)temp_output[element];
         }
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
@@ -292,15 +223,15 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     sum[i] = grad_reg[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       sum[i] += grad_reg[i][it];
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
+  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
 
 // store result
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
@@ -310,25 +241,25 @@
         output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] =
               (output_t)(scale * (grad_reg[i][it + element] -
                                   output_reg[i][it + element] * sum[i]));
         }
-        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
-            gradInput + i * element_count + it * WARP_SIZE, out);
+        copy<output_t, ELEMENTS_PER_LDG_STG>(
+          out, gradInput + i * element_count + it * WARP_SIZE);
       }
     }
   }
 }
-}  // end of anonymous namespace
+
 
 int get_batch_per_block(int query_seq_len, int key_seq_len, int batches,
                         int attn_heads) {
-  int log2_elements = log2_ceil(key_seq_len);
+  int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
   const int next_power_of_two = 1 << log2_elements;
 
   int warp_size =
       (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
   int batches_per_warp = (next_power_of_two <= 128) ? 2 : 1;
 
   constexpr int threads_per_block = 128;
@@ -345,15 +276,15 @@
                                             int query_seq_len, int key_seq_len,
                                             int batches, int attn_heads,
                                             int pad_batches) {
   TORCH_INTERNAL_ASSERT(key_seq_len >= 0 && key_seq_len <= 2048);
   if (key_seq_len == 0) {
     return;
   } else {
-    int log2_elements = log2_ceil(key_seq_len);
+    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
     const int next_power_of_two = 1 << log2_elements;
     int batch_count = batches * attn_heads * query_seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_forward.
     int warp_size =
         (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
@@ -445,15 +376,15 @@
                                              const acc_t scale,
                                              int query_seq_len, int key_seq_len,
                                              int batches, int attn_heads) {
   TORCH_INTERNAL_ASSERT(key_seq_len >= 0 && key_seq_len <= 2048);
   if (key_seq_len == 0) {
     return;
   } else {
-    int log2_elements = log2_ceil(key_seq_len);
+    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
     const int next_power_of_two = 1 << log2_elements;
     int batch_count = batches * attn_heads * query_seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_backward.
     int warp_size =
         (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
@@ -532,7 +463,71 @@
                 grad_input, grad, output, scale, batch_count, key_seq_len);
         break;
       default:
         break;
     }
   }
 }
+
+torch::Tensor fwd_cuda(torch::Tensor const& input, torch::Tensor const& mask,
+                       float scale_factor) {
+  // input is a 4d tensor with dimensions [batches, attn_heads, seq_len,
+  // seq_len]
+  const int batches = input.size(0);
+  const int pad_batches = mask.size(0);
+  const int attn_heads = input.size(1);
+  const int query_seq_len = input.size(2);
+  const int key_seq_len = input.size(3);
+  TORCH_INTERNAL_ASSERT(key_seq_len <= 2048);
+  TORCH_INTERNAL_ASSERT(query_seq_len > 1);
+  TORCH_INTERNAL_ASSERT(pad_batches == 1 || pad_batches == batches);
+  TORCH_INTERNAL_ASSERT(mask.size(1) == 1);
+  TORCH_INTERNAL_ASSERT(mask.size(2) == query_seq_len);
+  TORCH_INTERNAL_ASSERT(mask.size(3) == key_seq_len);
+
+  // Output
+  auto act_options = input.options().requires_grad(false);
+  torch::Tensor softmax_results = torch::empty(
+      {batches, attn_heads, query_seq_len, key_seq_len}, act_options);
+
+  // Softmax Intermediate Result Ptr
+  void* input_ptr = static_cast<void*>(input.data_ptr());
+  void* mask_ptr = static_cast<void*>(mask.data_ptr());
+  void* softmax_results_ptr = static_cast<void*>(softmax_results.data_ptr());
+
+  DISPATCH_HALF_AND_BFLOAT(
+      input.scalar_type(), "dispatch_scaled_masked_softmax_forward",
+      dispatch_scaled_masked_softmax_forward<scalar_t, scalar_t, float>(
+          reinterpret_cast<scalar_t*>(softmax_results_ptr),
+          reinterpret_cast<const scalar_t*>(input_ptr),
+          reinterpret_cast<const uint8_t*>(mask_ptr), scale_factor,
+          query_seq_len, key_seq_len, batches, attn_heads, pad_batches););
+  return softmax_results;
+}
+
+torch::Tensor bwd_cuda(torch::Tensor const& output_grads_,
+                       torch::Tensor const& softmax_results_,
+                       float scale_factor) {
+  auto output_grads = output_grads_.contiguous();
+  auto softmax_results = softmax_results_.contiguous();
+
+  // output grads is a 4d tensor with dimensions [batches, attn_heads, seq_len,
+  // seq_len]
+  const int batches = output_grads.size(0);
+  const int attn_heads = output_grads.size(1);
+  const int query_seq_len = output_grads.size(2);
+  const int key_seq_len = output_grads.size(3);
+
+  void* output_grads_ptr = static_cast<void*>(output_grads.data_ptr());
+
+  // Softmax Grad
+  DISPATCH_HALF_AND_BFLOAT(
+      output_grads_.scalar_type(), "dispatch_scaled_masked_softmax_backward",
+      dispatch_scaled_masked_softmax_backward<scalar_t, scalar_t, float>(
+          reinterpret_cast<scalar_t*>(output_grads_ptr),
+          reinterpret_cast<scalar_t*>(output_grads_ptr),
+          reinterpret_cast<scalar_t const*>(softmax_results.data_ptr()),
+          scale_factor, query_seq_len, key_seq_len, batches, attn_heads););
+
+  // backward pass is completely in-place
+  return output_grads;
+}
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp` & `colossalai-nightly-2024.6.1/extensions/pybind/softmax/scaled_masked_softmax.cpp`

 * *Files 20% similar despite different names*

```diff
@@ -2,53 +2,53 @@
  *     with minor changes. */
 
 #include <cuda_fp16.h>
 #include <torch/extension.h>
 
 #include <vector>
 
-namespace multihead_attn {
-namespace fused_softmax {
-namespace scaled_upper_triang_masked_softmax {
-
-torch::Tensor fwd_cuda(torch::Tensor const& input, float scale_factor);
+torch::Tensor fwd_cuda(torch::Tensor const& input, torch::Tensor const& mask,
+                       float scale_factor);
 
 torch::Tensor bwd_cuda(torch::Tensor const& output_grads,
                        torch::Tensor const& softmax_results,
                        float scale_factor);
 
-torch::Tensor fwd(torch::Tensor const& input, float scale_factor) {
-  AT_ASSERTM(input.dim() == 3, "expected 3D tensor");
+int get_batch_per_block(int query_seq_len, int key_seq_len, int batches,
+                        int attn_heads);
+
+torch::Tensor fwd(torch::Tensor const& input, torch::Tensor const& mask,
+                  float scale_factor) {
+  AT_ASSERTM(input.dim() == 4, "expected 4D tensor");
   AT_ASSERTM((input.scalar_type() == at::ScalarType::Half) ||
                  (input.scalar_type() == at::ScalarType::BFloat16),
              "Only fp16 and bf16 are supported");
+  AT_ASSERTM(mask.dim() == 4, "expected 4D tensor");
 
-  return fwd_cuda(input, scale_factor);
+  return fwd_cuda(input, mask, scale_factor);
 }
 
 torch::Tensor bwd(torch::Tensor const& output_grads,
                   torch::Tensor const& softmax_results, float scale_factor) {
-  AT_ASSERTM(output_grads.dim() == 3, "expected 3D tensor");
-  AT_ASSERTM(softmax_results.dim() == 3, "expected 3D tensor");
+  AT_ASSERTM(output_grads.dim() == 4, "expected 3D tensor");
+  AT_ASSERTM(softmax_results.dim() == 4, "expected 3D tensor");
 
   AT_ASSERTM((output_grads.scalar_type() == at::ScalarType::Half) ||
                  (output_grads.scalar_type() == at::ScalarType::BFloat16),
              "Only fp16 and bf16 are supported");
   AT_ASSERTM((softmax_results.scalar_type() == at::ScalarType::Half) ||
                  (softmax_results.scalar_type() == at::ScalarType::BFloat16),
              "Only fp16 and bf16 are supported");
 
   return bwd_cuda(output_grads, softmax_results, scale_factor);
 }
 
-}  // end namespace scaled_upper_triang_masked_softmax
-}  // end namespace fused_softmax
-}  // end namespace multihead_attn
-
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-  m.def("forward",
-        &multihead_attn::fused_softmax::scaled_upper_triang_masked_softmax::fwd,
+  m.def("forward", &fwd,
         "Self Multihead Attention scaled, time masked softmax -- Forward.");
-  m.def("backward",
-        &multihead_attn::fused_softmax::scaled_upper_triang_masked_softmax::bwd,
+
+  m.def("backward", &bwd,
         "Self Multihead Attention scaled, time masked softmax -- Backward.");
+
+  m.def("get_batch_per_block", &get_batch_per_block,
+        "Return Batch per block size.");
 }
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu`

 * *Files 10% similar despite different names*

```diff
@@ -1,128 +1,34 @@
 /*This code from NVIDIA Megatron:
  *     with minor changes. */
 
-#pragma once
-
+#include <ATen/ATen.h>
+#include <ATen/cuda/CUDAContext.h>
+#include <cuda.h>
+#include <cuda_fp16.h>
+#include <cuda_profiler_api.h>
+#include <cuda_runtime.h>
+#include <torch/extension.h>
 #include <assert.h>
 #include <c10/macros/Macros.h>
-#include <cuda_fp16.h>
 #include <stdint.h>
-
 #include <cfloat>
 #include <limits>
 
-namespace {
-
-template <typename Datatype, int ELEMENTS_PER_LDG>
-__device__ __inline__ void copy_vector(Datatype *dst, const Datatype *src);
-
-template <>
-__device__ __inline__ void copy_vector<c10::BFloat16, 1>(
-    c10::BFloat16 *dst, const c10::BFloat16 *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<c10::BFloat16, 4>(
-    c10::BFloat16 *dst, const c10::BFloat16 *src) {
-  *((float2 *)dst) = *((float2 *)src);
-}
-
-template <>
-__device__ __inline__ void copy_vector<c10::Half, 1>(c10::Half *dst,
-                                                     const c10::Half *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<c10::Half, 4>(c10::Half *dst,
-                                                     const c10::Half *src) {
-  *((float2 *)dst) = *((float2 *)src);
-}
-
-template <>
-__device__ __inline__ void copy_vector<uint8_t, 1>(uint8_t *dst,
-                                                   const uint8_t *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<uint8_t, 4>(uint8_t *dst,
-                                                   const uint8_t *src) {
-  *((half2 *)dst) = *((half2 *)src);
-}
-
-template <typename Datatype, int ELEMENTS_PER_LDG>
-__device__ __inline__ void copy_zero_vector(Datatype *dst);
-
-template <>
-__device__ __inline__ void copy_zero_vector<c10::BFloat16, 1>(
-    c10::BFloat16 *dst) {
-  *dst = 0.0;
-}
-
-template <>
-__device__ __inline__ void copy_zero_vector<c10::BFloat16, 4>(
-    c10::BFloat16 *dst) {
-  *((float2 *)dst) = make_float2(0.0f, 0.0f);
-}
-
-template <>
-__device__ __inline__ void copy_zero_vector<c10::Half, 1>(c10::Half *dst) {
-  *dst = 0.0;
-}
-
-template <>
-__device__ __inline__ void copy_zero_vector<c10::Half, 4>(c10::Half *dst) {
-  *((float2 *)dst) = make_float2(0.0f, 0.0f);
-}
-
-int log2_ceil(int value) {
-  int log2_value = 0;
-  while ((1 << log2_value) < value) ++log2_value;
-  return log2_value;
-}
-
-template <typename T>
-struct Add {
-  __device__ __forceinline__ T operator()(T a, T b) const { return a + b; }
-};
-
-template <typename T>
-struct Max {
-  __device__ __forceinline__ T operator()(T a, T b) const {
-    return a < b ? b : a;
-  }
-};
-
-template <typename T>
-__device__ __forceinline__ T
-WARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,
-                     unsigned int mask = 0xffffffff) {
-#if CUDA_VERSION >= 9000
-  return __shfl_xor_sync(mask, value, laneMask, width);
-#else
-  return __shfl_xor(value, laneMask, width);
-#endif
-}
-
-template <typename acc_t, int WARP_BATCH, int WARP_SIZE,
-          template <typename> class ReduceOp>
-__device__ __forceinline__ void warp_reduce(acc_t *sum) {
-  ReduceOp<acc_t> r;
-#pragma unroll
-  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
-#pragma unroll
-    for (int i = 0; i < WARP_BATCH; ++i) {
-      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);
-      sum[i] = r(sum[i], b);
-    }
-  }
-}
+#include "common/micros.h"
+#include "utils/vec_copy.h"
+#include "funcs/reduce_function.h"
+#include "funcs/unary_functor.h"
+
+using colossalAI::funcs::UnaryOpFunctor;
+using colossalAI::funcs::UnaryOpType;
+using colossalAI::funcs::warp_reduce;
+using colossalAI::funcs::ReduceType;
+using colossalAI::cuda::utils::copy;
+using colossalAI::cuda::utils::copy_zero;
 
 /*
  * Extended softmax (from native aten pytorch) with following additional
  * features 1) input scaling 2) Implicit time (diagonal masking)
  */
 template <typename input_t, typename output_t, typename acc_t,
           int log2_elements>
@@ -165,16 +71,16 @@
     int batch_element_count = (i >= local_batches) ? 0 : local_seq;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
 
       if (element_index < batch_element_count) {
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
-            temp_data, src + i * element_count * stride + it * WARP_SIZE);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(
+            src + i * element_count * stride + it * WARP_SIZE, temp_data);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if ((element_index + element) < batch_element_count) {
             elements[i][it + element] = (acc_t)temp_data[element] * scale;
           } else {
             elements[i][it + element] = -std::numeric_limits<acc_t>::infinity();
@@ -196,28 +102,29 @@
     max_value[i] = elements[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       max_value[i] =
           (max_value[i] > elements[i][it]) ? max_value[i] : elements[i][it];
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Max>(max_value);
+  warp_reduce<acc_t,ReduceType::kMax,WARP_BATCH,WARP_SIZE>(max_value);
 
   acc_t sum[WARP_BATCH]{0.0f};
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; ++it) {
       if (it < warp_iteration_limit) {
         elements[i][it] = std::exp((elements[i][it] - max_value[i]));
         sum[i] += elements[i][it];
       }
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
+  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
+
 
   // store result
   output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
@@ -229,18 +136,18 @@
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (element_index + element < local_seq) {
             out[element] = elements[i][it + element] / sum[i];
           } else {
             out[element] = 0;
           }
         }
-        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
-            dst + i * element_count * stride + it * WARP_SIZE, out);
+        copy<output_t, ELEMENTS_PER_LDG_STG>(
+            out, dst + i * element_count * stride + it * WARP_SIZE);
       } else if (element_index < element_count) {
-        copy_zero_vector<output_t, ELEMENTS_PER_LDG_STG>(
+        copy_zero<output_t, ELEMENTS_PER_LDG_STG>(
             dst + i * element_count * stride + it * WARP_SIZE);
       } else {
         break;
       }
     }
   }
 }
@@ -288,18 +195,18 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     int batch_element_count = (i >= local_batches) ? 0 : local_seq;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < batch_element_count) {
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
-            temp_grad, grad + i * element_count * stride + it * WARP_SIZE);
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
-            temp_output, output + i * element_count * stride + it * WARP_SIZE);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(
+            grad + i * element_count * stride + it * WARP_SIZE, temp_grad);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(
+            output + i * element_count * stride + it * WARP_SIZE, temp_output);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (element_index + element < batch_element_count) {
             output_reg[i][it + element] = (acc_t)temp_output[element];
           }
         }
@@ -319,15 +226,15 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     sum[i] = grad_reg[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       sum[i] += grad_reg[i][it];
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
+  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
 
 // store result
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
@@ -337,32 +244,30 @@
         output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] =
               (output_t)(scale * (grad_reg[i][it + element] -
                                   output_reg[i][it + element] * sum[i]));
         }
-        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
-            gradInput + i * element_count * stride + it * WARP_SIZE, out);
+        copy<output_t, ELEMENTS_PER_LDG_STG>(
+            out, gradInput + i * element_count * stride + it * WARP_SIZE);
       }
     }
   }
 }
 
-}  // end of anonymous namespace
-
 template <typename input_t, typename output_t, typename acc_t>
 void dispatch_scaled_upper_triang_masked_softmax_forward(
     output_t *dst, const input_t *src, const input_t scale,
     int softmax_elements, int softmax_elements_stride, int attn_batches) {
   TORCH_INTERNAL_ASSERT(softmax_elements >= 0 && softmax_elements <= 2048);
   if (softmax_elements == 0) {
     return;
   } else {
-    int log2_elements = log2_ceil(softmax_elements);
+    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(softmax_elements);
     const int next_power_of_two = 1 << log2_elements;
     int seq_len = softmax_elements;
     int batch_count = attn_batches * seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_forward.
     int warp_size =
@@ -479,15 +384,15 @@
     output_t *grad_input, input_t *grad, const input_t *output,
     const acc_t scale, int softmax_elements, int softmax_elements_stride,
     int attn_batches) {
   TORCH_INTERNAL_ASSERT(softmax_elements >= 0 && softmax_elements <= 2048);
   if (softmax_elements == 0) {
     return;
   } else {
-    int log2_elements = log2_ceil(softmax_elements);
+    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(softmax_elements);
     const int next_power_of_two = 1 << log2_elements;
     int seq_len = softmax_elements;
     int batch_count = attn_batches * seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_backward.
     int warp_size =
@@ -594,7 +499,65 @@
                 softmax_elements_stride, softmax_elements);
         break;
       default:
         break;
     }
   }
 }
+
+
+
+
+torch::Tensor fwd_cuda(torch::Tensor const& input, float scale_factor) {
+  // input is a 3d tensor with dimensions [attn_batches, seq_len, seq_len]
+  const int attn_batches = input.size(0);
+  const int seq_len = input.size(1);
+  TORCH_INTERNAL_ASSERT(seq_len <= 2048);
+
+  // Output
+  auto act_options = input.options().requires_grad(false);
+  torch::Tensor softmax_results =
+      torch::empty({attn_batches, seq_len, seq_len}, act_options);
+
+  // Softmax Intermediate Result Ptr
+  void* input_ptr = static_cast<void*>(input.data_ptr());
+  void* softmax_results_ptr = static_cast<void*>(softmax_results.data_ptr());
+
+  DISPATCH_HALF_AND_BFLOAT(
+      input.scalar_type(),
+      "dispatch_scaled_upper_triang_masked_softmax_forward",
+      dispatch_scaled_upper_triang_masked_softmax_forward<scalar_t, scalar_t,
+                                                          float>(
+          reinterpret_cast<scalar_t*>(softmax_results_ptr),
+          reinterpret_cast<const scalar_t*>(input_ptr), scale_factor, seq_len,
+          seq_len, attn_batches););
+  return softmax_results;
+}
+
+torch::Tensor bwd_cuda(torch::Tensor const& output_grads_,
+                       torch::Tensor const& softmax_results_,
+                       float scale_factor) {
+  auto output_grads = output_grads_.contiguous();
+  auto softmax_results = softmax_results_.contiguous();
+
+  // output grads is a 3d tensor with dimensions [attn_batches, seq_len,
+  // seq_len]
+  const int attn_batches = output_grads.size(0);
+  const int seq_len = output_grads.size(1);
+  TORCH_INTERNAL_ASSERT(output_grads.size(1) == output_grads.size(2));
+
+  void* output_grads_ptr = static_cast<void*>(output_grads.data_ptr());
+
+  // Softmax Grad
+  DISPATCH_HALF_AND_BFLOAT(
+      output_grads_.scalar_type(),
+      "dispatch_scaled_upper_triang_masked_softmax_backward",
+      dispatch_scaled_upper_triang_masked_softmax_backward<scalar_t, scalar_t,
+                                                           float>(
+          reinterpret_cast<scalar_t*>(output_grads_ptr),
+          reinterpret_cast<scalar_t*>(output_grads_ptr),
+          reinterpret_cast<scalar_t const*>(softmax_results.data_ptr()),
+          scale_factor, seq_len, seq_len, attn_batches););
+
+  // backward pass is completely in-place
+  return output_grads;
+}
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/scaled_softmax.py` & `colossalai-nightly-2024.6.1/colossalai/nn/layer/scaled_softmax.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,19 +4,21 @@
 import enum
 
 import torch
 import torch.nn as nn
 
 from colossalai.kernel.kernel_loader import ScaledMaskedSoftmaxLoader, ScaledUpperTriangleMaskedSoftmaxLoader
 
-try:
-    from colossalai._C import scaled_masked_softmax, scaled_upper_triang_masked_softmax
-except ImportError:
-    scaled_masked_softmax = None
-    scaled_upper_triang_masked_softmax = None
+# NOTE: These kernels are compiled on specific GPU arch and not widely applicable.
+# try:
+#     from colossalai._C import scaled_masked_softmax as scaled_masked_softmax, scaled_upper_triangle_masked_softmax_cuda as scaled_upper_triang_masked_softmax
+# except ImportError:
+
+scaled_masked_softmax = None
+scaled_upper_triang_masked_softmax = None
 
 
 class AttnMaskType(enum.Enum):
     padding = 1
     causal = 2
     paddedcausal = 3
 
@@ -181,10 +183,10 @@
 
         return probs
 
     def get_batch_per_block(self, sq, sk, b, np):
         # build and load kernel if not pre-built
         global scaled_masked_softmax
         if scaled_masked_softmax is None:
-            scaled_masked_softmax = ScaledMaskedSoftmaxBuilder().load()
+            scaled_masked_softmax = ScaledMaskedSoftmaxLoader().load()
 
         return scaled_masked_softmax.get_batch_per_block(sq, sk, b, np)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cuda_extension.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/cuda_extension.py`

 * *Files 13% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 
 class _CudaExtension(_CppExtension):
     @abstractmethod
     def nvcc_flags(self) -> List[str]:
         """
         This function should return a list of nvcc compilation flags for extensions.
         """
+        return ["-DCOLOSSAL_WITH_CUDA"]
 
     def is_available(self) -> bool:
         # cuda extension can only be built if cuda is available
         try:
             import torch
 
             cuda_available = torch.cuda.is_available()
@@ -49,14 +50,20 @@
         from torch.utils.cpp_extension import CUDA_HOME
 
         if CUDA_HOME is None:
             raise RuntimeError("CUDA_HOME is None, please set CUDA_HOME to compile C++/CUDA kernels in ColossalAI.")
         cuda_include = os.path.join(CUDA_HOME, "include")
         return cuda_include
 
+    def include_dirs(self) -> List[str]:
+        """
+        This function should return a list of include files for extensions.
+        """
+        return super().include_dirs() + [self.get_cuda_home_include()]
+
     def build_jit(self) -> None:
         from torch.utils.cpp_extension import CUDA_HOME, load
 
         set_cuda_arch_list(CUDA_HOME)
 
         # get build dir
         build_directory = _Extension.get_jit_extension_folder_path()
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_dao_cuda.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_dao_cuda.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ..base_extension import _Extension
+from ...base_extension import _Extension
 
 
 class FlashAttentionDaoCudaExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_dao_cuda", support_aot=False, support_jit=False, priority=10)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_npu.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_npu.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ..base_extension import _Extension
+from ...base_extension import _Extension
 
 
 class FlashAttentionNpuExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_npu", support_aot=False, support_jit=False)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_sdpa_cuda.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ..base_extension import _Extension
+from ...base_extension import _Extension
 
 
 class FlashAttentionSdpaCudaExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_sdpa_cuda", support_aot=False, support_jit=False)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/layernorm_cuda.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax_cuda.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,24 +1,28 @@
-from ..cuda_extension import _CudaExtension
-from ..utils import append_nvcc_threads, get_cuda_cc_flag
+from ...cuda_extension import _CudaExtension
+from ...utils import append_nvcc_threads
 
 
-class LayerNormCudaExtension(_CudaExtension):
+class ScaledMaskedSoftmaxCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="layernorm_cuda")
+        super().__init__(name="scaled_masked_softmax_cuda")
 
     def sources_files(self):
-        ret = [self.csrc_abs_path(fname) for fname in ["cuda/layer_norm_cuda.cpp", "cuda/layer_norm_cuda_kernel.cu"]]
-        return ret
-
-    def include_dirs(self):
-        ret = [self.get_cuda_home_include()]
+        ret = [self.csrc_abs_path(fname) for fname in ["kernel/cuda/scaled_masked_softmax_kernel.cu"]] + [
+            self.pybind_abs_path("softmax/scaled_masked_softmax.cpp")
+        ]
         return ret
 
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
-        extra_cuda_flags = ["-maxrregcount=50"]
-        extra_cuda_flags.extend(get_cuda_cc_flag())
-        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags + self.version_dependent_macros
+        extra_cuda_flags = [
+            "-std=c++14",
+            "-std=c++17",
+            "-U__CUDA_NO_HALF_OPERATORS__",
+            "-U__CUDA_NO_HALF_CONVERSIONS__",
+            "-U__CUDA_NO_HALF2_OPERATORS__",
+            "-DTHRUST_IGNORE_CUB_VERSION_CHECK",
+        ]
+        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags + super().nvcc_flags()
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/moe_cuda.py` & `colossalai-nightly-2024.6.1/extensions/pybind/softmax/scaled_masked_softmax_cuda.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,29 +1,28 @@
-from ..cuda_extension import _CudaExtension
-from ..utils import append_nvcc_threads, get_cuda_cc_flag
+from ...cuda_extension import _CudaExtension
+from ...utils import append_nvcc_threads
 
 
-class MoeCudaExtension(_CudaExtension):
+class ScaledMaskedSoftmaxCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="moe_cuda")
-
-    def include_dirs(self):
-        ret = [self.csrc_abs_path("cuda/include"), self.get_cuda_home_include()]
-        return ret
+        super().__init__(name="scaled_masked_softmax_cuda")
 
     def sources_files(self):
-        ret = [self.csrc_abs_path(fname) for fname in ["cuda/moe_cuda.cpp", "cuda/moe_cuda_kernel.cu"]]
+        ret = [self.csrc_abs_path(fname) for fname in ["kernel/cuda/scaled_masked_softmax_kernel.cu"]] + [
+            self.pybind_abs_path("softmax/scaled_masked_softmax.cpp")
+        ]
         return ret
 
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
         extra_cuda_flags = [
+            "-std=c++14",
+            "-std=c++17",
             "-U__CUDA_NO_HALF_OPERATORS__",
             "-U__CUDA_NO_HALF_CONVERSIONS__",
-            "--expt-relaxed-constexpr",
-            "--expt-extended-lambda",
+            "-U__CUDA_NO_HALF2_OPERATORS__",
+            "-DTHRUST_IGNORE_CUB_VERSION_CHECK",
         ]
-        extra_cuda_flags.extend(get_cuda_cc_flag())
-        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags
+        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags + super().nvcc_flags()
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_masked_softmax_cuda.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,32 +1,30 @@
-from ..cuda_extension import _CudaExtension
-from ..utils import append_nvcc_threads
+from ...cuda_extension import _CudaExtension
+from ...utils import append_nvcc_threads, get_cuda_cc_flag
 
 
-class ScaledMaskedSoftmaxCudaExtension(_CudaExtension):
+class ScaledUpperTriangleMaskedSoftmaxCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="scaled_masked_softmax_cuda")
+        super().__init__(name="scaled_upper_triangle_masked_softmax_cuda")
 
     def sources_files(self):
         ret = [
             self.csrc_abs_path(fname)
-            for fname in ["cuda/scaled_masked_softmax.cpp", "cuda/scaled_masked_softmax_cuda.cu"]
-        ]
+            for fname in [
+                "kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu",
+            ]
+        ] + [self.pybind_abs_path("softmax/scaled_upper_triang_masked_softmax.cpp")]
         return ret
 
-    def include_dirs(self):
-        return [self.get_cuda_home_include()]
-
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
         extra_cuda_flags = [
-            "-std=c++14",
-            "-std=c++17",
             "-U__CUDA_NO_HALF_OPERATORS__",
             "-U__CUDA_NO_HALF_CONVERSIONS__",
-            "-U__CUDA_NO_HALF2_OPERATORS__",
-            "-DTHRUST_IGNORE_CUB_VERSION_CHECK",
+            "--expt-relaxed-constexpr",
+            "--expt-extended-lambda",
         ]
-        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags
+        extra_cuda_flags.extend(get_cuda_cc_flag())
+        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags + super().nvcc_flags()
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/pybind/layernorm/layernorm_cuda.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,34 +1,26 @@
-from ..cuda_extension import _CudaExtension
-from ..utils import append_nvcc_threads, get_cuda_cc_flag
+from ...cuda_extension import _CudaExtension
+from ...utils import append_nvcc_threads, get_cuda_cc_flag
 
 
-class ScaledUpperTriangleMaskedSoftmaxCudaExtension(_CudaExtension):
+class LayerNormCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="scaled_upper_triangle_masked_softmax_cuda")
-
-    def include_dirs(self):
-        return [self.get_cuda_home_include()]
+        super().__init__(name="layernorm_cuda")
 
     def sources_files(self):
-        ret = [
-            self.csrc_abs_path(fname)
-            for fname in [
-                "cuda/scaled_upper_triang_masked_softmax.cpp",
-                "cuda/scaled_upper_triang_masked_softmax_cuda.cu",
-            ]
+        ret = [self.csrc_abs_path(fname) for fname in ["kernel/cuda/layer_norm_kernel.cu"]] + [
+            self.pybind_abs_path("layernorm/layer_norm.cpp")
         ]
         return ret
 
+    def include_dirs(self):
+        ret = [self.get_cuda_home_include()] + [self.csrc_abs_path("")]
+        return ret
+
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
-        extra_cuda_flags = [
-            "-U__CUDA_NO_HALF_OPERATORS__",
-            "-U__CUDA_NO_HALF_CONVERSIONS__",
-            "--expt-relaxed-constexpr",
-            "--expt-extended-lambda",
-        ]
+        extra_cuda_flags = ["-maxrregcount=50"]
         extra_cuda_flags.extend(get_cuda_cc_flag())
-        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags
+        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags + self.version_dependent_macros + super().nvcc_flags()
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/triton_extension.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/triton_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/utils.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/extensions/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_dropout_add.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/jit/bias_dropout_add.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_gelu.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/jit/bias_gelu.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/jit/option.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/jit/option.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/kernel_loader.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/kernel_loader.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,27 +4,29 @@
 from .extensions import (
     CpuAdamArmExtension,
     CpuAdamX86Extension,
     FlashAttentionDaoCudaExtension,
     FlashAttentionNpuExtension,
     FlashAttentionSdpaCudaExtension,
     FusedOptimizerCudaExtension,
+    InferenceOpsCudaExtension,
     LayerNormCudaExtension,
     MoeCudaExtension,
     ScaledMaskedSoftmaxCudaExtension,
     ScaledUpperTriangleMaskedSoftmaxCudaExtension,
 )
 from .extensions.base_extension import _Extension
 
 __all__ = [
     "KernelLoader",
     "CPUAdamLoader",
     "LayerNormLoader",
     "MoeLoader",
     "FusedOptimizerLoader",
+    "InferenceOpsLoader",
     "ScaledMaskedSoftmaxLoader",
     "ScaledUpperTriangleMaskedSoftmaxLoader",
 ]
 
 
 class KernelLoader:
     """
@@ -93,14 +95,18 @@
     REGISTRY = [MoeCudaExtension]
 
 
 class FusedOptimizerLoader(KernelLoader):
     REGISTRY = [FusedOptimizerCudaExtension]
 
 
+class InferenceOpsLoader(KernelLoader):
+    REGISTRY = [InferenceOpsCudaExtension]
+
+
 class ScaledMaskedSoftmaxLoader(KernelLoader):
     REGISTRY = [ScaledMaskedSoftmaxCudaExtension]
 
 
 class ScaledUpperTriangleMaskedSoftmaxLoader(KernelLoader):
     REGISTRY = [ScaledUpperTriangleMaskedSoftmaxCudaExtension]
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/triton/gptq_triton.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/triton/flash_decoding.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,543 +1,533 @@
-# Adapted from AutoGPTQ auto_gptq: https://github.com/PanQiWei/AutoGPTQ
-
+# Applying Flash-Decoding as descibed in
+# https://pytorch.org/blog/flash-decoding/
+# by Tri Dao, 2023
 import torch
 import triton
 import triton.language as tl
 
-from .custom_autotune import autotune, matmul248_kernel_config_pruner
-
-
-@triton.jit
-def tanh(x):
-    # Tanh is just a scaled sigmoid
-    return 2 * tl.sigmoid(2 * x) - 1
-
 
+# Triton 2.1.0
 @triton.jit
-def cosh(x):
-    exp_x = tl.exp(x)
-    return (exp_x + 1.0 / exp_x) * 0.5
-
-
-# a Triton implementation of the most used activations
-# See for instance http://arxiv.org/abs/1606.08415 for an overview
-
-
-# ReLU
-@triton.jit
-def relu(x):
-    """
-    ReLU_ activation function
-
-    .. _ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html
-    """
-    return tl.where(x >= 0, x, 0.0)
-
-
-@triton.jit
-def squared_relu(x):
-    """
-    Squared ReLU activation, as proposed in the Primer_ paper.
-
-    .. _Primer: https://arxiv.org/abs/2109.08668
-    """
-    x_sq = x * x
-    return tl.where(x > 0.0, x_sq, 0.0)
-
-
-@triton.jit
-def star_relu(x):
-    """
-    Star ReLU activation, as proposed in the "MetaFormer Baselines for Vision"_ paper.
-
-    .. _ "MetaFormer Baselines for Vision": https://arxiv.org/pdf/2210.13452.pdf
-    """
-    x_sq = x * x
-    return 0.8944 * tl.where(x > 0.0, x_sq, 0.0) - 0.4472
-
-
-# Leaky ReLU
-@triton.jit
-def leaky_relu(x):
-    """
-    LeakyReLU_ activation
-
-    .. _LeakyReLU: https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html
-    """
-    return tl.where(x >= 0.0, x, 0.01 * x)
-
-
-@triton.jit
-def gelu(x):
-    """
-    GeLU_ activation - Gaussian error linear unit
+def _flash_decoding_fwd_kernel(
+    Q,  # [batch_size * q_len, head_num, head_dim]
+    KCache,  # [num_blocks, num_kv_heads, block_size, head_dim]
+    VCache,  # [num_blocks, num_kv_heads, block_size, head_dim],
+    # or [num_blocks, num_kv_heads, head_dim//x, block_size, x], depends on strides provided
+    block_tables,  # [batch_size, max_blocks_per_sequence]
+    mid_o,  # [batch_size * q_len, head_num, kv_split_num, head_dim]
+    mid_o_lse,  # [batch_size * q_len, head_num, kv_split_num]
+    kv_seq_len,  # [batch_size]
+    q_len,
+    batch_size,
+    kv_group_num,
+    x,
+    sm_scale,
+    stride_qt,
+    stride_qh,
+    stride_qd,
+    stride_kcb,
+    stride_kch,
+    stride_kcsplit_x,
+    stride_kcs,
+    stride_kcd,
+    stride_vcb,
+    stride_vch,
+    stride_vcs,
+    stride_vcd,
+    stride_bts,
+    stride_btb,
+    stride_mid_ot,
+    stride_mid_oh,
+    stride_mid_ob,
+    stride_mid_od,
+    stride_mid_o_lset,
+    stride_mid_o_lseh,
+    stride_mid_o_lseb,
+    BLOCK_KV: tl.constexpr,
+    BLOCK_SIZE: tl.constexpr,
+    HEAD_DIM: tl.constexpr,
+):
+    cur_token_idx = tl.program_id(0)
+    cur_seq_idx = cur_token_idx // q_len
+    if cur_seq_idx >= batch_size:
+        return
+    cur_token_off = (cur_token_idx % q_len) - q_len + 1
+    cur_head_idx = tl.program_id(1)
+    block_start_kv = tl.program_id(2)  # for splitting k/v
+
+    # NOTE It requires BLOCK_KV and BLOCK_SIZE to be the same
+    # TODO might want to replace with BLOCK_KV % BLOCK_SIZE == 0 (optimize BLOCK_KV as multiple of BLOCK_SIZE)
+    #      and then support calculating multiple kv cache blocks on an instance
+    tl.static_assert(BLOCK_KV == BLOCK_SIZE)
+    # get the current (kv) sequence length
+    # cur_token_off is used as a "mask" here for spec-dec during verification process
+    cur_kv_seq_len = tl.load(kv_seq_len + cur_seq_idx) + cur_token_off
+    if block_start_kv * BLOCK_KV >= cur_kv_seq_len:
+        return
+    offsets_dmodel = tl.arange(0, HEAD_DIM)
+    offsets_block = tl.arange(0, BLOCK_SIZE)
+
+    # block table for the current sequence
+    block_table_ptr = block_tables + cur_seq_idx * stride_bts
+    # cur_bt_start_idx = block_start_kv * (BLOCK_KV // BLOCK_SIZE)
+    # cur_block_id = tl.load(block_table_ptr + cur_bt_start_idx * stride_btb)
+    cur_block_id = tl.load(block_table_ptr + block_start_kv * stride_btb)
+    cur_occupied_size = tl.where(
+        (block_start_kv + 1) * BLOCK_SIZE <= cur_kv_seq_len, BLOCK_SIZE, cur_kv_seq_len - block_start_kv * BLOCK_SIZE
+    )
+    tl.device_assert(cur_occupied_size >= 0)
 
-    .. _GeLU: https://arxiv.org/pdf/1606.08415.pdf
-    """
-    return 0.5 * x * (1 + tanh(_kAlpha * (x + 0.044715 * x * x * x)))
+    offsets_q = cur_token_idx * stride_qt + cur_head_idx * stride_qh + offsets_dmodel * stride_qd
+    q = tl.load(Q + offsets_q)
+    cur_kv_head_idx = cur_head_idx // kv_group_num
+    offset_kvcache = cur_block_id * stride_kcb + cur_kv_head_idx * stride_kch
+    offsets_k = (
+        offset_kvcache
+        + (offsets_dmodel[None, :] // x) * stride_kcsplit_x
+        + (offsets_dmodel[None, :] % x) * stride_kcd
+        + offsets_block[:, None] * stride_kcs
+    )
+    k_cur_block = tl.load(KCache + offsets_k)
+    V_block_ptr = tl.make_block_ptr(
+        base=VCache + offset_kvcache,
+        shape=(cur_occupied_size, HEAD_DIM),
+        strides=(stride_vcs, stride_vcd),
+        offsets=(0, 0),
+        block_shape=(BLOCK_SIZE, HEAD_DIM),
+        order=(0, 1),
+    )
+    v_cur_block = tl.load(V_block_ptr)
+    acc = tl.zeros([HEAD_DIM], dtype=tl.float32)
+    # use block size of the paged/blocked kv cache
+    S_ij = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
+
+    # NOTE a trick to come across triton's requirement that values in both first and second input shapes must be >= 16,
+    # Multiplying two tensors with shapes [1, d] * [d, block_size] will fail.
+    # Refer to https://github.com/openai/triton/discussions/895
+    S_ij += tl.sum(q[None, :] * k_cur_block, 1)
+    S_ij *= sm_scale
+    S_ij += tl.where(block_start_kv * BLOCK_KV + offsets_block < cur_kv_seq_len, 0, float("-inf"))
+
+    m = tl.max(S_ij, 0)
+    S_ij -= m
+    p_ij_hat = tl.exp(S_ij)
+    l_i = tl.sum(p_ij_hat, 0)
+    p_ij_hat = p_ij_hat.to(v_cur_block.type.element_ty)
+    acc += tl.sum(v_cur_block * p_ij_hat[:, None], 0)
+    acc = acc / l_i
+
+    offsets_mid_o = (
+        cur_token_idx * stride_mid_ot
+        + cur_head_idx * stride_mid_oh
+        + block_start_kv * stride_mid_ob
+        + offsets_dmodel * stride_mid_od
+    )
+    tl.store(mid_o + offsets_mid_o, acc)
+    offsets_mid_o_lse = (
+        cur_token_idx * stride_mid_o_lset + cur_head_idx * stride_mid_o_lseh + block_start_kv * stride_mid_o_lseb
+    )
+    # logsumexp l_i^(j) = m^(j) + log(l_i^(j))
+    tl.store(mid_o_lse + offsets_mid_o_lse, m + tl.log(l_i))
 
 
+# Triton 2.1.0
 @triton.jit
-def smelu(x):
-    """
-    SmeLU_ activation -  Smooth ReLU with beta=2.0
-
-    .. _SmeLU: https://arxiv.org/pdf/2202.06499.pdf
-    """
-    beta = 2.0
-
-    relu = tl.where(x >= beta, x, 0.0)
-    return tl.where(tl.abs(x) <= beta, (x + beta) * (x + beta) / (4.0 * beta), relu)
-
+def _alibi_flash_decoding_fwd_kernel(
+    Q,  # [batch_size * q_len, head_num, head_dim]
+    KCache,  # [num_blocks, num_kv_heads, block_size, head_dim]
+    VCache,  # [num_blocks, num_kv_heads, block_size, head_dim]
+    block_tables,  # [batch_size, max_blocks_per_sequence]
+    mid_o,  # [batch_size * q_len, head_num, kv_split_num, head_dim]
+    mid_o_lse,  # [batch_size * q_len, head_num, kv_split_num]
+    kv_seq_len,  # [batch_size]
+    q_len,
+    batch_size,
+    alibi_slopes,
+    stride_qt,
+    stride_qh,
+    stride_qd,
+    stride_cacheb,
+    stride_cacheh,
+    stride_cachebs,
+    stride_cached,
+    stride_bts,
+    stride_btb,
+    stride_mid_ot,
+    stride_mid_oh,
+    stride_mid_ob,
+    stride_mid_od,
+    stride_mid_o_lset,
+    stride_mid_o_lseh,
+    stride_mid_o_lseb,
+    sm_scale,
+    KV_GROUPS: tl.constexpr,
+    BLOCK_KV: tl.constexpr,
+    BLOCK_SIZE: tl.constexpr,
+    HEAD_DIM: tl.constexpr,
+):
+    cur_token_idx = tl.program_id(0)
+    cur_seq_idx = cur_token_idx // q_len
+    if cur_seq_idx >= batch_size:
+        return
+    cur_token_off = (cur_token_idx % q_len) - q_len + 1
+    cur_head_idx = tl.program_id(1)
+    block_start_kv = tl.program_id(2)  # for splitting k/v
+
+    # NOTE It requires BLOCK_KV and BLOCK_SIZE to be the same
+    # TODO might want to replace with BLOCK_KV % BLOCK_SIZE == 0 (optimize BLOCK_KV as multiple of BLOCK_SIZE)
+    #      and then support calculating multiple kv cache blocks on an instance
+    tl.static_assert(BLOCK_KV == BLOCK_SIZE)
+    # get the current (kv) sequence length
+    # cur_token_off is used as a "mask" here for spec-dec during verification process
+    cur_kv_seq_len = tl.load(kv_seq_len + cur_seq_idx) + cur_token_off
+    if block_start_kv * BLOCK_KV >= cur_kv_seq_len:
+        return
+
+    offsets_dmodel = tl.arange(0, HEAD_DIM)
+    offsets_q = cur_token_idx * stride_qt + cur_head_idx * stride_qh + offsets_dmodel * stride_qd
+    q = tl.load(Q + offsets_q)
+    # block table for the current sequence
+    block_table_ptr = block_tables + cur_seq_idx * stride_bts
+    # cur_bt_start_idx = block_start_kv * (BLOCK_KV // BLOCK_SIZE)
+    # cur_block_id = tl.load(block_table_ptr + cur_bt_start_idx * stride_btb)
+    cur_block_id = tl.load(block_table_ptr + block_start_kv * stride_btb)
+    cur_occupied_size = tl.where(
+        (block_start_kv + 1) * BLOCK_SIZE <= cur_kv_seq_len, BLOCK_SIZE, cur_kv_seq_len - block_start_kv * BLOCK_SIZE
+    )
+    tl.device_assert(cur_occupied_size >= 0)
 
-@triton.jit
-def silu(x):
-    return x * tl.sigmoid(x)
+    cur_kv_head_idx = cur_head_idx // KV_GROUPS
+    offset_kvcache = cur_block_id * stride_cacheb + cur_kv_head_idx * stride_cacheh
+    K_block_ptr = tl.make_block_ptr(
+        base=KCache + offset_kvcache,
+        shape=(cur_occupied_size, HEAD_DIM),
+        strides=(stride_cachebs, stride_cached),
+        offsets=(0, 0),
+        block_shape=(BLOCK_SIZE, HEAD_DIM),
+        order=(0, 1),
+    )
+    V_block_ptr = tl.make_block_ptr(
+        base=VCache + offset_kvcache,
+        shape=(cur_occupied_size, HEAD_DIM),
+        strides=(stride_cachebs, stride_cached),
+        offsets=(0, 0),
+        block_shape=(BLOCK_SIZE, HEAD_DIM),
+        order=(0, 1),
+    )
+    k_cur_block = tl.load(K_block_ptr)
+    v_cur_block = tl.load(V_block_ptr)
+    acc = tl.zeros([HEAD_DIM], dtype=tl.float32)
+    # use block size of the paged/blocked kv cache
+    S_ij = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
+
+    alibi_slope = tl.load(alibi_slopes + cur_head_idx)
+    position_k_offset = block_start_kv * BLOCK_KV + tl.arange(0, BLOCK_SIZE)
+
+    # NOTE a trick to come across triton's requirement that values in both first and second input shapes must be >= 16,
+    # Multiplying two tensors with shapes [1, d] * [d, block_size] will fail.
+    # Refer to https://github.com/openai/triton/discussions/895
+    S_ij += tl.sum(q[None, :] * k_cur_block, 1)
+    S_ij *= sm_scale
+    S_ij -= alibi_slope * (cur_kv_seq_len - 1 - position_k_offset)
+    S_ij = tl.where(cur_kv_seq_len > position_k_offset, S_ij, float("-inf"))
+
+    m = tl.max(S_ij, 0)
+    S_ij -= m
+    p_ij_hat = tl.exp(S_ij)
+    l_i = tl.sum(p_ij_hat, 0)
+    p_ij_hat = p_ij_hat.to(v_cur_block.type.element_ty)
+    acc += tl.sum(v_cur_block * p_ij_hat[:, None], 0)
+    acc = acc / l_i
+
+    offsets_mid_o = (
+        cur_token_idx * stride_mid_ot
+        + cur_head_idx * stride_mid_oh
+        + block_start_kv * stride_mid_ob
+        + offsets_dmodel * stride_mid_od
+    )
+    tl.store(mid_o + offsets_mid_o, acc)
+    offsets_mid_o_lse = (
+        cur_token_idx * stride_mid_o_lset + cur_head_idx * stride_mid_o_lseh + block_start_kv * stride_mid_o_lseb
+    )
+    # logsumexp l_i^(j) = m^(j) + log(l_i^(j))
+    tl.store(mid_o_lse + offsets_mid_o_lse, m + tl.log(l_i))
 
 
-@autotune(
-    configs=[
-        triton.Config(
-            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 256, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 32, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 64, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=2, num_warps=8
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 64, "BLOCK_SIZE_K": 64, "GROUP_SIZE_M": 8}, num_stages=3, num_warps=8
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 32, "BLOCK_SIZE_N": 32, "BLOCK_SIZE_K": 128, "GROUP_SIZE_M": 8}, num_stages=2, num_warps=4
-        ),
-    ],
-    key=["M", "N", "K"],
-    nearest_power_of_two=True,
-    prune_configs_by={
-        "early_config_prune": matmul248_kernel_config_pruner,
-        "perf_model": None,
-        "top_k": None,
-    },
-)
+# Triton 2.1.0
 @triton.jit
-def cai_gptq_matmul_248_kernel(
-    a_ptr,
-    b_ptr,
-    c_ptr,
-    scales_ptr,
-    zeros_ptr,
-    bias_ptr,
-    residual_ptr,
-    M,
-    N,
-    K,
-    bits,
-    maxq,
-    gptq_group_size,
-    stride_am,
-    stride_ak,
-    stride_bk,
-    stride_bn,
-    stride_cm,
-    stride_cn,
-    stride_scales,
-    stride_zeros,
-    QKV_FUSED: tl.constexpr,
-    ADD_BIAS: tl.constexpr,
-    ADD_RESIDUAL: tl.constexpr,
-    ACT_TYPE: tl.constexpr,
-    BLOCK_SIZE_M: tl.constexpr,
-    BLOCK_SIZE_N: tl.constexpr,
-    BLOCK_SIZE_K: tl.constexpr,
-    GROUP_SIZE_M: tl.constexpr,
+def _flash_decoding_fwd_reduce_kernel(
+    mid_o,  # [batch_size, head_num, kv_split_num, head_dim]
+    mid_o_lse,  # [batch_size, head_num, kv_split_num]
+    O,  # [batch_size, num_heads, head_dim] or [batch_size, 1, num_heads, head_dim]
+    kv_seq_len,
+    q_len,
+    batch_size,
+    stride_mid_ot,
+    stride_mid_oh,
+    stride_mid_ob,
+    stride_mid_od,
+    stride_o_lset,
+    stride_o_lseh,
+    stride_o_lseb,
+    stride_ot,
+    stride_oh,
+    stride_od,
+    BLOCK_KV: tl.constexpr,
+    HEAD_DIM: tl.constexpr,
+):
+    cur_token_idx = tl.program_id(0)
+    cur_seq_idx = cur_token_idx // q_len
+    if cur_seq_idx >= batch_size:
+        return
+    cur_head_idx = tl.program_id(1)
+
+    # cur_token_off is used as a "mask" here for spec-dec during verification process
+    cur_token_off = (cur_token_idx % q_len) - q_len + 1
+    cur_kv_seq_len = tl.load(kv_seq_len + cur_seq_idx) + cur_token_off
+    offsets_dmodel = tl.arange(0, HEAD_DIM)
+
+    # NOTE currently the block size BLOCK_KV splitting kv is relatively small as we have
+    # BLOCK_KV == BLOCK_SIZE for now. We might want to decrease the number of blocks of kv splitted.
+    kv_split_num = (cur_kv_seq_len + BLOCK_KV - 1) // BLOCK_KV
+    m_i = float("-inf")  # max logic
+    l_i = 0.0  # sum exp
+    acc = tl.zeros([HEAD_DIM], dtype=tl.float32)
+
+    offsets_mid_o = cur_token_idx * stride_mid_ot + cur_head_idx * stride_mid_oh + offsets_dmodel
+    offset_mid_lse = cur_token_idx * stride_o_lset + cur_head_idx * stride_o_lseh
+    for block_i in range(0, kv_split_num, 1):
+        mid_o_block = tl.load(mid_o + offsets_mid_o + block_i * stride_mid_ob)
+        lse = tl.load(mid_o_lse + offset_mid_lse + block_i * stride_o_lseb)
+        m_ij = tl.maximum(m_i, lse)
+        scale = tl.exp(m_i - m_ij)
+        acc = acc * scale
+        lse -= m_ij
+        exp_logic = tl.exp(lse)
+        acc += exp_logic * mid_o_block
+        l_i = scale * l_i + exp_logic
+        m_i = m_ij
+
+    acc = acc / l_i
+    offsets_O = cur_token_idx * stride_ot + cur_head_idx * stride_oh + offsets_dmodel
+    tl.store(O + offsets_O, acc.to(O.type.element_ty))
+    return
+
+
+# Decoding Stage
+# Used with blocked KV Cache (PagedAttention)
+def flash_decoding_attention(
+    q: torch.Tensor,
+    k_cache: torch.Tensor,
+    v_cache: torch.Tensor,
+    kv_seq_len: torch.Tensor,
+    block_tables: torch.Tensor,
+    block_size: int,
+    max_seq_len_in_batch: int = None,
+    output: torch.Tensor = None,
+    mid_output: torch.Tensor = None,
+    mid_output_lse: torch.Tensor = None,
+    alibi_slopes: torch.Tensor = None,
+    sm_scale: int = None,
+    kv_group_num: int = 1,
+    q_len: int = 1,  # NOTE alibi flash decoding does not support q_len > 1 at this moment.
+    use_new_kcache_layout: bool = False,
 ):
     """
-    Compute the matrix multiplication C = A x B.
-    A is of shape (M, K) float16
-    B is of shape (K//8, N) int32
-    C is of shape (M, N) float16
-    scales is of shape (G, N) float16
-    zeros is of shape (G, N) float16
-    """
-    infearure_per_bits = 32 // bits
+    Flash decoding implemented with a blocked KV Cache (PagedAttention) during decoding stage.
 
-    pid = tl.program_id(axis=0)
-    NK = K
+    Args:
+        q (torch.Tensor): [bsz * q_len, num_heads, head_dim]
+            q_len > 1 only for verification process in speculative-decoding.
+        k_cache (torch.Tensor): [num_blocks, num_kv_heads, block_size, head_dim]
+        v_cache (torch.Tensor): [num_blocks, num_kv_heads, block_size, head_dim]
+        kv_seq_len (torch.Tensor): [batch_size]
+            records the (kv) sequence lengths incorporating past kv sequence lengths.
+        block_tables (torch.Tensor): [batch_size, max_blocks_per_sequence]
+        max_seq_len_in_batch (int): Maximum sequence length in the batch.
+        output (torch.Tensor):  [bsz, num_heads * head_dim]
+        mid_output (torch.Tensor): [max_bsz * q_len, num_heads, kv_max_split_num, head_dim]
+            Intermediate output tensor. `max_bsz` should be greater than or equal to `bsz`.
+            q_len > 1 only for verification process in speculative-decoding.
+        mid_output_lse (torch.Tensor): [max_bsz * q_len, num_heads, kv_max_split_num]
+            Log-sum-exp of intermediate output. `max_bsz` should be greater than or equal to `bsz`.
+            q_len > 1 only for verification process in speculative-decoding.
+        alibi_slopes (torch.Tensor): [num_heads] alibi slopes used for alibi flash decoding.
+        block_size (int): Size of each block in the blocked key/value cache.
+        num_kv_group (int, optional): Number of key/value groups. Defaults to 1.
+        q_length (int): Query length. Use for speculative decoding when `q_length` > 1 (i.e. the last n tokens).
+            Defaults to 1.
+        use_new_kcache_layout (bool): Whether to use the new kcache layout. Defaults to False.
+
+    Returns:
+        Output tensor with shape [bsz * q_len, num_heads * head_dim]
+    """
+    q = q.squeeze() if q.dim() == 4 else q
+    assert q.dim() == 3, f"Incompatible q dim: {q.dim()}"
+    n_tokens, num_heads, head_dim = q.shape
+    assert n_tokens % q_len == 0, "Invalid q_len"
+    bsz = n_tokens // q_len
+
+    assert head_dim in {32, 64, 128, 256}
+    assert kv_seq_len.shape[0] == block_tables.shape[0] == bsz, (
+        f"Got incompatible batch size (number of seqs):\n"
+        f"  KV seq lengths bsz {kv_seq_len.size(0)}, Block tables bsz {block_tables.size(0)}, "
+        f"batch size {bsz}"
+    )
+    assert k_cache.size(-2) == v_cache.size(-2) == block_size, (
+        f"Got incompatible block size on kv caches:\n"
+        f"  assigned block_size {block_size}, k_cache block_size {k_cache.size(-2)}, "
+        f"v_cache block_size {v_cache.size(-2)}"
+    )
 
-    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
-    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
-    num_pid_k = tl.cdiv(NK, BLOCK_SIZE_K)
-    qkv_offset = pid // (num_pid_m * num_pid_n)
-    pid = pid % (num_pid_m * num_pid_n)
-    num_pid_in_group = GROUP_SIZE_M * num_pid_n
-    group_id = pid // num_pid_in_group
-    first_pid_m = group_id * GROUP_SIZE_M
-    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
-    pid_m = first_pid_m + (pid % group_size_m)
-    pid_n = (pid % num_pid_in_group) // group_size_m
-
-    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
-    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
-    offs_k = tl.arange(0, BLOCK_SIZE_K)
-    # offs_bk = offs_k + qkv_offset * NK
-    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
-
-    a_mask = offs_am[:, None] < M
-    # b_ptrs is set up such that it repeats elements along the K axis 8 times
-    b_ptrs = (
-        b_ptr
-        + qkv_offset * N * NK // infearure_per_bits
-        + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)
-    )  # (BLOCK_SIZE_K, BLOCK_SIZE_N)
-    # g_ptrs = g_ptr + offs_k
-    # shifter is used to extract the N bits of each element in the 32-bit word from B
-    scales_ptrs = scales_ptr + qkv_offset * NK * N // gptq_group_size + offs_bn[None, :]
-    zeros_ptrs = (
-        zeros_ptr
-        + qkv_offset * NK * N // gptq_group_size // infearure_per_bits
-        + (offs_bn[None, :] // infearure_per_bits)
-    )
-
-    shifter = (offs_k % infearure_per_bits) * bits
-    zeros_shifter = (offs_bn % infearure_per_bits) * bits
-    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
-    g_idx_base = tl.arange(0, BLOCK_SIZE_K)
-    g_idx_base = g_idx_base // gptq_group_size
-    g_idx = g_idx_base
-    # tl.device_print("gidx, ", g_idx)
-
-    scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
-    zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
-    zeros = (zeros >> zeros_shifter[None, :]) & maxq
-    zeros = zeros + 1
-
-    for k in range(0, num_pid_k):
-        # g_idx = tl.load(g_ptrs)
-        # if (k + 1) * BLOCK_SIZE_K > currend_group_end:
-        scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
-        zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
-        zeros = (zeros >> zeros_shifter[None, :]) & maxq
-        zeros = zeros + 1
-        # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop
-        a = tl.load(a_ptrs, mask=a_mask, other=0.0)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
-        b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated
-        # Now we need to unpack b (which is N-bit values) into 32-bit values
-        b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values
-        b = (b - zeros).to(tl.float16) * scales  # Scale and shift
-        accumulator += tl.dot(a, b)
-
-        a_ptrs += BLOCK_SIZE_K
-        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk
-        g_idx = g_idx_base + ((k + 1) * BLOCK_SIZE_K) // gptq_group_size
-        # if (k + 2) * BLOCK_SIZE_K > currend_group_end:
-
-    c_ptrs = c_ptr + qkv_offset * M * N + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]
-    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)
-
-    if ADD_BIAS:
-        bias_mask = offs_bn < N
-        offs_bn += qkv_offset * N
-        bias_ptrs = bias_ptr + stride_cn * offs_bn
-        bias = tl.load(bias_ptrs, mask=bias_mask, other=0.0)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
-        accumulator += bias[None, :]
-
-    if ACT_TYPE == 1:
-        accumulator = relu(accumulator)
-    elif ACT_TYPE == 2:
-        accumulator = gelu(accumulator)
-    elif ACT_TYPE == 3:
-        accumulator = silu(accumulator)
-
-    if ADD_RESIDUAL:
-        residual_ptrs = residual_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]
-        res = tl.load(residual_ptrs, mask=c_mask, other=0.0)
-        accumulator += res
-
-    tl.store(c_ptrs, accumulator, mask=c_mask)
-
-
-# Adapted from AutoGPTQ auto_gptq: https://github.com/PanQiWei/AutoGPTQ
-@autotune(
-    configs=[
-        triton.Config(
-            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 256, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 128, "BLOCK_SIZE_N": 32, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 64, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=4, num_warps=4
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 128, "BLOCK_SIZE_K": 32, "GROUP_SIZE_M": 8}, num_stages=2, num_warps=8
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 64, "BLOCK_SIZE_N": 64, "BLOCK_SIZE_K": 64, "GROUP_SIZE_M": 8}, num_stages=3, num_warps=8
-        ),
-        triton.Config(
-            {"BLOCK_SIZE_M": 32, "BLOCK_SIZE_N": 32, "BLOCK_SIZE_K": 128, "GROUP_SIZE_M": 8}, num_stages=2, num_warps=4
-        ),
-    ],
-    key=["M", "N", "K"],
-    nearest_power_of_two=True,
-    prune_configs_by={
-        "early_config_prune": matmul248_kernel_config_pruner,
-        "perf_model": None,
-        "top_k": None,
-    },
-)
-@triton.jit
-def cai_gptq_idx_matmul_248_kernel(
-    a_ptr,
-    b_ptr,
-    c_ptr,
-    scales_ptr,
-    zeros_ptr,
-    idx_ptr,
-    bias_ptr,
-    residual_ptr,
-    M,
-    N,
-    K,
-    bits,
-    maxq,
-    gptq_group_size,
-    stride_am,
-    stride_ak,
-    stride_bk,
-    stride_bn,
-    stride_cm,
-    stride_cn,
-    stride_scales,
-    stride_zeros,
-    QKV_FUSED: tl.constexpr,
-    ADD_BIAS: tl.constexpr,
-    ADD_RESIDUAL: tl.constexpr,
-    ACT_TYPE: tl.constexpr,
-    BLOCK_SIZE_M: tl.constexpr,
-    BLOCK_SIZE_N: tl.constexpr,
-    BLOCK_SIZE_K: tl.constexpr,
-    GROUP_SIZE_M: tl.constexpr,
-):
-    """
-    Compute the matrix multiplication C = A x B.
-    A is of shape (M, K) float16
-    B is of shape (K//8, N) int32
-    C is of shape (M, N) float16
-    scales is of shape (G, N) float16
-    zeros is of shape (G, N) float16
-    """
-    infearure_per_bits = 32 // bits
+    # NOTE BLOCK_KV could be considered as block splitting the sequence on k/v
+    # For now, BLOCK_KV is supposed to be equivalent with the size of physical cache block (i.e.`block_size`)
+    assert block_size in {16, 32, 64, 128}
+    BLOCK_KV = block_size
+
+    sm_scale = 1.0 / (head_dim**0.5) if sm_scale is None else sm_scale
+    max_seq_len_in_batch = kv_seq_len.max().item() if max_seq_len_in_batch is None else max_seq_len_in_batch
+    # For compatibility (TODO revise modeling in future)
+    kv_max_split_num = (max_seq_len_in_batch + BLOCK_KV - 1) // BLOCK_KV
+
+    if mid_output is None:
+        mid_output = torch.empty(
+            (bsz * q_len, num_heads, kv_max_split_num, head_dim), dtype=torch.float32, device=q.device
+        )
+    if mid_output_lse is None:
+        mid_output_lse = torch.empty((bsz * q_len, num_heads, kv_max_split_num), dtype=torch.float32, device=q.device)
+    if output is None:
+        # A hack to prevent `view` operation in modeling
+        output = torch.empty((bsz * q_len, num_heads * head_dim), dtype=q.dtype, device=q.device)
+
+    assert (
+        mid_output.size(2) == mid_output_lse.size(2) >= kv_max_split_num
+    ), "Incompatible kv split number of intermediate output tensors"
+    assert (
+        mid_output.size(0) == mid_output_lse.size(0) >= output.size(0) == n_tokens
+    ), f"Incompatible first dimension of output tensors"
+
+    # NOTE use `triton.next_power_of_2` here to utilize the cache mechanism of triton
+    # To optimize, revise batching/scheduling to batch 2^n sequences in a batch (preferred)
+    grid = lambda META: (
+        triton.next_power_of_2(bsz * q_len),
+        num_heads,
+        triton.cdiv(triton.next_power_of_2(max_seq_len_in_batch), META["BLOCK_KV"]),
+    )
 
-    pid = tl.program_id(axis=0)
-    NK = K
+    if alibi_slopes is not None:
+        # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
+        # the code (alibi kernel) will be refactored later to avoid code duplication, when
+        # the whole triton flow with new k cache layout has been supported and tested.
+        assert (
+            not use_new_kcache_layout
+        ), "Alibi Slopes will be supported with new kcache layout later when the whole triton flow is ready"
+
+        _alibi_flash_decoding_fwd_kernel[grid](
+            q,
+            k_cache,
+            v_cache,
+            block_tables,
+            mid_output,
+            mid_output_lse,
+            kv_seq_len,
+            q_len,
+            bsz,
+            alibi_slopes,
+            q.stride(0),
+            q.stride(1),
+            q.stride(2),
+            k_cache.stride(0),
+            k_cache.stride(1),
+            k_cache.stride(2),
+            k_cache.stride(3),
+            block_tables.stride(0),
+            block_tables.stride(1),
+            mid_output.stride(0),
+            mid_output.stride(1),
+            mid_output.stride(2),
+            mid_output.stride(3),
+            mid_output_lse.stride(0),
+            mid_output_lse.stride(1),
+            mid_output_lse.stride(2),
+            sm_scale,
+            KV_GROUPS=kv_group_num,
+            BLOCK_KV=block_size,
+            BLOCK_SIZE=block_size,
+            HEAD_DIM=head_dim,
+        )
+    else:
+        # For KCache and VCache with the same layout
+        x = head_dim
+        kcsplit_x_stride, kcs_stride, kcd_stride = 0, k_cache.stride(2), k_cache.stride(3)
+        # For KCache layout [num_blocks, num_kv_heads, head_dim//x, block_size, x]
+        if use_new_kcache_layout:
+            assert (
+                k_cache.dim() == 5
+                and k_cache.shape[1] == v_cache.shape[1]
+                and k_cache.shape[2] * k_cache.shape[4] == v_cache.shape[3]
+            ), f"Invalid KCache shape {k_cache.shape} and VCache shape {v_cache.shape}"
+            x = k_cache.size(-1)
+            kcsplit_x_stride, kcs_stride, kcd_stride = k_cache.stride()[-3:]
+
+        _flash_decoding_fwd_kernel[grid](
+            q,
+            k_cache,
+            v_cache,
+            block_tables,
+            mid_output,
+            mid_output_lse,
+            kv_seq_len,
+            q_len,
+            bsz,
+            kv_group_num,
+            x,
+            sm_scale,
+            q.stride(0),
+            q.stride(1),
+            q.stride(2),
+            k_cache.stride(0),
+            k_cache.stride(1),
+            kcsplit_x_stride,
+            kcs_stride,
+            kcd_stride,
+            v_cache.stride(0),
+            v_cache.stride(1),
+            v_cache.stride(2),
+            v_cache.stride(3),
+            block_tables.stride(0),
+            block_tables.stride(1),
+            mid_output.stride(0),
+            mid_output.stride(1),
+            mid_output.stride(2),
+            mid_output.stride(3),
+            mid_output_lse.stride(0),
+            mid_output_lse.stride(1),
+            mid_output_lse.stride(2),
+            BLOCK_KV=block_size,
+            BLOCK_SIZE=block_size,
+            HEAD_DIM=head_dim,
+        )
+
+    grid = (triton.next_power_of_2(bsz * q_len), num_heads)
+    _flash_decoding_fwd_reduce_kernel[grid](
+        mid_output,
+        mid_output_lse,
+        output,
+        kv_seq_len,
+        q_len,
+        bsz,
+        mid_output.stride(0),
+        mid_output.stride(1),
+        mid_output.stride(2),
+        mid_output.stride(3),
+        mid_output_lse.stride(0),
+        mid_output_lse.stride(1),
+        mid_output_lse.stride(2),
+        output.stride(0),
+        head_dim,
+        1,
+        BLOCK_KV=block_size,
+        HEAD_DIM=head_dim,
+    )
 
-    # if QKV_FUSED:
-    #     NK = K//3
-    # else:
-    #     NK = K
-    # NK = K
-
-    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
-    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
-    num_pid_k = tl.cdiv(NK, BLOCK_SIZE_K)
-    qkv_offset = pid // (num_pid_m * num_pid_n)
-    pid = pid % (num_pid_m * num_pid_n)
-    num_pid_in_group = GROUP_SIZE_M * num_pid_n
-    group_id = pid // num_pid_in_group
-    first_pid_m = group_id * GROUP_SIZE_M
-    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
-    pid_m = first_pid_m + (pid % group_size_m)
-    pid_n = (pid % num_pid_in_group) // group_size_m
-
-    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
-    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
-    offs_k = tl.arange(0, BLOCK_SIZE_K)
-    # offs_bk = offs_k + qkv_offset * NK
-    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
-
-    a_mask = offs_am[:, None] < M
-    # b_ptrs is set up such that it repeats elements along the K axis 8 times
-    b_ptrs = (
-        b_ptr
-        + qkv_offset * N * NK // infearure_per_bits
-        + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)
-    )  # (BLOCK_SIZE_K, BLOCK_SIZE_N)
-    # g_ptrs = g_ptr + offs_k
-    # shifter is used to extract the N bits of each element in the 32-bit word from B
-    scales_ptrs = scales_ptr + qkv_offset * NK * N // gptq_group_size + offs_bn[None, :]
-    zeros_ptrs = (
-        zeros_ptr
-        + qkv_offset * NK * N // gptq_group_size // infearure_per_bits
-        + (offs_bn[None, :] // infearure_per_bits)
-    )
-
-    shifter = (offs_k % infearure_per_bits) * bits
-    zeros_shifter = (offs_bn % infearure_per_bits) * bits
-    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
-    g_ptrs = idx_ptr + offs_k
-    g_idx = tl.load(g_ptrs)
-    # tl.device_print("gidx, ", g_idx)
-    zeros_ptrs = zeros_ptr + (offs_bn[None, :] // infearure_per_bits)
-
-    scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
-
-    for k in range(0, num_pid_k):
-        g_idx = tl.load(g_ptrs)
-        scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
-        zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)
-
-        zeros = (zeros >> zeros_shifter[None, :]) & maxq
-        zeros = zeros + 1
-
-        # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop
-        a = tl.load(a_ptrs, mask=a_mask, other=0.0)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
-        b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated
-        # Now we need to unpack b (which is N-bit values) into 32-bit values
-        b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values
-        b = (b - zeros).to(tl.float16) * scales  # Scale and shift
-        accumulator += tl.dot(a, b)
-
-        a_ptrs += BLOCK_SIZE_K
-        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk
-        g_ptrs += BLOCK_SIZE_K
-
-    c_ptrs = c_ptr + qkv_offset * M * N + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]
-    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)
-
-    if ADD_BIAS:
-        bias_mask = offs_bn < N
-        offs_bn += qkv_offset * N
-        bias_ptrs = bias_ptr + stride_cn * offs_bn
-        bias = tl.load(bias_ptrs, mask=bias_mask, other=0.0)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)
-        accumulator += bias[None, :]
-
-    if ACT_TYPE == 1:
-        accumulator = relu(accumulator)
-    elif ACT_TYPE == 2:
-        accumulator = gelu(accumulator)
-    elif ACT_TYPE == 3:
-        accumulator = silu(accumulator)
-
-    if ADD_RESIDUAL:
-        residual_ptrs = residual_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]
-        res = tl.load(residual_ptrs, mask=c_mask, other=0.0)
-        accumulator += res
-
-    tl.store(c_ptrs, accumulator, mask=c_mask)
-
-
-def gptq_fused_linear_triton(
-    input,
-    qweight,
-    scales,
-    qzeros,
-    bias,
-    residual,
-    bits,
-    maxq,
-    gptq_group_size,
-    qkv_fused,
-    add_bias,
-    add_residual,
-    g_idx=None,
-    act_type=0,
-):
-    # print("gptq fused ", qkv_fused, add_bias, add_residual)
-    assert input.is_cuda, "input is not in cuda"
-    assert qweight.is_cuda, "qweight is not in cuda"
-    assert scales.is_cuda, "scales is not in cuda"
-    assert qzeros.is_cuda, "qzeros is not in cuda"
-
-    with torch.cuda.device(input.device):
-        if qkv_fused:
-            grid = lambda META: (
-                triton.cdiv(input.shape[0], META["BLOCK_SIZE_M"])
-                * triton.cdiv(qweight.shape[1], META["BLOCK_SIZE_N"])
-                * 3,
-            )
-            output = torch.empty((input.shape[0] * 3, qweight.shape[1]), device=input.device, dtype=torch.float16)
-        else:
-            grid = lambda META: (
-                triton.cdiv(input.shape[0], META["BLOCK_SIZE_M"]) * triton.cdiv(qweight.shape[1], META["BLOCK_SIZE_N"]),
-            )
-            output = torch.empty((input.shape[0], qweight.shape[1]), device=input.device, dtype=torch.float16)
-        # print("dtype, ", qweight.dtype, output.dtype, scales.dtype, qzeros.dtype, bias.dtype, residual.dtype)
-        if g_idx is None:
-            cai_gptq_matmul_248_kernel[grid](
-                input,
-                qweight,
-                output,
-                scales,
-                qzeros,
-                bias,
-                residual,
-                input.shape[0],
-                qweight.shape[1],
-                input.shape[1],
-                bits,
-                maxq,
-                gptq_group_size,
-                input.stride(0),
-                input.stride(1),
-                qweight.stride(0),
-                qweight.stride(1),
-                output.stride(0),
-                output.stride(1),
-                scales.stride(0),
-                qzeros.stride(0),
-                QKV_FUSED=qkv_fused,
-                ADD_BIAS=add_bias,
-                ADD_RESIDUAL=add_residual,
-                ACT_TYPE=act_type,
-            )
-        else:
-            cai_gptq_idx_matmul_248_kernel[grid](
-                input,
-                qweight,
-                output,
-                scales,
-                qzeros,
-                g_idx,
-                bias,
-                residual,
-                input.shape[0],
-                qweight.shape[1],
-                input.shape[1],
-                bits,
-                maxq,
-                gptq_group_size,
-                input.stride(0),
-                input.stride(1),
-                qweight.stride(0),
-                qweight.stride(1),
-                output.stride(0),
-                output.stride(1),
-                scales.stride(0),
-                qzeros.stride(0),
-                QKV_FUSED=qkv_fused,
-                ADD_BIAS=add_bias,
-                ADD_RESIDUAL=add_residual,
-                ACT_TYPE=act_type,
-            )
-        if qkv_fused:
-            return output.view(3, input.shape[0], qweight.shape[1])
-        else:
-            return output
+    return output
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/triton/llama_act_combine_kernel.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/triton/llama_act_combine_kernel.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/triton/qkv_matmul_kernel.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/triton/qkv_matmul_kernel.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/kernel/triton/softmax.py` & `colossalai-nightly-2024.6.1/colossalai/kernel/triton/softmax.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/lazy/construction.py` & `colossalai-nightly-2024.6.1/colossalai/lazy/construction.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/lazy/lazy_init.py` & `colossalai-nightly-2024.6.1/colossalai/lazy/lazy_init.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/lazy/pretrained.py` & `colossalai-nightly-2024.6.1/colossalai/lazy/pretrained.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import copy
 import os
 from typing import Callable, Optional, Union
 
 import torch
 from torch.nn import Module
 
 from colossalai.interface import pretrained as pretrained_interface
@@ -70,14 +71,32 @@
     from_pipeline = kwargs.pop("_from_pipeline", None)
     from_auto_class = kwargs.pop("_from_auto", False)
     _fast_init = kwargs.pop("_fast_init", True)
     torch_dtype = kwargs.pop("torch_dtype", None)
     subfolder = kwargs.pop("subfolder", "")
     commit_hash = kwargs.pop("_commit_hash", None)
     variant = kwargs.pop("variant", None)
+
+    kwargs.pop("state_dict", None)
+    kwargs.pop("from_tf", False)
+    kwargs.pop("from_flax", False)
+    kwargs.pop("output_loading_info", False)
+    kwargs.pop("trust_remote_code", None)
+    kwargs.pop("low_cpu_mem_usage", None)
+    kwargs.pop("device_map", None)
+    kwargs.pop("max_memory", None)
+    kwargs.pop("offload_folder", None)
+    kwargs.pop("offload_state_dict", False)
+    kwargs.pop("load_in_8bit", False)
+    kwargs.pop("load_in_4bit", False)
+    kwargs.pop("quantization_config", None)
+    kwargs.pop("adapter_kwargs", {})
+    kwargs.pop("adapter_name", "default")
+    kwargs.pop("use_flash_attention_2", False)
+
     use_safetensors = kwargs.pop("use_safetensors", None if is_safetensors_available() else False)
 
     if len(kwargs) > 0:
         logger.warning(f"Below kwargs may be ignored: {list(kwargs.keys())}")
 
     from_pt = True
 
@@ -104,14 +123,18 @@
             revision=revision,
             subfolder=subfolder,
             _from_auto=from_auto_class,
             _from_pipeline=from_pipeline,
             **kwargs,
         )
     else:
+        config = copy.deepcopy(config)
+        kwarg_attn_imp = kwargs.pop("attn_implementation", None)
+        if kwarg_attn_imp is not None and config._attn_implementation != kwarg_attn_imp:
+            config._attn_implementation = kwarg_attn_imp
         model_kwargs = kwargs
 
     if commit_hash is None:
         commit_hash = getattr(config, "_commit_hash", None)
 
     # This variable will flag if we're loading a sharded checkpoint. In this case the archive file is just the
     # index of the files.
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/amp/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/amp/apex_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/apex_amp.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/amp/apex_amp/apex_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/amp/naive_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/amp/naive_amp/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/naive_amp.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/amp/naive_amp/naive_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/amp/torch_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/_grad_scaler.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/amp/torch_amp/_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/torch_amp.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/amp/torch_amp/torch_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/builder/builder.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/builder/builder.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/communication/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/communication/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/communication/collective.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/communication/collective.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/communication/p2p.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p_v2.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/communication/p2p_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/communication/ring.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/communication/ring.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/communication/utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/communication/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/constants.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_context.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/parallel_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_mode.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/parallel_mode.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_1d.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_1d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2d.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_3d.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_3d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_data.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_data.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_model.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_sequence.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_sequence.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_tensor.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/initializer_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/process_group_initializer.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/process_group_initializer/process_group_initializer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/random/_helper.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/random/_helper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/context/random/seed_manager.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/context/random/seed_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/_base_engine.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/_base_engine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_accumulation/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/gradient_handler/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_base_schedule.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/schedule/_base_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/schedule/_pipeline_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/global_variables.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/global_variables.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_engine.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/async_engine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_manager.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/async_manager.py`

 * *Files 1% similar despite different names*

```diff
@@ -51,22 +51,22 @@
         has_new_finished = False
         if self.running_batch is None:
             new_batch = self.req_queue.generate_new_batch(self.running_batch)
             if new_batch is not None:
                 self.stats_tool.count_prompt_tokens(new_batch)
                 self.running_batch = new_batch
                 has_new_finished, outputs = self._prefill_batch(self.running_batch)
-                self._filter_runing_batch()
+                self._filter_running_batch()
                 self.has_wait_tokens = 0
 
         else:
             if self.has_wait_tokens < self.max_wait_tokens:
                 self.stats_tool.count_output_tokens(self.running_batch)
                 has_new_finished, outputs = self._decode_batch(self.running_batch)
-                self._filter_runing_batch()
+                self._filter_running_batch()
                 self.has_wait_tokens += 1
 
             else:
                 new_mini_batch = self.req_queue.generate_new_batch(self.running_batch)
                 if new_mini_batch is not None:
                     self.stats_tool.count_prompt_tokens(new_mini_batch)
                     has_new_finished, outputs = self._prefill_batch(new_mini_batch)
@@ -74,15 +74,15 @@
                         self._merge_batch(self.running_batch, new_mini_batch)
                         self.running_batch.merge(new_mini_batch)
                     self.has_wait_tokens = 0
 
                 else:
                     self.stats_tool.count_output_tokens(self.running_batch)
                     has_new_finished, outputs = self._decode_batch(self.running_batch)
-                    self._filter_runing_batch()
+                    self._filter_running_batch()
                     self.has_wait_tokens += 1
 
         if has_new_finished:
             return outputs
         return None
 
     def _prefill_batch(self, batch):
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/infer_batch.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/infer_batch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/io_struct.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/io_struct.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_init_config.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/ray_init_config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/req_queue.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/req_queue.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/sampling_params.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/sampling_params.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/stats.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/dynamic_batching/stats.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/modeling/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/llama.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/modeling/bloom.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,489 +1,540 @@
-# This code is adapted from huggingface transformers: https://github.com/huggingface/transformers/blob/v4.34.1/src/transformers/models/llama/modeling_llama.py
 import math
-from typing import List, Optional, Tuple
+import warnings
+from typing import Optional, Tuple, Union
 
 import torch
-from transformers.models.llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaForCausalLM, LlamaModel
+import torch.distributed as dist
+from torch.nn import CrossEntropyLoss
+from torch.nn import functional as F
+from transformers.models.bloom.modeling_bloom import (
+    BaseModelOutputWithPastAndCrossAttentions,
+    BloomAttention,
+    BloomBlock,
+    BloomForCausalLM,
+    BloomModel,
+    CausalLMOutputWithCrossAttentions,
+)
 from transformers.utils import logging
 
 from colossalai.inference.tensor_parallel.batch_infer_state import BatchInferState
-from colossalai.kernel.triton import llama_context_attn_fwd, token_attention_fwd
-from colossalai.kernel.triton.token_attention_kernel import Llama2TokenAttentionForwards
-from colossalai.pipeline.stage_manager import PipelineStageManager
-
-from ._utils import copy_kv_to_mem_cache
+from colossalai.kernel.triton import bloom_context_attn_fwd, copy_kv_cache_to_dest, token_attention_fwd
 
 try:
-    from lightllm.models.llama2.triton_kernel.context_flashattention_nopad import (
-        context_attention_fwd as lightllm_llama2_context_attention_fwd,
-    )
-    from lightllm.models.llama.triton_kernel.context_flashattention_nopad import (
-        context_attention_fwd as lightllm_context_attention_fwd,
+    from lightllm.models.bloom.triton_kernel.context_flashattention_nopad import (
+        context_attention_fwd as lightllm_bloom_context_attention_fwd,
     )
-    from lightllm.models.llama.triton_kernel.rotary_emb import rotary_emb_fwd as llama_rotary_embedding_fwd
 
     HAS_LIGHTLLM_KERNEL = True
 except:
-    print("please install lightllm from source to run inference: https://github.com/ModelTC/lightllm")
     HAS_LIGHTLLM_KERNEL = False
 
-try:
-    from flash_attn import flash_attn_with_kvcache
-
-    HAS_FLASH_KERNEL = True
-except:
-    HAS_FLASH_KERNEL = False
-    print("please install flash attentiom from https://github.com/Dao-AILab/flash-attention")
 
+def generate_alibi(n_head, dtype=torch.float16):
+    """
+    This method is adapted from `_generate_alibi` function
+    in `lightllm/models/bloom/layer_weights/transformer_layer_weight.py`
+    of the ModelTC/lightllm GitHub repository.
+    This method is originally the `build_alibi_tensor` function
+    in `transformers/models/bloom/modeling_bloom.py`
+    of the huggingface/transformers GitHub repository.
+    """
 
-def rotate_half(x):
-    """Rotates half the hidden dims of the input."""
-    x1 = x[..., : x.shape[-1] // 2]
-    x2 = x[..., x.shape[-1] // 2 :]
-    return torch.cat((-x2, x1), dim=-1)
-
-
-def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
-    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.
-    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]
-    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]
-    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
-    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
-
-    q_embed = (q * cos) + (rotate_half(q) * sin)
-    k_embed = (k * cos) + (rotate_half(k) * sin)
-    return q_embed, k_embed
-
-
-def llama_triton_context_attention(
-    query_states, key_states, value_states, attn_output, infer_state, num_key_value_groups=1
-):
-    if num_key_value_groups == 1:
-        if HAS_LIGHTLLM_KERNEL is False:
-            llama_context_attn_fwd(
-                query_states,
-                key_states,
-                value_states,
-                attn_output,
-                infer_state.start_loc,
-                infer_state.seq_len,
-                # infer_state.cache_manager.past_key_values_length,
-                infer_state.max_len_in_batch,
-            )
+    def get_slopes_power_of_2(n):
+        start = 2 ** (-(2 ** -(math.log2(n) - 3)))
+        return [start * start**i for i in range(n)]
+
+    def get_slopes(n):
+        if math.log2(n).is_integer():
+            return get_slopes_power_of_2(n)
         else:
-            lightllm_context_attention_fwd(
-                query_states,
-                key_states,
-                value_states,
-                attn_output,
-                infer_state.start_loc,
-                infer_state.seq_len,
-                # infer_state.cache_manager.past_key_values_length,
-                infer_state.max_len_in_batch,
-            )
-    else:
-        assert HAS_LIGHTLLM_KERNEL is True, "You have to install lightllm kernels to run llama2 model"
-        lightllm_llama2_context_attention_fwd(
-            query_states,
-            key_states,
-            value_states,
-            attn_output,
-            infer_state.start_loc,
-            infer_state.seq_len,
-            # infer_state.cache_manager.past_key_values_length,
-            infer_state.max_len_in_batch,
-        )
+            closest_power_of_2 = 2 ** math.floor(math.log2(n))
+            slopes_power_of_2 = get_slopes_power_of_2(closest_power_of_2)
+            slopes_double = get_slopes(2 * closest_power_of_2)
+            slopes_combined = slopes_power_of_2 + slopes_double[0::2][: n - closest_power_of_2]
+            return slopes_combined
 
+    slopes = get_slopes(n_head)
+    return torch.tensor(slopes, dtype=dtype)
 
-def llama_triton_token_attention(query_states, attn_output, infer_state, num_key_value_groups=1):
-    assert HAS_LIGHTLLM_KERNEL is True, "You have to install lightllm kernel to run token attention for llama models"
-    if num_key_value_groups == 1:
-        token_attention_fwd(
-            query_states,
-            infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
-            infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
-            attn_output,
-            infer_state.block_loc,
-            infer_state.start_loc,
-            infer_state.seq_len,
-            # infer_state.cache_manager.past_key_values_length,
-            infer_state.max_len_in_batch,
-        )
-    else:
-        Llama2TokenAttentionForwards.token_attn(
-            query_states,
-            infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
-            infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
-            attn_output,
-            infer_state.block_loc,
-            infer_state.start_loc,
-            infer_state.seq_len,
-            # infer_state.cache_manager.past_key_values_length,
-            infer_state.max_len_in_batch,
-            infer_state.other_kv_index,
-        )
 
-
-class LlamaInferenceForwards:
+class BloomInferenceForwards:
     """
-    This class holds forwards for llama inference.
-    We intend to replace the forward methods for LlamaModel, LlamaDecoderLayer, and LlamaAttention for LlamaForCausalLM.
+    This class serves a micro library for bloom inference forwards.
+    We intend to replace the forward methods for BloomForCausalLM, BloomModel, BloomBlock, and BloomAttention,
+    as well as prepare_inputs_for_generation method for BloomForCausalLM.
+    For future improvement, we might want to skip replacing methods for BloomForCausalLM,
+    and call BloomModel.forward iteratively in TpInferEngine
     """
 
     @staticmethod
-    def llama_causal_lm_forward(
-        self: LlamaForCausalLM,
-        input_ids: torch.LongTensor = None,
+    def bloom_model_forward(
+        self: BloomModel,
+        input_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
         attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[List[torch.FloatTensor]] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
-        labels: Optional[torch.LongTensor] = None,
+        head_mask: Optional[torch.LongTensor] = None,
+        inputs_embeds: Optional[torch.LongTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
-        infer_state: BatchInferState = None,
-        stage_manager: Optional[PipelineStageManager] = None,
-        hidden_states: Optional[torch.FloatTensor] = None,
-        stage_index: Optional[List[int]] = None,
-    ):
-        r"""
-        Args:
-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
-
-        """
+        infer_state: Optional[BatchInferState] = None,
+        **deprecated_arguments,
+    ) -> Union[Tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:
         logger = logging.get_logger(__name__)
 
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-        if output_attentions:
-            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
-            output_attentions = False
-        if output_hidden_states:
-            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
-            output_hidden_states = False
+        if deprecated_arguments.pop("position_ids", False) is not False:
+            # `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`
+            warnings.warn(
+                "`position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore"
+                " passing `position_ids`.",
+                FutureWarning,
+            )
+        if len(deprecated_arguments) > 0:
+            raise ValueError(f"Got unexpected arguments: {deprecated_arguments}")
 
-        # If is first stage and after warmup, go throught lm_head first
-        if stage_manager.is_first_stage() and hidden_states is not None:
-            lm_logits = self.lm_head(hidden_states)
-            return {"logits": lm_logits}
-
-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-        outputs = LlamaInferenceForwards.llama_model_forward(
-            self.model,
-            input_ids=input_ids,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_values=past_key_values,
-            inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
-            output_attentions=output_attentions,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
-            infer_state=infer_state,
-            stage_manager=stage_manager,
-            hidden_states=hidden_states,
-            stage_index=stage_index,
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
         )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        return outputs
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
+        elif input_ids is not None:
+            batch_size, seq_length = input_ids.shape
+        elif inputs_embeds is not None:
+            batch_size, seq_length, _ = inputs_embeds.shape
+        else:
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
 
-    @staticmethod
-    def llama_model_forward(
-        self: LlamaModel,
-        input_ids: torch.LongTensor = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[List[torch.FloatTensor]] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
-        use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-        infer_state: BatchInferState = None,
-        stage_manager: Optional[PipelineStageManager] = None,
-        hidden_states: Optional[torch.FloatTensor] = None,
-        stage_index: Optional[List[int]] = None,
-    ):
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+        # still need to keep past_key_values to fit original forward flow
+        if past_key_values is None:
+            past_key_values = tuple([None] * len(self.h))
 
-        use_cache = use_cache if use_cache is not None else self.config.use_cache
-        # retrieve input_ids and inputs_embeds
-        if stage_manager is None or stage_manager.is_first_stage():
-            if input_ids is not None and inputs_embeds is not None:
-                raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
-            elif input_ids is not None:
-                batch_size, seq_length = input_ids.shape
-            elif inputs_embeds is not None:
-                batch_size, seq_length, _ = inputs_embeds.shape
-            else:
-                raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
-            device = input_ids.device if input_ids is not None else inputs_embeds.device
-            if inputs_embeds is None:
-                inputs_embeds = self.embed_tokens(input_ids)
-            hidden_states = inputs_embeds
-        else:
-            assert stage_manager is not None
-            assert hidden_states is not None, f"hidden_state should not be none in stage {stage_manager.stage}"
-            input_shape = hidden_states.shape[:-1]
-            batch_size, seq_length = input_shape
-            device = hidden_states.device
+        # Prepare head mask if needed
+        # 1.0 in head_mask indicate we keep the head
+        # attention_probs has shape batch_size x num_heads x N x N
+        # head_mask has shape n_layer x batch x num_heads x N x N
+        head_mask = self.get_head_mask(head_mask, self.config.n_layer)
+
+        if inputs_embeds is None:
+            inputs_embeds = self.word_embeddings(input_ids)
+
+        hidden_states = self.word_embeddings_layernorm(inputs_embeds)
+
+        presents = () if use_cache else None
+        all_self_attentions = () if output_attentions else None
+        all_hidden_states = () if output_hidden_states else None
+
+        if self.gradient_checkpointing and self.training:
+            if use_cache:
+                logger.warning_once(
+                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+                )
+                use_cache = False
+
+        # NOTE determine if BatchInferState is passed in via arg
+        #      if not, get the attr binded to the model
+        # We might wantto remove setattr later
+        if infer_state is None:
+            assert hasattr(self, "infer_state")
+            infer_state = self.infer_state
 
+        # infer_state.cache_manager = self.cache_manager
         if infer_state.is_context_stage:
             past_key_values_length = 0
         else:
             past_key_values_length = infer_state.max_len_in_batch - 1
 
-        # NOTE: differentiate with prefill stage
-        #       block_loc require different value-assigning method for two different stage
         if use_cache and seq_length != 1:
-            # NOTE assume prefill stage
-            # allocate memory block
+            # prefill stage
             infer_state.is_context_stage = True  # set prefill stage, notify attention layer
             infer_state.context_mem_index = infer_state.cache_manager.alloc(infer_state.total_token_num)
-            infer_state.init_block_loc(
+            BatchInferState.init_block_loc(
                 infer_state.block_loc, infer_state.seq_len, seq_length, infer_state.context_mem_index
             )
         else:
             infer_state.is_context_stage = False
             alloc_mem = infer_state.cache_manager.alloc_contiguous(batch_size)
             if alloc_mem is not None:
                 infer_state.decode_is_contiguous = True
                 infer_state.decode_mem_index = alloc_mem[0]
                 infer_state.decode_mem_start = alloc_mem[1]
                 infer_state.decode_mem_end = alloc_mem[2]
                 infer_state.block_loc[:, infer_state.max_len_in_batch - 1] = infer_state.decode_mem_index
             else:
+                print(f" *** Encountered allocation non-contiguous")
+                print(f"    infer_state.max_len_in_batch : {infer_state.max_len_in_batch}")
                 infer_state.decode_is_contiguous = False
                 alloc_mem = infer_state.cache_manager.alloc(batch_size)
                 infer_state.decode_mem_index = alloc_mem
+                # infer_state.decode_key_buffer = torch.empty((batch_size, self.tp_head_num_, self.head_dim_), dtype=torch.float16, device="cuda")
+                # infer_state.decode_value_buffer = torch.empty((batch_size, self.tp_head_num_, self.head_dim_), dtype=torch.float16, device="cuda")
                 infer_state.block_loc[:, infer_state.max_len_in_batch - 1] = infer_state.decode_mem_index
 
-        if position_ids is None:
-            position_ids = torch.arange(
-                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
-            )
-            position_ids = position_ids.repeat(batch_size, 1)
-            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
-        else:
-            position_ids = position_ids.view(-1, seq_length).long()
-
-        if infer_state.is_context_stage:
-            infer_state.position_cos = torch.index_select(self._cos_cached, 0, position_ids.view(-1)).view(
-                position_ids.view(-1).shape[0], -1
-            )
-            infer_state.position_sin = torch.index_select(self._sin_cached, 0, position_ids.view(-1)).view(
-                position_ids.view(-1).shape[0], -1
-            )
-
-        else:
-            seq_len = infer_state.seq_len
-            infer_state.position_cos = torch.index_select(self._cos_cached, 0, seq_len - 1).view(seq_len.shape[0], -1)
-            infer_state.position_sin = torch.index_select(self._sin_cached, 0, seq_len - 1).view(seq_len.shape[0], -1)
-            infer_state.other_kv_index = infer_state.block_loc[0, infer_state.max_len_in_batch - 1].item()
-
-        # embed positions
         if attention_mask is None:
-            attention_mask = torch.ones(
-                (batch_size, infer_state.max_len_in_batch), dtype=torch.bool, device=hidden_states.device
-            )
+            attention_mask = torch.ones((batch_size, infer_state.max_len_in_batch), device=hidden_states.device)
+        else:
+            attention_mask = attention_mask.to(hidden_states.device)
 
-        attention_mask = self._prepare_decoder_attention_mask(
-            attention_mask, (batch_size, seq_length), hidden_states, past_key_values_length
+        # NOTE revise: we might want to store a single 1D alibi(length is #heads) in model,
+        #      or store to BatchInferState to prevent re-calculating
+        #      When we have multiple process group (e.g. dp together with tp), we need to pass the pg to here
+        # alibi = generate_alibi(self.num_heads).contiguous().cuda()
+        tp_size = dist.get_world_size()
+        curr_tp_rank = dist.get_rank()
+        alibi = (
+            generate_alibi(self.num_heads * tp_size)
+            .contiguous()[curr_tp_rank * self.num_heads : (curr_tp_rank + 1) * self.num_heads]
+            .cuda()
+        )
+        causal_mask = self._prepare_attn_mask(
+            attention_mask,
+            input_shape=(batch_size, seq_length),
+            past_key_values_length=past_key_values_length,
         )
 
-        # decoder layers
         infer_state.decode_layer_id = 0
+        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
+            if output_hidden_states:
+                all_hidden_states = all_hidden_states + (hidden_states,)
+
+            if self.gradient_checkpointing and self.training:
+                # NOTE: currently our KV cache manager does not handle this condition
+                def create_custom_forward(module):
+                    def custom_forward(*inputs):
+                        # None for past_key_value
+                        return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)
+
+                    return custom_forward
+
+                outputs = torch.utils.checkpoint.checkpoint(
+                    create_custom_forward(block),
+                    hidden_states,
+                    alibi,
+                    causal_mask,
+                    layer_past,
+                    head_mask[i],
+                )
+            else:
+                outputs = block(
+                    hidden_states,
+                    layer_past=layer_past,
+                    attention_mask=causal_mask,
+                    head_mask=head_mask[i],
+                    use_cache=use_cache,
+                    output_attentions=output_attentions,
+                    alibi=alibi,
+                    infer_state=infer_state,
+                )
 
-        start_idx, end_idx = stage_index[0], stage_index[1]
-        if past_key_values is None:
-            past_key_values = tuple([None] * (end_idx - start_idx + 1))
-
-        for idx, past_key_value in zip(range(start_idx, end_idx), past_key_values):
-            decoder_layer = self.layers[idx]
-            # NOTE: modify here for passing args to decoder layer
-            layer_outputs = decoder_layer(
-                hidden_states,
-                attention_mask=attention_mask,
-                position_ids=position_ids,
-                past_key_value=past_key_value,
-                output_attentions=output_attentions,
-                use_cache=use_cache,
-                infer_state=infer_state,
-            )
             infer_state.decode_layer_id += 1
-            hidden_states = layer_outputs[0]
+            hidden_states = outputs[0]
+            if use_cache is True:
+                presents = presents + (outputs[1],)
+
+            if output_attentions:
+                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)
 
-        if stage_manager.is_last_stage() or stage_manager.num_stages == 1:
-            hidden_states = self.norm(hidden_states)
+        # Add last hidden state
+        hidden_states = self.ln_f(hidden_states)
+
+        if output_hidden_states:
+            all_hidden_states = all_hidden_states + (hidden_states,)
 
-        # update indices
-        # infer_state.block_loc[:, infer_state.max_len_in_batch-1] = infer_state.total_token_num + torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
-        infer_state.start_loc += torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
+        # update indices of kv cache block
+        # NOT READY FOR PRIME TIME
+        # might want to remove this part, instead, better to pass the BatchInferState from model forward,
+        #       and update these information in engine.generate after model foward called
+        infer_state.start_loc = infer_state.start_loc + torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
         infer_state.seq_len += 1
         infer_state.max_len_in_batch += 1
 
-        # if not return_dict:
-        #     return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+        if not return_dict:
+            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)
 
-        # return BaseModelOutputWithPast(
-        #     last_hidden_state=hidden_states,
-        #     past_key_values=next_cache,
-        #     hidden_states=all_hidden_states,
-        #     attentions=all_self_attns,
-        # )
-        return {"hidden_states": hidden_states}
+        return BaseModelOutputWithPastAndCrossAttentions(
+            last_hidden_state=hidden_states,
+            past_key_values=presents,  # should always be (None, None, ..., None)
+            hidden_states=all_hidden_states,
+            attentions=all_self_attentions,
+        )
 
     @staticmethod
-    def llama_decoder_layer_forward(
-        self: LlamaDecoderLayer,
-        hidden_states: torch.Tensor,
+    def bloom_for_causal_lm_forward(
+        self: BloomForCausalLM,
+        input_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
         attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        output_attentions: Optional[bool] = False,
-        use_cache: Optional[bool] = False,
+        head_mask: Optional[torch.Tensor] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        labels: Optional[torch.Tensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
         infer_state: Optional[BatchInferState] = None,
-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
-        residual = hidden_states
+        **deprecated_arguments,
+    ):
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
+            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
+            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
+        """
+        logging.get_logger(__name__)
 
-        hidden_states = self.input_layernorm(hidden_states)
-        # Self Attention
-        hidden_states, self_attn_weights, present_key_value = self.self_attn(
-            hidden_states=hidden_states,
+        if deprecated_arguments.pop("position_ids", False) is not False:
+            # `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`
+            warnings.warn(
+                "`position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore"
+                " passing `position_ids`.",
+                FutureWarning,
+            )
+        if len(deprecated_arguments) > 0:
+            raise ValueError(f"Got unexpected arguments: {deprecated_arguments}")
+
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        transformer_outputs = BloomInferenceForwards.bloom_model_forward(
+            self.transformer,
+            input_ids,
+            past_key_values=past_key_values,
             attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_value=past_key_value,
-            output_attentions=output_attentions,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
             use_cache=use_cache,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
             infer_state=infer_state,
         )
+        hidden_states = transformer_outputs[0]
 
-        hidden_states = residual + hidden_states
-
-        # Fully Connected
-        residual = hidden_states
-        hidden_states = self.post_attention_layernorm(hidden_states)
-        hidden_states = self.mlp(hidden_states)
-        hidden_states = residual + hidden_states
+        lm_logits = self.lm_head(hidden_states)
 
-        outputs = (hidden_states,)
+        loss = None
+        if labels is not None:
+            # move labels to correct device to enable model parallelism
+            labels = labels.to(lm_logits.device)
+            # Shift so that tokens < n predict n
+            shift_logits = lm_logits[..., :-1, :].contiguous()
+            shift_labels = labels[..., 1:].contiguous()
+            batch_size, seq_length, vocab_size = shift_logits.shape
+            # Flatten the tokens
+            loss_fct = CrossEntropyLoss()
+            loss = loss_fct(
+                shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length)
+            )
 
-        if output_attentions:
-            outputs += (self_attn_weights,)
+        if not return_dict:
+            output = (lm_logits,) + transformer_outputs[1:]
+            return ((loss,) + output) if loss is not None else output
+
+        return CausalLMOutputWithCrossAttentions(
+            loss=loss,
+            logits=lm_logits,
+            past_key_values=transformer_outputs.past_key_values,
+            hidden_states=transformer_outputs.hidden_states,
+            attentions=transformer_outputs.attentions,
+        )
 
-        if use_cache:
-            outputs += (present_key_value,)
+    @staticmethod
+    def bloom_for_causal_lm_prepare_inputs_for_generation(
+        self: BloomForCausalLM,
+        input_ids: torch.LongTensor,
+        past_key_values: Optional[torch.Tensor] = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        **kwargs,
+    ) -> dict:
+        # only last token for input_ids if past is not None
+        if past_key_values:
+            input_ids = input_ids[:, -1].unsqueeze(-1)
+
+            # NOTE we won't use past key values here
+            # the cache may be in the stardard format (e.g. in contrastive search), convert to bloom's format if needed
+            # if past_key_values[0][0].shape[0] == input_ids.shape[0]:
+            #     past_key_values = self._convert_to_bloom_cache(past_key_values)
+
+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step
+        if inputs_embeds is not None and past_key_values is None:
+            model_inputs = {"inputs_embeds": inputs_embeds}
+        else:
+            model_inputs = {"input_ids": input_ids}
 
-        return outputs
+        model_inputs.update(
+            {
+                "past_key_values": past_key_values,
+                "use_cache": kwargs.get("use_cache"),
+                "attention_mask": attention_mask,
+            }
+        )
+        return model_inputs
 
     @staticmethod
-    def llama_flash_attn_kvcache_forward(
-        self: LlamaAttention,
+    def bloom_block_forward(
+        self: BloomBlock,
         hidden_states: torch.Tensor,
-        attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        output_attentions: bool = False,
+        alibi: torch.Tensor,
+        attention_mask: torch.Tensor,
+        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
+        head_mask: Optional[torch.Tensor] = None,
         use_cache: bool = False,
+        output_attentions: bool = False,
         infer_state: Optional[BatchInferState] = None,
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-        assert use_cache is True, "use_cache should be set to True using this llama attention"
+    ):
+        # hidden_states: [batch_size, seq_length, hidden_size]
+
+        # Layer norm at the beginning of the transformer layer.
+        layernorm_output = self.input_layernorm(hidden_states)
+
+        # Layer norm post the self attention.
+        if self.apply_residual_connection_post_layernorm:
+            residual = layernorm_output
+        else:
+            residual = hidden_states
 
-        bsz, q_len, _ = hidden_states.size()
+        # Self attention.
+        attn_outputs = self.self_attention(
+            layernorm_output,
+            residual,
+            layer_past=layer_past,
+            attention_mask=attention_mask,
+            alibi=alibi,
+            head_mask=head_mask,
+            use_cache=use_cache,
+            output_attentions=output_attentions,
+            infer_state=infer_state,
+        )
+
+        attention_output = attn_outputs[0]
+
+        outputs = attn_outputs[1:]
+
+        layernorm_output = self.post_attention_layernorm(attention_output)
+
+        # Get residual
+        if self.apply_residual_connection_post_layernorm:
+            residual = layernorm_output
+        else:
+            residual = attention_output
 
-        # NOTE might think about better way to handle transposed k and v
-        # key_states            [bs, seq_len, num_heads, head_dim/embed_size_per_head]
-        # key_states_transposed [bs, num_heads, seq_len, head_dim/embed_size_per_head]
+        # MLP.
+        output = self.mlp(layernorm_output, residual)
 
-        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim)
-        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim)
-        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim)
+        if use_cache:
+            outputs = (output,) + outputs
+        else:
+            outputs = (output,) + outputs[1:]
 
-        # NOTE might want to revise
-        #   need some way to record the length of past key values cache
-        #   since we won't return past_key_value_cache right now
+        return outputs  # hidden_states, present, attentions
 
-        cos, sin = infer_state.position_cos, infer_state.position_sin
+    @staticmethod
+    def bloom_attention_forward(
+        self: BloomAttention,
+        hidden_states: torch.Tensor,
+        residual: torch.Tensor,
+        alibi: torch.Tensor,
+        attention_mask: torch.Tensor,
+        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
+        head_mask: Optional[torch.Tensor] = None,
+        use_cache: bool = False,
+        output_attentions: bool = False,
+        infer_state: Optional[BatchInferState] = None,
+    ):
+        fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
 
-        llama_rotary_embedding_fwd(query_states.view(-1, self.num_heads, self.head_dim), cos, sin)
-        llama_rotary_embedding_fwd(key_states.view(-1, self.num_key_value_heads, self.head_dim), cos, sin)
+        # 3 x [batch_size, seq_length, num_heads, head_dim]
+        (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
+        batch_size, q_length, H, D_HEAD = query_layer.shape
+        k = key_layer.reshape(-1, H, D_HEAD)  # batch_size * q_length, H, D_HEAD, q_lenth == 1
+        v = value_layer.reshape(-1, H, D_HEAD)  # batch_size * q_length, H, D_HEAD, q_lenth == 1
 
-        query_states = query_states.reshape(-1, self.num_heads, self.head_dim)
-        key_states = key_states.reshape(-1, self.num_key_value_heads, self.head_dim)
-        value_states = value_states.reshape(-1, self.num_key_value_heads, self.head_dim)
+        mem_manager = infer_state.cache_manager
+        layer_id = infer_state.decode_layer_id
 
         if infer_state.is_context_stage:
-            # first token generation
-            # copy key and value calculated in current step to memory manager
-            copy_kv_to_mem_cache(
-                infer_state.decode_layer_id,
-                key_states,
-                value_states,
-                infer_state.context_mem_index,
-                infer_state.cache_manager,
-            )
-            attn_output = torch.empty_like(query_states)
+            # context process
+            max_input_len = q_length
+            b_start_loc = infer_state.start_loc
+            b_seq_len = infer_state.seq_len[:batch_size]
+            q = query_layer.reshape(-1, H, D_HEAD)
 
-            llama_triton_context_attention(
-                query_states,
-                key_states,
-                value_states,
-                attn_output,
-                infer_state,
-                num_key_value_groups=self.num_key_value_groups,
-            )
+            copy_kv_cache_to_dest(k, infer_state.context_mem_index, mem_manager.key_buffer[layer_id])
+            copy_kv_cache_to_dest(v, infer_state.context_mem_index, mem_manager.value_buffer[layer_id])
+
+            # output = self.output[:batch_size*q_length, :, :]
+            output = torch.empty_like(q)
+
+            if HAS_LIGHTLLM_KERNEL:
+                lightllm_bloom_context_attention_fwd(q, k, v, output, alibi, b_start_loc, b_seq_len, max_input_len)
+            else:
+                bloom_context_attn_fwd(q, k, v, output, b_start_loc, b_seq_len, max_input_len, alibi)
+
+            context_layer = output.view(batch_size, q_length, H * D_HEAD)
         else:
+            # query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, q_length, self.head_dim)
+            # need shape: batch_size, H, D_HEAD (q_length == 1), input q shape : (batch_size, q_length(1), H, D_HEAD)
+            assert q_length == 1, "for non-context process, we only support q_length == 1"
+            q = query_layer.reshape(-1, H, D_HEAD)
+
             if infer_state.decode_is_contiguous:
                 # if decode is contiguous, then we copy to key cache and value cache in cache manager directly
-                cache_k = infer_state.cache_manager.key_buffer[infer_state.decode_layer_id][
+                cache_k = infer_state.cache_manager.key_buffer[layer_id][
                     infer_state.decode_mem_start : infer_state.decode_mem_end, :, :
                 ]
-                cache_v = infer_state.cache_manager.value_buffer[infer_state.decode_layer_id][
+                cache_v = infer_state.cache_manager.value_buffer[layer_id][
                     infer_state.decode_mem_start : infer_state.decode_mem_end, :, :
                 ]
-                cache_k.copy_(key_states)
-                cache_v.copy_(value_states)
+                cache_k.copy_(k)
+                cache_v.copy_(v)
             else:
                 # if decode is not contiguous, use triton kernel to copy key and value cache
-                # k, v shape: [batch_size, num_heads, head_dim/embed_size_per_head
-                copy_kv_to_mem_cache(
-                    infer_state.decode_layer_id,
-                    key_states,
-                    value_states,
-                    infer_state.decode_mem_index,
-                    infer_state.cache_manager,
-                )
+                # k, v shape: [batch_size, num_heads, head_dim/embed_size_per_head]
+                copy_kv_cache_to_dest(k, infer_state.decode_mem_index, mem_manager.key_buffer[layer_id])
+                copy_kv_cache_to_dest(v, infer_state.decode_mem_index, mem_manager.value_buffer[layer_id])
+
+            b_start_loc = infer_state.start_loc
+            b_loc = infer_state.block_loc
+            b_seq_len = infer_state.seq_len
+            output = torch.empty_like(q)
+            token_attention_fwd(
+                q,
+                mem_manager.key_buffer[layer_id],
+                mem_manager.value_buffer[layer_id],
+                output,
+                b_loc,
+                b_start_loc,
+                b_seq_len,
+                infer_state.max_len_in_batch,
+                alibi,
+            )
 
-            if HAS_LIGHTLLM_KERNEL:
-                attn_output = torch.empty_like(query_states)
-                llama_triton_token_attention(
-                    query_states, attn_output, infer_state, num_key_value_groups=self.num_key_value_groups
-                )
-            else:
-                self.num_heads // self.num_key_value_heads
-                cache_k = infer_state.cache_manager.key_buffer[infer_state.decode_layer_id]
-                cache_v = infer_state.cache_manager.value_buffer[infer_state.decode_layer_id]
-
-                query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim)
-                copy_cache_k = cache_k.view(bsz, -1, self.num_key_value_heads, self.head_dim)
-                copy_cache_v = cache_v.view(bsz, -1, self.num_key_value_heads, self.head_dim)
-
-                attn_output = flash_attn_with_kvcache(
-                    q=query_states,
-                    k_cache=copy_cache_k,
-                    v_cache=copy_cache_v,
-                    softmax_scale=1 / math.sqrt(self.head_dim),
-                    causal=True,
+            context_layer = output.view(batch_size, q_length, H * D_HEAD)
+
+        # NOTE: always set present as none for now, instead of returning past key value to the next decoding,
+        #       we create the past key value pair from the cache manager
+        present = None
+
+        # aggregate results across tp ranks. See here: https://github.com/pytorch/pytorch/issues/76232
+        if self.pretraining_tp > 1 and self.slow_but_exact:
+            slices = self.hidden_size / self.pretraining_tp
+            output_tensor = torch.zeros_like(context_layer)
+            for i in range(self.pretraining_tp):
+                output_tensor = output_tensor + F.linear(
+                    context_layer[:, :, int(i * slices) : int((i + 1) * slices)],
+                    self.dense.weight[:, int(i * slices) : int((i + 1) * slices)],
                 )
+        else:
+            output_tensor = self.dense(context_layer)
 
-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)
+        # dropout is not required here during inference
+        output_tensor = residual + output_tensor
 
-        attn_output = self.o_proj(attn_output)
+        outputs = (output_tensor, present)
+        assert output_attentions is False, "we do not support output_attentions at this time"
 
-        # return past_key_value as None
-        return attn_output, None, None
+        return outputs
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/llama.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/hybridengine/polices/llama.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/manager.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -127,22 +127,22 @@
 
         if self.running_batch is None:
             new_batch = self.req_queue.generate_new_batch(self.running_batch)
             if new_batch is not None:
                 self.stats_tool.count_prompt_tokens(new_batch)
                 self.running_batch = new_batch
                 yield from self._prefill_batch(self.running_batch)
-                self._filter_runing_batch()
+                self._filter_running_batch()
                 self.has_wait_tokens = 0
             return
 
         if self.has_wait_tokens < self.max_wait_tokens:
             self.stats_tool.count_output_tokens(self.running_batch)
             yield from self._decode_batch(self.running_batch)
-            self._filter_runing_batch()
+            self._filter_running_batch()
             self.has_wait_tokens += 1
             return
         else:
             new_mini_batch = self.req_queue.generate_new_batch(self.running_batch)
             if new_mini_batch is not None:
                 self.stats_tool.count_prompt_tokens(new_mini_batch)
                 yield from self._prefill_batch(new_mini_batch)
@@ -150,15 +150,15 @@
                     self._merge_batch(self.running_batch, new_mini_batch)
                     self.running_batch.merge(new_mini_batch)
                 self.has_wait_tokens = 0
 
             else:
                 self.stats_tool.count_output_tokens(self.running_batch)
                 yield from self._decode_batch(self.running_batch)
-                self._filter_runing_batch()
+                self._filter_running_batch()
                 self.has_wait_tokens += 1
 
         return
 
     def _init_batch(self, batch: Batch, dtype="fp16"):
         reqs = [r.to_rpc_obj() for r in batch.reqs]
         batch_id = batch.batch_id
@@ -239,15 +239,15 @@
             finished_reqs = batch.filter_finished()
             if batch.is_clear():
                 self._remove_batch(batch)
             else:
                 self._filter_batch(batch)
             yield from self._output_process(finished_reqs)
 
-    def _filter_runing_batch(self):
+    def _filter_running_batch(self):
         if self.running_batch is not None and self.running_batch.is_clear():
             self.running_batch = None
 
     def _add_token_id_to_req(self, batch: Batch, req_ans):
         for req_id, (new_token_id, new_gen_metadata) in req_ans.items():
             req = batch.id_to_reqs[req_id]
             req.output_ids.append(new_token_id)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/engine.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/engine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/inference/utils.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,29 +1,19 @@
 """
 Utils for model inference
 """
 import os
+import re
+from pathlib import Path
+from typing import Optional, Tuple
 
 import torch
+from torch import nn
 
-from colossalai.kernel.triton.copy_kv_cache_dest import copy_kv_cache_to_dest
-
-
-def copy_kv_to_mem_cache(layer_id, key_buffer, value_buffer, context_mem_index, mem_manager):
-    """
-    This function copies the key and value cache to the memory cache
-    Args:
-        layer_id : id of current layer
-        key_buffer : key cache
-        value_buffer : value cache
-        context_mem_index : index of memory cache in kv cache manager
-        mem_manager : cache manager
-    """
-    copy_kv_cache_to_dest(key_buffer, context_mem_index, mem_manager.key_buffer[layer_id])
-    copy_kv_cache_to_dest(value_buffer, context_mem_index, mem_manager.value_buffer[layer_id])
+from colossalai.testing import free_port
 
 
 def init_to_get_rotary(self, base=10000, use_elem=False):
     """
     This function initializes the rotary positional embedding, it is compatible for all models and is called in ShardFormer
     Args:
         self : Model that holds the rotary positional embedding
@@ -59,9 +49,67 @@
     if use_elem:
         n_elem //= 2
 
     inv_freq = 1.0 / (base ** (torch.arange(0, n_elem, 2, device="cpu", dtype=torch.float32) / n_elem))
     t = torch.arange(max_seq_len + 1024 * 64, device="cpu", dtype=torch.float32) / rope_scaling_factor
     freqs = torch.outer(t, inv_freq)
 
-    self._cos_cached = torch.cos(freqs).to(torch.float16).cuda()
-    self._sin_cached = torch.sin(freqs).to(torch.float16).cuda()
+    self._cos_cached = torch.cos(freqs).to(self.dtype).cuda()
+    self._sin_cached = torch.sin(freqs).to(self.dtype).cuda()
+
+
+def has_index_file(checkpoint_path: str) -> Tuple[bool, Optional[Path]]:
+    """
+    Check whether the checkpoint has an index file.
+
+    Args:
+        checkpoint_path (str): path to the checkpoint.
+
+    Returns:
+        Tuple[bool, Optional[Path]]: a tuple of (has_index_file, index_file_path)
+    """
+    checkpoint_path = Path(checkpoint_path)
+    if checkpoint_path.is_file():
+        # check if it is .index.json
+        reg = re.compile("(.*?).index((\..*)?).json")
+        if reg.fullmatch(checkpoint_path.name) is not None:
+            return True, checkpoint_path
+        else:
+            return False, None
+    elif checkpoint_path.is_dir():
+        index_files = list(checkpoint_path.glob("*.index.*json"))
+
+        for index_file in index_files:
+            if "safetensors" in index_file.__str__():
+                return True, index_file.__str__()  # return the safetensors file first
+
+        if len(index_files) == 1:
+            return True, index_files[0]
+        else:
+            assert (
+                len(index_files) == 1
+            ), f"Expected to find one .index.json file in {checkpoint_path}, but found {len(index_files)}"
+            return False, None
+    else:
+        raise RuntimeError(f"Invalid checkpoint path {checkpoint_path}. Expected a file or a directory.")
+
+
+def get_model_size(model: nn.Module):
+    """Calculates the total size of the model weights (including biases) in bytes.
+    Args:
+        model: The PyTorch model to analyze.
+    Returns:
+        The total size of the model weights in bytes.
+    """
+    total_size = 0
+    for key, param in model.named_parameters():
+        total_size += param.element_size() * param.numel()
+    return total_size / (1024**3)
+
+
+def find_available_ports(num: int):
+    try:
+        free_ports = [free_port() for i in range(num)]
+    except OSError as e:
+        print(f"An OS error occurred: {e}")
+        raise RuntimeError("Error finding available ports")
+    return free_ports
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/chatglm2.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,242 +1,357 @@
-import os
-from typing import Optional, Tuple
+""" PyTorch ChatGLM model. """
+
+from typing import List, Optional, Tuple
 
 import torch
+import torch.utils.checkpoint
 from torch.nn import CrossEntropyLoss
 from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
+from transformers.utils import logging
+
+from colossalai.pipeline.stage_manager import PipelineStageManager
+from colossalai.shardformer import ShardConfig
+from colossalai.shardformer.layer import AttnMaskType, ColoAttention
+from colossalai.shardformer.layer._operation import gather_forward_split_backward, split_forward_gather_backward
+
+
+def get_flash_core_attention_forward():
+    from .chatglm2_6b.modeling_chatglm import CoreAttention
+
+    def forward(self: CoreAttention, query_layer, key_layer, value_layer, attention_mask):
+        query_layer, key_layer, value_layer = [k.permute(1, 2, 0, 3) for k in [query_layer, key_layer, value_layer]]
+        if attention_mask is None and query_layer.shape[2] == key_layer.shape[2]:
+            attention_mask_type = AttnMaskType.CAUSAL
+            attn_bias = torch.zeros(
+                query_layer.shape[0],
+                1,
+                query_layer.shape[2],
+                key_layer.shape[2],
+                dtype=query_layer.dtype,
+                device=query_layer.device,
+            )
+            temp_mask = (
+                torch.ones(
+                    query_layer.shape[2],
+                    key_layer.shape[2],
+                    dtype=torch.bool,
+                    device=query_layer.device,
+                )
+                .tril(diagonal=0)
+                .expand(query_layer.shape[0], 1, -1, -1)
+            )
+            attn_bias.masked_fill_(temp_mask.logical_not(), torch.finfo(query_layer.dtype).min)
+        else:
+            attention_mask_type = AttnMaskType.CUSTOM
+            if attention_mask is not None:
+                attn_bias = torch.zeros_like(attention_mask, dtype=query_layer.dtype)
+                attn_bias.masked_fill_(attention_mask, torch.finfo(query_layer.dtype).min)
+        dropout_p = self.attention_dropout.p if self.training else 0.0
+        context_layer = ColoAttention.attention(
+            query_layer,
+            key_layer,
+            value_layer,
+            attention_mask=attn_bias,
+            attention_mask_type=attention_mask_type,
+            dropout_p=dropout_p,
+            scale=1.0 / self.norm_factor,
+        )
+        context_layer = context_layer.permute(2, 0, 1, 3)
+        new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)
+        context_layer = context_layer.reshape(*new_context_layer_shape)
+        return context_layer
+
+    return forward
+
 
-from colossalai.inference.tensor_parallel.batch_infer_state import BatchInferState
-from colossalai.kernel.triton.token_attention_kernel import Llama2TokenAttentionForwards
-from colossalai.shardformer.modeling.chatglm2_6b.modeling_chatglm import (
-    ChatGLMForConditionalGeneration,
-    ChatGLMModel,
-    GLMBlock,
-    GLMTransformer,
-    SelfAttention,
-    split_tensor_along_last_dim,
-)
-
-from ._utils import copy_kv_to_mem_cache
-
-try:
-    from lightllm.models.chatglm2.triton_kernel.rotary_emb import rotary_emb_fwd as chatglm2_rotary_emb_fwd
-    from lightllm.models.llama2.triton_kernel.context_flashattention_nopad import (
-        context_attention_fwd as lightllm_llama2_context_attention_fwd,
-    )
-
-    HAS_LIGHTLLM_KERNEL = True
-except:
-    print("please install lightllm from source to run inference: https://github.com/ModelTC/lightllm")
-    HAS_LIGHTLLM_KERNEL = False
-
-
-# This func is same as Llama model init_to_get_rotary, we should move them into _utils.py
-def _init_to_get_rotary(self, base=10000):
-    self.config.head_dim_ = self.config.hidden_size // self.config.num_attention_heads
-    if not hasattr(self.config, "rope_scaling"):
-        rope_scaling_factor = 1.0
-    else:
-        rope_scaling_factor = self.config.rope_scaling.factor if self.config.rope_scaling is not None else 1.0
-    if hasattr(self.config, "max_sequence_length"):
-        max_seq_len = self.config.max_sequence_length
-    elif hasattr(self.config, "max_position_embeddings"):
-        max_seq_len = self.config.max_position_embeddings * rope_scaling_factor
-    else:
-        max_seq_len = 2048 * rope_scaling_factor
-    base = float(base)
-
-    # NTK  ref: https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/
-    try:
-        ntk_alpha = float(os.environ.get("INFER_NTK_ALPHA", 1))
-        assert ntk_alpha >= 1
-        if ntk_alpha > 1:
-            print(f"Note: NTK enabled, alpha set to {ntk_alpha}")
-        max_seq_len *= ntk_alpha
-        base = base * (ntk_alpha ** (self.head_dim_ / (self.head_dim_ - 2)))  # Base change formula
-    except:
-        pass
-    n_elem = self.config.head_dim_ // 2
-    inv_freq = 1.0 / (base ** (torch.arange(0, n_elem, 2, device="cpu", dtype=torch.float32) / n_elem))
-    t = torch.arange(max_seq_len + 1024 * 64, device="cpu", dtype=torch.float32) / rope_scaling_factor
-    freqs = torch.outer(t, inv_freq)
-
-    self._cos_cached = torch.cos(freqs).to(torch.float16).cuda()
-    self._sin_cached = torch.sin(freqs).to(torch.float16).cuda()
-    return
-
-
-def get_masks(self, input_ids, past_length, padding_mask=None):
-    batch_size, seq_length = input_ids.shape
-    full_attention_mask = torch.ones(batch_size, seq_length, seq_length, device=input_ids.device)
-    full_attention_mask.tril_()
-    if past_length:
-        full_attention_mask = torch.cat(
-            (
-                torch.ones(batch_size, seq_length, past_length, device=input_ids.device),
-                full_attention_mask,
-            ),
-            dim=-1,
+def get_jit_fused_glm_block_forward():
+    from .chatglm2_6b.modeling_chatglm import GLMBlock
+
+    def forward(
+        self: GLMBlock,
+        hidden_states,
+        attention_mask,
+        rotary_pos_emb,
+        kv_cache=None,
+        use_cache=True,
+    ):
+        # hidden_states: [s, b, h]
+        # Layer norm at the beginning of the transformer layer.
+        layernorm_output = self.input_layernorm(hidden_states)
+        # Self attention.
+        attention_output, kv_cache = self.self_attention(
+            layernorm_output,
+            attention_mask,
+            rotary_pos_emb,
+            kv_cache=kv_cache,
+            use_cache=use_cache,
         )
 
-    if padding_mask is not None:
-        full_attention_mask = full_attention_mask * padding_mask.unsqueeze(1)
-    if not past_length and padding_mask is not None:
-        full_attention_mask -= padding_mask.unsqueeze(-1) - 1
-    full_attention_mask = (full_attention_mask < 0.5).bool()
-    full_attention_mask.unsqueeze_(1)
-    return full_attention_mask
+        # Residual connection.
+        if self.apply_residual_connection_post_layernorm:
+            residual = layernorm_output
+        else:
+            residual = hidden_states
 
+        layernorm_input = self.dropout_add(attention_output, residual, self.hidden_dropout, self.training)
 
-class ChatGLM2InferenceForwards:
+        # Layer norm post the self attention.
+        layernorm_output = self.post_attention_layernorm(layernorm_input)
+
+        # MLP.
+        mlp_output = self.mlp(layernorm_output)
+
+        # Second residual connection.
+        if self.apply_residual_connection_post_layernorm:
+            residual = layernorm_output
+        else:
+            residual = layernorm_input
+
+        output = self.dropout_add(mlp_output, residual, self.hidden_dropout, self.training)
+
+        return output, kv_cache
+
+    return forward
+
+
+class ChatGLMPipelineForwards:
     """
-    This class holds forwards for Chatglm2 inference.
-    We intend to replace the forward methods for ChatGLMModel, ChatGLMEecoderLayer, and ChatGLMAttention.
+    This class serves as a micro library for ChatGLM model forwards under pipeline parallelism.
     """
 
     @staticmethod
-    def chatglm_for_conditional_generation_forward(
-        self: ChatGLMForConditionalGeneration,
-        input_ids: Optional[torch.Tensor] = None,
+    def chatglm_model_forward(
+        self: "ChatGLMModel",
+        input_ids,
         position_ids: Optional[torch.Tensor] = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,
+        attention_mask: Optional[torch.BoolTensor] = None,
+        full_attention_mask: Optional[torch.BoolTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
         inputs_embeds: Optional[torch.Tensor] = None,
-        labels: Optional[torch.Tensor] = None,
         use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
-        return_last_logit: Optional[bool] = False,
+        stage_manager: Optional[PipelineStageManager] = None,
+        hidden_states: Optional[torch.FloatTensor] = None,
+        stage_index: Optional[List[int]] = None,
+        shard_config: ShardConfig = None,
     ):
+        logger = logging.get_logger(__name__)
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
         use_cache = use_cache if use_cache is not None else self.config.use_cache
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-        infer_state = self.infer_state
-
-        if input_ids is not None and inputs_embeds is not None:
-            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
-        elif input_ids is not None:
+        # TODO(jianghai): left the recording kv-value tensors as () or None type, this feature may be added in the future.
+        if past_key_values:
+            logger.warning_once("Non-empty past_key_values is not supported for pipeline models at the moment.")
+            past_key_values = None
+        if output_hidden_states:
+            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
+            output_hidden_states = False
+        if use_cache:
+            logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
+            use_cache = False
+        if stage_manager.is_first_stage():
             batch_size, seq_length = input_ids.shape
-        elif inputs_embeds is not None:
-            batch_size, seq_length, _ = inputs_embeds.shape
+            if inputs_embeds is None:
+                inputs_embeds = self.embedding(input_ids)
+            hidden_states = inputs_embeds
         else:
-            raise ValueError("You have to specify either input_ids or inputs_embeds")
-
-        if infer_state.is_context_stage:
-            past_key_values_length = 0
+            seq_length, batch_size = hidden_states.shape[:2]
+        if self.pre_seq_len is not None:
+            if past_key_values is None:
+                past_key_values = self.get_prompt(
+                    batch_size=batch_size,
+                    device=input_ids.device,
+                    dtype=inputs_embeds.dtype,
+                )
+            if attention_mask is not None:
+                attention_mask = torch.cat(
+                    [
+                        attention_mask.new_ones((batch_size, self.pre_seq_len)),
+                        attention_mask,
+                    ],
+                    dim=-1,
+                )
+        if full_attention_mask is None:
+            if (attention_mask is not None and not attention_mask.all()) or (past_key_values and seq_length != 1):
+                full_attention_mask = self.get_masks(input_ids, past_key_values, padding_mask=attention_mask)
+        # Rotary positional embeddings
+        rotary_pos_emb = self.rotary_pos_emb(self.seq_length)
+        if position_ids is not None:
+            rotary_pos_emb = rotary_pos_emb[position_ids]
         else:
-            past_key_values_length = infer_state.max_len_in_batch - 1
-
-        seq_length_with_past = seq_length + past_key_values_length
+            rotary_pos_emb = rotary_pos_emb[None, :seq_length]
+        rotary_pos_emb = rotary_pos_emb.transpose(0, 1).contiguous()
+        if not past_key_values:
+            past_key_values = [None for _ in range(self.num_layers)]
+        presents = () if use_cache else None
+        if self.encoder.gradient_checkpointing and self.encoder.training:
+            if use_cache:
+                logger.warning_once(
+                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+                )
+                use_cache = False
+        all_self_attentions = None
+        all_hidden_states = () if output_hidden_states else None
+        start_idx, end_idx = stage_index[0], stage_index[1]
 
-        # prefill stage at first
-        if use_cache and seq_length != 1:
-            infer_state.is_context_stage = True
-            infer_state.context_mem_index = infer_state.cache_manager.alloc(infer_state.total_token_num)
-            infer_state.init_block_loc(
-                infer_state.block_loc, infer_state.seq_len, seq_length, infer_state.context_mem_index
-            )
-        else:
-            infer_state.is_context_stage = False
-            alloc_mem = infer_state.cache_manager.alloc_contiguous(batch_size)
-            if alloc_mem is not None:
-                infer_state.decode_is_contiguous = True
-                infer_state.decode_mem_index = alloc_mem[0]
-                infer_state.decode_mem_start = alloc_mem[1]
-                infer_state.decode_mem_end = alloc_mem[2]
-                infer_state.block_loc[:, seq_length_with_past - 1] = infer_state.decode_mem_index
+        if shard_config and shard_config.enable_sequence_parallelism:
+            if shard_config.sequence_parallelism_mode == "split_gather":
+                hidden_states = split_forward_gather_backward(
+                    hidden_states,
+                    dim=0,
+                    process_group=shard_config.tensor_parallel_process_group,
+                )
+        for idx in range(start_idx, end_idx):
+            layer = self.encoder._get_layer(idx)
+            if output_hidden_states:
+                all_hidden_states = all_hidden_states + (hidden_states,)
+            if self.encoder.gradient_checkpointing and self.encoder.training:
+                layer_ret = torch.utils.checkpoint.checkpoint(
+                    layer,
+                    hidden_states,
+                    attention_mask,
+                    rotary_pos_emb,
+                    past_key_values[idx],
+                    use_cache,
+                )
             else:
-                print(f" *** Encountered allocation non-contiguous")
-                print(
-                    f"    infer_state.cache_manager.past_key_values_length: {infer_state.cache_manager.past_key_values_length}"
-                )
-                infer_state.decode_is_contiguous = False
-                alloc_mem = infer_state.cache_manager.alloc(batch_size)
-                infer_state.decode_mem_index = alloc_mem
-                # infer_state.decode_key_buffer = torch.empty((batch_size, self.tp_head_num_, self.head_dim_), dtype=torch.float16, device="cuda")
-                # infer_state.decode_value_buffer = torch.empty((batch_size, self.tp_head_num_, self.head_dim_), dtype=torch.float16, device="cuda")
-                infer_state.block_loc[:, seq_length_with_past - 1] = infer_state.decode_mem_index
-
-        # related to rotary embedding
-        if infer_state.is_context_stage:
-            infer_state.position_cos = torch.index_select(self._cos_cached, 0, position_ids.view(-1)).view(
-                position_ids.view(-1).shape[0], -1
-            )
-            infer_state.position_sin = torch.index_select(self._sin_cached, 0, position_ids.view(-1)).view(
-                position_ids.view(-1).shape[0], -1
+                layer_ret = layer(
+                    hidden_states,
+                    full_attention_mask,
+                    rotary_pos_emb,
+                    kv_cache=past_key_values[idx],
+                    use_cache=use_cache,
+                )
+            hidden_states, kv_cache = layer_ret
+            if use_cache:
+                presents = presents + (kv_cache,)
+
+        if shard_config and shard_config.enable_sequence_parallelism:
+            if shard_config.sequence_parallelism_mode == "split_gather":
+                hidden_states = gather_forward_split_backward(
+                    hidden_states,
+                    dim=0,
+                    process_group=shard_config.tensor_parallel_process_group,
+                )
+        if output_hidden_states:
+            all_hidden_states = all_hidden_states + (hidden_states,)
+        if stage_manager.is_last_stage():
+            # final layer_norm
+            if self.encoder.post_layer_norm:
+                hidden_states = self.encoder.final_layernorm(hidden_states)
+            if not return_dict:
+                return tuple(
+                    v
+                    for v in [
+                        hidden_states,
+                        presents,
+                        all_hidden_states,
+                        all_self_attentions,
+                    ]
+                    if v is not None
+                )
+            return BaseModelOutputWithPast(
+                last_hidden_state=hidden_states,
+                past_key_values=presents,
+                hidden_states=all_hidden_states,
+                attentions=all_self_attentions,
             )
         else:
-            seq_len = infer_state.seq_len
-            infer_state.position_cos = torch.index_select(self._cos_cached, 0, seq_len - 1).view(seq_len.shape[0], -1)
-            infer_state.position_sin = torch.index_select(self._sin_cached, 0, seq_len - 1).view(seq_len.shape[0], -1)
-            infer_state.other_kv_index = infer_state.block_loc[0, infer_state.max_len_in_batch - 1].item()
+            return {"hidden_states": hidden_states}
 
-        transformer_outputs = self.transformer(
+    @staticmethod
+    def chatglm_for_conditional_generation_forward(
+        self: "ChatGLMForConditionalGeneration",
+        input_ids: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.Tensor] = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        labels: Optional[torch.Tensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        return_last_logit: Optional[bool] = False,
+        stage_manager: Optional[PipelineStageManager] = None,
+        hidden_states: Optional[torch.FloatTensor] = None,
+        stage_index: Optional[List[int]] = None,
+        shard_config: ShardConfig = None,
+    ):
+        logging.get_logger(__name__)
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+        transformer_outputs = ChatGLMPipelineForwards.chatglm_model_forward(
+            self.transformer,
             input_ids=input_ids,
             position_ids=position_ids,
             attention_mask=attention_mask,
             past_key_values=past_key_values,
             inputs_embeds=inputs_embeds,
             use_cache=use_cache,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
-            infer_state=infer_state,
-        )
-
-        hidden_states = transformer_outputs[0]
-        if return_last_logit:
-            hidden_states = hidden_states[-1:]
-        lm_logits = self.transformer.output_layer(hidden_states)
-        lm_logits = lm_logits.transpose(0, 1).contiguous()
-
-        loss = None
-        if labels is not None:
-            lm_logits = lm_logits.to(torch.float32)
-
-            # Shift so that tokens < n predict n
-            shift_logits = lm_logits[..., :-1, :].contiguous()
-            shift_labels = labels[..., 1:].contiguous()
-            # Flatten the tokens
-            loss_fct = CrossEntropyLoss(ignore_index=-100)
-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
-
-            lm_logits = lm_logits.to(hidden_states.dtype)
-            loss = loss.to(hidden_states.dtype)
-
-        if not return_dict:
-            output = (lm_logits,) + transformer_outputs[1:]
-            return ((loss,) + output) if loss is not None else output
+            stage_manager=stage_manager,
+            hidden_states=hidden_states,
+            stage_index=stage_index,
+            shard_config=shard_config,
+        )
+        if stage_manager.is_last_stage():
+            hidden_states = transformer_outputs[0]
+            if return_last_logit:
+                hidden_states = hidden_states[-1:]
+            lm_logits = self.transformer.output_layer(hidden_states)
+            lm_logits = lm_logits.transpose(0, 1).contiguous()
+            loss = None
+            if labels is not None:
+                lm_logits = lm_logits.to(torch.float32)
+                # Shift so that tokens < n predict n
+                shift_logits = lm_logits[..., :-1, :].contiguous()
+                shift_labels = labels[..., 1:].contiguous()
+                # Flatten the tokens
+                loss_fct = CrossEntropyLoss(ignore_index=-100)
+                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
+                lm_logits = lm_logits.to(hidden_states.dtype)
+                loss = loss.to(hidden_states.dtype)
+            if not return_dict:
+                output = (lm_logits,) + transformer_outputs[1:]
+                return ((loss,) + output) if loss is not None else output
+            return CausalLMOutputWithPast(
+                loss=loss,
+                logits=lm_logits,
+                past_key_values=transformer_outputs.past_key_values,
+                hidden_states=transformer_outputs.hidden_states,
+                attentions=transformer_outputs.attentions,
+            )
+        else:
+            return transformer_outputs
 
-        return CausalLMOutputWithPast(
-            loss=loss,
-            logits=lm_logits,
-            past_key_values=transformer_outputs.past_key_values,
-            hidden_states=transformer_outputs.hidden_states,
-            attentions=transformer_outputs.attentions,
-        )
 
-    @staticmethod
-    def chatglm_model_forward(
-        self: ChatGLMModel,
+def get_chatglm_sequence_parallel_forward_fn(shard_config: ShardConfig):
+    def forward(
+        self,
         input_ids,
         position_ids: Optional[torch.Tensor] = None,
         attention_mask: Optional[torch.BoolTensor] = None,
         full_attention_mask: Optional[torch.BoolTensor] = None,
         past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
         inputs_embeds: Optional[torch.Tensor] = None,
         use_cache: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
-        infer_state: BatchInferState = None,
     ):
         output_hidden_states = (
             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
         )
         use_cache = use_cache if use_cache is not None else self.config.use_cache
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
         batch_size, seq_length = input_ids.shape
 
         if inputs_embeds is None:
             inputs_embeds = self.embedding(input_ids)
 
         if self.pre_seq_len is not None:
             if past_key_values is None:
@@ -249,35 +364,48 @@
                 attention_mask = torch.cat(
                     [
                         attention_mask.new_ones((batch_size, self.pre_seq_len)),
                         attention_mask,
                     ],
                     dim=-1,
                 )
+
         if full_attention_mask is None:
             if (attention_mask is not None and not attention_mask.all()) or (past_key_values and seq_length != 1):
-                full_attention_mask = get_masks(
-                    self, input_ids, infer_state.cache_manager.past_key_values_length, padding_mask=attention_mask
-                )
+                full_attention_mask = self.get_masks(input_ids, past_key_values, padding_mask=attention_mask)
+
+        # Rotary positional embeddings
+        rotary_pos_emb = self.rotary_pos_emb(self.seq_length)
+        if position_ids is not None:
+            rotary_pos_emb = rotary_pos_emb[position_ids]
+        else:
+            rotary_pos_emb = rotary_pos_emb[None, :seq_length]
+        rotary_pos_emb = rotary_pos_emb.transpose(0, 1).contiguous()
 
         # Run encoder.
+        # [seq_len, batch_size, hidden_size] -> [seq_len/TP_size, batch_size, hidden_size]
+        inputs_embeds = split_forward_gather_backward(
+            inputs_embeds,
+            dim=0,
+            process_group=shard_config.tensor_parallel_process_group,
+        )
         hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(
             inputs_embeds,
             full_attention_mask,
+            rotary_pos_emb=rotary_pos_emb,
             kv_caches=past_key_values,
             use_cache=use_cache,
             output_hidden_states=output_hidden_states,
-            infer_state=infer_state,
         )
 
-        # update indices
-        # infer_state.block_loc[:, infer_state.max_len_in_batch-1] = infer_state.total_token_num + torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
-        infer_state.start_loc = infer_state.start_loc + torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
-        infer_state.seq_len += 1
-        infer_state.max_len_in_batch += 1
+        hidden_states = gather_forward_split_backward(
+            hidden_states,
+            dim=0,
+            process_group=shard_config.tensor_parallel_process_group,
+        )
 
         if not return_dict:
             return tuple(
                 v
                 for v in [
                     hidden_states,
                     presents,
@@ -290,256 +418,8 @@
         return BaseModelOutputWithPast(
             last_hidden_state=hidden_states,
             past_key_values=presents,
             hidden_states=all_hidden_states,
             attentions=all_self_attentions,
         )
 
-    @staticmethod
-    def chatglm_encoder_forward(
-        self: GLMTransformer,
-        hidden_states,
-        attention_mask,
-        kv_caches=None,
-        use_cache: Optional[bool] = True,
-        output_hidden_states: Optional[bool] = False,
-        infer_state: Optional[BatchInferState] = None,
-    ):
-        hidden_states = hidden_states.transpose(0, 1).contiguous()
-        if not kv_caches:
-            kv_caches = [None for _ in range(self.num_layers)]
-        presents = () if use_cache else None
-        all_self_attentions = None
-        all_hidden_states = () if output_hidden_states else None
-
-        infer_state.decode_layer_id = 0
-        for index in range(self.num_layers):
-            layer = self.layers[index]
-
-            layer_ret = layer(
-                hidden_states,
-                attention_mask,
-                kv_cache=kv_caches[index],
-                use_cache=use_cache,
-                infer_state=infer_state,
-            )
-
-            infer_state.decode_layer_id += 1
-
-            hidden_states, kv_cache = layer_ret
-            if use_cache:
-                presents = presents + (kv_cache,)
-
-        if output_hidden_states:
-            all_hidden_states = all_hidden_states + (hidden_states,)
-
-        # Final layer norm.
-        hidden_states = hidden_states.transpose(0, 1).contiguous()
-
-        if self.post_layer_norm:
-            hidden_states = self.final_layernorm(hidden_states)
-
-        return hidden_states, presents, all_hidden_states, all_self_attentions
-
-    @staticmethod
-    def chatglm_glmblock_forward(
-        self: GLMBlock,
-        hidden_states,
-        attention_mask,
-        kv_cache=None,
-        use_cache=True,
-        infer_state: Optional[BatchInferState] = None,
-    ):
-        # hidden_states: [s, b, h]
-
-        # Layer norm at the beginning of the transformer layer.
-        layernorm_output = self.input_layernorm(hidden_states)
-        # Self attention.
-        attention_output, kv_cache = self.self_attention(
-            layernorm_output,
-            attention_mask,
-            kv_cache=kv_cache,
-            use_cache=use_cache,
-            infer_state=infer_state,
-        )
-        # Residual connection.
-        if self.apply_residual_connection_post_layernorm:
-            residual = layernorm_output
-        else:
-            residual = hidden_states
-        layernorm_input = torch.nn.functional.dropout(attention_output, p=self.hidden_dropout, training=self.training)
-        layernorm_input = residual + layernorm_input
-        # Layer norm post the self attention.
-        layernorm_output = self.post_attention_layernorm(layernorm_input)
-        # MLP.
-        mlp_output = self.mlp(layernorm_output)
-
-        # Second residual connection.
-        if self.apply_residual_connection_post_layernorm:
-            residual = layernorm_output
-        else:
-            residual = layernorm_input
-
-        output = torch.nn.functional.dropout(mlp_output, p=self.hidden_dropout, training=self.training)
-        output = residual + output
-        return output, kv_cache
-
-    @staticmethod
-    def chatglm_flash_attn_kvcache_forward(
-        self: SelfAttention,
-        hidden_states,
-        attention_mask,
-        kv_cache=None,
-        use_cache=True,
-        infer_state: Optional[BatchInferState] = None,
-    ):
-        assert use_cache is True, "use_cache should be set to True using this chatglm attention"
-        # hidden_states: original :[sq, b, h] --> this [b, sq, h]
-        batch_size = hidden_states.shape[0]
-        hidden_size = hidden_states.shape[-1]
-        # Attention heads [sq, b, h] --> [sq, b, (np * 3 * hn)]
-        mixed_x_layer = self.query_key_value(hidden_states)
-        if self.multi_query_attention:
-            (query_layer, key_layer, value_layer) = mixed_x_layer.split(
-                [
-                    self.num_attention_heads_per_partition * self.hidden_size_per_attention_head,
-                    self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,
-                    self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,
-                ],
-                dim=-1,
-            )
-            query_layer = query_layer.view(
-                query_layer.size()[:-1]
-                + (
-                    self.num_attention_heads_per_partition,
-                    self.hidden_size_per_attention_head,
-                )
-            )
-            key_layer = key_layer.view(
-                key_layer.size()[:-1]
-                + (
-                    self.num_multi_query_groups_per_partition,
-                    self.hidden_size_per_attention_head,
-                )
-            )
-            value_layer = value_layer.view(
-                value_layer.size()[:-1]
-                + (
-                    self.num_multi_query_groups_per_partition,
-                    self.hidden_size_per_attention_head,
-                )
-            )
-
-        else:
-            new_tensor_shape = mixed_x_layer.size()[:-1] + (
-                self.num_attention_heads_per_partition,
-                3 * self.hidden_size_per_attention_head,
-            )
-            mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)
-            # [sq, b, np, 3 * hn] --> 3 [sq, b, np, hn]
-            (query_layer, key_layer, value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)
-        cos, sin = infer_state.position_cos, infer_state.position_sin
-
-        chatglm2_rotary_emb_fwd(
-            query_layer.view(-1, self.num_attention_heads_per_partition, self.hidden_size_per_attention_head), cos, sin
-        )
-        if self.multi_query_attention:
-            chatglm2_rotary_emb_fwd(
-                key_layer.view(-1, self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head),
-                cos,
-                sin,
-            )
-        else:
-            chatglm2_rotary_emb_fwd(
-                key_layer.view(-1, self.num_attention_heads_per_partition, self.hidden_size_per_attention_head),
-                cos,
-                sin,
-            )
-
-        # reshape q k v  to [bsz*sql, num_heads, head_dim]   2*1 ,32/2 ,128
-        query_layer = query_layer.reshape(
-            -1, self.num_attention_heads_per_partition, self.hidden_size_per_attention_head
-        )
-        key_layer = key_layer.reshape(
-            -1, self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head
-        )
-        value_layer = value_layer.reshape(
-            -1, self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head
-        )
-
-        if infer_state.is_context_stage:
-            # first token generation:
-            # copy key and value calculated in current step to memory manager
-            copy_kv_to_mem_cache(
-                infer_state.decode_layer_id,
-                key_layer,
-                value_layer,
-                infer_state.context_mem_index,
-                infer_state.cache_manager,
-            )
-            attn_output = torch.empty_like(query_layer.contiguous().view(-1, self.projection_size))
-
-            # NOTE: no bug in context attn fwd (del it )
-            lightllm_llama2_context_attention_fwd(
-                query_layer,
-                key_layer,
-                value_layer,
-                attn_output.view(-1, self.num_attention_heads_per_partition, self.hidden_size_per_attention_head),
-                infer_state.start_loc,
-                infer_state.seq_len,
-                infer_state.max_len_in_batch,
-            )
-
-        else:
-            if infer_state.decode_is_contiguous:
-                # if decode is contiguous, then we copy to key cache and value cache in cache manager directly
-                cache_k = infer_state.cache_manager.key_buffer[infer_state.decode_layer_id][
-                    infer_state.decode_mem_start : infer_state.decode_mem_end, :, :
-                ]
-                cache_v = infer_state.cache_manager.value_buffer[infer_state.decode_layer_id][
-                    infer_state.decode_mem_start : infer_state.decode_mem_end, :, :
-                ]
-                cache_k.copy_(key_layer)
-                cache_v.copy_(value_layer)
-            else:
-                # if decode is not contiguous, use triton kernel to copy key and value cache
-                # k, v shape: [batch_size, num_heads, head_dim/embed_size_per_head
-                copy_kv_to_mem_cache(
-                    infer_state.decode_layer_id,
-                    key_layer,
-                    value_layer,
-                    infer_state.decode_mem_index,
-                    infer_state.cache_manager,
-                )
-
-            # second token and follows
-            attn_output = torch.empty_like(query_layer.contiguous().view(-1, self.projection_size))
-            cache_k = infer_state.cache_manager.key_buffer[infer_state.decode_layer_id][
-                : infer_state.decode_mem_end, :, :
-            ]
-            cache_v = infer_state.cache_manager.value_buffer[infer_state.decode_layer_id][
-                : infer_state.decode_mem_end, :, :
-            ]
-
-            # ==================================
-            # core attention computation is replaced by triton kernel
-            # ==================================
-            Llama2TokenAttentionForwards.token_attn(
-                query_layer,
-                cache_k,
-                cache_v,
-                attn_output,
-                infer_state.block_loc,
-                infer_state.start_loc,
-                infer_state.seq_len,
-                infer_state.max_len_in_batch,
-                infer_state.other_kv_index,
-            )
-
-            # print('after attention',torch.isnan(attn_output).any())
-
-        # =================
-        # Output:[b,sq, h]
-        # =================
-        output = self.dense(attn_output).reshape(batch_size, -1, hidden_size)
-
-        return output, kv_cache
+    return forward
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/bloom.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/policies/bloom.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/llama.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/inference/tensor_parallel/policies/llama.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/initialize.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/initialize.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/_ops/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/base_layer.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/base_layer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/dropout.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/dropout.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/embedding.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/linear.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/normalization.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/colossalai_layer/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_operation.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_1d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_1d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/layers.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_1d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_operation.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/layers.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/layers.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_2p5d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_operation.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_3d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_3d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/layers.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_3d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/_operation.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_sequence/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/layers.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/parallel_sequence/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/common.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/utils/common.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/layers.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/vanilla/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/loss/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_1d.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/loss/loss_1d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2d.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/loss/loss_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2p5d.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/loss/loss_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_3d.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/loss/loss_3d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/metric/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2d.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/metric/accuracy_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2p5d.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/metric/accuracy_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_3d.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/metric/accuracy_3d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/data_parallel.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/data_parallel.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/colo_module.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/colo_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/embedding.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/linear.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/module_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/layers/module_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/reducer.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/nn/parallel/reducer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/layer_spec.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/layer_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/fx.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/middleware/adaptor/fx.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/topo.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/middleware/topo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipelinable.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/pipelinable.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipeline_process_group.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/pipeline_process_group.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_base.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/rpc/_pipeline_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/rpc/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/pipeline/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/registry/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/registry/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/registry/registry.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/registry/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/compute_spec.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/tensor/compute_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/dist_spec_mgr.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/tensor/dist_spec_mgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/distspec.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/tensor/distspec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/op_wrapper.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/tensor/op_wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/process_group.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/tensor/process_group.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/tensor_spec.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/tensor/tensor_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/_trainer.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/trainer/_trainer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_base_hook.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/_base_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_checkpoint_hook.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/_checkpoint_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_log_hook.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/_log_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_metric_hook.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/trainer/hooks/_metric_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/activation_checkpoint.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/activation_checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/module_checkpoint.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/checkpoint/module_checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/checkpoint/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpointing.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/checkpointing.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/common.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/common.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/memory.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/memory.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/comm_profiler.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/legacy/comm_profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/prof_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/legacy/prof_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/profiler.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/colo_init_context.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/colo_init_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/gemini_context.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/gemini_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/ophooks/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/stateful_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_placement_policy.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/tensor_placement_policy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/gemini/tensor_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/init_context.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/init_ctx/init_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/base_shard_strategy.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/shard_utils/base_shard_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/commons.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/shard_utils/commons.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/reduce_scatter.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/reduce_scatter.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/sharded_model_v2.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/sharded_model_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/utils.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/zero_hook.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_model/zero_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_param.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_param/sharded_param.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_tensor.py` & `colossalai-nightly-2024.6.1/colossalai/legacy/zero/sharded_param/sharded_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/logging/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/logging/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/logging/logger.py` & `colossalai-nightly-2024.6.1/colossalai/logging/logger.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/moe/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/moe/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/moe/_operation.py` & `colossalai-nightly-2024.6.1/colossalai/moe/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/moe/checkpoint.py` & `colossalai-nightly-2024.6.1/colossalai/moe/checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/moe/experts.py` & `colossalai-nightly-2024.6.1/colossalai/moe/experts.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/moe/layers.py` & `colossalai-nightly-2024.6.1/colossalai/moe/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/moe/load_balance.py` & `colossalai-nightly-2024.6.1/colossalai/moe/load_balance.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/moe/loss.py` & `colossalai-nightly-2024.6.1/colossalai/moe/loss.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/moe/manager.py` & `colossalai-nightly-2024.6.1/colossalai/moe/manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/moe/routers.py` & `colossalai-nightly-2024.6.1/colossalai/moe/routers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/moe/utils.py` & `colossalai-nightly-2024.6.1/colossalai/moe/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/init.py` & `colossalai-nightly-2024.6.1/colossalai/nn/init.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/layer/layernorm.py` & `colossalai-nightly-2024.6.1/colossalai/nn/layer/layernorm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/cosine.py` & `colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/cosine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/delayed.py` & `colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/delayed.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/linear.py` & `colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/multistep.py` & `colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/multistep.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/onecycle.py` & `colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/onecycle.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/poly.py` & `colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/poly.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/torch.py` & `colossalai-nightly-2024.6.1/colossalai/nn/lr_scheduler/torch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/cpu_adam.py` & `colossalai-nightly-2024.6.1/colossalai/nn/optimizer/cpu_adam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_adam.py` & `colossalai-nightly-2024.6.1/colossalai/nn/optimizer/fused_adam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_lamb.py` & `colossalai-nightly-2024.6.1/colossalai/nn/optimizer/fused_lamb.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_sgd.py` & `colossalai-nightly-2024.6.1/colossalai/nn/optimizer/fused_sgd.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/hybrid_adam.py` & `colossalai-nightly-2024.6.1/colossalai/nn/optimizer/hybrid_adam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lamb.py` & `colossalai-nightly-2024.6.1/colossalai/nn/optimizer/lamb.py`

 * *Files 12% similar despite different names*

```diff
@@ -22,24 +22,26 @@
         adam (bool, optional): always use trust ratio = 1, which turns this into
             Adam. Useful for comparison purposes.
 
     .. _Large Batch Optimization for Deep Learning\: Training BERT in 76 minutes:
         https://arxiv.org/abs/1904.00962
     """
 
-    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0, adam=False):
+    def __init__(
+        self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0, adam=False, bias_correction=False
+    ):
         if not 0.0 <= lr:
             raise ValueError("Invalid learning rate: {}".format(lr))
         if not 0.0 <= eps:
             raise ValueError("Invalid epsilon value: {}".format(eps))
         if not 0.0 <= betas[0] < 1.0:
             raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
         if not 0.0 <= betas[1] < 1.0:
             raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
-        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
+        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, bias_correction=bias_correction)
         self.adam = adam
         super(Lamb, self).__init__(params, defaults)
 
     def step(self, closure=None):
         """Performs a single optimization step.
 
         Arguments:
@@ -75,34 +77,35 @@
 
                 # Decay the first and second moment running average coefficient
                 # m_t
                 exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                 # v_t
                 exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
 
-                # Paper v3 does not use debiasing.
-                # bias_correction1 = 1 - beta1 ** state['step']
-                # bias_correction2 = 1 - beta2 ** state['step']
-                # Apply bias to lr to avoid broadcast.
-                # * math.sqrt(bias_correction2) / bias_correction1
-                step_size = group["lr"]
+                # NOTE: Paper v3 does not use debiasing.
+                scaled_lr = group["lr"]
+                if group["bias_correction"]:
+                    bias_correction1 = 1 - beta1 ** state["step"]
+                    bias_correction2 = 1 - beta2 ** state["step"]
+                    # Apply debiasing to lr to avoid broadcast
+                    scaled_lr *= (bias_correction2**0.5) / bias_correction1
+                    # exp_avg.div_(bias_correction1)
+                    # exp_avg_sq.div_(bias_correction2)
 
                 weight_norm = p.data.pow(2).sum().sqrt()
 
                 adam_step = exp_avg / exp_avg_sq.sqrt().add(group["eps"])
                 if group["weight_decay"] != 0:
                     adam_step.add_(p.data, alpha=group["weight_decay"])
 
                 adam_norm = adam_step.pow(2).sum().sqrt()
                 if weight_norm == 0 or adam_norm == 0:
                     trust_ratio = 1
                 else:
                     trust_ratio = weight_norm / adam_norm
-                state["weight_norm"] = weight_norm
-                state["adam_norm"] = adam_norm
-                state["trust_ratio"] = trust_ratio
+
                 if self.adam:
                     trust_ratio = 1
 
-                p.data.add_(adam_step, alpha=-step_size * trust_ratio)
+                p.data.add_(adam_step, alpha=-scaled_lr * trust_ratio)
 
         return loss
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lars.py` & `colossalai-nightly-2024.6.1/colossalai/nn/optimizer/lars.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/nvme_optimizer.py` & `colossalai-nightly-2024.6.1/colossalai/nn/optimizer/nvme_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/pipeline/p2p.py` & `colossalai-nightly-2024.6.1/colossalai/pipeline/p2p.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/base.py` & `colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/generate.py` & `colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/generate.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/interleaved_pp.py` & `colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/interleaved_pp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/one_f_one_b.py` & `colossalai-nightly-2024.6.1/colossalai/pipeline/schedule/one_f_one_b.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/pipeline/stage_manager.py` & `colossalai-nightly-2024.6.1/colossalai/pipeline/stage_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/quantization/bnb.py` & `colossalai-nightly-2024.6.1/colossalai/quantization/bnb.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/quantization/bnb_config.py` & `colossalai-nightly-2024.6.1/colossalai/quantization/bnb_config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/layer/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/_operation.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/layer/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/attn.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/layer/attn.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/dropout.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/layer/dropout.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/embedding.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/layer/embedding.py`

 * *Files 0% similar despite different names*

```diff
@@ -245,15 +245,14 @@
             i.e. it remains as a fixed pad, defaults to None.
         dtype (:class:`torch.dtype`, optional): The dtype of parameters, defaults to None.
         weight_initializer (:class:`typing.Callable`, optional):
             he initializer of weight, defaults to normal initializer.
 
     The ``args`` and ``kwargs`` used in :class:``torch.nn.functional.embedding`` should contain:
     ::
-
         max_norm (float, optional): If given, each embedding vector with norm larger than max_norm is
                     renormalized to have norm max_norm. Note: this will modify weight in-place.
         norm_type (float, optional): The p of the p-norm to compute for the max_norm option. Default 2.
         scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse
                     of frequency of the words in the mini-batch. Default False.
         sparse (bool, optional): If True, gradient w.r.t. weight will be a sparse tensor. Default False.
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/linear.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/layer/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/loss.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/layer/loss.py`

 * *Files 6% similar despite different names*

```diff
@@ -18,27 +18,28 @@
     def forward(
         ctx,
         vocab_logits: torch.Tensor,
         target: torch.Tensor,
         ignore_index: int,
         process_group: ProcessGroup,
         vocab_size: int,
+        dtype=torch.float32,
     ):
         r"""
         Calculate the cross entropy loss before gather, the origin loss function is as follows:
         loss = -log(exp(x[class])/sum(exp(x[i]))
         and can be rewrite as:
         loss = log(sum(exp(x[i])) - x[class]
 
         To avoid the `nan` of log(sum(exp(x[i]))), we minus the max of x[i]
 
         Args:
             vocab_logits (:class:`torch.Tensor`): The logits of the vocabulary, shape is
               [batch_size, seq_len, vocab_size]
-            labels (:class:`torch.Tensor`): The labels of the vocabulary, shape is
+            target (:class:`torch.Tensor`): The labels of the vocabulary, shape is
               [batch_size, seq_len]
 
         Returns:
             :class:`torch.Tensor`: The cross entropy loss
         """
         # get the max
         logits_max = torch.max(vocab_logits, dim=-1)[0]
@@ -82,50 +83,52 @@
         pred_logits = pred_logits_1d.view_as(target)
         pred_logits[mask] = 0.0
 
         # allreduce the get all x(i,y)
         dist.all_reduce(pred_logits, op=dist.ReduceOp.SUM, group=process_group)
         exp_logits = vocab_logits
         torch.exp(vocab_logits, out=exp_logits)
-        sum_exp_logits = torch.sum(exp_logits, dim=-1)
+        sum_exp_logits = torch.sum(exp_logits, dim=-1, dtype=torch.float32)
         dist.all_reduce(sum_exp_logits, op=dist.ReduceOp.SUM, group=process_group)
 
         # calculate the loss
         # loss = log(sum(exp(x[i]))) - x[class]
         loss = torch.where(target == ignore_index, 0.0, torch.log(sum_exp_logits) - pred_logits)
         num_non_zero = torch.sum(loss != 0.0)
         ctx.inv_num_non_zero = 1.0 / num_non_zero
         loss = torch.sum(loss).div_(num_non_zero)
 
         # calculate the softmax
-        exp_logits.div_(sum_exp_logits.unsqueeze(dim=-1))
+        exp_logits = exp_logits.div(sum_exp_logits.unsqueeze(dim=-1)).to(dtype)
         exp_logits[target == ignore_index] = 0.0
         ctx.save_for_backward(exp_logits, mask, masked_target_1d)
+        ctx.dtype = dtype
 
         return loss
 
     @staticmethod
     def backward(ctx, grad_output):
         # retrieve the saved tensors
         grad_output = grad_output * ctx.inv_num_non_zero
         exp_logits, mask, masked_target_1d = ctx.saved_tensors
 
         # use exp logits as the input grad
         grad_logits = exp_logits
         partion_vocab_size = grad_logits.shape[-1]
         grad_logits_2d = grad_logits.view(-1, partion_vocab_size)
 
-        update = 1.0 - mask.view(-1).float()
+        update = 1.0 - mask.view(-1).float().to(ctx.dtype)
         grad_logits_2d[torch.arange(0, grad_logits_2d.shape[0]), masked_target_1d] -= update
 
         grad_logits.mul_(grad_output.unsqueeze(dim=-1))
-        return grad_logits, None, None, None, None
+        return grad_logits, None, None, None, None, None
 
 
 def cross_entropy_1d(
     vocab_logits: torch.Tensor,
     labels: torch.Tensor,
     ignore_index: int = -100,
     process_group: ProcessGroup = None,
     vocab_size: int = None,
+    dtype: torch.dtype = None,
 ) -> torch.Tensor:
-    return DistCrossEntropy.apply(vocab_logits, labels, ignore_index, process_group, vocab_size)
+    return DistCrossEntropy.apply(vocab_logits, labels, ignore_index, process_group, vocab_size, dtype)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/normalization.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/layer/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/parallel_module.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/layer/parallel_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/qkv_fused_linear.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/layer/qkv_fused_linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/utils.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/layer/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bert.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/bert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/blip2.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/blip2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bloom.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/bloom.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,14 +6,15 @@
 from torch.distributed import ProcessGroup
 from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
 from torch.nn import functional as F
 from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
 from transformers.modeling_outputs import (
     BaseModelOutputWithPastAndCrossAttentions,
     CausalLMOutputWithCrossAttentions,
+    CausalLMOutputWithPast,
     QuestionAnsweringModelOutput,
     SequenceClassifierOutputWithPast,
     TokenClassifierOutput,
 )
 from transformers.models.bloom.modeling_bloom import (
     BloomForCausalLM,
     BloomForQuestionAnswering,
@@ -23,14 +24,16 @@
 )
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer.layer._operation import gather_forward_split_backward, split_forward_gather_backward
 from colossalai.shardformer.shard import ShardConfig
 
+from ..layer import cross_entropy_1d
+
 logger = logging.get_logger(__name__)
 
 
 def build_bloom_alibi_tensor_fn(process_group: ProcessGroup) -> torch.Tensor:
     def build_bloom_alibi_tensor(
         self, attention_mask: torch.Tensor, num_heads: int, dtype: torch.dtype
     ) -> torch.Tensor:
@@ -350,29 +353,40 @@
             hidden_states=hidden_states,
             stage_index=stage_index,
             shard_config=shard_config,
         )
         past_key_values = None
         if stage_manager.is_last_stage():
             hidden_states = transformer_outputs[0]
-            lm_logits = self.lm_head(hidden_states)
+            lm_logits = self.lm_head(hidden_states).contiguous()
 
             loss = None
             if labels is not None:
                 # move labels to correct device to enable model parallelism
                 labels = labels.to(lm_logits.device)
                 # Shift so that tokens < n predict n
                 shift_logits = lm_logits[..., :-1, :].contiguous()
                 shift_labels = labels[..., 1:].contiguous()
                 batch_size, seq_length, vocab_size = shift_logits.shape
                 # Flatten the tokens
-                loss_fct = CrossEntropyLoss()
-                loss = loss_fct(
-                    shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length)
-                )
+                if shard_config.enable_tensor_parallelism and shard_config.parallel_output:
+                    new_vocab_size = lm_logits.shape[-1]
+                    shift_logits = shift_logits.view(-1, new_vocab_size)
+                    shift_labels = shift_labels.view(-1)
+                    loss = cross_entropy_1d(
+                        shift_logits,
+                        shift_labels,
+                        process_group=shard_config.tensor_parallel_process_group,
+                        vocab_size=self.lm_head.out_features,
+                        dtype=self.transformer.dtype,
+                    )
+                else:
+                    loss_fct = CrossEntropyLoss()
+                    shift_logits = shift_logits.view(-1, self.config.vocab_size)
+                    loss = loss_fct(shift_logits, shift_labels.view(-1))
 
             if not return_dict:
                 output = (lm_logits,) + transformer_outputs[1:]
                 return ((loss,) + output) if loss is not None else output
 
             return CausalLMOutputWithCrossAttentions(
                 loss=loss,
@@ -1061,7 +1075,83 @@
             last_hidden_state=hidden_states,
             past_key_values=presents,
             hidden_states=all_hidden_states,
             attentions=all_self_attentions,
         )
 
     return forward
+
+
+def get_lm_forward_with_dist_cross_entropy(shard_config: ShardConfig):
+    from transformers import BloomForCausalLM
+
+    def forward(
+        self: BloomForCausalLM,
+        input_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        head_mask: Optional[torch.Tensor] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        labels: Optional[torch.Tensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, CausalLMOutputWithPast]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
+            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
+            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
+        """
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        transformer_outputs = self.transformer(
+            input_ids=input_ids,
+            past_key_values=past_key_values,
+            attention_mask=attention_mask,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            use_cache=use_cache,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+        past_key_values = None
+        hidden_states = transformer_outputs[0]
+        lm_logits = self.lm_head(hidden_states)
+
+        loss = None
+        if labels is not None:
+            # move labels to correct device to enable model parallelism
+            labels = labels.to(lm_logits.device)
+            # Shift so that tokens < n predict n
+            shift_logits = lm_logits[..., :-1, :].contiguous()
+            shift_labels = labels[..., 1:].contiguous()
+            # Flatten the tokens
+            new_vocab_size = lm_logits.shape[-1]
+            shift_logits = shift_logits.view(-1, new_vocab_size)
+            shift_labels = shift_labels.view(-1)
+            loss = cross_entropy_1d(
+                shift_logits,
+                shift_labels,
+                process_group=shard_config.tensor_parallel_process_group,
+                vocab_size=self.lm_head.out_features,
+                dtype=self.transformer.dtype,
+            )
+        if not return_dict:
+            output = (lm_logits,) + transformer_outputs[1:]
+            return ((loss,) + output) if loss is not None else output
+
+        return CausalLMOutputWithPast(
+            loss=loss,
+            logits=lm_logits,
+            past_key_values=transformer_outputs.past_key_values,
+            hidden_states=transformer_outputs.hidden_states,
+            attentions=transformer_outputs.attentions,
+        )
+
+    return forward
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2.py` & `colossalai-nightly-2024.6.1/colossalai/inference/modeling/models/glide_llama.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,425 +1,475 @@
-""" PyTorch ChatGLM model. """
-
-from typing import List, Optional, Tuple
+# This is modified from huggingface transformers
+# https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/models/llama/modeling_llama.py
+import warnings
+from types import MethodType
+from typing import List, Optional, Tuple, Union
 
 import torch
-import torch.utils.checkpoint
-from torch.nn import CrossEntropyLoss
+import torch.nn as nn
+from transformers.cache_utils import Cache, DynamicCache
+from transformers.modeling_attn_mask_utils import (
+    _prepare_4d_causal_attention_mask,
+    _prepare_4d_causal_attention_mask_for_sdpa,
+)
 from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
-from transformers.utils import logging
-
-from colossalai.pipeline.stage_manager import PipelineStageManager
-from colossalai.shardformer import ShardConfig
-from colossalai.shardformer.layer import AttnMaskType, ColoAttention
-from colossalai.shardformer.layer._operation import gather_forward_split_backward, split_forward_gather_backward
-
-
-def get_flash_core_attention_forward():
-    from .chatglm2_6b.modeling_chatglm import CoreAttention
-
-    def forward(self: CoreAttention, query_layer, key_layer, value_layer, attention_mask):
-        query_layer, key_layer, value_layer = [k.permute(1, 2, 0, 3) for k in [query_layer, key_layer, value_layer]]
-        if attention_mask is None and query_layer.shape[2] == key_layer.shape[2]:
-            attention_mask_type = AttnMaskType.CAUSAL
-            attn_bias = torch.zeros(
-                query_layer.shape[0],
-                1,
-                query_layer.shape[2],
-                key_layer.shape[2],
-                dtype=query_layer.dtype,
-                device=query_layer.device,
-            )
-            temp_mask = (
-                torch.ones(
-                    query_layer.shape[2],
-                    key_layer.shape[2],
-                    dtype=torch.bool,
-                    device=query_layer.device,
-                )
-                .tril(diagonal=0)
-                .expand(query_layer.shape[0], 1, -1, -1)
-            )
-            attn_bias.masked_fill_(temp_mask.logical_not(), torch.finfo(query_layer.dtype).min)
-        else:
-            attention_mask_type = AttnMaskType.CUSTOM
-            if attention_mask is not None:
-                attn_bias = torch.zeros_like(attention_mask, dtype=query_layer.dtype)
-                attn_bias.masked_fill_(attention_mask, torch.finfo(query_layer.dtype).min)
-        dropout_p = self.attention_dropout.p if self.training else 0.0
-        context_layer = ColoAttention.attention(
-            query_layer,
-            key_layer,
-            value_layer,
-            attention_mask=attn_bias,
-            attention_mask_type=attention_mask_type,
-            dropout_p=dropout_p,
-            scale=1.0 / self.norm_factor,
+from transformers.models.llama.modeling_llama import (
+    LlamaAttention,
+    LlamaConfig,
+    LlamaDecoderLayer,
+    LlamaDynamicNTKScalingRotaryEmbedding,
+    LlamaForCausalLM,
+    LlamaLinearScalingRotaryEmbedding,
+    LlamaMLP,
+    LlamaModel,
+    LlamaRMSNorm,
+    LlamaRotaryEmbedding,
+)
+
+from colossalai.inference.spec import GlideInput
+from colossalai.kernel.triton import flash_decoding_attention
+from colossalai.logging import get_dist_logger
+
+logger = get_dist_logger(__name__)
+
+
+def rotate_half(x):
+    """Rotates half the hidden dims of the input."""
+    x1 = x[..., : x.shape[-1] // 2]
+    x2 = x[..., x.shape[-1] // 2 :]
+    return torch.cat((-x2, x1), dim=-1)
+
+
+def apply_single_rotary_pos_emb(q, cos, sin, position_ids):
+    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.
+    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]
+    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]
+    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
+    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
+    q_embed = (q * cos) + (rotate_half(q) * sin)
+    return q_embed
+
+
+def glide_llama_causal_lm_forward(
+    self: LlamaForCausalLM,
+    input_ids: torch.LongTensor = None,
+    glide_input: Optional[GlideInput] = None,
+    attention_mask: Optional[torch.Tensor] = None,
+    position_ids: Optional[torch.LongTensor] = None,
+    past_key_values: Optional[List[torch.FloatTensor]] = None,
+    inputs_embeds: Optional[torch.FloatTensor] = None,
+    labels: Optional[torch.LongTensor] = None,
+    use_cache: Optional[bool] = None,
+    output_attentions: Optional[bool] = None,
+    output_hidden_states: Optional[bool] = None,
+    return_dict: Optional[bool] = None,
+) -> Union[Tuple, CausalLMOutputWithPast]:
+    r"""
+    Args:
+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
+
+    Returns:
+
+    Example:
+
+    ```python
+    >>> from transformers import AutoTokenizer, LlamaForCausalLM
+
+    >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
+    >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)
+
+    >>> prompt = "Hey, are you conscious? Can you talk to me?"
+    >>> inputs = tokenizer(prompt, return_tensors="pt")
+
+    >>> # Generate
+    >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
+    >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
+    "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
+    ```"""
+    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+    output_hidden_states = (
+        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+    )
+    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+    outputs = self.model(
+        input_ids=input_ids,
+        glide_input=glide_input,
+        attention_mask=attention_mask,
+        position_ids=position_ids,
+        past_key_values=past_key_values,
+        inputs_embeds=inputs_embeds,
+        use_cache=use_cache,
+        output_attentions=output_attentions,
+        output_hidden_states=output_hidden_states,
+        return_dict=return_dict,
+    )
+
+    hidden_states = outputs[0]
+    logits = self.lm_head(hidden_states)
+    logits = logits.float()
+
+    if not return_dict:
+        output = (logits,) + outputs[1:]
+        return output
+
+    return CausalLMOutputWithPast(
+        loss=None,
+        logits=logits,
+        past_key_values=outputs.past_key_values,
+        hidden_states=outputs.hidden_states,
+        attentions=outputs.attentions,
+    )
+
+
+def glide_llama_model_forward(
+    self: LlamaModel,
+    input_ids: torch.LongTensor = None,
+    glide_input: GlideInput = None,
+    attention_mask: Optional[torch.Tensor] = None,
+    position_ids: Optional[torch.LongTensor] = None,
+    past_key_values: Optional[List[torch.FloatTensor]] = None,
+    inputs_embeds: Optional[torch.FloatTensor] = None,
+    use_cache: Optional[bool] = None,
+    output_attentions: Optional[bool] = None,
+    output_hidden_states: Optional[bool] = None,
+    return_dict: Optional[bool] = None,
+) -> Union[Tuple, BaseModelOutputWithPast]:
+    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+    output_hidden_states = (
+        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+    )
+    use_cache = use_cache if use_cache is not None else self.config.use_cache
+
+    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+    # retrieve input_ids and inputs_embeds
+    if input_ids is not None and inputs_embeds is not None:
+        raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
+    elif input_ids is not None:
+        batch_size, seq_length = input_ids.shape[:2]
+    elif inputs_embeds is not None:
+        batch_size, seq_length = inputs_embeds.shape[:2]
+    else:
+        raise ValueError("You have to specify either input_ids or inputs_embeds")
+
+    past_key_values_length = 0
+    if use_cache:
+        use_legacy_cache = not isinstance(past_key_values, Cache)
+        if use_legacy_cache:
+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)
+        past_key_values_length = past_key_values.get_usable_length(seq_length)
+
+    if position_ids is None:
+        device = input_ids.device if input_ids is not None else inputs_embeds.device
+        position_ids = torch.arange(
+            past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
         )
-        context_layer = context_layer.permute(2, 0, 1, 3)
-        new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)
-        context_layer = context_layer.reshape(*new_context_layer_shape)
-        return context_layer
+        position_ids = position_ids.unsqueeze(0)
 
-    return forward
+    if inputs_embeds is None:
+        inputs_embeds = self.embed_tokens(input_ids)
 
-
-def get_jit_fused_glm_block_forward():
-    from .chatglm2_6b.modeling_chatglm import GLMBlock
-
-    def forward(
-        self: GLMBlock,
-        hidden_states,
-        attention_mask,
-        rotary_pos_emb,
-        kv_cache=None,
-        use_cache=True,
-    ):
-        # hidden_states: [s, b, h]
-        # Layer norm at the beginning of the transformer layer.
-        layernorm_output = self.input_layernorm(hidden_states)
-        # Self attention.
-        attention_output, kv_cache = self.self_attention(
-            layernorm_output,
+    if self._use_flash_attention_2:
+        # 2d mask is passed through the layers
+        attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
+    elif self._use_sdpa and not output_attentions:
+        # output_attentions=True can not be supported when using SDPA, and we fall back on
+        # the manual implementation that requires a 4D causal mask in all cases.
+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
             attention_mask,
-            rotary_pos_emb,
-            kv_cache=kv_cache,
-            use_cache=use_cache,
+            (batch_size, seq_length),
+            inputs_embeds,
+            past_key_values_length,
+        )
+    else:
+        # 4d mask is passed through the layers
+        attention_mask = _prepare_4d_causal_attention_mask(
+            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
         )
 
-        # Residual connection.
-        if self.apply_residual_connection_post_layernorm:
-            residual = layernorm_output
-        else:
-            residual = hidden_states
+    # embed positions
+    hidden_states = inputs_embeds
 
-        layernorm_input = self.dropout_add(attention_output, residual, self.hidden_dropout, self.training)
+    # decoder layers
+    all_hidden_states = () if output_hidden_states else None
+    all_self_attns = () if output_attentions else None
+    next_decoder_cache = () if use_cache else None
 
-        # Layer norm post the self attention.
-        layernorm_output = self.post_attention_layernorm(layernorm_input)
+    for decoder_layer in self.layers:
+        if output_hidden_states:
+            all_hidden_states += (hidden_states,)
 
-        # MLP.
-        mlp_output = self.mlp(layernorm_output)
+        # GlideLlamaDecoderLayer
+        layer_outputs = decoder_layer(
+            hidden_states,
+            glide_input=glide_input,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_values,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+        )
 
-        # Second residual connection.
-        if self.apply_residual_connection_post_layernorm:
-            residual = layernorm_output
-        else:
-            residual = layernorm_input
+        hidden_states = layer_outputs[0]
 
-        output = self.dropout_add(mlp_output, residual, self.hidden_dropout, self.training)
+        if use_cache:
+            next_decoder_cache = layer_outputs[2 if output_attentions else 1]
 
-        return output, kv_cache
+        if output_attentions:
+            all_self_attns += (layer_outputs[1],)
 
-    return forward
+    hidden_states = self.norm(hidden_states)
 
+    # add hidden states from the last decoder layer
+    if output_hidden_states:
+        all_hidden_states += (hidden_states,)
+
+    next_cache = None
+    if use_cache:
+        next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache
+    if not return_dict:
+        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+    return BaseModelOutputWithPast(
+        last_hidden_state=hidden_states,
+        past_key_values=next_cache,
+        hidden_states=all_hidden_states,
+        attentions=all_self_attns,
+    )
 
-class ChatGLMPipelineForwards:
-    """
-    This class serves as a micro library for ChatGLM model forwards under pipeline parallelism.
-    """
 
-    @staticmethod
-    def chatglm_model_forward(
-        self: "ChatGLMModel",
-        input_ids,
-        position_ids: Optional[torch.Tensor] = None,
-        attention_mask: Optional[torch.BoolTensor] = None,
-        full_attention_mask: Optional[torch.BoolTensor] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
-        inputs_embeds: Optional[torch.Tensor] = None,
-        use_cache: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-        stage_manager: Optional[PipelineStageManager] = None,
-        hidden_states: Optional[torch.FloatTensor] = None,
-        stage_index: Optional[List[int]] = None,
-        shard_config: ShardConfig = None,
+class GlideLlamaConfig(LlamaConfig):
+    """Configuration class with specific arguments used by GLIDE llama model as a drafter"""
+
+    def __init__(
+        self,
+        large_hidden_size=4096,
+        large_num_attention_heads=32,
+        **kwargs,
     ):
-        logger = logging.get_logger(__name__)
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-        )
-        use_cache = use_cache if use_cache is not None else self.config.use_cache
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-        # TODO(jianghai): left the recording kv-value tensors as () or None type, this feature may be added in the future.
-        if past_key_values:
-            logger.warning_once("Non-empty past_key_values is not supported for pipeline models at the moment.")
-            past_key_values = None
-        if output_hidden_states:
-            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
-            output_hidden_states = False
-        if use_cache:
-            logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
-            use_cache = False
-        if stage_manager.is_first_stage():
-            batch_size, seq_length = input_ids.shape
-            if inputs_embeds is None:
-                inputs_embeds = self.embedding(input_ids)
-            hidden_states = inputs_embeds
-        else:
-            seq_length, batch_size = hidden_states.shape[:2]
-        if self.pre_seq_len is not None:
-            if past_key_values is None:
-                past_key_values = self.get_prompt(
-                    batch_size=batch_size,
-                    device=input_ids.device,
-                    dtype=inputs_embeds.dtype,
-                )
-            if attention_mask is not None:
-                attention_mask = torch.cat(
-                    [
-                        attention_mask.new_ones((batch_size, self.pre_seq_len)),
-                        attention_mask,
-                    ],
-                    dim=-1,
-                )
-        if full_attention_mask is None:
-            if (attention_mask is not None and not attention_mask.all()) or (past_key_values and seq_length != 1):
-                full_attention_mask = self.get_masks(input_ids, past_key_values, padding_mask=attention_mask)
-        # Rotary positional embeddings
-        rotary_pos_emb = self.rotary_pos_emb(self.seq_length)
-        if position_ids is not None:
-            rotary_pos_emb = rotary_pos_emb[position_ids]
-        else:
-            rotary_pos_emb = rotary_pos_emb[None, :seq_length]
-        rotary_pos_emb = rotary_pos_emb.transpose(0, 1).contiguous()
-        if not past_key_values:
-            past_key_values = [None for _ in range(self.num_layers)]
-        presents = () if use_cache else None
-        if self.encoder.gradient_checkpointing and self.encoder.training:
-            if use_cache:
-                logger.warning_once(
-                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
-                )
-                use_cache = False
-        all_self_attentions = None
-        all_hidden_states = () if output_hidden_states else None
-        start_idx, end_idx = stage_index[0], stage_index[1]
-
-        if shard_config and shard_config.enable_sequence_parallelism:
-            if shard_config.sequence_parallelism_mode == "split_gather":
-                hidden_states = split_forward_gather_backward(
-                    hidden_states,
-                    dim=0,
-                    process_group=shard_config.tensor_parallel_process_group,
-                )
-        for idx in range(start_idx, end_idx):
-            layer = self.encoder._get_layer(idx)
-            if output_hidden_states:
-                all_hidden_states = all_hidden_states + (hidden_states,)
-            if self.encoder.gradient_checkpointing and self.encoder.training:
-                layer_ret = torch.utils.checkpoint.checkpoint(
-                    layer,
-                    hidden_states,
-                    attention_mask,
-                    rotary_pos_emb,
-                    past_key_values[idx],
-                    use_cache,
-                )
-            else:
-                layer_ret = layer(
-                    hidden_states,
-                    full_attention_mask,
-                    rotary_pos_emb,
-                    kv_cache=past_key_values[idx],
-                    use_cache=use_cache,
-                )
-            hidden_states, kv_cache = layer_ret
-            if use_cache:
-                presents = presents + (kv_cache,)
-
-        if shard_config and shard_config.enable_sequence_parallelism:
-            if shard_config.sequence_parallelism_mode == "split_gather":
-                hidden_states = gather_forward_split_backward(
-                    hidden_states,
-                    dim=0,
-                    process_group=shard_config.tensor_parallel_process_group,
-                )
-        if output_hidden_states:
-            all_hidden_states = all_hidden_states + (hidden_states,)
-        if stage_manager.is_last_stage():
-            # final layer_norm
-            if self.encoder.post_layer_norm:
-                hidden_states = self.encoder.final_layernorm(hidden_states)
-            if not return_dict:
-                return tuple(
-                    v
-                    for v in [
-                        hidden_states,
-                        presents,
-                        all_hidden_states,
-                        all_self_attentions,
-                    ]
-                    if v is not None
-                )
-            return BaseModelOutputWithPast(
-                last_hidden_state=hidden_states,
-                past_key_values=presents,
-                hidden_states=all_hidden_states,
-                attentions=all_self_attentions,
+        super().__init__(**kwargs)
+        self.large_hidden_size = large_hidden_size
+        self.large_num_attention_heads = large_num_attention_heads
+
+
+class LlamaCrossAttention(nn.Module):
+    """Multi-headed attention from 'Attention Is All You Need' paper"""
+
+    def __init__(self, config: GlideLlamaConfig):
+        super().__init__()
+        self.config = config
+        self.hidden_size = config.hidden_size
+        self.num_heads = config.num_attention_heads
+        self.head_dim = self.hidden_size // self.num_heads
+        self.num_key_value_heads = config.num_key_value_heads
+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
+        self.max_position_embeddings = config.max_position_embeddings
+
+        if (self.head_dim * self.num_heads) != self.hidden_size:
+            raise ValueError(
+                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
+                f" and `num_heads`: {self.num_heads})."
             )
-        else:
-            return {"hidden_states": hidden_states}
 
-    @staticmethod
-    def chatglm_for_conditional_generation_forward(
-        self: "ChatGLMForConditionalGeneration",
-        input_ids: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.Tensor] = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,
-        inputs_embeds: Optional[torch.Tensor] = None,
-        labels: Optional[torch.Tensor] = None,
-        use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-        return_last_logit: Optional[bool] = False,
-        stage_manager: Optional[PipelineStageManager] = None,
-        hidden_states: Optional[torch.FloatTensor] = None,
-        stage_index: Optional[List[int]] = None,
-        shard_config: ShardConfig = None,
-    ):
-        logging.get_logger(__name__)
-        use_cache = use_cache if use_cache is not None else self.config.use_cache
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-        transformer_outputs = ChatGLMPipelineForwards.chatglm_model_forward(
-            self.transformer,
-            input_ids=input_ids,
-            position_ids=position_ids,
-            attention_mask=attention_mask,
-            past_key_values=past_key_values,
-            inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
-            stage_manager=stage_manager,
-            hidden_states=hidden_states,
-            stage_index=stage_index,
-            shard_config=shard_config,
-        )
-        if stage_manager.is_last_stage():
-            hidden_states = transformer_outputs[0]
-            if return_last_logit:
-                hidden_states = hidden_states[-1:]
-            lm_logits = self.transformer.output_layer(hidden_states)
-            lm_logits = lm_logits.transpose(0, 1).contiguous()
-            loss = None
-            if labels is not None:
-                lm_logits = lm_logits.to(torch.float32)
-                # Shift so that tokens < n predict n
-                shift_logits = lm_logits[..., :-1, :].contiguous()
-                shift_labels = labels[..., 1:].contiguous()
-                # Flatten the tokens
-                loss_fct = CrossEntropyLoss(ignore_index=-100)
-                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
-                lm_logits = lm_logits.to(hidden_states.dtype)
-                loss = loss.to(hidden_states.dtype)
-            if not return_dict:
-                output = (lm_logits,) + transformer_outputs[1:]
-                return ((loss,) + output) if loss is not None else output
-            return CausalLMOutputWithPast(
-                loss=loss,
-                logits=lm_logits,
-                past_key_values=transformer_outputs.past_key_values,
-                hidden_states=transformer_outputs.hidden_states,
-                attentions=transformer_outputs.attentions,
+        # large model (verifier) configs
+        self.large_hidden_size = config.large_hidden_size
+        self.large_num_heads = config.large_num_attention_heads
+        self.large_head_dim = self.large_hidden_size // self.large_num_heads
+
+        self.q_proj = nn.Linear(self.hidden_size, self.large_num_heads * self.large_head_dim, bias=False)
+        self.o_proj = nn.Linear(self.large_num_heads * self.large_head_dim, self.hidden_size, bias=False)
+        self._init_rope()
+
+    def _init_rope(self):
+        if self.config.rope_scaling is None:
+            self.rotary_emb = LlamaRotaryEmbedding(
+                self.large_head_dim,
+                max_position_embeddings=self.max_position_embeddings,
             )
         else:
-            return transformer_outputs
+            scaling_type = self.config.rope_scaling["type"]
+            scaling_factor = self.config.rope_scaling["factor"]
+            if scaling_type == "linear":
+                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(
+                    self.large_head_dim,
+                    max_position_embeddings=self.max_position_embeddings,
+                    scaling_factor=scaling_factor,
+                )
+            elif scaling_type == "dynamic":
+                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(
+                    self.large_head_dim,
+                    max_position_embeddings=self.max_position_embeddings,
+                    scaling_factor=scaling_factor,
+                )
+            else:
+                raise ValueError(f"Unknown RoPE scaling type {scaling_type}")
 
+    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
+        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()
 
-def get_chatglm_sequence_parallel_forward_fn(shard_config: ShardConfig):
     def forward(
         self,
-        input_ids,
-        position_ids: Optional[torch.Tensor] = None,
-        attention_mask: Optional[torch.BoolTensor] = None,
-        full_attention_mask: Optional[torch.BoolTensor] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
-        inputs_embeds: Optional[torch.Tensor] = None,
-        use_cache: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-    ):
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-        )
-        use_cache = use_cache if use_cache is not None else self.config.use_cache
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+        hidden_states: torch.Tensor,
+        glide_input: GlideInput = None,  # Used for glimpsing main model's KV caches
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        output_attentions: bool = False,
+        use_cache: bool = False,
+    ) -> Optional[torch.Tensor]:
+        bsz, q_len, _ = hidden_states.size()
+
+        block_tables = glide_input.block_tables
+        large_k_cache = glide_input.large_k_cache
+        large_v_cache = glide_input.large_v_cache
+        sequence_lengths = glide_input.sequence_lengths
+        cache_block_size = large_k_cache.size(-2)
+
+        query_states = self.q_proj(hidden_states)
+        kv_seq_len = sequence_lengths.max().item()
+
+        query_states = query_states.view(bsz, -1, self.large_num_heads, self.large_head_dim).transpose(1, 2)
+
+        # for RoPE
+        cos, sin = self.rotary_emb(query_states, seq_len=kv_seq_len + 32)
+        query_states = apply_single_rotary_pos_emb(query_states, cos, sin, position_ids)
+        query_states = query_states.transpose(1, 2)
+        query_states = query_states.reshape(-1, self.large_num_heads, self.large_head_dim)
+
+        attn_output = flash_decoding_attention(
+            q=query_states,
+            k_cache=large_k_cache,
+            v_cache=large_v_cache,
+            kv_seq_len=sequence_lengths,
+            block_tables=block_tables,
+            block_size=cache_block_size,
+            max_seq_len_in_batch=kv_seq_len,
+        )  # attn_output: [bsz * q_len, num_heads * head_dim]
+
+        attn_output = attn_output.reshape(bsz, q_len, self.large_hidden_size)
+
+        attn_output = self.o_proj(attn_output)
+
+        return attn_output
+
+
+# A class to be used to replace LlamaDecoderLayer in a Llama Model as Drafter in speculative decoding.
+# Refer to GLIDE with a CAPE https://arxiv.org/pdf/2402.02082.pdf
+class GlideLlamaDecoderLayer(nn.Module):
+    def __init__(self, config: GlideLlamaConfig, layer_idx: Optional[int] = None):
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)
+        self.cross_attn = LlamaCrossAttention(config=config)
+        self.mlp = LlamaMLP(config)
+        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
 
-        batch_size, seq_length = input_ids.shape
+    @staticmethod
+    def from_native_module(module: LlamaDecoderLayer, *args, **kwargs) -> "GlideLlamaDecoderLayer":
+        """Build a GlideLlamaDecoderLayer from a native LlamaDecoderLayer"""
+        config: LlamaConfig = module.mlp.config  # XXX
+        layer_idx = module.self_attn.layer_idx
+        glide_config = GlideLlamaConfig(**config.to_dict())
+        glide_decoder_layer = GlideLlamaDecoderLayer(glide_config, layer_idx=layer_idx)
 
-        if inputs_embeds is None:
-            inputs_embeds = self.embedding(input_ids)
+        return glide_decoder_layer
 
-        if self.pre_seq_len is not None:
-            if past_key_values is None:
-                past_key_values = self.get_prompt(
-                    batch_size=batch_size,
-                    device=input_ids.device,
-                    dtype=inputs_embeds.dtype,
-                )
-            if attention_mask is not None:
-                attention_mask = torch.cat(
-                    [
-                        attention_mask.new_ones((batch_size, self.pre_seq_len)),
-                        attention_mask,
-                    ],
-                    dim=-1,
-                )
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        glide_input: GlideInput = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        output_attentions: Optional[bool] = False,
+        use_cache: Optional[bool] = False,
+        **kwargs,
+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
+        """
+        Args:
+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
+            attention_mask (`torch.FloatTensor`, *optional*):
+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,
+                query_sequence_length, key_sequence_length)` if default attention is used.
+            output_attentions (`bool`, *optional*):
+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
+                returned tensors for more detail.
+            use_cache (`bool`, *optional*):
+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
+                (see `past_key_values`).
+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
+        """
+        if "padding_mask" in kwargs:
+            warnings.warn(
+                "Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`"
+            )
 
-        if full_attention_mask is None:
-            if (attention_mask is not None and not attention_mask.all()) or (past_key_values and seq_length != 1):
-                full_attention_mask = self.get_masks(input_ids, past_key_values, padding_mask=attention_mask)
-
-        # Rotary positional embeddings
-        rotary_pos_emb = self.rotary_pos_emb(self.seq_length)
-        if position_ids is not None:
-            rotary_pos_emb = rotary_pos_emb[position_ids]
-        else:
-            rotary_pos_emb = rotary_pos_emb[None, :seq_length]
-        rotary_pos_emb = rotary_pos_emb.transpose(0, 1).contiguous()
+        residual = hidden_states
 
-        # Run encoder.
-        # [seq_len, batch_size, hidden_size] -> [seq_len/TP_size, batch_size, hidden_size]
-        inputs_embeds = split_forward_gather_backward(
-            inputs_embeds,
-            dim=0,
-            process_group=shard_config.tensor_parallel_process_group,
-        )
-        hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(
-            inputs_embeds,
-            full_attention_mask,
-            rotary_pos_emb=rotary_pos_emb,
-            kv_caches=past_key_values,
+        hidden_states = self.input_layernorm(hidden_states)
+
+        # Self Attention
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
             use_cache=use_cache,
-            output_hidden_states=output_hidden_states,
+            **kwargs,
         )
+        hidden_states = residual + hidden_states
 
-        hidden_states = gather_forward_split_backward(
-            hidden_states,
-            dim=0,
-            process_group=shard_config.tensor_parallel_process_group,
-        )
+        curr_q_len = hidden_states.size(1)
+        # Cross attention
+        if glide_input is None or not glide_input.glimpse_ready:
+            warnings.warn(
+                "Data used for glimpsing the past KV caches of the main model (verifier) is not complete. "
+                "Fall back to normal decoder layer modeling (drafter). "
+                "This might lead to incorrect results when using the Glide Models for speculative decoding."
+            )
+        elif curr_q_len == 1:
+            # Notice that we skip prefill stage
+            # always use the output of the main model as the inputs for the next round of speculation
+            residual = hidden_states
 
-        if not return_dict:
-            return tuple(
-                v
-                for v in [
-                    hidden_states,
-                    presents,
-                    all_hidden_states,
-                    all_self_attentions,
-                ]
-                if v is not None
+            hidden_states = self.cross_attn(
+                hidden_states=hidden_states,
+                glide_input=glide_input,
+                attention_mask=attention_mask,
+                position_ids=position_ids,
+                output_attentions=output_attentions,
+                use_cache=True,
             )
+            hidden_states = residual + hidden_states
 
-        return BaseModelOutputWithPast(
-            last_hidden_state=hidden_states,
-            past_key_values=presents,
-            hidden_states=all_hidden_states,
-            attentions=all_self_attentions,
-        )
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = self.post_attention_layernorm(hidden_states)
+        hidden_states = self.mlp(hidden_states)
+        hidden_states = residual + hidden_states
+
+        outputs = (hidden_states,)
 
-    return forward
+        if use_cache:
+            outputs += (present_key_value,)
+
+        return outputs
+
+
+class GlideLlamaForCausalLM(LlamaForCausalLM):
+    def __init__(self, config: GlideLlamaConfig):
+        super().__init__(config)
+        self.config = config
+        bound_method = MethodType(glide_llama_causal_lm_forward, self)
+        setattr(self, "forward", bound_method)
+        bound_method = MethodType(glide_llama_model_forward, self.model)
+        model = getattr(self, "model")
+        setattr(model, "forward", bound_method)
+        replaced_layers = nn.ModuleList(
+            [GlideLlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
+        )
+        setattr(model, "layers", replaced_layers)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/falcon.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/falcon.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,14 +10,15 @@
     AttentionMaskConverter,
     _prepare_4d_causal_attention_mask,
     _prepare_4d_causal_attention_mask_for_sdpa,
 )
 from transformers.modeling_outputs import (
     BaseModelOutputWithPastAndCrossAttentions,
     CausalLMOutputWithCrossAttentions,
+    CausalLMOutputWithPast,
     QuestionAnsweringModelOutput,
     SequenceClassifierOutputWithPast,
     TokenClassifierOutput,
 )
 from transformers.models.falcon.modeling_falcon import (
     FalconForCausalLM,
     FalconForQuestionAnswering,
@@ -27,14 +28,16 @@
     build_alibi_tensor,
 )
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer.shard import ShardConfig
 
+from ..layer import cross_entropy_1d
+
 
 def build_falcon_alibi_tensor_fn(process_group: ProcessGroup) -> torch.Tensor:
     def build_falcon_alibi_tensor(
         self, attention_mask: torch.Tensor, num_heads: int, dtype: torch.dtype
     ) -> torch.Tensor:
         """
         Link to paper: https://arxiv.org/abs/2108.12409 Alibi tensor is not causal as the original paper mentions, it
@@ -433,22 +436,36 @@
         if stage_manager.is_last_stage():
             hidden_states = transformer_outputs[0]
             lm_logits = self.lm_head(hidden_states)
 
             loss = None
             if labels is not None:
                 # Shift so that tokens < n predict n
+                labels = labels.to(lm_logits.device)
                 shift_logits = lm_logits[..., :-1, :].contiguous()
                 shift_labels = labels[..., 1:].contiguous()
                 batch_size, seq_length, vocab_size = shift_logits.shape
                 # Flatten the tokens
                 loss_fct = CrossEntropyLoss()
-                loss = loss_fct(
-                    shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length)
-                )
+                if shard_config.enable_tensor_parallelism and shard_config.parallel_output:
+                    new_vocab_size = shift_logits.shape[-1]
+                    shift_logits = shift_logits.view(-1, new_vocab_size)
+                    shift_labels = shift_labels.view(-1)
+                    loss = cross_entropy_1d(
+                        shift_logits,
+                        shift_labels,
+                        process_group=shard_config.tensor_parallel_process_group,
+                        vocab_size=self.lm_head.out_features,
+                        dtype=self.transformer.dtype,
+                    )
+                else:
+                    loss = loss_fct(
+                        shift_logits.view(batch_size * seq_length, vocab_size),
+                        shift_labels.view(batch_size * seq_length),
+                    )
 
             if not return_dict:
                 output = (lm_logits,) + transformer_outputs[1:]
                 return ((loss,) + output) if loss is not None else output
 
             return CausalLMOutputWithCrossAttentions(
                 loss=loss,
@@ -743,7 +760,83 @@
                 end_logits=end_logits,
                 hidden_states=outputs.hidden_states,
                 attentions=outputs.attentions,
             )
         else:
             hidden_states = outputs.get("hidden_states")
             return {"hidden_states": hidden_states}
+
+
+def get_lm_forward_with_dist_cross_entropy(shard_config: ShardConfig):
+    from transformers import FalconForCausalLM
+
+    def forward(
+        self: FalconForCausalLM,
+        input_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        head_mask: Optional[torch.Tensor] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        labels: Optional[torch.Tensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, CausalLMOutputWithPast]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
+            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
+            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
+        """
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        transformer_outputs = self.transformer(
+            input_ids,
+            past_key_values=past_key_values,
+            attention_mask=attention_mask,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            use_cache=use_cache,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+        past_key_values = None
+        hidden_states = transformer_outputs[0]
+        lm_logits = self.lm_head(hidden_states)
+        loss = None
+        if labels is not None:
+            # Shift so that tokens < n predict n
+            labels = labels.to(lm_logits.device)
+            shift_logits = lm_logits[..., :-1, :].contiguous()
+            shift_labels = labels[..., 1:].contiguous()
+            batch_size, seq_length, vocab_size = shift_logits.shape
+            # Flatten the tokens
+            new_vocab_size = shift_logits.shape[-1]
+            shift_logits = shift_logits.view(-1, new_vocab_size)
+            shift_labels = shift_labels.view(-1)
+            loss = cross_entropy_1d(
+                shift_logits,
+                shift_labels,
+                process_group=shard_config.tensor_parallel_process_group,
+                vocab_size=self.lm_head.out_features,
+                dtype=self.transformer.dtype,
+            )
+
+        if not return_dict:
+            output = (lm_logits,) + transformer_outputs[1:]
+            return ((loss,) + output) if loss is not None else output
+
+        return CausalLMOutputWithPast(
+            loss=loss,
+            logits=lm_logits,
+            past_key_values=transformer_outputs.past_key_values,
+            hidden_states=transformer_outputs.hidden_states,
+            attentions=transformer_outputs.attentions,
+        )
+
+    return forward
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gpt2.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/gpt2.py`

 * *Files 0% similar despite different names*

```diff
@@ -385,14 +385,15 @@
             shift_labels = shift_labels.view(-1)
             if shard_config.enable_tensor_parallelism and shard_config.parallel_output:
                 loss = cross_entropy_1d(
                     shift_logits,
                     shift_labels,
                     process_group=shard_config.tensor_parallel_process_group,
                     vocab_size=self.lm_head.out_features,
+                    dtype=self.transformer.dtype,
                 )
             else:
                 loss = loss_fct(shift_logits, shift_labels)
 
         if not return_dict:
             output = (lm_logits,) + outputs[1:]
             return ((loss,) + output) if loss is not None else output
@@ -1290,14 +1291,15 @@
             shift_logits = shift_logits.view(-1, shift_logits.size(-1))
             shift_labels = shift_labels.view(-1)
             loss = cross_entropy_1d(
                 shift_logits,
                 shift_labels,
                 process_group=shard_config.tensor_parallel_process_group,
                 vocab_size=self.lm_head.out_features,
+                dtype=self.transformer.dtype,
             )
 
         if not return_dict:
             output = (lm_logits,) + transformer_outputs[1:]
             return ((loss,) + output) if loss is not None else output
 
         return CausalLMOutputWithCrossAttentions(
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gptj.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/gptj.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/jit.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/jit.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/llama.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/llama.py`

 * *Files 0% similar despite different names*

```diff
@@ -328,14 +328,15 @@
                     new_vocab_size = logits.shape[-1]
                     shift_logits = shift_logits.view(-1, new_vocab_size)
                     loss = cross_entropy_1d(
                         shift_logits,
                         shift_labels,
                         process_group=shard_config.tensor_parallel_process_group,
                         vocab_size=self.lm_head.out_features,
+                        dtype=self.model.dtype,
                     )
                 else:
                     shift_logits = shift_logits.view(-1, self.config.vocab_size)
                     loss = loss_fct(shift_logits, shift_labels)
 
             if not return_dict:
                 output = (logits,) + outputs[1:]
@@ -764,14 +765,15 @@
             new_vocab_size = logits.shape[-1]
             shift_logits = shift_logits.view(-1, new_vocab_size)
             loss = cross_entropy_1d(
                 shift_logits,
                 shift_labels,
                 process_group=shard_config.tensor_parallel_process_group,
                 vocab_size=self.lm_head.out_features,
+                dtype=self.model.dtype,
             )
 
         if not return_dict:
             output = (logits,) + outputs[1:]
             return (loss,) + output if loss is not None else output
 
         return CausalLMOutputWithPast(
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/mistral.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/mistral.py`

 * *Files 9% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 )
 from transformers.models.mistral.modeling_mistral import MistralForCausalLM, MistralModel
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer.shard import ShardConfig
 
-from ..layer import ColoAttention
+from ..layer import ColoAttention, cross_entropy_1d
 
 logger = logging.get_logger(__name__)
 
 
 class MistralForwards:
     @staticmethod
     def mistral_model_forward(
@@ -266,19 +266,30 @@
             loss = None
             if labels is not None:
                 # Shift so that tokens < n predict n
                 shift_logits = logits[..., :-1, :].contiguous()
                 shift_labels = labels[..., 1:].contiguous()
                 # Flatten the tokens
                 loss_fct = CrossEntropyLoss()
-                shift_logits = shift_logits.view(-1, self.config.vocab_size)
                 shift_labels = shift_labels.view(-1)
                 # Enable model parallelism
                 shift_labels = shift_labels.to(shift_logits.device)
-                loss = loss_fct(shift_logits, shift_labels)
+                if shard_config.enable_tensor_parallelism and shard_config.parallel_output:
+                    new_vocab_size = logits.shape[-1]
+                    shift_logits = shift_logits.view(-1, new_vocab_size)
+                    loss = cross_entropy_1d(
+                        shift_logits,
+                        shift_labels,
+                        process_group=shard_config.tensor_parallel_process_group,
+                        vocab_size=self.lm_head.out_features,
+                        dtype=self.model.dtype,
+                    )
+                else:
+                    shift_logits = shift_logits.view(-1, self.config.vocab_size)
+                    loss = loss_fct(shift_logits, shift_labels)
 
             if not return_dict:
                 output = (logits,) + outputs[1:]
                 return (loss,) + output if loss is not None else output
 
             return CausalLMOutputWithPast(
                 loss=loss,
@@ -605,7 +616,105 @@
         attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
 
         attn_output = self.o_proj(attn_output)
 
         return attn_output, None, past_key_value
 
     return forward
+
+
+def get_lm_forward_with_dist_cross_entropy(shard_config: ShardConfig):
+    from transformers import MistralForCausalLM
+
+    def forward(
+        self: MistralForCausalLM,
+        input_ids: torch.LongTensor = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[List[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        labels: Optional[torch.LongTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, CausalLMOutputWithPast]:
+        r"""
+        Args:
+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
+
+        Returns:
+
+        Example:
+
+        ```python
+        >>> from transformers import AutoTokenizer, MistralForCausalLM
+
+        >>> model = MistralForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
+        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)
+
+        >>> prompt = "Hey, are you conscious? Can you talk to me?"
+        >>> inputs = tokenizer(prompt, return_tensors="pt")
+
+        >>> # Generate
+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
+        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
+        ```"""
+
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+        outputs = self.model(
+            input_ids=input_ids,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_values=past_key_values,
+            inputs_embeds=inputs_embeds,
+            use_cache=use_cache,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+
+        hidden_states = outputs[0]
+        logits = self.lm_head(hidden_states)
+        logits = logits.float()
+
+        loss = None
+        if labels is not None:
+            # Shift so that tokens < n predict n
+            shift_logits = logits[..., :-1, :].contiguous()
+            shift_labels = labels[..., 1:].contiguous()
+            shift_labels = shift_labels.view(-1)
+            # Enable model parallelism
+            shift_labels = shift_labels.to(shift_logits.device)
+            new_vocab_size = logits.shape[-1]
+            shift_logits = shift_logits.view(-1, new_vocab_size)
+            loss = cross_entropy_1d(
+                shift_logits,
+                shift_labels,
+                process_group=shard_config.tensor_parallel_process_group,
+                vocab_size=self.lm_head.out_features,
+                dtype=self.model.dtype,
+            )
+
+        if not return_dict:
+            output = (logits,) + outputs[1:]
+            return (loss,) + output if loss is not None else output
+
+        return CausalLMOutputWithPast(
+            loss=loss,
+            logits=logits,
+            past_key_values=outputs.past_key_values,
+            hidden_states=outputs.hidden_states,
+            attentions=outputs.attentions,
+        )
+
+    return forward
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/opt.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/opt.py`

 * *Files 10% similar despite different names*

```diff
@@ -18,14 +18,16 @@
 )
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer.layer import ColoAttention
 from colossalai.shardformer.shard import ShardConfig
 
+from ..layer import cross_entropy_1d
+
 logger = logging.get_logger(__name__)
 
 
 def _get_attention_mask(
     self: OPTModel,
     shard_config: ShardConfig,
     hidden_states: torch.Tensor,
@@ -332,16 +334,31 @@
             if labels is not None:
                 # move labels to correct device to enable model parallelism
                 labels = labels.to(logits.device)
                 # Shift so that tokens < n predict n
                 shift_logits = logits[..., :-1, :].contiguous()
                 shift_labels = labels[..., 1:].contiguous()
                 # Flatten the tokens
-                loss_fct = CrossEntropyLoss()
-                loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))
+
+                if shard_config.enable_tensor_parallelism and shard_config.parallel_output:
+                    new_vocab_size = logits.shape[-1]
+                    shift_logits = shift_logits.view(-1, new_vocab_size)
+                    shift_labels = shift_labels.view(-1)
+                    loss = cross_entropy_1d(
+                        shift_logits,
+                        shift_labels,
+                        process_group=shard_config.tensor_parallel_process_group,
+                        vocab_size=self.lm_head.out_features,
+                        dtype=self.model.decoder.dtype,
+                    )
+                else:
+                    loss_fct = CrossEntropyLoss()
+                    shift_logits = shift_logits.view(-1, self.config.vocab_size)
+                    loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))
+
             if not return_dict:
                 output = (logits,) + outputs[1:]
                 return (loss,) + output if loss is not None else output
 
             return CausalLMOutputWithPast(
                 loss=loss,
                 logits=logits,
@@ -840,7 +857,151 @@
 
         if use_cache:
             outputs += (present_key_value,)
 
         return outputs
 
     return forward
+
+
+def get_lm_forward_with_dist_cross_entropy(shard_config: ShardConfig):
+    def forward(
+        self: OPTForCausalLM,
+        input_ids: torch.LongTensor = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        head_mask: Optional[torch.Tensor] = None,
+        past_key_values: Optional[List[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        labels: Optional[torch.LongTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, CausalLMOutputWithPast]:
+        r"""
+        Args:
+            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
+                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
+                provide it.
+
+                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
+                [`PreTrainedTokenizer.__call__`] for details.
+
+                [What are input IDs?](../glossary#input-ids)
+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
+
+                - 1 for tokens that are **not masked**,
+                - 0 for tokens that are **masked**.
+
+                [What are attention masks?](../glossary#attention-mask)
+            head_mask (`torch.Tensor` of shape `(num_hidden_layers, num_attention_heads)`, *optional*):
+                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:
+
+                - 1 indicates the head is **not masked**,
+                - 0 indicates the head is **masked**.
+
+            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
+                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
+                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
+                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional
+                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.
+
+                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
+                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
+
+                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
+                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
+                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors
+                than the model's internal embedding lookup matrix.
+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
+            use_cache (`bool`, *optional*):
+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
+                (see `past_key_values`).
+            output_attentions (`bool`, *optional*):
+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
+                returned tensors for more detail.
+            output_hidden_states (`bool`, *optional*):
+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
+                for more detail.
+            return_dict (`bool`, *optional*):
+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
+
+        Returns:
+
+        Example:
+
+        ```python
+        >>> from transformers import AutoTokenizer, OPTForCausalLM
+
+        >>> model = OPTForCausalLM.from_pretrained("facebook/opt-350m")
+        >>> tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
+
+        >>> prompt = "Hey, are you conscious? Can you talk to me?"
+        >>> inputs = tokenizer(prompt, return_tensors="pt")
+
+        >>> # Generate
+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
+        "Hey, are you conscious? Can you talk to me?\nI'm not conscious. I'm just a little bit of a weirdo."
+        ```"""
+
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+        outputs = self.model.decoder(
+            input_ids=input_ids,
+            attention_mask=attention_mask,
+            head_mask=head_mask,
+            past_key_values=past_key_values,
+            inputs_embeds=inputs_embeds,
+            use_cache=use_cache,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+
+        logits = self.lm_head(outputs[0]).contiguous()
+
+        loss = None
+        if labels is not None:
+            # move labels to correct device to enable model parallelism
+            labels = labels.to(logits.device)
+            # Shift so that tokens < n predict n
+            shift_logits = logits[..., :-1, :].contiguous()
+            shift_labels = labels[..., 1:].contiguous()
+            shift_labels = shift_labels.view(-1)
+            # Enable model parallelism
+            shift_labels = shift_labels.to(shift_logits.device)
+            new_vocab_size = logits.shape[-1]
+            shift_logits = shift_logits.view(-1, new_vocab_size)
+            loss = cross_entropy_1d(
+                shift_logits,
+                shift_labels,
+                process_group=shard_config.tensor_parallel_process_group,
+                vocab_size=self.lm_head.out_features,
+                dtype=self.model.decoder.dtype,
+            )
+
+        if not return_dict:
+            output = (logits,) + outputs[1:]
+            return (loss,) + output if loss is not None else output
+
+        return CausalLMOutputWithPast(
+            loss=loss,
+            logits=logits,
+            past_key_values=outputs.past_key_values,
+            hidden_states=outputs.hidden_states,
+            attentions=outputs.attentions,
+        )
+
+    return forward
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/sam.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/sam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/t5.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/t5.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/vit.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/vit.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/whisper.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/modeling/whisper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/auto_policy.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/auto_policy.py`

 * *Files 2% similar despite different names*

```diff
@@ -178,14 +178,24 @@
     ),
     "transformers.models.mistral.modeling_mistral.MistralForCausalLM": PolicyLocation(
         file_name="mistral", class_name="MistralForCausalLMPolicy"
     ),
     "transformers.models.mistral.modeling_mistral.MistralForSequenceClassification": PolicyLocation(
         file_name="mistral", class_name="MistralForSequenceClassificationPolicy"
     ),
+    # Qwen2
+    "transformers.models.qwen2.modeling_qwen2.Qwen2Model": PolicyLocation(
+        file_name="qwen2", class_name="Qwen2ModelPolicy"
+    ),
+    "transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM": PolicyLocation(
+        file_name="qwen2", class_name="Qwen2ForCausalLMPolicy"
+    ),
+    "transformers.models.qwen2.modeling_qwen2.Qwen2ForSequenceClassification": PolicyLocation(
+        file_name="qwen2", class_name="Qwen2ForSequenceClassificationPolicy"
+    ),
 }
 
 
 def import_policy(policy_location: PolicyLocation) -> Policy:
     """
     Dynamically import a Policy class based on the policy location.
     """
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/base_policy.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/shard/sharder.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,208 +1,236 @@
-# part of code modified from https://github.com/tunib-ai/parallelformers
-
-from abc import ABC, abstractmethod
-from dataclasses import dataclass
-from typing import Any, Callable, Dict, List, Optional, Union
+from types import MethodType
+from typing import Any, Callable, Dict, List, Optional, Set, Union
 
 import torch.nn as nn
 from torch import Tensor
-from torch.nn import Module
-
-from colossalai.pipeline.stage_manager import PipelineStageManager
-
-from ..layer.normalization import BaseLayerNorm
-from ..layer.parallel_module import ParallelModule
-from ..shard.shard_config import ShardConfig
-
-__all__ = ["ParallelModule", "SubModuleReplacementDescription", "ModulePolicyDescription", "Policy"]
-
 
-@dataclass
-class SubModuleReplacementDescription:
-    r"""
-    Describe how a submodule will be replaced
+from colossalai.lazy import LazyInitContext
 
-    Args:
-        suffix (str): used to get the submodule object
-        target_module (ParallelModule): specifies the module class used to replace to submodule
-        kwargs (Dict[str, Any]): the dictionary used to pass extra arguments to the `ParallelModule.from_native_module` method.
-        ignore_if_not_exist (bool): if the submodule does not exist, ignore it or raise an exception
-    """
+from .._utils import getattr_, setattr_
+from ..policies.auto_policy import get_autopolicy
+from ..policies.base_policy import Policy, SubModuleReplacementDescription
+from .shard_config import ShardConfig
+from .utils import set_tensors_to_none
 
-    suffix: str
-    target_module: Union[ParallelModule, BaseLayerNorm]
-    kwargs: Dict[str, Any] = None
-    ignore_if_not_exist: bool = False
+__all__ = ["ModelSharder", "shard_model"]
 
 
-@dataclass
-class ModulePolicyDescription:
+class ModelSharder(object):
     r"""
-    Describe how the attributes and parameters will be transformed in a policy.
+    Shard the original huggingface model according to the policy
 
     Args:
-        attribute_replacement (Dict[str, Any]): key is the attribute name, value is the attribute value after sharding
-        param_replacement (List[Callable]): a list of functions to perform in-place param replacement. The function
-                    must receive only one arguments: module. One example is
-
-                    ```python
-                    def example_replace_weight(module: torch.nn.Module):
-                        weight = module.weight
-                        new_weight = shard_rowwise(weight, process_group)
-                        module.weight = torch.nn.Parameter(new_weight)
-                    ```
-        sub_module_replacement (List[SubModuleReplacementDescription]): each element in the list is a SubModuleReplacementDescription
-                    object which specifies the module to be replaced and the target module used to replacement.
-        method_replace (Dict[str, Callable]): key is the method name, value is the method for replacement
-    """
-
-    attribute_replacement: Dict[str, Any] = None
-    param_replacement: List[Callable] = None
-    sub_module_replacement: List[SubModuleReplacementDescription] = None
-    method_replacement: Dict[str, Callable] = None
-
-
-class Policy(ABC):
-    r"""
-    The base class for all the policies. For each different model, it should have a different policy class,
-    like BertPolicy for Bert Model or OPTPolicy for OPT model.
-
-    Shardformer has provided many built-in sharding policies for the mainstream models. You can use the
-    built-in policies by setting `policy = None`, which is already the default argument for `Shardformer.optimize`.
-    If you want to define your own policy, you can inherit from this class and overwrite the methods you want to modify.
+        policy (:class:`Policy`): The policy to shard the model
+        model (:class:`torch.Module`): The model to shard
+        shard_config: The setting of distributed model
     """
 
-    def __init__(self) -> None:
-        self.shard_config: Optional[ShardConfig] = None
-        self.model: Optional[Module] = None
-
-    def set_model(self, model: nn.Module) -> None:
-        r"""
-        Set model as an attribute of the Policy object so that we can access the model's attributes.
-        Args:
-            model (:class:`nn.Module`): The model to be perform
-        """
+    def __init__(self, model: nn.Module, policy: Policy, shard_config: ShardConfig = None) -> None:
         self.model = model
+        self.shard_config = shard_config
+        self.policy = get_autopolicy(self.model) if policy is None else policy
 
-    def set_shard_config(self, shard_config: ShardConfig) -> None:
+    def shard(self) -> List[Dict[int, Tensor]]:
         r"""
-        Set shard config as an attribute of the Policy object.
-        Args:
-            shard_config (:class:`ShardConfig`): The shard config to be perform
+        Shard the model according to the policy
         """
-        self.shard_config = shard_config
-
-        self.config_sanity_check()
+        self.policy.set_model(self.model)
+        self.policy.set_shard_config(self.shard_config)
+        self._preprocess()
+        # get shared params before release unheld layers, this avoid misjudgment of shared params (None is None)
+        shared_params = self.policy.get_shared_params()
+        held_layers = self._release_unheld_layers()
+        self._replace_module(include=held_layers)
+        self._materialize()
+        self._postprocess()
+        return shared_params
 
-    @property
-    def pipeline_stage_manager(self) -> Optional[PipelineStageManager]:
-        if self.shard_config is not None:
-            return self.shard_config.pipeline_stage_manager
-        return None
+    def _preprocess(self) -> None:
+        self.model = self.policy.preprocess()
 
-    @abstractmethod
-    def config_sanity_check(self):
-        """
-        Check if the shard config is valid for the model. Raise an exception if the config is invalid.
-        This method is made abstractmethod with no default implementation because we want to the policy writer
-        to take note of the feature supported by his/her model and policy.
-        """
+    def _postprocess(self) -> None:
+        self.model = self.policy.postprocess()
 
-    @abstractmethod
-    def preprocess(self) -> nn.Module:
+    def _replace_module(self, include: Optional[Set[nn.Module]] = None) -> None:
         r"""
-        Perform some preprocessing of the model, like reshaping the embedding layer.
+        Replace the module according to the policy, and replace the module one by one
+
+        Args:
+            model (:class:`torch.nn.Module`): The model to shard
         """
+        module_descriptions = self.policy.module_policy()
+        for layer_cls, module_description in module_descriptions.items():
+            attr_replacement = module_description.attribute_replacement
+            param_replacement = module_description.param_replacement
+            sub_module_replacement = module_description.sub_module_replacement
+            method_replacement = module_description.method_replacement
+            self._recursive_replace_layer(
+                self.model,
+                layer_cls,
+                attr_replacement,
+                param_replacement,
+                method_replacement,
+                sub_module_replacement,
+                include=include,
+            )
 
-    @abstractmethod
-    def module_policy(self) -> Dict[Union[str, nn.Module], ModulePolicyDescription]:
+    def _recursive_replace_layer(
+        self,
+        module: nn.Module,
+        origin_cls: Union[str, nn.Module],
+        attr_replacement: Dict[str, Any],
+        param_replacement: List[Callable],
+        method_replacement: Dict[str, Callable],
+        sub_module_replacement: List[SubModuleReplacementDescription],
+        include: Optional[Set[nn.Module]] = None,
+    ) -> None:
         r"""
-        This method returns the module policy, which is a dictionary. The key is the module name or the module object,
-        and the value is the ModulePolicyDescription object. The ModulePolicyDescription object describes how the module
-        will be transformed.
-        """
+        Reverse the replace layer operation
 
-    @abstractmethod
-    def postprocess(self) -> nn.Module:
+        Args:
+            module (torch.nn.Module): The object of layer to shard
+            origin_cls (Union[str, torch.nn.Module]): The origin layer class or a string of layer class name
+            attr_replacement (Dict[str, Any]): The attribute dict to modify
+            param_replacement (List[Callable]): The function list to get parameter shard information in policy
+            method_replacement (Dict[str, Callable]):  Key is the method name, value is the method for replacement
+            sub_module_replacement ((List[SubModuleReplacementDescription]): The function list to get sub module shard information in policy
+            include (Set[nn.Module], optional): The set of modules to keep on current device when pipeline parallel is enabled. Defaults to None
+        """
+        if (isinstance(origin_cls, str) and origin_cls == module.__class__.__name__) or (
+            module.__class__ == origin_cls
+        ):
+            if attr_replacement is not None:
+                self._replace_attr(module, attr_replacement)
+
+            if param_replacement is not None and (include is None or module in include):
+                self._replace_param(module, param_replacement)
+
+            if method_replacement is not None:
+                self._replace_method(module, method_replacement)
+
+            if sub_module_replacement is not None:
+                self._replace_sub_module(module, sub_module_replacement, include)
+
+        for name, child in module.named_children():
+            self._recursive_replace_layer(
+                child,
+                origin_cls,
+                attr_replacement,
+                param_replacement,
+                method_replacement,
+                sub_module_replacement,
+                include=include,
+            )
+
+    def _replace_attr(
+        self,
+        module: nn.Module,
+        attr_replacement: Dict[str, Any],
+    ) -> None:
         r"""
-        Perform some postprocessing of the model, like binding the weight of embedding layer with
-        the classifier layer
+        Replace the attribute of the layer
+
+        Args:
+            module (:class:`torch.nn.Module`): The object of layer to shard
+            attr_replacement (Dict): The attribute dict to modify
         """
+        for k, v in attr_replacement.items():
+            setattr_(module, k, v, ignore=True)
 
-    def append_or_create_submodule_replacement(
+    def _replace_param(
         self,
-        description: Union[SubModuleReplacementDescription, List[SubModuleReplacementDescription]],
-        policy: Dict[Union[str, nn.Module], ModulePolicyDescription],
-        target_key: Union[str, nn.Module],
-    ) -> Dict[Union[str, nn.Module], ModulePolicyDescription]:
+        module: nn.Module,
+        param_replacement: List[Callable],
+    ) -> None:
         r"""
-        Append or create a new submodule replacement description to the policy for the given key.
+        Replace the parameter of the layer
 
         Args:
-            submodule_replace_desc (Union[SubModuleReplacementDescription, List[SubModuleReplacementDescription]]): the submodule replacement description to be appended
-            policy (Dict[Union[str, nn.Module], ModulePolicyDescription]): the policy to be updated
-            target_key (Union[str, nn.Module]): the key of the policy to be updated
-        """
-        # convert to list
-        if isinstance(description, SubModuleReplacementDescription):
-            description = [description]
-
-        # append or create a new description
-        if target_key in policy:
-            if policy[target_key].sub_module_replacement is None:
-                policy[target_key].sub_module_replacement = description
-            else:
-                policy[target_key].sub_module_replacement.extend(description)
-        else:
-            policy[target_key] = ModulePolicyDescription(sub_module_replacement=description)
+            module (:class:`torch.nn.Module`): The object of layer to shard
+            param_replacement (List[Callable]): The function list to get parameter shard information in policy
+        """
+        for param_func in param_replacement:
+            param_func(module)
 
-        return policy
+    def _replace_method(self, module: nn.Module, method_replacement: Dict[str, Callable]):
+        for method_name, new_method in method_replacement.items():
+            # bind the new method to the module
+            bound_method = MethodType(new_method, module)
+            setattr(module, method_name, bound_method)
 
-    def append_or_create_method_replacement(
+    def _replace_sub_module(
         self,
-        description: Dict[str, Callable],
-        policy: Dict[Union[str, nn.Module], ModulePolicyDescription],
-        target_key: Union[str, nn.Module],
-    ) -> Dict[Union[str, nn.Module], ModulePolicyDescription]:
+        org_layer: nn.Module,
+        sub_module_replacement: List[SubModuleReplacementDescription],
+        include: Optional[Set[nn.Module]] = None,
+    ) -> None:
         r"""
-        Append or create a new method replacement description to the policy for the given key.
+        Shard one layer according to the policy, the layer should be the same class as the key in policy's argument_policy return dict
 
         Args:
-            description (Union[SubModuleReplacementDescription, List[SubModuleReplacementDescription]]): the submodule replacement description to be appended
-            policy (Dict[Union[str, nn.Module], ModulePolicyDescription]): the policy to be updated
-            target_key (Union[str, nn.Module]): the key of the policy to be updated
-        """
-        if target_key in policy:
-            if policy[target_key].method_replacement is None:
-                policy[target_key].method_replacement = description
-            else:
-                policy[target_key].method_replacement.update(description)
-        else:
-            policy[target_key] = ModulePolicyDescription(method_replacement=description)
-
-        return policy
-
-    def get_held_layers(self) -> List[Module]:
-        """Get layers that should be held in current stage. This method should be implemented by subclass.
-
-        Returns:
-            List[Module]: List of layers that should be hold in current stage
-        """
-        raise NotImplementedError
-
-    def get_shared_params(self) -> List[Dict[int, Tensor]]:
-        """Get parameters that should be shared across stages. This method should be implemented by subclass.
-
-        Returns:
-            List[Dict[int, Tensor]]: List of parameters that should be shared across stages. E.g. [{0: module.model.embed_tokens.weight, 3: module.lm_head.weight}]
-        """
-        return []
-
-    def tie_weight_check(self):
-        input_embedding = self.model.get_input_embeddings()
-        output_embedding = self.model.get_output_embeddings()
-        return (
-            input_embedding is not None
-            and output_embedding is not None
-            and id(input_embedding.weight) == id(output_embedding.weight)
-        )
+            org_layer (torch.nn.Module): The origin layer object to shard
+            sub_module_replacement (List[SubModuleReplacementDescription]): The sub module replacement description list
+            include (Set[nn.Module], optional): The set of modules to keep on current device when pipeline parallel is enabled. Defaults to None
+        """
+        for description in sub_module_replacement:
+            suffix = description.suffix
+            target_module = description.target_module
+            kwargs = {} if description.kwargs is None else description.kwargs
+
+            assert target_module is not None, "target_module should not be None"
+
+            native_sub_module = getattr_(org_layer, suffix, ignore=True)
+            # Skip replacement if submodule is not kept by current device when pipeline parallel is enabled.
+            if (include is not None) and (native_sub_module is not None) and (native_sub_module not in include):
+                continue
+
+            assert not isinstance(
+                native_sub_module, target_module
+            ), f"The module with suffix {suffix} has been replaced, please check the policy"
+
+            # if it is None and we are allowed to ignore this module
+            # just skip
+            if description.ignore_if_not_exist and native_sub_module is None:
+                continue
+
+            try:
+                replace_layer = target_module.from_native_module(
+                    native_sub_module, process_group=self.shard_config.tensor_parallel_process_group, **kwargs
+                )
+            except Exception as e:
+                raise RuntimeError(
+                    f"Failed to replace {suffix} of type {native_sub_module.__class__.__qualname__}"
+                    f" with {target_module.__qualname__} with the exception: {e}. "
+                    "Please check your model configuration or sharding policy, you can set up an issue for us to help you as well."
+                )
+
+            setattr_(org_layer, suffix, replace_layer)
+
+    def _get_recursive_held_layers(self, held_layers: Optional[List[nn.Module]]) -> Optional[List[nn.Module]]:
+        def collect_sub_modules(module: nn.Module):
+            if module is None:
+                return
+            recursive_held_layers.append(module)
+            for name, child in module.named_children():
+                collect_sub_modules(child)
+
+        recursive_held_layers = []
+        for module in held_layers:
+            collect_sub_modules(module)
+        return recursive_held_layers
+
+    def _release_unheld_layers(self) -> Optional[Set[nn.Module]]:
+        r"""
+        Release the unheld layers in the model
+        """
+        if self.shard_config and self.shard_config.pipeline_stage_manager:
+            held_layers = self.policy.get_held_layers()
+            set_tensors_to_none(self.model, exclude=set(held_layers))
+            return set(self._get_recursive_held_layers(held_layers))
+        return None
+
+    def _materialize(self) -> None:
+        r"""
+        Materialize the model if lazy initialization is used
+        """
+        LazyInitContext.materialize(self.model)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bert.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/bert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/blip2.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/blip2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bloom.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/bloom.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,14 +12,15 @@
     BloomPipelineForwards,
     build_bloom_alibi_tensor_fn,
     get_bloom_flash_attention_forward,
     get_bloom_sequence_parallel_forward_fn,
     get_jit_fused_bloom_attention_forward,
     get_jit_fused_bloom_gelu_forward,
     get_jit_fused_bloom_mlp_forward,
+    get_lm_forward_with_dist_cross_entropy,
 )
 from ..modeling.jit import get_dropout_add_func, get_jit_fused_dropout_add_func, get_jit_fused_gelu_forward_func
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 
 class BloomPolicy(Policy):
     def __init__(self) -> None:
@@ -283,20 +284,26 @@
         # handle tensor parallelism
         if self.shard_config.enable_tensor_parallelism:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=col_nn.VocabParallelLMHead1D,
                     kwargs=dict(
-                        gather_output=True, make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by
+                        gather_output=not self.shard_config.parallel_output,
+                        make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by,
                     ),
                 ),
                 policy=policy,
                 target_key=BloomForCausalLM,
             )
+            if self.shard_config.parallel_output:
+                method_replacement = {"forward": get_lm_forward_with_dist_cross_entropy(self.shard_config)}
+                self.append_or_create_method_replacement(
+                    description=method_replacement, policy=policy, target_key=BloomForCausalLM
+                )
         else:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=col_nn.PaddingLMHead,
                     kwargs=dict(make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by),
                 ),
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/chatglm2.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/falcon.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/falcon.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,20 @@
 from typing import Callable, Dict, List
 
 from torch import Tensor, nn
 from torch.nn import Module
 
 import colossalai.shardformer.layer as col_nn
 
-from ..modeling.falcon import FalconPipelineForwards, build_falcon_alibi_tensor_fn, get_tp_falcon_decoder_layer_forward
+from ..modeling.falcon import (
+    FalconPipelineForwards,
+    build_falcon_alibi_tensor_fn,
+    get_lm_forward_with_dist_cross_entropy,
+    get_tp_falcon_decoder_layer_forward,
+)
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = ["FalconPolicy"]
 
 
 class FalconPolicy(Policy):
     def __init__(self) -> None:
@@ -229,20 +234,27 @@
         # handle tensor parallelism
         if self.shard_config.enable_tensor_parallelism:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=col_nn.VocabParallelLMHead1D,
                     kwargs=dict(
-                        gather_output=True, make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by
+                        gather_output=not self.shard_config.parallel_output,
+                        make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by,
                     ),
                 ),
                 policy=policy,
                 target_key=FalconForCausalLM,
             )
+            if self.shard_config.parallel_output:
+                method_replacement = {"forward": get_lm_forward_with_dist_cross_entropy(self.shard_config)}
+                self.append_or_create_method_replacement(
+                    description=method_replacement, policy=policy, target_key=FalconForCausalLM
+                )
+
         else:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=col_nn.PaddingLMHead,
                     kwargs=dict(make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by),
                 ),
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/gpt2.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/gpt2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/gptj.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/gptj.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/llama.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/llama.py`

 * *Files 2% similar despite different names*

```diff
@@ -137,17 +137,19 @@
                 target_key=LlamaModel,
             )
 
         if self.shard_config.enable_tensor_parallelism:
             assert (
                 self.model.config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
             ), f"The number of attention heads must be divisible by tensor parallel size."
-            assert (
-                self.model.config.num_key_value_heads % self.shard_config.tensor_parallel_size == 0
-            ), f"The number of key_value heads must be divisible by tensor parallel size."
+            if hasattr(self.model.config, "num_key_value_heads"):
+                assert (
+                    self.model.config.num_key_value_heads >= self.shard_config.tensor_parallel_size
+                    and self.model.config.num_key_value_heads % self.shard_config.tensor_parallel_size == 0
+                ), f"The number of key_value heads must be divisible by, and must not be less than tensor parallel size."
             decoder_attribute_replacement = {
                 "self_attn.hidden_size": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
                 "self_attn.num_heads": self.model.config.num_attention_heads // self.shard_config.tensor_parallel_size,
             }
             if getattr(self.model.config, "num_key_value_heads", False):
                 decoder_attribute_replacement["self_attn.num_key_value_heads"] = (
                     self.model.config.num_key_value_heads // self.shard_config.tensor_parallel_size
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/mistral.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/mistral.py`

 * *Files 6% similar despite different names*

```diff
@@ -14,14 +14,15 @@
     PaddingLMHead,
     VocabParallelEmbedding1D,
     VocabParallelLMHead1D,
 )
 
 from ..modeling.mistral import (
     MistralForwards,
+    get_lm_forward_with_dist_cross_entropy,
     get_mistral_flash_attention_forward,
     get_mistral_model_forward_for_flash_attn,
 )
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = ["MistralPolicy", "MistralModelPolicy", "MistralForCausalLMPolicy", "MistralForSequenceClassificationPolicy"]
 
@@ -271,22 +272,26 @@
             # add a new item for casual lm
             new_item = {
                 MistralForCausalLM: ModulePolicyDescription(
                     sub_module_replacement=[
                         SubModuleReplacementDescription(
                             suffix="lm_head",
                             target_module=VocabParallelLMHead1D,
-                            kwargs=dict(
-                                gather_output=True,
-                                make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by,
-                            ),
+                            kwargs={
+                                "gather_output": not self.shard_config.parallel_output,
+                                "make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by,
+                            },
                         )
                     ]
                 )
             }
+            if self.shard_config.parallel_output:
+                new_item[MistralForCausalLM].method_replacement = {
+                    "forward": get_lm_forward_with_dist_cross_entropy(self.shard_config)
+                }
         else:
             new_item = {
                 MistralForCausalLM: ModulePolicyDescription(
                     sub_module_replacement=[
                         SubModuleReplacementDescription(
                             suffix="lm_head",
                             target_module=PaddingLMHead,
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/opt.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/opt.py`

 * *Files 1% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 )
 
 from .._utils import getattr_
 from ..modeling.jit import get_jit_fused_dropout_add_func
 from ..modeling.opt import (
     OPTPipelineForwards,
     get_jit_fused_opt_decoder_layer_forward,
+    get_lm_forward_with_dist_cross_entropy,
     get_opt_decoder_forward_for_flash_attention,
     get_opt_flash_attention_forward,
 )
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = [
     "OPTPolicy",
@@ -265,20 +266,26 @@
         policy = super().module_policy()
         if self.shard_config.enable_tensor_parallelism:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=VocabParallelLMHead1D,
                     kwargs=dict(
-                        gather_output=True, make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by
+                        gather_output=not self.shard_config.parallel_output,
+                        make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by,
                     ),
                 ),
                 policy=policy,
                 target_key=OPTForCausalLM,
             )
+            if self.shard_config.parallel_output:
+                method_replacement = {"forward": get_lm_forward_with_dist_cross_entropy(self.shard_config)}
+                self.append_or_create_method_replacement(
+                    description=method_replacement, policy=policy, target_key=OPTForCausalLM
+                )
         else:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="lm_head",
                     target_module=PaddingLMHead,
                     kwargs=dict(make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by),
                 ),
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/sam.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/sam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/t5.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/t5.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/vit.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/vit.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/whisper.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/policies/whisper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/grad_ckpt_config.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/shard/grad_ckpt_config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/shard_config.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/shard/shard_config.py`

 * *Files 2% similar despite different names*

```diff
@@ -121,13 +121,7 @@
         self.enable_fused_normalization = apex_avail
         self.enable_flash_attention = True
         self.enable_jit_fused = True
         # This can cause non-in-place param sharding when used without ZeRO.
         # It may also slow down training when seq len is small. Plz enable manually.
         # self.enable_sequence_parallelism = True
         # self.enable_sequence_overlap = True
-
-    def _infer(self):
-        """
-        Set default params for inference.
-        """
-        # assert self.pipeline_stage_manager is None, "pipeline parallelism is not supported in inference for now"
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/shardformer.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/shard/shardformer.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 import os
 from typing import Dict, List, Tuple
 
+import torch.distributed as dist
 import torch.nn as nn
 from torch import Tensor
 
 from colossalai.cluster import DistCoordinator
 
 from ..policies.base_policy import Policy
 from .shard_config import ShardConfig
@@ -32,15 +33,19 @@
     shard_config = ShardConfig()
     shard_former = ShardFormer(shard_config=shard_config)
     model, shared_params = shard_former.optimize(org_model)
     ```
     """
 
     def __init__(self, shard_config: ShardConfig):
-        self.coordinator = DistCoordinator()
+        self.is_distributed = dist.is_initialized()
+        if self.is_distributed:
+            self.coordinator = DistCoordinator()
+        else:
+            self.coordinator = None
         self.shard_config = shard_config
 
     def optimize(self, model: nn.Module, policy: Policy = None) -> Tuple[nn.Module, List[Dict[int, Tensor]]]:
         r"""
         This method will optimize the model based on the given policy.
 
         Args:
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/utils.py` & `colossalai-nightly-2024.6.1/colossalai/shardformer/shard/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/colo_parameter.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/colo_parameter.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/colo_tensor.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/colo_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/comm_spec.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/comm_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,14 +2,15 @@
     compute_global_numel,
     customized_distributed_tensor_to_param,
     distribute_tensor,
     distribute_tensor_with_customization,
     get_device_mesh,
     get_global_shape,
     get_layout,
+    get_shard_dim_1d,
     get_sharding_spec,
     init_as_dtensor,
     init_tensor_as_customization_distributed,
     is_customized_distributed_tensor,
     is_distributed_tensor,
     is_sharded,
     redistribute,
@@ -33,14 +34,15 @@
     "sharded_tensor_to_param",
     "compute_global_numel",
     "get_sharding_spec",
     "get_global_shape",
     "get_device_mesh",
     "redistribute",
     "get_layout",
+    "get_shard_dim_1d",
     "is_customized_distributed_tensor",
     "distribute_tensor_with_customization",
     "init_tensor_as_customization_distributed",
     "to_global_for_customized_distributed_tensor",
     "customized_distributed_tensor_to_param",
     "Layout",
     "ShardingSpec",
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/api.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/api.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,21 +4,38 @@
 from typing import Union
 
 import torch
 import torch.distributed as dist
 from torch.distributed import ProcessGroup
 
 from colossalai.device.device_mesh import DeviceMesh
+from colossalai.tensor.d_tensor.sharding_spec import DimSpec
 
 from .layout import Layout
 from .layout_converter import LayoutConverter
 from .sharding_spec import ShardingSpec
 
 layout_converter = LayoutConverter()
 
+_SHARD_DIM = DimSpec([0])
+
+
+def get_shard_dim_1d(p: torch.Tensor):
+    """
+    Get the dimension along which the tensor is sharded, for example in 1D Tensor Parallel.
+    Args:
+        p (torch.Tensor): the input tensor
+    Returns:
+        int: the dimension along which the tensor is sharded
+    """
+    if not is_distributed_tensor(p):
+        raise ValueError("p is not a distributed tensor")
+    sharding = p.dist_layout.sharding_spec.sharding_sequence
+    return sharding.index(_SHARD_DIM)
+
 
 def clear_layout_converter():
     global layout_converter
     layout_converter.cached_solution.clear()
 
 
 def is_distributed_tensor(tensor: torch.Tensor) -> bool:
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/comm_spec.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/comm_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/layout.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout_converter.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/layout_converter.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/sharding_spec.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/sharding_spec.py`

 * *Files 5% similar despite different names*

```diff
@@ -136,16 +136,17 @@
         """
         difference = self.difference_dict[(str(self), str(other))]
         return difference
 
 
 class ShardingSpec:
     """
-    Sharding spec describes how to shard a tensor with dim_size dimensions. The sharding sequence looks like
-    [R, R, S0, S1], which means
+    Sharding spec describes how to shard a tensor with dim_size dimensions. For example for a 3D tensor, the sharding sequence
+    [R, S0, S1] means not sharding the first dim, sharding the 3rd along the 1st device mesh axis (Process group)
+    and sharding the 3th dim along the 2nd device mesh axis. Useful for say, 2D Tensor Parallel.
 
     Argument:
         dim_partition_dict(Dict[int, List[int]], optional): The key is the dimension of tensor to be sharded,
             and the value of the key describe which logical axis will be sharded in that dimension.
         sharding_sequence(List[DimSpec], optional): A straight view of ShardingSpec looks like [R, R, S0, S1].
     """
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/utils.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/d_tensor/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/api.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/moe_tensor/api.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/moe_info.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/moe_tensor/moe_info.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/padded_tensor/api.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/padded_tensor/api.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/param_op_hook.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/param_op_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/shape_consistency.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/shape_consistency.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/sharding_spec.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/sharding_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/tensor/utils.py` & `colossalai-nightly-2024.6.1/colossalai/tensor/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/testing/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/testing/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/testing/comparison.py` & `colossalai-nightly-2024.6.1/colossalai/testing/comparison.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/testing/pytest_wrapper.py` & `colossalai-nightly-2024.6.1/colossalai/testing/pytest_wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/testing/random.py` & `colossalai-nightly-2024.6.1/colossalai/testing/random.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/testing/utils.py` & `colossalai-nightly-2024.6.1/colossalai/testing/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/utils/__init__.py` & `colossalai-nightly-2024.6.1/colossalai/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/utils/common.py` & `colossalai-nightly-2024.6.1/colossalai/utils/common.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/utils/memory.py` & `colossalai-nightly-2024.6.1/colossalai/utils/memory.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/utils/model/utils.py` & `colossalai-nightly-2024.6.1/colossalai/utils/model/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py` & `colossalai-nightly-2024.6.1/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/rank_recorder.py` & `colossalai-nightly-2024.6.1/colossalai/utils/rank_recorder/rank_recorder.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/tensor_detector.py` & `colossalai-nightly-2024.6.1/colossalai/utils/tensor_detector/tensor_detector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/utils/timer.py` & `colossalai-nightly-2024.6.1/colossalai/utils/timer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/chunk.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/chunk/chunk.py`

 * *Files 4% similar despite different names*

```diff
@@ -160,14 +160,16 @@
         self.cpu_vis_flag = False
 
         # whether to record l2 norm for the gradient clipping calculation
         self.l2_norm_flag = False
         self.l2_norm = None
 
         self.grad_chunk = None
+        # the async all-reduce/reduce-scatter work of this grad chunk (None means sync)
+        self.grad_reduce_work = None
 
     @property
     def memory_usage(self) -> Dict[str, int]:
         cuda_memory = 0
         cpu_memory = 0
 
         if self.chunk_temp is not None:
@@ -240,15 +242,15 @@
         """Check if the chunk has inf or nan values on CUDA."""
         if self.is_gathered:
             valid_tensor = self.cuda_global_chunk[: self.utilized_size]
         else:
             assert self.cuda_shard is not None  # only check on CUDA
             valid_tensor = self.cuda_shard[: self.valid_end]
 
-        return torch.isinf(valid_tensor).any().item() | torch.isnan(valid_tensor).any().item()
+        return torch.isinf(valid_tensor).any() | torch.isnan(valid_tensor).any()
 
     def set_l2_norm(self) -> None:
         """Record l2 norm of this chunks on CUDA."""
         assert self.l2_norm is None, "you are calculating the l2 norm twice"
         if self.is_gathered:
             valid_tensor = self.cuda_global_chunk[: self.utilized_size]
         else:
@@ -353,62 +355,75 @@
             else:
                 self.cpu_shard = self.cuda_shard.cpu()
             self.cpu_vis_flag = True
             self.cuda_shard = None
         else:
             raise NotImplementedError
 
-    def access_chunk(self):
+    def access_chunk(self, async_access: bool = False) -> Optional[dist.Work]:
         """Make the chunk usable for the parameters inside it. It's an operation done in CUDA."""
         # sanity check
         assert self.chunk_temp is None
-
+        maybe_work = None
         if not self.is_gathered:
-            self.__gather()
+            maybe_work = self.__gather(async_op=async_access)
         self.__update_tensors_ptr()
+        return maybe_work
 
     def release_chunk(self):
         """Release the usable chunk. It's an operation done in CUDA."""
         # sanity check
         assert self.chunk_temp is None
 
         if self.is_gathered:
             self.__scatter()
 
-    def reduce(self):
+    def reduce(self, async_op: bool = False):
         """Reduce scatter all the gradients. It's an operation done in CUDA."""
         # sanity check
         assert self.is_gathered
-
+        assert self.grad_reduce_work is None
         if self.pg_size == 1:
             # tricky code here
             # just move cuda_global_chunk to cuda_shard
             # the communication is not necessary
             self.__scatter()
             if self.extra_dp_group is not None:
-                dist.all_reduce(self.cuda_shard, group=self.extra_dp_group)
+                self.grad_reduce_work = dist.all_reduce(self.cuda_shard, group=self.extra_dp_group, async_op=async_op)
         elif self.keep_gathered:
             # we use all-reduce here
-            dist.all_reduce(self.cuda_global_chunk, group=self.torch_pg)
-            if self.extra_dp_group is not None:
-                dist.all_reduce(self.cuda_global_chunk, group=self.extra_dp_group)
+            self.grad_reduce_work = dist.all_reduce(self.cuda_global_chunk, group=self.torch_pg, async_op=async_op)
+            if self.extra_dp_group is not None:  # cannot guranatee the order of multiple all-reduce
+                self.wait_async_reduce()
+                self.grad_reduce_work = dist.all_reduce(
+                    self.cuda_global_chunk, group=self.extra_dp_group, async_op=async_op
+                )
         else:
             self.cuda_shard = torch.empty(
                 self.shard_size, dtype=self.dtype, device=get_accelerator().get_current_device()
             )
 
             input_list = list(torch.chunk(self.cuda_global_chunk, chunks=self.pg_size, dim=0))
-            dist.reduce_scatter(self.cuda_shard, input_list, group=self.torch_pg)
+            self.grad_reduce_work = dist.reduce_scatter(
+                self.cuda_shard, input_list, group=self.torch_pg, async_op=async_op
+            )
+
             if self.extra_dp_group is not None:
-                dist.all_reduce(self.cuda_shard, group=self.extra_dp_group)
+                self.wait_async_reduce()
+                self.grad_reduce_work = dist.all_reduce(self.cuda_shard, group=self.extra_dp_group, async_op=async_op)
 
             free_storage(self.cuda_global_chunk)
             self.is_gathered = False
         self.__update_tensors_state(TensorState.HOLD)
 
+    def wait_async_reduce(self) -> None:
+        if self.grad_reduce_work is not None:
+            self.grad_reduce_work.wait()
+            self.grad_reduce_work = None
+
     def tensor_trans_state(self, tensor: torch.Tensor, tensor_state: TensorState) -> None:
         """
         Make a transition of the tensor into the next state.
 
         Args:
             tensor (torch.Tensor): a torch Tensor object.
             tensor_state (TensorState): the target state for transition.
@@ -494,25 +509,27 @@
             assert self.device_type == "cpu"
             self.optim_sync_flag = False
             self.cpu_vis_flag = False
 
     def get_tensors(self) -> List[torch.Tensor]:
         return list(self.tensors_info.keys())
 
-    def __gather(self):
+    def __gather(self, async_op: bool = False) -> Optional[dist.Work]:
         if not self.is_gathered:
             # sanity check
             assert self.cuda_shard is not None
 
             alloc_storage(self.cuda_global_chunk)
             gather_list = list(torch.chunk(input=self.cuda_global_chunk, chunks=self.pg_size, dim=0))
-            dist.all_gather(gather_list, self.cuda_shard, self.torch_pg)
+            work = dist.all_gather(gather_list, self.cuda_shard, self.torch_pg, async_op=async_op)
 
             self.cuda_shard = None
             self.is_gathered = True
+            return work
+        return None
 
     def __scatter(self):
         if self.keep_gathered:
             return
 
         if self.is_gathered:
             # sanity check
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/manager.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/chunk/manager.py`

 * *Files 3% similar despite different names*

```diff
@@ -16,27 +16,36 @@
     A manager class to manipulate the tensors in chunks.
 
     Args:
         chunk_configuration (Dict[int, Dict]): the configuration dictionary of this chunk manager.
         init_device (torch.device): optional, the device on which the chunk is initialized. The default is None.
     """
 
-    def __init__(self, chunk_configuration, init_device: Optional[torch.device] = None) -> None:
+    def __init__(
+        self,
+        chunk_configuration,
+        init_device: Optional[torch.device] = None,
+        reuse_fp16_chunk: bool = True,
+    ) -> None:
         self.device = init_device or get_accelerator().get_current_device()
         self.dp_degree_chunk_size_dict: Dict[int, int] = dict()
         self.kwargs_config = chunk_configuration
         for k, v in self.kwargs_config.items():
             self.dp_degree_chunk_size_dict[k] = v.pop("chunk_size")
             v["init_device"] = self.device
 
         self.chunk_groups: Dict[str, Deque[Chunk]] = dict()
         self.tensor_chunk_map: Dict[torch.Tensor, Chunk] = dict()
         self.accessed_chunks: Set[Chunk] = set()
         self.accessed_mem: int = 0
         self.total_mem: Dict[str, int] = {"cpu": 0, "cuda": 0}
+        self.reuse_fp16_chunk = reuse_fp16_chunk
+        # Whether model is accumulating gradients,
+        self.accumulating_grads = False
+        self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
 
     def register_tensor(
         self,
         tensor: torch.Tensor,
         group_type: str,
         config_key: int,
         zero_group: ProcessGroup,
@@ -98,23 +107,24 @@
         self.tensor_chunk_map[tensor] = chunk_group[-1]
 
     def close_all_groups(self):
         """Close all the chunks of all groups."""
         for group_name in self.chunk_groups:
             self.__close_one_chunk(self.chunk_groups[group_name][-1])
 
-    def access_chunk(self, chunk: Chunk) -> None:
+    def access_chunk(self, chunk: Chunk, async_access: bool = False) -> Optional[dist.Work]:
         """Make the chunk can be used for calculation."""
         if chunk in self.accessed_chunks:
-            return
+            return None
         self.__sub_memory_usage(chunk.memory_usage)
         if chunk.device_type == "cpu":
             chunk.shard_move(get_accelerator().get_current_device())
-        self.__add_accessed_chunk(chunk)
+        maybe_work = self.__add_accessed_chunk(chunk, async_access=async_access)
         self.__add_memory_usage(chunk.memory_usage)
+        return maybe_work
 
     def release_chunk(self, chunk: Chunk) -> None:
         """Scatter the chunk in CUDA."""
         if chunk not in self.accessed_chunks:
             return
         if chunk.can_release:
             self.__sub_memory_usage(chunk.memory_usage)
@@ -130,20 +140,20 @@
         self.__add_memory_usage(chunk.memory_usage)
 
     def trans_tensor_state(self, tensor: torch.Tensor, state: TensorState) -> None:
         """Transit tensor state according to pre-defined state machine."""
         chunk = self.tensor_chunk_map[tensor]
         chunk.tensor_trans_state(tensor, state)
 
-    def reduce_chunk(self, chunk: Chunk) -> bool:
+    def reduce_chunk(self, chunk: Chunk, async_op: bool = False) -> bool:
         """Reduce or all reduce the chunk."""
         if not chunk.can_reduce:
             return False
         self.__sub_memory_usage(chunk.memory_usage)
-        chunk.reduce()
+        chunk.reduce(async_op=async_op)
         self.__sub_accessed_chunk(chunk)
         self.__add_memory_usage(chunk.memory_usage)
         return True
 
     def fake_release_chunk(self, chunk: Chunk) -> None:
         """Release gathered chunk in a fake mode.
         This function is used for keep-gathered chunk in the inference mode.
@@ -238,18 +248,19 @@
         for k, v in usage.items():
             self.total_mem[k] -= v
 
     def __add_memory_usage(self, usage: Dict[str, int]):
         for k, v in usage.items():
             self.total_mem[k] += v
 
-    def __add_accessed_chunk(self, chunk: Chunk):
-        chunk.access_chunk()
+    def __add_accessed_chunk(self, chunk: Chunk, async_access: bool = False) -> Optional[dist.Work]:
+        maybe_work = chunk.access_chunk(async_access=async_access)
         self.accessed_chunks.add(chunk)
         self.accessed_mem += chunk.chunk_mem
+        return maybe_work
 
     def __sub_accessed_chunk(self, chunk: Chunk):
         chunk.release_chunk()
         self.accessed_chunks.remove(chunk)
         self.accessed_mem -= chunk.chunk_mem
 
     def init_grad_chunk(self, chunk: Chunk) -> Chunk:
@@ -259,15 +270,15 @@
         self.__add_memory_usage(grad_chunk.memory_usage)
         if grad_chunk not in self.accessed_chunks:
             self.accessed_chunks.add(grad_chunk)
             self.accessed_mem += grad_chunk.chunk_mem
         return grad_chunk
 
     def rearrange_accumulated_grad_chunk(self, chunk: Chunk) -> Chunk:
-        """Rearrange gradients accumulated in chunk.grad_chunk, and getP prepared for gradient reduction."""
+        """Rearrange gradients accumulated in chunk.grad_chunk, and get prepared for gradient reduction."""
 
         assert chunk.grad_chunk is not None
 
         # Make a backup for gradient accumulated before.
         # Here backup gradients should be multiplied, since it will be divided after gradient reduction.
         if chunk.grad_chunk.is_gathered:
             accumulated_grad = chunk.grad_chunk.cuda_global_chunk.clone().detach().mul_(chunk.pg_size)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/search_utils.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/chunk/search_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/utils.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/chunk/utils.py`

 * *Files 7% similar despite different names*

```diff
@@ -15,14 +15,15 @@
     return a / b
 
 
 def init_chunk_manager(
     model: nn.Module,
     init_device: Optional[torch.device] = None,
     hidden_dim: Optional[int] = None,
+    reuse_fp16_chunk: bool = True,
     verbose: bool = False,
     **kwargs,
 ) -> ChunkManager:
     if hidden_dim:
         search_interval = hidden_dim
     else:
         search_interval = 1024  # defaults to 1024
@@ -46,9 +47,13 @@
             "used number: {:.2f} * 2^20, wasted number: {:.2f} * 2^20\n".format(total_size, wasted_size),
             "total wasted percentage is {:.2f}%".format(100 * safe_div(wasted_size, total_size + wasted_size)),
             sep="",
             flush=True,
         )
     dist.barrier()
 
-    chunk_manager = ChunkManager(config_dict, init_device)
+    chunk_manager = ChunkManager(
+        config_dict,
+        init_device,
+        reuse_fp16_chunk=reuse_fp16_chunk,
+    )
     return chunk_manager
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_ddp.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/gemini_ddp.py`

 * *Files 7% similar despite different names*

```diff
@@ -74,14 +74,15 @@
     def __init__(
         self,
         module: torch.nn.Module,
         chunk_config_dict: Optional[dict] = None,
         chunk_init_device: torch.device = torch.device("cpu"),
         placement_policy: str = "static",
         enable_gradient_accumulation: bool = False,
+        max_prefetch: int = 0,
         shard_param_frac: float = 1.0,  # only for static placement
         offload_optim_frac: float = 0.0,  # only for static placement
         offload_param_frac: float = 0.0,  # only for static placement
         warmup_non_model_data_ratio: float = 0.8,  # only for auto placement
         steady_cuda_cap_ratio: float = 0.9,  # only for auto placement
         search_range_m: int = 32,  # chunk search options
         hidden_dim: Optional[int] = None,  # chunk search options
@@ -92,63 +93,65 @@
         scatter_after_inference: bool = True,
         mixed_precision: torch.dtype = torch.float16,
         zero_group: Optional[ProcessGroup] = None,
         memstats: Optional[MemStats] = None,  # genimi memory stats
         master_weights: bool = True,
         extra_dp_group: Optional[ProcessGroup] = None,
         verbose: bool = False,
+        enable_async_reduce: bool = True,
     ) -> None:
         assert mixed_precision in (torch.float16, torch.bfloat16)
+        reuse_fp16_chunk = master_weights if not enable_gradient_accumulation else False
+        self.enable_gradient_accumulation = enable_gradient_accumulation
         if chunk_config_dict is not None:
-            self.chunk_manager = ChunkManager(chunk_config_dict, chunk_init_device)
+            self.chunk_manager = ChunkManager(
+                chunk_config_dict,
+                chunk_init_device,
+                reuse_fp16_chunk=reuse_fp16_chunk,
+            )
         else:
             # some ugly hotfix for the compatibility with Lightning
             if search_range_m is None:
                 search_range_m = 32
             self.chunk_manager = init_chunk_manager(
                 model=module,
                 init_device=chunk_init_device,
                 hidden_dim=hidden_dim,
                 search_range_m=search_range_m,
                 min_chunk_size_m=min_chunk_size_m,
                 strict_ddp_flag=strict_ddp_mode,
                 process_group=zero_group,
+                reuse_fp16_chunk=reuse_fp16_chunk,
                 verbose=verbose,
             )
         self.gemini_manager = GeminiManager(
             placement_policy,
             self.chunk_manager,
             memstats,
             shard_param_frac=shard_param_frac,
             offload_optim_frac=offload_optim_frac,
             offload_param_frac=offload_param_frac,
             warmup_non_model_data_ratio=warmup_non_model_data_ratio,
             steady_cuda_cap_ratio=steady_cuda_cap_ratio,
+            max_prefetch=max_prefetch,
         )
         self.force_outputs_fp32 = force_outputs_fp32
         self.param_op_hook = GeminiZeROHook(self.gemini_manager)
         self.fp32_params: List[torch.Tensor] = list()
         self.fp16_params: List[ColoParameter] = list()
-        self.overflow_counter = 0
         self.grads_device: Dict[torch.Tensor, torch.device] = dict()
         self.param2name: Dict[nn.Parameter, str] = dict()
         self.name2param: Dict[str, nn.Parameter] = dict()
         self.scatter_after_inference = scatter_after_inference
         self.mixed_precision = mixed_precision
         self.zero_group = zero_group or _get_default_group()
         self.extra_dp_group = extra_dp_group
 
-        self.reuse_fp16_chunk = master_weights
         self.master_weights = master_weights
 
-        self.enable_gradient_accumulation = enable_gradient_accumulation
-        if self.enable_gradient_accumulation:
-            self.reuse_fp16_chunk = False
-        self.accumulating_grads = False  # Whether model is accumulating gradients
-
         self._logger = get_dist_logger()
 
         if self.gemini_manager._premade_memstats_:
             # build chunk in param runtime visited order.
             param_order = self.gemini_manager.memstats()._param_runtime_order
         else:
             # build chunk in param initialized order.
@@ -174,15 +177,39 @@
         self._non_persistent_buffers_set = self._get_non_persistent_buffers_set(module)
         self._cast_buffers()
         # register grad hook
         for p in module.parameters():
             if is_ddp_ignored(p):
                 continue
             if p.requires_grad:
-                p.register_hook(partial(self.grad_handle, p))
+                assert not hasattr(p, "_grad_handle")
+                p._grad_handle = p.register_hook(
+                    partial(
+                        GeminiDDP.grad_handle,
+                        chunk_manager=self.chunk_manager,
+                        param2name=self.param2name,
+                        grads_device=self.grads_device,
+                        master_weights=self.master_weights,
+                        enable_gradient_accumulation=self.enable_gradient_accumulation,
+                        p=p,
+                        async_reduce=enable_async_reduce,
+                    )
+                )
+
+    def remove_hooks(self):
+        for p in self.module.parameters():
+            if is_ddp_ignored(p):
+                continue
+            if p.requires_grad:
+                assert hasattr(p, "_grad_handle")
+                p._grad_handle.remove()
+                delattr(p, "_grad_handle")
+
+    def __del__(self):
+        self.remove_hooks()
 
     def parameters(self, recurse: bool = True):
         return self.module.parameters(recurse)
 
     def named_parameters(self, prefix: str = "", recurse: bool = True):
         return self.module.named_parameters(prefix, recurse)
 
@@ -308,102 +335,143 @@
         # set a visit label for all parameters
         # the label is used to check whether the parameter is correctly reduced
         for param in self.param2name:
             if not is_ddp_ignored(param):
                 setattr(param, "_gemini_reduced", False)
 
     def _post_backward(self):
+        for param in self.param2name:
+            if hasattr(param, "_release_grad_chunk_cb"):
+                param._release_grad_chunk_cb()
+                delattr(param, "_release_grad_chunk_cb")
+
         if self.chunk_manager.accessed_mem != 0:
             error_params = ["Reduction failed at followed parameters:"]
             for param in self.param2name:
                 if not is_ddp_ignored(param) and not getattr(param, "_gemini_reduced"):
                     error_params.append(self.param2name[param])
             error_str = "\n\t".join(error_params)
             raise RuntimeError(
                 "ZERO DDP error: the synchronization of gradients doesn't exit properly.",
                 "The most possible reason is that the model is not compatible with GeminiDDP.\n",
                 f"{error_str}",
             )
         self._setup_grads_ptr()
-        if self.enable_gradient_accumulation and not self.accumulating_grads:
-            self.accumulating_grads = True  # Turn on the state of gradient accumulation.
+        if self.enable_gradient_accumulation and not self.chunk_manager.accumulating_grads:
+            self.chunk_manager.accumulating_grads = True  # Turn on the state of gradient accumulation.
         self._logger.debug(
             f"comp cuda demand time: {self.gemini_manager._comp_cuda_demand_time}, layout time: {self.gemini_manager._layout_time}, evict time: {self.gemini_manager._evict_time}, CPU->CUDA vol: {self.gemini_manager._h2d_volume}B, CUDA->CPU vol: {self.gemini_manager._d2h_volume}"
         )
         self.gemini_manager.post_iter()
 
     def backward(self, loss: torch.Tensor):
         self._pre_backward()
         with self.param_op_hook.switch_to_backward(), ColoParamOpHookManager.use_hooks(self.param_op_hook):
             loss.backward()
         self._post_backward()
 
     def backward_by_grad(self, tensor, grad):
         raise RuntimeError("Gemini is not compatible with pipeline. backward_by_grad shoudn't be called in Gemini.")
 
-    def grad_handle(self, p, grad):
+    @staticmethod
+    def grad_handle(
+        grad,
+        chunk_manager: ChunkManager,
+        param2name: Dict,
+        grads_device: Dict,
+        master_weights: bool,
+        enable_gradient_accumulation: bool,
+        p: nn.Parameter,
+        async_reduce: bool,
+    ):
         setattr(p, "_gemini_reduced", True)
         empty_grad = torch.empty_like(grad)
         free_storage(empty_grad)
         with torch._C.DisableTorchFunction():
-            chunk = self.chunk_manager.get_chunk(p)
+            chunk = chunk_manager.get_chunk(p)
             if chunk.tensors_info[p].state != TensorState.HOLD_AFTER_BWD:
                 raise RuntimeError(
-                    f"Parameter `{self.param2name[p]}` failed at the gradient reduction. "
+                    f"Parameter `{param2name[p]}` failed at the gradient reduction. "
                     "Some unsupported torch function is operated upon this parameter."
                 )
             grad_chunk = chunk
-            if not self.reuse_fp16_chunk:
-                if not self.accumulating_grads:
-                    grad_chunk = self.chunk_manager.init_grad_chunk(chunk)
+            if not chunk_manager.reuse_fp16_chunk:
+                if not chunk_manager.accumulating_grads:
+                    grad_chunk = chunk_manager.init_grad_chunk(chunk)
                 else:
                     assert chunk.grad_chunk is not None
-                    if chunk.grad_chunk not in self.chunk_manager.accessed_chunks:
-                        grad_chunk = self.chunk_manager.rearrange_accumulated_grad_chunk(chunk)
+                    if chunk.grad_chunk not in chunk_manager.accessed_chunks:
+                        grad_chunk = chunk_manager.rearrange_accumulated_grad_chunk(chunk)
                     else:
                         grad_chunk = chunk.grad_chunk
                         chunk.grad_chunk.l2_norm = None
 
                 # hold -> compute -> hold after bwd
                 grad_chunk.tensor_trans_state(p, TensorState.COMPUTE)
                 grad_chunk.tensor_trans_state(p, TensorState.HOLD_AFTER_BWD)
                 # fp16 param chunk: hold after bwd -> ready for reduce -> hold
                 chunk.tensor_trans_state(p, TensorState.READY_FOR_REDUCE)
                 chunk.tensor_trans_state(p, TensorState.HOLD)
 
             grad_chunk.tensor_trans_state(p, TensorState.READY_FOR_REDUCE)
-            if not self.accumulating_grads:
-                grad_chunk.copy_tensor_to_chunk_slice(p, grad, update_ptr=self.reuse_fp16_chunk)
+            if not chunk_manager.accumulating_grads:
+                grad_chunk.copy_tensor_to_chunk_slice(p, grad, update_ptr=chunk_manager.reuse_fp16_chunk)
             else:
                 grad_chunk.add_tensor_to_chunk_slice(p, grad)
-            reduced = self.chunk_manager.reduce_chunk(grad_chunk)
-            if reduced:
-                if not self.reuse_fp16_chunk:
-                    if chunk.keep_gathered:
-                        self.chunk_manager.fake_release_chunk(chunk)
-                    else:
-                        self.chunk_manager.release_chunk(chunk)
-                if grad_chunk.is_gathered:
-                    grad_chunk.cuda_global_chunk.div_(chunk.pg_size)
-                    if self.extra_dp_group is not None:
-                        grad_chunk.cuda_global_chunk.div_(chunk.extra_dp_size)
+            reduced = chunk_manager.reduce_chunk(grad_chunk, async_op=async_reduce)
+            if reduced:  # if not async, can release immediately, else release in when work finished
+                if async_reduce:
+                    # dirty fix by installing callback
+                    assert not hasattr(p, "_release_grad_chunk_cb")
+
+                    def _release_grad_chunk_cb():
+                        grad_chunk.wait_async_reduce()
+                        GeminiDDP.release_grad_chunk_handle(
+                            chunk_manager,
+                            grads_device,
+                            master_weights,
+                            enable_gradient_accumulation,
+                            p,
+                            chunk,
+                            grad_chunk,
+                        )
+
+                    p._release_grad_chunk_cb = _release_grad_chunk_cb
                 else:
-                    grad_chunk.cuda_shard.div_(chunk.pg_size)
-                    if self.extra_dp_group is not None:
-                        grad_chunk.cuda_shard.div_(chunk.extra_dp_size)
-                # check overflow elements
-                self.overflow_counter += grad_chunk.has_inf_or_nan
-                # record l2 norm for gradient clipping. flag is bound to fp16 chunk
-                if chunk.l2_norm_flag:
-                    grad_chunk.set_l2_norm()
-                self.chunk_manager.move_chunk(grad_chunk, self.grads_device[p], force_copy=True)
-                if not (self.master_weights) or (self.enable_gradient_accumulation):
-                    self.chunk_manager.move_chunk(chunk, self.grads_device[p], force_copy=True)
+                    GeminiDDP.release_grad_chunk_handle(
+                        chunk_manager, grads_device, master_weights, enable_gradient_accumulation, p, chunk, grad_chunk
+                    )
         return empty_grad
 
+    @staticmethod
+    def release_grad_chunk_handle(
+        chunk_manager, grads_device, master_weights, enable_gradient_accumulation, p, chunk, grad_chunk
+    ):
+        if not chunk_manager.reuse_fp16_chunk:
+            if chunk.keep_gathered:
+                chunk_manager.fake_release_chunk(chunk)
+            else:
+                chunk_manager.release_chunk(chunk)
+        if grad_chunk.is_gathered:
+            grad_chunk.cuda_global_chunk.div_(chunk.pg_size)
+            if chunk.extra_dp_group is not None:
+                grad_chunk.cuda_global_chunk.div_(chunk.extra_dp_size)
+        else:
+            grad_chunk.cuda_shard.div_(chunk.pg_size)
+            if chunk.extra_dp_group is not None:
+                grad_chunk.cuda_shard.div_(chunk.extra_dp_size)
+                # check overflow elements
+        chunk_manager.overflow_counter += grad_chunk.has_inf_or_nan
+        # record l2 norm for gradient clipping. flag is bound to fp16 chunk
+        if chunk.l2_norm_flag:
+            grad_chunk.set_l2_norm()
+        chunk_manager.move_chunk(grad_chunk, grads_device[p], force_copy=True)
+        if not (master_weights) or (enable_gradient_accumulation):
+            chunk_manager.move_chunk(chunk, grads_device[p], force_copy=True)
+
     def zero_grad(self, set_to_none: bool = False) -> None:
         self.module.zero_grad(set_to_none=True)
 
     def set_chunk_grad_device(self, chunk: Chunk, device: torch.device) -> None:
         for tensor in chunk.get_tensors():
             self.grads_device[tensor] = device
 
@@ -509,19 +577,19 @@
             prefix (str): the prefix for parameters and buffers used in this
                 module
         """
         assert keep_vars is False, "`state_dict` with parameter, `keep_vars=True`, is not supported now."
 
         # get copies of fp32 parameters in CPU
         # as memory of fp16_params may be reused by grad, it's not reliable, we should use fp32_params and convert to fp16
-        params = self.fp32_params if self.reuse_fp16_chunk else self.fp16_params
+        params = self.fp32_params if self.chunk_manager.reuse_fp16_chunk else self.fp16_params
         param_to_save_data = self._get_param_to_save_data(params, only_rank_0)
         # get the mapping between copies and fp16 parameters
         p_mapping = dict()
-        if self.reuse_fp16_chunk:
+        if self.chunk_manager.reuse_fp16_chunk:
             for p, fp32_p in zip(self.fp16_params, self.fp32_params):
                 name = self.param2name[p]
                 assert fp32_p in param_to_save_data, "Parameter '{}' is neglected in the chunk list".format(name)
                 record_parameter = param_to_save_data[fp32_p]
                 p_mapping[p] = record_parameter
         else:
             p_mapping = param_to_save_data
@@ -709,30 +777,32 @@
 
         fp32_to_name = dict()
         for p, fp32_p in zip(self.fp16_params, self.fp32_params):
             if p is not None:
                 name = self.param2name[p]
                 fp32_to_name[fp32_p] = name
 
-        params_to_load = self.fp32_params if self.reuse_fp16_chunk else self.fp16_params
+        params_to_load = self.fp32_params if self.chunk_manager.reuse_fp16_chunk else self.fp16_params
         chunk_list = self.chunk_manager.get_chunks(params_to_load)
         for chunk in chunk_list:
             temp_chunk = get_temp_total_chunk_on_cuda(chunk, self.mixed_precision)
 
             for tensor, tensor_info in chunk.tensors_info.items():
                 source_device_mesh, source_sharding_spec, shard_fn, gather_fn = None, None, None, None
                 if is_distributed_tensor(tensor):
                     # shard the input param
                     source_device_mesh = get_device_mesh(tensor)
                     source_sharding_spec = get_sharding_spec(tensor)
                 elif is_customized_distributed_tensor(tensor):
                     shard_fn = tensor.shard_fn
                     gather_fn = tensor.gather_fn
 
-                parameter_name = fp32_to_name[tensor] if self.reuse_fp16_chunk else self.param2name[tensor]
+                parameter_name = (
+                    fp32_to_name[tensor] if self.chunk_manager.reuse_fp16_chunk else self.param2name[tensor]
+                )
                 parameter_slice = temp_chunk[tensor_info.offset : tensor_info.end]
                 load(
                     parameter_name,
                     tensor,
                     partial(load_parameter, parameter_slice),
                     source_device_mesh,
                     source_sharding_spec,
@@ -896,15 +966,15 @@
         for name, param in self.name2param.items():
             if param is not None:
                 if is_ddp_ignored(param):
                     # deal with ddp ignored parameters
                     gathered_param = param if keep_vars else param.detach()
                 else:
                     # as memory of fp16 param may be reused, we should use fp32 param and then convert to fp16
-                    param_to_save = fp16_to_fp32[param] if self.reuse_fp16_chunk else param
+                    param_to_save = fp16_to_fp32[param] if self.chunk_manager.reuse_fp16_chunk else param
                     if param_to_save not in gathered_param_buffer:
                         chunk = self.chunk_manager.get_chunk(param_to_save)
                         gathered_param_buffer.update(self._get_chunk_to_save_data(chunk, only_rank_0))
                     gathered_param = gathered_param_buffer.pop(param_to_save)
 
                 block, block_size = sharder.append_param(prefix + name, gathered_param)
                 if block is not None:
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_hook.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/gemini_hook.py`

 * *Files 24% similar despite different names*

```diff
@@ -20,24 +20,50 @@
     def __init__(self, gemini_manager: GeminiManager) -> None:
         super().__init__()
         self._gemini_manager = gemini_manager
         self._chunk_manager = gemini_manager.chunk_manager
         self._training_phase = TrainingPhase.FORWARD
 
     def pre_op(self, params):
+        # map params to chunks
         params = [p for p in params if not is_ddp_ignored(p)]
-        chunks = self._chunk_manager.get_chunks(params)
+        all_chunks = self._chunk_manager.get_chunks(params)
+
+        # wait for prefetched chunks, filter those are not prefetched
+        chunks_fetch_sync = self._gemini_manager.wait_chunks(all_chunks)
+
+        # transfer state
         for p in params:
             self._chunk_manager.trans_tensor_state(p, TensorState.COMPUTE)
         self._gemini_manager.sample_overall_data()
-        self._gemini_manager.adjust_layout(chunks)
-        for chunk in chunks:
+
+        # evit chunks, aware of async fetched
+        self._gemini_manager.adjust_layout(
+            all_chunks, record_anyway=self._gemini_manager.placement_policy.max_prefetch > 0
+        )
+
+        # fetch the rest synchronously
+        for chunk in chunks_fetch_sync:
             self._chunk_manager.access_chunk(chunk)
 
-        # record cuda model data of the current OP
+        # get possible chunks to prefetch
+        chunks_fetch_async = self._gemini_manager.placement_policy.get_prefetch_chunks(
+            is_warmup=self._gemini_manager.is_warmup(),
+            compute_list=self._gemini_manager.compute_list,
+            compute_idx=self._gemini_manager.compute_idx,
+            async_works=self._gemini_manager.async_works,
+        )
+
+        # prefetch
+        for chunk in chunks_fetch_async:
+            maybe_work = self._chunk_manager.access_chunk(chunk, async_access=True)
+            if maybe_work is not None:
+                self._gemini_manager.add_work(chunk, maybe_work)
+
+        # record cuda model data of the current OP, including memory for prefetched chunks
         self._gemini_manager.record_model_data_volume()
 
     def post_op(self, params):
         params = [p for p in params if not is_ddp_ignored(p)]
         for p in params:
             tensor_state = (
                 TensorState.HOLD
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_mgr.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/gemini_mgr.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 import functools
 from time import time
-from typing import Dict, List, Optional, Tuple
+from typing import Dict, Iterable, List, Optional, Tuple
 
 import torch
+import torch.distributed as dist
 
 from .chunk import Chunk, ChunkManager
 from .memory_tracer import ChunkMemStatsCollector, MemStats
-from .placement_policy import PlacementPolicyFactory
+from .placement_policy import PlacementPolicy, PlacementPolicyFactory
 
 
 class GeminiManager:
     """
     Stateful Tensor Manager, inspired from PatrickStar
 
     PatrickStar: Parallel Training of Pre-trained Models via Chunk-based Memory Management
@@ -37,17 +38,20 @@
         self._chunk_manager = chunk_manager
 
         self._premade_memstats_ = memstats is not None
         self._memstats = memstats
         self._mem_stats_collector = (
             ChunkMemStatsCollector(chunk_manager, self._memstats) if policy_cls.need_mem_stats else None
         )
-        self._placement_policy = policy_cls(chunk_manager, self._mem_stats_collector, **placement_kwargs)
+        self._placement_policy = policy_cls(
+            chunk_manager=chunk_manager, mem_stats_collector=self._mem_stats_collector, **placement_kwargs
+        )
         self._compute_list: List[Tuple[Chunk, ...]] = []
         self._compute_idx: int = -1
+        self._async_works: Dict[Chunk, dist.Work] = {}
 
         self._h2d_volume = 0
         self._d2h_volume = 0
         self._layout_time = 0
         self._evict_time = 0
         self._warmup = True
         self._comp_cuda_demand_time = 0
@@ -87,37 +91,54 @@
     def post_iter(self):
         """This function must be called when each iteration finishes"""
         if self._mem_stats_collector and self._warmup:
             self._mem_stats_collector.finish_collection()
         self._warmup = False
         self.reset_attributes()
 
-    def adjust_layout(self, chunks: Tuple[Chunk, ...]) -> None:
+    def adjust_layout(self, chunks: Tuple[Chunk, ...], record_anyway: bool = False) -> None:
         """Adjust the layout of stateful tensors according to the information provided
         by mem_stats_collector, which should belongs to a Sharded Model.
         """
         # find stateful tensor in state COMPUTE
         start = time()
-        self._record_chunks_order(chunks)
-        cuda_demand, hold_cuda_tensor_list = self._get_layout_info(self._compute_idx, self._warmup, chunks)
+        self._record_warmup_chunks_order(chunks, record_anyway=record_anyway)
+        cuda_demand, can_evict_chunks = self._get_layout_info(self._compute_idx, self._warmup, chunks)
+        # don't evict chunks that are asynchronously fetched
+        can_evict_chunks = [chunk for chunk in can_evict_chunks if chunk not in self._async_works]
         self._layout_time += time() - start
 
         vol, evict_time = self._placement_policy.evict_tensors(
-            can_evict_chunks=hold_cuda_tensor_list,
+            can_evict_chunks=can_evict_chunks,
             cuda_demand=cuda_demand,
             warmup=self._warmup,
             compute_list=self._compute_list,
             compute_idx=self._compute_idx,
         )
 
         self._d2h_volume += vol
         self._evict_time += evict_time
         # move COMPUTE tensors to CUDA
         self._h2d_volume += cuda_demand
 
+    def wait_chunks(self, chunks: Iterable[Chunk]) -> Tuple[Chunk]:
+        non_prefetched_chunks = []
+        for chunk in chunks:
+            if chunk in self._async_works:
+                self._async_works[chunk].wait()
+                del self._async_works[chunk]
+            else:
+                non_prefetched_chunks.append(chunk)
+        return tuple(non_prefetched_chunks)
+
+    def add_work(self, chunk: Chunk, work: dist.Work):
+        assert work is not None
+        assert chunk not in self._async_works
+        self._async_works[chunk] = work
+
     @functools.lru_cache(maxsize=None)
     def _get_layout_info(self, compute_idx: int, warmup: bool, chunks: Tuple[Chunk, ...]):
         start = time()
         cuda_demand = 0
         for chunk in chunks:
             if chunk.device_type == "cuda" or chunk.device_type == "npu":
                 if chunk.is_gathered:
@@ -129,17 +150,17 @@
             else:
                 raise RuntimeError
         self._comp_cuda_demand_time += time() - start
 
         can_evict_chunks = self._chunk_manager.get_cuda_movable_chunks()
         return cuda_demand, can_evict_chunks
 
-    def _record_chunks_order(self, chunks: Tuple[Chunk, ...]) -> None:
+    def _record_warmup_chunks_order(self, chunks: Tuple[Chunk, ...], record_anyway: bool = False) -> None:
         self._compute_idx += 1
-        if self._warmup and self._placement_policy.need_mem_stats:
+        if self._warmup and (self._placement_policy.need_mem_stats or record_anyway):
             self._compute_list.append(chunks)
 
     def sample_overall_data(self):
         if self._mem_stats_collector:
             self._mem_stats_collector.sample_overall_data()
 
     def record_model_data_volume(self):
@@ -153,14 +174,30 @@
     @property
     def cuda_margin_mem(self) -> Optional[float]:
         if self._mem_stats_collector:
             return self._mem_stats_collector.cuda_margin_mem
         return None
 
     @property
+    def placement_policy(self) -> PlacementPolicy:
+        return self._placement_policy
+
+    @property
+    def compute_list(self) -> List[Tuple[Chunk, ...]]:
+        return self._compute_list
+
+    @property
+    def compute_idx(self) -> int:
+        return self._compute_idx
+
+    @property
+    def async_works(self) -> Dict[Chunk, dist.Work]:
+        return self._async_works
+
+    @property
     def is_cuda_margin_mem_avail(self) -> bool:
         return self._placement_policy.need_mem_stats
 
     def setup_grads_device(
         self, params: List[torch.Tensor], grads_device_map: Dict[torch.Tensor, torch.device]
     ) -> None:
         self._placement_policy.setup_grads_device(params, grads_device_map)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_optimizer.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/gemini_optimizer.py`

 * *Files 1% similar despite different names*

```diff
@@ -58,18 +58,18 @@
     ) -> None:
         super().__init__(
             initial_scale, min_scale, growth_factor, backoff_factor, growth_interval, hysteresis, max_scale
         )
         self.module = module
 
     def check_local_overflow(self) -> bool:
-        return self.module.overflow_counter > 0
+        return self.module.chunk_manager.overflow_counter.item() > 0
 
     def pre_zero_grad(self) -> None:
-        self.module.overflow_counter = 0
+        self.module.chunk_manager.overflow_counter.zero_()
 
 
 class GeminiOptimizer(OptimizerWrapper):
     """A wrapper for optimizer. ``GeminiDDP`` and ``GeminiOptimizer`` implement Zero Redundancy Optimizer (ZeRO state-3).
 
     Note:
         You must use ``GeminiDDP`` with ``GeminiOptimizer``.
@@ -198,15 +198,15 @@
 
     def _set_grad_ptr(self):
         for group in self.param_groups:
             for fake_param in group["params"]:
                 chunk16 = self.param_to_chunk16[fake_param]
                 begin, end = self.param_to_range[fake_param]
 
-                grad_chunk16 = chunk16 if self.module.reuse_fp16_chunk else chunk16.grad_chunk
+                grad_chunk16 = chunk16 if self.module.chunk_manager.reuse_fp16_chunk else chunk16.grad_chunk
                 fake_param.data = grad_chunk16.payload[begin:end]
                 fake_param.grad = fake_param.data
 
                 to_update_chunk = chunk16.paired_chunk if self.module.master_weights else chunk16
                 fake_param.data = to_update_chunk.payload[begin:end]
 
     def _update_fp16_params(self):
@@ -217,22 +217,22 @@
                 fake_param.data = none_tensor.to(fake_param.device)
 
         for chunk16 in self.chunk16_set:
             chunk16.optim_update()
 
     def _clear_global_norm(self) -> None:
         for c16 in self.chunk16_set:
-            grad_chunk = c16 if self.module.reuse_fp16_chunk else c16.grad_chunk
+            grad_chunk = c16 if self.module.chunk_manager.reuse_fp16_chunk else c16.grad_chunk
             grad_chunk.l2_norm = None
 
     def _calc_global_norm(self) -> float:
         norm_sqr: float = 0.0
         group_to_norm = dict()
         for c16 in self.chunk16_set:
-            grad_chunk = c16 if self.module.reuse_fp16_chunk else c16.grad_chunk
+            grad_chunk = c16 if self.module.chunk_manager.reuse_fp16_chunk else c16.grad_chunk
             assert grad_chunk.l2_norm is not None
 
             if grad_chunk.is_gathered:
                 norm_sqr += grad_chunk.l2_norm
             else:
                 # this chunk is sharded, use communication to collect total norm
                 if grad_chunk.torch_pg not in group_to_norm:
@@ -271,28 +271,28 @@
         self._set_grad_ptr()
 
         if self.mix_precision_mixin.should_skip_step():
             if self.verbose:
                 self._logger.info(f"Found overflow. Skip step")
             self._clear_global_norm()  # clear recorded norm
             self.zero_grad()  # reset all gradients
-            if self.module.reuse_fp16_chunk:
+            if self.module.chunk_manager.reuse_fp16_chunk:
                 self._update_fp16_params()
             return
 
         # get combined scale. combined scale = loss scale * clipping norm
         # so that gradient = gradient / combined scale
         combined_scale = self._get_combined_scale()
 
         ret = self.optim.step(div_scale=combined_scale, *args, **kwargs)
         self._register_states()
         self.zero_grad()
         if self.module.master_weights:
             self._update_fp16_params()
-        self.module.accumulating_grads = False
+        self.module.chunk_manager.accumulating_grads = False
         return ret
 
     def clip_grad_norm(self, model: torch.nn.Module, max_norm: float, norm_type: float = 2.0):
         raise NotImplementedError
 
     def backward(self, loss: torch.Tensor):
         loss = self.mix_precision_mixin.pre_backward(loss)
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_monitor.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/memory_monitor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_stats.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/memory_stats.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memstats_collector.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/memstats_collector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/param_runtime_order.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/param_runtime_order.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/utils.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/memory_tracer/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/placement_policy.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/placement_policy.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,54 +1,66 @@
 import functools
 import warnings
 from abc import ABC, abstractmethod
 from time import time
 from typing import Dict, List, Optional, Tuple, Type
 
 import torch
+import torch.distributed as dist
 
 from colossalai.accelerator import get_accelerator
 from colossalai.legacy.utils.memory import colo_device_memory_capacity
 from colossalai.zero.gemini.chunk import Chunk
 
 from .chunk import Chunk, ChunkManager
 from .memory_tracer import ChunkMemStatsCollector
 
 
 class PlacementPolicy(ABC):
     need_mem_stats: bool = False
 
     def __init__(
-        self, chunk_manager: ChunkManager, mem_stats_collector: Optional[ChunkMemStatsCollector] = None, **kwargs
+        self,
+        chunk_manager: ChunkManager,
+        mem_stats_collector: Optional[ChunkMemStatsCollector] = None,
+        max_prefetch: int = 0,
+        **kwargs,
     ) -> None:
         self.chunk_manager = chunk_manager
         self.mem_stats_collector: Optional[ChunkMemStatsCollector] = mem_stats_collector
+        self.max_prefetch = max_prefetch
 
     @abstractmethod
     def evict_tensors(self, can_evict_chunks: List[Chunk], **kwargs) -> Tuple[int, float]:
         raise NotImplementedError
 
     @abstractmethod
     def setup_grads_device(
         self, params: List[torch.Tensor], grads_device_map: Dict[torch.Tensor, torch.device]
     ) -> None:
         raise NotImplementedError
 
+    def get_prefetch_chunks(
+        self, is_warmup, compute_list: tuple, compute_idx: int, async_works: Dict[Chunk, dist.Work]
+    ) -> List[Chunk]:
+        return []  # no prefetch by default
+
 
 class StaticPlacementPolicy(PlacementPolicy):
     def __init__(
         self,
         chunk_manager: ChunkManager,
         mem_stats_collector: Optional[ChunkMemStatsCollector] = None,
+        max_prefetch: int = 0,
         shard_param_frac: float = 1.0,
         offload_optim_frac: float = 0.0,
         offload_param_frac: float = 0.0,
         **kwargs,
     ) -> None:
-        super().__init__(chunk_manager, mem_stats_collector=mem_stats_collector)
+        super().__init__(chunk_manager, mem_stats_collector=mem_stats_collector, max_prefetch=max_prefetch)
         if offload_param_frac > 0.0 and (shard_param_frac != 1.0 or offload_optim_frac != 1.0):
             warnings.warn("offload_param_frac is ignored when shard_param_frac != 1.0 or offload_optim_frac != 1.0")
             offload_param_frac = 0.0
         self.shard_param_frac = shard_param_frac
         self.offload_optim_frac = offload_optim_frac
         self.offload_param_frac = offload_param_frac
         # these should be initialized in setup_grads_device
@@ -91,33 +103,54 @@
                 # real offloaded mem is chunk.shard_mem, for simplicity we use chunk mem here
                 offloaded_optim_chunk_mem += chunk.chunk_mem
             for p in params:
                 grads_device_map[p] = device
         self.keep_gathered_chunk_mem = total_chunk_mem * (1 - self.shard_param_frac)
         self.keep_cuda_chunk_mem = total_chunk_mem * (1 - self.offload_param_frac)
 
+    def get_prefetch_chunks(
+        self, is_warmup: bool, compute_list: tuple, compute_idx: int, async_works: Dict[Chunk, dist.Work]
+    ) -> List[Chunk]:
+        if is_warmup:  # no prefetch during warmup since we need compute_list
+            return []
+        can_prefetch = self.max_prefetch - len(async_works)
+        prefetch = []
+        for i in range(compute_idx + 1, len(compute_list)):
+            for chunk in compute_list[i]:
+                if len(prefetch) >= can_prefetch:
+                    break
+                if chunk not in prefetch and chunk not in self.chunk_manager.accessed_chunks:
+                    prefetch.append(chunk)
+            else:
+                continue
+            break
+        return prefetch
+
 
 class AutoPlacementPolicy(PlacementPolicy):
     need_mem_stats: bool = True
 
     def __init__(
         self,
         chunk_manager: ChunkManager,
         mem_stats_collector: Optional[ChunkMemStatsCollector] = None,
+        max_prefetch: int = 0,
         warmup_non_model_data_ratio: float = 0.8,
         steady_cuda_cap_ratio: float = 0.9,
         **kwargs,
     ) -> None:
-        super().__init__(chunk_manager, mem_stats_collector=mem_stats_collector)
+        super().__init__(chunk_manager, mem_stats_collector=mem_stats_collector, max_prefetch=max_prefetch)
         # model data will use 1-_warmup_non_model_data_ratio CUDA memory in warmup phase
         # you can set them by AutoPlacementPolicy.set_warmup_non_model_data_ratio()
         # and AutoPlacementPolicy.set_steady_cuda_cap_ratio()
         self._warmup_non_model_data_ratio = warmup_non_model_data_ratio
         self._steady_cuda_cap_ratio = steady_cuda_cap_ratio
 
+        self.__avail_cuda_model_data_for_prefetch = None
+
     def evict_tensors(
         self,
         can_evict_chunks: List[Chunk],
         cuda_demand: int = 0,
         warmup: bool = True,
         compute_list: Optional[List[Tuple[Chunk, ...]]] = None,
         compute_idx: int = 0,
@@ -169,14 +202,15 @@
                 self.chunk_manager.move_chunk(chunk, torch.device("cpu"))
                 freed_cuda_model_data += chunk.chunk_mem
             if freed_cuda_model_data < to_free_cuda_model_data:
                 raise RuntimeError(
                     f"Adjust layout failed! No enough CUDA memory! "
                     f"Need {to_free_cuda_model_data}, freed {freed_cuda_model_data}"
                 )
+        self.__avail_cuda_model_data_for_prefetch = avail_cuda_model_data + freed_cuda_model_data
         return freed_cuda_model_data, time() - start
 
     @staticmethod
     @functools.lru_cache(maxsize=None)
     def _sort_can_evict_chunks(can_evict_chunks: tuple, compute_idx: int, compute_list: tuple) -> list:
         next_compute_idx = {chunk: len(compute_list) for chunk in can_evict_chunks}
         for i in range(len(compute_list) - 1, compute_idx, -1):
@@ -194,14 +228,38 @@
             # init offload optim settings
             # keep gathered chunks are in CUDA
             if chunk.keep_gathered:
                 grads_device_map[p] = get_accelerator().get_current_device()
             else:
                 grads_device_map[p] = torch.device("cpu")
 
+    def get_prefetch_chunks(
+        self, is_warmup: bool, compute_list: tuple, compute_idx: int, async_works: Dict[Chunk, dist.Work]
+    ) -> List[Chunk]:
+        if is_warmup:  # no prefetch during warmup since we need compute_list
+            return []
+
+        avail_cuda_model_data = self.__avail_cuda_model_data_for_prefetch
+        self.__avail_cuda_model_data_for_prefetch = None  # incase of double use
+
+        prefetch_chunk_memory = 0
+        can_prefetch = self.max_prefetch - len(async_works)
+        prefetch = []
+        for i in range(compute_idx + 1, len(compute_list)):
+            for chunk in compute_list[i]:
+                if len(prefetch) >= can_prefetch or prefetch_chunk_memory + chunk.chunk_mem > avail_cuda_model_data:
+                    break
+                if chunk not in prefetch and chunk not in self.chunk_manager.accessed_chunks:
+                    prefetch_chunk_memory += chunk.chunk_mem
+                    prefetch.append(chunk)
+            else:
+                continue
+            break
+        return prefetch
+
 
 class PlacementPolicyFactory:
     policies: Dict[str, Type[PlacementPolicy]] = {
         "auto": AutoPlacementPolicy,
         "static": StaticPlacementPolicy,
     }
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/gemini/utils.py` & `colossalai-nightly-2024.6.1/colossalai/zero/gemini/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/low_level/_utils.py` & `colossalai-nightly-2024.6.1/colossalai/zero/low_level/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/gradient_store.py` & `colossalai-nightly-2024.6.1/colossalai/zero/low_level/bookkeeping/gradient_store.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,29 +2,32 @@
 
 from torch import Tensor
 
 from .base_store import BaseStore
 
 
 class GradientStore(BaseStore):
-    def __init__(self, *args, partition_grad: bool = False):
+    def __init__(self, *args, partition_grad: bool = False, require_grad_sync: bool = True):
         super().__init__(*args)
         """
         self._grads_of_params mapping the parameter and its gradient slices
         data structure:
         {
          group_id:{
             param_id: [grad_rank0, grad_rank1, ...]
           }
         }
         """
         self._grads_of_params = dict()
-        # for zero2, it's `param_id: [grad_local_rank]`
+        # stage 2
+        self._partition_grads = partition_grad
+        # grad accumulation
+        self.require_grad_sync = require_grad_sync
         self._working_index = 0 if partition_grad else self._local_rank
-
+        # for zero2, it's `param_id: [grad_local_rank]`
         self.grad_to_param_mapping = dict()
 
     def get_partitioned_gradients_by_param_id(self, group_id: int, param_id: int) -> List:
         """Return list of gradient slices of a specific parameter
 
         Args:
             group_id (int): The index of a parameter group
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/parameter_store.py` & `colossalai-nightly-2024.6.1/colossalai/zero/low_level/bookkeeping/parameter_store.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+from typing import Dict
+
 from torch import Tensor
 from torch.distributed import ProcessGroup
 
 from .base_store import BaseStore
 
 
 class ParameterStore(BaseStore):
@@ -43,7 +45,16 @@
         Args:
             master_param (Tensor): The parameter copy in optimizer
             working_param (Tensor): The parameter of the model
         """
 
         self.master_to_working_param[id(master_param)] = working_param
         self.working_to_master_param[id(working_param)] = master_param
+
+    def get_padding_map(self) -> Dict[int, Tensor]:
+        """Return the padding map
+
+        Returns:
+            Dict[int, Tensor]: The padding map
+        """
+
+        return self._padding_map
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/tensor_bucket.py` & `colossalai-nightly-2024.6.1/colossalai/zero/low_level/bookkeeping/tensor_bucket.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/low_level/low_level_optim.py` & `colossalai-nightly-2024.6.1/colossalai/zero/low_level/low_level_optim.py`

 * *Files 9% similar despite different names*

```diff
@@ -86,46 +86,20 @@
     ):
         super(LowLevelZeroOptimizer, self).__init__(optim=optimizer)
 
         self._dtype = self.optim.param_groups[0]["params"][0].dtype
         self._logger = get_dist_logger()
         self._verbose = verbose
 
-        # stage 2
-        self._partition_grads = partition_grad
-
         self._cpu_offload = cpu_offload
 
-        # grad accumulation
-        self.require_grad_sync = True
-
-        # if process_group is none, will use the default one
-        self.dp_pg = dp_process_group
-        self._local_rank = dist.get_rank(group=self.dp_pg)
-        self._world_size = dist.get_world_size(group=self.dp_pg)
-
-        # extra dp
-        # This group is used to sync moe param, dp_world_size = moe_duplicates * extra_dp_size.
-        # Non moe param will be sync by global dp pg, moe param will be sync by extra dp pg.
-        # Moe param grad is be split as non moe param by global dp pg, and grad will be merged in step.
-        # And moe working and master param are split by extra dp pg.
-        self.moe_extra_dp_pg = moe_extra_dp_process_group
-        if self.moe_extra_dp_pg is not None:
-            self.moe_extra_dp_pg_size = dist.get_world_size(group=self.moe_extra_dp_pg)
-            self.moe_extra_dp_pg_rank = dist.get_rank(group=self.moe_extra_dp_pg)
-
         # working and master params for mixed precision training
         self._working_param_groups = dict()
         self._master_param_groups_of_current_rank = dict()
 
-        # communication params
-        self._overlap_communication = overlap_communication
-        self._reduce_bucket_size = reduce_bucket_size
-        self._communication_dtype = communication_dtype
-
         # gradient clipping
         self._clip_grad_norm = clip_grad_norm
 
         # master weights copy
         self._master_weights = master_weights
 
         if forced_dtype:
@@ -136,32 +110,34 @@
             self._dtype = forced_dtype
 
         # check argument conflict
         self._sanity_checks()
 
         # ParameterStore will manage the tensor buffers used for zero
         # it will not manage the tensors used by mixed precision training
-        self._param_store = ParameterStore(self.dp_pg)
-        self._grad_store = GradientStore(self.dp_pg, partition_grad=partition_grad)
-        self._bucket_store = BucketStore(self.dp_pg)
+        self._param_store = ParameterStore(dp_process_group)
+        self._grad_store = GradientStore(dp_process_group, partition_grad=partition_grad, require_grad_sync=True)
+        self._bucket_store = BucketStore(
+            dp_process_group, reduce_bucket_size, overlap_communication, communication_dtype, moe_extra_dp_process_group
+        )
 
         # moe param should not be stored in working_groups
         # because they have different parallel strategy
         # so we need to store them separately in param_groups
         # instead of working_groups
         self.working_moe_params = list()
 
         # iterate over the param group in the optimizer
         # partition these param groups for data parallel training
         # and add buffers to parameter store for future access
         for group_id, param_group in enumerate(self.optim.param_groups):
             group_params = list()
             for param in param_group["params"]:
                 if param.requires_grad:
-                    if self.moe_extra_dp_pg is None:
+                    if self._bucket_store.moe_extra_dp_pg is None:
                         # skip moe param
                         if is_moe_tensor(param):
                             self.working_moe_params.append(param)
                             continue
                     group_params.append(param)
 
             # add the working params to working_param_groups for bookkeeping
@@ -190,23 +166,18 @@
             self.moe_master_to_working_map = {}
             for master_moe_param, working_moe_param in zip(self.master_moe_params, self.working_moe_params):
                 self.moe_master_to_working_map[id(master_moe_param)] = working_moe_param
             # add to optim
             param_group["params"] = self.master_moe_params
             self.optim.param_groups.append(param_group)
 
-        # initialize communication stream for
-        # communication-computation overlapping
-        if self._overlap_communication:
-            self._comm_stream = get_accelerator().Stream()
-
         # reduction hook is only used if overlapping communication
         # or stage 2 is used
         # if it is stage 1 without overlapping, no hook will be attached
-        if self._overlap_communication or self._partition_grads:
+        if self._bucket_store._overlap_communication or self._grad_store._partition_grads:
             self._attach_reduction_hook()
 
         # initialize mixed precision mixin
         self.mixed_precision_mixin: Optional[MixedPrecisionMixin] = None
         if self._dtype is torch.float16:
             self.mixed_precision_mixin = LowLevelZeroFP16MixedPrecisionMixin(
                 self.num_param_groups,
@@ -218,14 +189,17 @@
                 growth_interval=growth_interval,
                 hysteresis=hysteresis,
                 max_scale=max_scale,
             )
         elif self._dtype is torch.bfloat16:
             self.mixed_precision_mixin = BF16MixedPrecisionMixin()
 
+    def __del__(self):
+        self.remove_hooks()
+
     @property
     def dtype(self):
         return self._dtype
 
     @property
     def num_param_groups(self):
         return len(self._working_param_groups)
@@ -242,83 +216,102 @@
 
     def _create_master_param_current_rank(self, param_list):
         # split each param evenly by world size
         params_current_rank = []
         device = "cpu" if self._cpu_offload else get_accelerator().get_current_device()
 
         for param in param_list:
-            padding_size = (self._world_size - param.numel() % self._world_size) % self._world_size
+            padding_size = (
+                self._bucket_store.zero_world_size - param.numel() % self._bucket_store.zero_world_size
+            ) % self._bucket_store.zero_world_size
             self._param_store.record_param_padding_size(param, padding_size)
 
             with torch.no_grad():
                 if padding_size > 0:
                     padding_param = torch.nn.functional.pad(param.data.view(-1), [0, padding_size])
                     # reset working params' ptr when no master weights
                     if self._master_weights == False:
                         param.data = padding_param[: param.numel()].view(param.shape)
                 else:
                     padding_param = param.data.view(-1)
 
-                if self.moe_extra_dp_pg is not None and is_moe_tensor(param):
-                    splited_params = padding_param.split(padding_param.numel() // self.moe_extra_dp_pg_size)
-                    splited_params = splited_params[self.moe_extra_dp_pg_rank]
+                if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(param):
+                    splited_params = padding_param.split(
+                        padding_param.numel() // self._bucket_store.moe_extra_dp_pg_size
+                    )
+                    splited_params = splited_params[self._bucket_store.moe_extra_dp_pg_rank]
                 else:
-                    splited_params = padding_param.split(padding_param.numel() // self._world_size)
-                    splited_params = splited_params[self._local_rank]
+                    splited_params = padding_param.split(padding_param.numel() // self._bucket_store.zero_world_size)
+                    splited_params = splited_params[self._bucket_store.zero_local_rank]
 
                 # use fp32 when master_weights is True
                 if self._master_weights is True:
                     splited_param_current_rank = splited_params.detach().float().to(device)
                 else:
                     splited_param_current_rank = splited_params
 
+                # Send the splited view to the optimizer to match ZeRO 2 grad shape
                 params_current_rank.append(splited_param_current_rank)
                 self._param_store.link_master_and_working_param(splited_param_current_rank, param)
 
         return params_current_rank
 
     ###########################
     # Backward Reduction Hook #
     ###########################
 
-    def _grad_handler(self, group_id, param):
+    @staticmethod
+    def grad_handler(
+        param: nn.Parameter,
+        group_id: int,
+        bucket_store: BucketStore,
+        param_store: ParameterStore,
+        grad_store: GradientStore,
+    ):
         # if run with no_sync context, would not sync grad when backward
-        if self.require_grad_sync:
-            self._add_to_bucket(param, group_id)
+        if grad_store.require_grad_sync:
+            LowLevelZeroOptimizer.add_to_bucket(param, group_id, bucket_store, param_store, grad_store)
 
     def _attach_reduction_hook(self):
         # we iterate over the working params
         # on each param, we register a hook to its AccumulateGrad object
         for group_id in range(self.num_param_groups):
             param_group = self._working_param_groups[group_id]
             for param in param_group:
                 if param.requires_grad:
-                    param.register_post_accumulate_grad_hook(partial(self._grad_handler, group_id))
+                    param._grad_handle = param.register_post_accumulate_grad_hook(
+                        partial(
+                            LowLevelZeroOptimizer.grad_handler,
+                            group_id=group_id,
+                            bucket_store=self._bucket_store,
+                            param_store=self._param_store,
+                            grad_store=self._grad_store,
+                        )
+                    )
 
     #######################
     # Reduction Functions #
     #######################
-
-    def _run_reduction(self):
-        if self._bucket_store.num_elements_in_bucket() > 0:
-            self._bucket_store.build_grad_in_bucket()
-
-            if self.moe_extra_dp_pg is None:
-                flat_grads = self._bucket_store.get_flatten_grad()
-                flat_grads /= self._world_size
+    @staticmethod
+    def run_reduction(bucket_store: BucketStore, grad_store: GradientStore):
+        if bucket_store.num_elements_in_bucket() > 0:
+            bucket_store.build_grad_in_bucket()
+            if bucket_store.moe_extra_dp_pg is None:
+                flat_grads = bucket_store.get_flatten_grad()
+                flat_grads /= bucket_store.zero_world_size
             else:
                 # record moe and non moe param
                 moe_list = []
-                for param in self._bucket_store._param_list:
+                for param in bucket_store._param_list:
                     moe_list.append(is_moe_tensor(param))
 
                 # divide them into different groups
                 moe_grad_list = []
                 non_moe_grad_list = []
-                for grad_list in self._bucket_store._grad_in_bucket.values():
+                for grad_list in bucket_store._grad_in_bucket.values():
                     non_moe_cur_grad = []
                     moe_cur_grad = []
                     for i in range(len(grad_list)):
                         if moe_list[i] == True:
                             moe_cur_grad.append(grad_list[i])
                         else:
                             non_moe_cur_grad.append(grad_list[i])
@@ -328,218 +321,249 @@
                         non_moe_grad_list.append(non_moe_cur_grad)
 
                 if len(non_moe_grad_list) > 0:
                     non_moe_flat_grads = []
                     for grad_list in non_moe_grad_list:
                         non_moe_flat_grads.append(_flatten_dense_tensors(grad_list))
                     non_moe_flat_grads = _flatten_dense_tensors(non_moe_flat_grads)
-                    non_moe_flat_grads /= self._world_size
+                    non_moe_flat_grads /= bucket_store.zero_world_size
 
                 if len(moe_grad_list) > 0:
                     moe_flat_grads = []
                     for grad_list in moe_grad_list:
                         moe_flat_grads.append(_flatten_dense_tensors(grad_list))
                     moe_flat_grads = _flatten_dense_tensors(moe_flat_grads)
 
             # ready to add other tensors to bucket
-            self._bucket_store.reset_num_elements_in_bucket()
+            bucket_store.reset_num_elements_in_bucket()
 
-            if self._overlap_communication:
-                stream = self._comm_stream
+            if bucket_store._overlap_communication:
+                stream = bucket_store.comm_stream
                 # in case of the memory being reused in the default stream
-                if self.moe_extra_dp_pg is None:
+                if bucket_store.moe_extra_dp_pg is None:
                     flat_grads.record_stream(stream)
                 else:
                     if len(non_moe_grad_list) > 0:
                         non_moe_flat_grads.record_stream(stream)
                     if len(moe_grad_list) > 0:
                         moe_flat_grads.record_stream(stream)
                 # waiting for ops in the default stream finishing
                 stream.wait_stream(get_accelerator().current_stream())
             else:
                 stream = get_accelerator().current_stream()
 
             with get_accelerator().stream(stream):
-                group_id = self._bucket_store.current_group_id
+                group_id = bucket_store.current_group_id
 
-                if self.moe_extra_dp_pg is None:
+                if bucket_store.moe_extra_dp_pg is None:
                     grad_dtype = flat_grads.dtype
-                    if self._communication_dtype is not None:
-                        flat_grads = flat_grads.to(self._communication_dtype)
+                    if bucket_store._communication_dtype is not None:
+                        flat_grads = flat_grads.to(bucket_store._communication_dtype)
 
-                if not self._partition_grads:
-                    if self.moe_extra_dp_pg is None:
-                        dist.all_reduce(flat_grads, group=self.dp_pg)
+                if not grad_store._partition_grads:
+                    if bucket_store.moe_extra_dp_pg is None:
+                        dist.all_reduce(flat_grads, group=bucket_store.torch_pg)
                         if flat_grads.dtype != grad_dtype:
                             flat_grads = flat_grads.to(grad_dtype)
 
-                        flat_grads_per_rank = flat_grads.split(flat_grads.numel() // self._world_size)
-                        grad_in_bucket = self._bucket_store.get_grad()
-                        self._update_unpartitoned_grad(grad_in_bucket.values(), flat_grads_per_rank, group_id)
+                        flat_grads_per_rank = flat_grads.split(flat_grads.numel() // bucket_store.zero_world_size)
+                        grad_in_bucket = bucket_store.get_grad()
+                        LowLevelZeroOptimizer.update_unpartitoned_grad(
+                            bucket_store, grad_store, grad_in_bucket.values(), flat_grads_per_rank, group_id
+                        )
 
                     # sync extra zero group
                     else:
                         # sync non moe param in global dp group
                         if len(non_moe_grad_list) > 0:
-                            dist.all_reduce(non_moe_flat_grads, group=self.dp_pg)
+                            dist.all_reduce(non_moe_flat_grads, group=bucket_store.torch_pg)
                             flat_grads_per_rank = non_moe_flat_grads.split(
-                                non_moe_flat_grads.numel() // self._world_size
+                                non_moe_flat_grads.numel() // bucket_store.zero_world_size
+                            )
+                            LowLevelZeroOptimizer.update_unpartitoned_grad(
+                                bucket_store, grad_store, non_moe_grad_list, flat_grads_per_rank, group_id
                             )
-                            self._update_unpartitoned_grad(non_moe_grad_list, flat_grads_per_rank, group_id)
 
                         # sync moe param only in zero group
                         if len(moe_grad_list) > 0:
-                            dist.all_reduce(moe_flat_grads, group=self.moe_extra_dp_pg)
-                            flat_grads_per_rank = moe_flat_grads.split(moe_flat_grads.numel() // self._world_size)
-                            self._update_unpartitoned_grad(moe_grad_list, flat_grads_per_rank, group_id)
+                            dist.all_reduce(moe_flat_grads, group=bucket_store.moe_extra_dp_pg)
+                            flat_grads_per_rank = moe_flat_grads.split(
+                                moe_flat_grads.numel() // bucket_store.zero_world_size
+                            )
+                            LowLevelZeroOptimizer.update_unpartitoned_grad(
+                                bucket_store, grad_store, moe_grad_list, flat_grads_per_rank, group_id
+                            )
 
                 else:
-                    if self.moe_extra_dp_pg is None:
-                        flat_grads_list = list(flat_grads.split(len(flat_grads) // self._world_size))
-                        recieved_grad = torch.zeros_like(flat_grads_list[0])
-                        dist.reduce_scatter(recieved_grad, flat_grads_list, group=self.dp_pg)
-
-                        if recieved_grad.dtype != grad_dtype:
-                            recieved_grad = recieved_grad.to(grad_dtype)
-
-                        grad_in_bucket_current_rank = self._bucket_store.get_grad()[self._local_rank]
-                        self._update_partitoned_grad(grad_in_bucket_current_rank, recieved_grad, group_id, 1)
+                    if bucket_store.moe_extra_dp_pg is None:
+                        flat_grads_list = list(flat_grads.split(len(flat_grads) // bucket_store.zero_world_size))
+                        received_grad = torch.zeros_like(flat_grads_list[0])
+                        dist.reduce_scatter(received_grad, flat_grads_list, group=bucket_store.torch_pg)
+
+                        if received_grad.dtype != grad_dtype:
+                            received_grad = received_grad.to(grad_dtype)
+
+                        grad_in_bucket_current_rank = bucket_store.get_grad()[bucket_store.zero_local_rank]
+                        LowLevelZeroOptimizer.update_partitoned_grad(
+                            bucket_store, grad_store, grad_in_bucket_current_rank, received_grad, group_id, 1
+                        )
                     else:
                         # categorize moe and non moe param
-                        grad_in_bucket_current_rank = self._bucket_store.get_grad()[self._local_rank]
+                        grad_in_bucket_current_rank = bucket_store.get_grad()[bucket_store.zero_local_rank]
                         moe_grad_in_bucket_current_rank = []
                         non_moe_grad_in_bucket_current_rank = []
                         for idx, grad in enumerate(grad_in_bucket_current_rank):
                             if moe_list[idx] == True:
                                 moe_grad_in_bucket_current_rank.append(grad)
                             else:
                                 non_moe_grad_in_bucket_current_rank.append(grad)
 
                         if len(non_moe_grad_list) > 0:
                             flat_grads_list = list(
-                                non_moe_flat_grads.split(len(non_moe_flat_grads) // self._world_size)
+                                non_moe_flat_grads.split(len(non_moe_flat_grads) // bucket_store.zero_world_size)
                             )
-                            recieved_grad = torch.zeros_like(flat_grads_list[0])
-                            dist.reduce_scatter(recieved_grad, flat_grads_list, group=self.dp_pg)
-                            self._update_partitoned_grad(
+                            received_grad = torch.zeros_like(flat_grads_list[0])
+                            dist.reduce_scatter(received_grad, flat_grads_list, group=bucket_store.torch_pg)
+                            LowLevelZeroOptimizer.update_partitoned_grad(
+                                bucket_store,
+                                grad_store,
                                 non_moe_grad_in_bucket_current_rank,
-                                recieved_grad,
+                                received_grad,
                                 group_id,
                                 1,
                             )
 
                         if len(moe_grad_list) > 0:
                             flat_grads_list = list(
-                                moe_flat_grads.split(len(moe_flat_grads) // self.moe_extra_dp_pg_size)
+                                moe_flat_grads.split(len(moe_flat_grads) // bucket_store.moe_extra_dp_pg_size)
                             )
-                            recieved_grad = torch.zeros_like(flat_grads_list[0])
+                            received_grad = torch.zeros_like(flat_grads_list[0])
                             dist.reduce_scatter(
-                                recieved_grad,
+                                received_grad,
                                 flat_grads_list,
-                                group=self.moe_extra_dp_pg,
+                                group=bucket_store.moe_extra_dp_pg,
                             )
-                            param_slice = self._world_size // self.moe_extra_dp_pg_size
-                            recieved_grad = list(recieved_grad.split(len(recieved_grad) // param_slice))
-                            for split_recieved_grad in recieved_grad:
+                            param_slice = bucket_store.zero_world_size // bucket_store.moe_extra_dp_pg_size
+                            received_grad = list(received_grad.split(len(received_grad) // param_slice))
+                            for split_recieved_grad in received_grad:
                                 split_recieved_grad = _unflatten_dense_tensors(
                                     split_recieved_grad, moe_grad_in_bucket_current_rank
                                 )
                                 for real_grad, grad in zip(split_recieved_grad, moe_grad_in_bucket_current_rank):
-                                    param_id = self._bucket_store.get_param_id_of_grad(grad)
-                                    self._add_grad(real_grad, param_slice, group_id, param_id)
-
-                self._bucket_store.reset()
-
-    def _update_unpartitoned_grad(self, origin_grad_list: List, flat_grad_list: List, group_id: int) -> None:
+                                    param_id = bucket_store.get_param_id_of_grad(grad)
+                                    LowLevelZeroOptimizer.add_grad(
+                                        grad_store, real_grad, param_slice, group_id, param_id
+                                    )
+
+                bucket_store.reset()
+
+    @staticmethod
+    def update_unpartitoned_grad(
+        bucket_store: BucketStore,
+        grad_store: GradientStore,
+        origin_grad_list: List,
+        flat_grad_list: List,
+        group_id: int,
+    ) -> None:
         for rank, grad_list in enumerate(origin_grad_list):
             sync_tensor(flat_grad_list[rank], grad_list)
             for grad in grad_list:
-                param_id = self._bucket_store.get_param_id_of_grad(grad)
-                self._add_grad(grad, self._world_size, group_id, param_id, rank)
+                param_id = bucket_store.get_param_id_of_grad(grad)
+                LowLevelZeroOptimizer.add_grad(grad_store, grad, bucket_store.zero_world_size, group_id, param_id, rank)
 
-    def _update_partitoned_grad(
-        self,
+    @staticmethod
+    def update_partitoned_grad(
+        bucket_store: BucketStore,
+        grad_store: GradientStore,
         origin_grad_list: List,
         flat_grad: torch.Tensor,
         group_id: int,
         partition_num: int,
     ) -> None:
         sync_tensor(flat_grad, origin_grad_list)
         for grad in origin_grad_list:
-            param_id = self._bucket_store.get_param_id_of_grad(grad)
-            self._add_grad(grad, partition_num, group_id, param_id)
+            param_id = bucket_store.get_param_id_of_grad(grad)
+            LowLevelZeroOptimizer.add_grad(grad_store, grad, partition_num, group_id, param_id)
 
-    def _add_grad(
-        self,
+    @staticmethod
+    def add_grad(
+        grad_store: GradientStore,
         grad: torch.Tensor,
         partition_num: int,
         group_id: int,
         param_id: int,
         rank: int = 0,
     ) -> None:
-        if len(self._grad_store.get_partitioned_gradients_by_param_id(group_id, param_id)) < partition_num:
-            self._grad_store.append_gradients_by_param_id(grad, group_id, param_id)
+        if len(grad_store.get_partitioned_gradients_by_param_id(group_id, param_id)) < partition_num:
+            grad_store.append_gradients_by_param_id(grad, group_id, param_id)
         else:
-            self._grad_store.add_gradients_by_param_id(grad, rank, group_id, param_id)
+            grad_store.add_gradients_by_param_id(grad, rank, group_id, param_id)
 
-    def _add_to_bucket(self, param, group_id):
+    @staticmethod
+    def add_to_bucket(
+        param: nn.Parameter,
+        group_id: int,
+        bucket_store: BucketStore,
+        param_store: ParameterStore,
+        grad_store: GradientStore,
+    ):
         param_size = param.numel()
 
         # check if the bucket is full
         # if full, will reduce the grads already in the bucket
         # or got a grad of param from another group
         # after reduction, the bucket will be empty
         if (
-            self._bucket_store.num_elements_in_bucket() + param_size > self._reduce_bucket_size
-            or group_id != self._bucket_store.current_group_id
+            bucket_store.num_elements_in_bucket() + param_size > bucket_store.reduce_bucket_size
+            or group_id != bucket_store.current_group_id
         ):
-            self._run_reduction()
+            LowLevelZeroOptimizer.run_reduction(bucket_store, grad_store)
 
-        padding_size = self._param_store.get_param_padding_size(param)
-        self._bucket_store.add_param_grad(group_id, param, padding_size)
+        padding_size = param_store.get_param_padding_size(param)
+        bucket_store.add_param_grad(group_id, param, padding_size)
 
     ################################
     # torch.optim.Optimizer methods
     ################################
 
     def backward(self, loss, retain_graph=False):
         assert not (
-            self._partition_grads and not self.require_grad_sync
+            self._grad_store._partition_grads and not self._grad_store.require_grad_sync
         ), "ZeRO2(partition_grads) and no_sync are not compatible"
 
         if self.mixed_precision_mixin is not None:
             loss = self.mixed_precision_mixin.pre_backward(loss)
 
         loss.backward(retain_graph=retain_graph)
 
-        if not self.require_grad_sync:
+        if not self._grad_store.require_grad_sync:
             return
 
-        self._reduce_grad(self._partition_grads)
+        self._reduce_grad(self._grad_store._partition_grads)
 
         # clear reduced grads
-        if self._overlap_communication:
+        if self._bucket_store._overlap_communication:
             get_accelerator().synchronize()
         self.zero_grad()
 
     def backward_by_grad(self, tensor, grad):
         assert not (
-            self._partition_grads and not self.require_grad_sync
+            self._grad_store._partition_grads and not self._grad_store.require_grad_sync
         ), "ZeRO2(partition_grads) and gradient accumulation(no_sync) are not compatible"
 
         if self.mixed_precision_mixin is not None:
             grad = self.mixed_precision_mixin.pre_backward_by_grad(tensor, grad)
         torch.autograd.backward(tensor, grad)
 
-        if not self.require_grad_sync:
+        if not self._grad_store.require_grad_sync:
             return
-        self._reduce_grad(self._partition_grads)
+        self._reduce_grad(self._grad_store._partition_grads)
 
         # clear reduced grads
-        if self._overlap_communication:
+        if self._bucket_store._overlap_communication:
             get_accelerator().synchronize()
 
         self.zero_grad()
 
     def zero_grad(self, set_to_none=True):
         """
         Set parameter gradients to zero. If set_to_none = True, gradient
@@ -562,15 +586,15 @@
 
     ####################
     # Update Parameter #
     ####################
 
     def step(self, closure=None):
         assert closure is None, "closure is not supported by step()"
-        if not self.require_grad_sync:
+        if not self._grad_store.require_grad_sync:
             return
 
         if self.mixed_precision_mixin is not None and self.mixed_precision_mixin.should_skip_step():
             self._grad_store.reset_all_gradients()
             if self._verbose:
                 self._logger.info(f"Found overflow. Skip step")
             self.zero_grad()
@@ -581,35 +605,37 @@
         norm_groups = []
 
         # sometimes not all params are 'really' working
         # for instance, when layer drop, the dropped layer has no grad
         # and should not be updated
         real_working_params = dict()
         real_master_params = dict()
-        grad_index = 0 if self._partition_grads else self._local_rank
+        grad_index = 0 if self._grad_store._partition_grads else self._bucket_store.zero_local_rank
         for group_id in range(self.num_param_groups):
             master_params = self._master_param_groups_of_current_rank[group_id]
             real_working_params[group_id] = []
             real_master_params[group_id] = []
             for splited_param in master_params:
                 working_param = self._param_store.master_to_working_param[id(splited_param)]
                 # if a working param requires grad and has no grad
                 # it is not 'really' working, e.g. the droped layer
                 # else the splited grad should be attached to the splited param
                 grads = self._grad_store.get_partitioned_gradients_by_param_id(group_id, id(working_param))
                 if len(grads) > 0:
                     # moe hybrid zero
-                    if self.moe_extra_dp_pg is not None and is_moe_tensor(working_param):
+                    if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(working_param):
                         real_working_params[group_id].append(working_param)
-                        if self._partition_grads:
+                        if self._grad_store._partition_grads:
                             grad = grads
                         else:
-                            param_slice = self._world_size // self.moe_extra_dp_pg_size
+                            param_slice = self._bucket_store.zero_world_size // self._bucket_store.moe_extra_dp_pg_size
                             grad = grads[
-                                self.moe_extra_dp_pg_rank * param_slice : (self.moe_extra_dp_pg_rank + 1) * param_slice
+                                self._bucket_store.moe_extra_dp_pg_rank
+                                * param_slice : (self._bucket_store.moe_extra_dp_pg_rank + 1)
+                                * param_slice
                             ]
                         grad = flatten(grad)
                     else:
                         real_working_params[group_id].append(working_param)
                         grad = grads[grad_index]
                     # no need to copy fp32 grad if master_weights is False
                     if self._master_weights:
@@ -670,33 +696,33 @@
 
         # update working partition updated by the current rank
         device = get_accelerator().get_current_device()
         for group_id in range(self.num_param_groups):
             master_working_param = self.optim.param_groups[group_id]["params"]
             for idx, splited_param in enumerate(master_working_param):
                 working_param = real_working_params[group_id][idx]
-                if self.moe_extra_dp_pg is not None and is_moe_tensor(working_param):
+                if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(working_param):
                     all_splited_param = [
                         torch.zeros(splited_param.shape, device=device, dtype=self._dtype)
-                        for _ in range(self.moe_extra_dp_pg_size)
+                        for _ in range(self._bucket_store.moe_extra_dp_pg_size)
                     ]
                     dist.all_gather(
                         all_splited_param,
                         splited_param.to(device).to(self._dtype),
-                        group=self.moe_extra_dp_pg,
+                        group=self._bucket_store.moe_extra_dp_pg,
                     )
                 else:
                     all_splited_param = [
                         torch.zeros(splited_param.shape, device=device, dtype=self._dtype)
-                        for _ in range(self._world_size)
+                        for _ in range(self._bucket_store.zero_world_size)
                     ]
                     dist.all_gather(
                         all_splited_param,
                         splited_param.to(device).to(self._dtype),
-                        group=self.dp_pg,
+                        group=self._bucket_store.torch_pg,
                     )
                 working_param.data.copy_(flatten(all_splited_param)[: working_param.numel()].reshape_as(working_param))
             self.optim.param_groups[group_id]["params"] = self._master_param_groups_of_current_rank[group_id]
 
     def _compute_grad_norm(self, gradients: List[Tensor], norm_type: int = 2) -> float:
         r"""
         Compute and return the gradient norm for gradient clipping.
@@ -716,15 +742,15 @@
         if norm_type == inf:
             total_norm = max(grad.data.abs().max() for grad in gradients)
             total_norm_cuda = torch.tensor(
                 [float(total_norm)],
                 device=get_accelerator().get_current_device(),
                 dtype=torch.float,
             )
-            dist.all_reduce(total_norm_cuda, op=torch.distributed.ReduceOp.MAX, group=self.dp_pg)
+            dist.all_reduce(total_norm_cuda, op=torch.distributed.ReduceOp.MAX, group=self._bucket_store.torch_pg)
             total_norm = total_norm_cuda.item()
 
         else:
             total_norm_exponentiated = 0.0
             for grad in gradients:
                 grad_norm_exponentiated = grad.data.double().norm(norm_type) ** norm_type
                 total_norm_exponentiated += grad_norm_exponentiated
@@ -734,15 +760,15 @@
                 [float(total_norm_exponentiated)],
                 device=get_accelerator().get_current_device(),
                 dtype=torch.float,
             )
             torch.distributed.all_reduce(
                 total_norm_exponentiated_cuda,
                 op=torch.distributed.ReduceOp.SUM,
-                group=self.dp_pg,
+                group=self._bucket_store.torch_pg,
             )
             total_norm = total_norm_exponentiated_cuda.item() ** (1.0 / norm_type)
 
         return total_norm
 
     #############################
     # Mixed Precision Utilities #
@@ -769,35 +795,41 @@
 
     # this method is used to sync gradient manually
     def _sync_grad(self):
         for group_id in range(self.num_param_groups):
             param_group = self._working_param_groups[group_id]
             for param in param_group:
                 if param.requires_grad and param.grad is not None:
-                    self._add_to_bucket(param, group_id)
+                    LowLevelZeroOptimizer.add_to_bucket(
+                        param,
+                        group_id,
+                        self._bucket_store,
+                        self._param_store,
+                        self._grad_store,
+                    )
 
-        self._run_reduction()
+        LowLevelZeroOptimizer.run_reduction(self._bucket_store, self._grad_store)
 
     def _reduce_grad(self, partition_grad):
         # if not overlapping communication (no reduction hook is attached) when zero1
         # we need to manually reduce these gradients
-        if not partition_grad and not self._overlap_communication:
+        if not partition_grad and not self._bucket_store._overlap_communication:
             self._sync_grad()
         else:
-            self._run_reduction()
+            LowLevelZeroOptimizer.run_reduction(self._bucket_store, self._grad_store)
 
     # this context comes from pytorch DDP
     @contextmanager
     def no_sync(self):
-        old_require_grad_sync = self.require_grad_sync
-        self.require_grad_sync = False
+        old_require_grad_sync = self._grad_store.require_grad_sync
+        self._grad_store.require_grad_sync = False
         try:
             yield
         finally:
-            self.require_grad_sync = old_require_grad_sync
+            self._grad_store.require_grad_sync = old_require_grad_sync
 
     ##############
     # State Dict #
     ##############
 
     def _pack_state(self, state: Dict) -> Dict:
         # comes from pytorch optimizer.state_dict()
@@ -829,24 +861,26 @@
         zero_state = dict()
         device = get_accelerator().get_current_device()
         for param, state in self.optim.state.items():
             zero_state[param] = copy.deepcopy(state)
             for k, v in state.items():
                 if isinstance(v, torch.Tensor) and k != "step":
                     working_param = self._param_store.master_to_working_param[id(param)]
-                    if self.moe_extra_dp_pg is not None and is_moe_tensor(v):
+                    if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(v):
                         gather_tensor = [
-                            torch.zeros(v.shape, device=device, dtype=v.dtype) for _ in range(self.moe_extra_dp_pg_size)
+                            torch.zeros(v.shape, device=device, dtype=v.dtype)
+                            for _ in range(self._bucket_store.moe_extra_dp_pg_size)
                         ]
-                        dist.all_gather(gather_tensor, v.to(device), group=self.moe_extra_dp_pg)
+                        dist.all_gather(gather_tensor, v.to(device), group=self._bucket_store.moe_extra_dp_pg)
                     else:
                         gather_tensor = [
-                            torch.zeros(v.shape, device=device, dtype=v.dtype) for _ in range(self._world_size)
+                            torch.zeros(v.shape, device=device, dtype=v.dtype)
+                            for _ in range(self._bucket_store.zero_world_size)
                         ]
-                        dist.all_gather(gather_tensor, v.to(device), group=self.dp_pg)
+                        dist.all_gather(gather_tensor, v.to(device), group=self._bucket_store.torch_pg)
                     param_state = (
                         torch.stack(gather_tensor).view(-1)[: working_param.numel()].reshape_as(working_param).cpu()
                     )
                     zero_state[param][k] = param_state
 
         states_dict = self._pack_state(zero_state)
 
@@ -858,25 +892,31 @@
         Args:
             state_dict (dict): A pytorch form state_dict
         """
         zero_state_dict = copy.deepcopy(state_dict)
         for param_idx, state in zero_state_dict["state"].items():
             for k, v in state.items():
                 if isinstance(v, torch.Tensor) and k != "step":
-                    padding_size = (self._world_size - v.numel() % self._world_size) % self._world_size
+                    padding_size = (
+                        self._bucket_store.zero_world_size - v.numel() % self._bucket_store.zero_world_size
+                    ) % self._bucket_store.zero_world_size
                     with torch.no_grad():
                         v = v.flatten()
                         if padding_size > 0:
                             v = torch.nn.functional.pad(v, [0, padding_size])
-                        if self.moe_extra_dp_pg is not None and is_moe_tensor(v):
-                            v_list = v.split(v.numel() // self.moe_extra_dp_pg_size)
-                            zero_state_dict["state"][param_idx][k] = v_list[self.moe_extra_dp_pg_rank].detach().clone()
+                        if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(v):
+                            v_list = v.split(v.numel() // self._bucket_store.moe_extra_dp_pg_size)
+                            zero_state_dict["state"][param_idx][k] = (
+                                v_list[self._bucket_store.moe_extra_dp_pg_rank].detach().clone()
+                            )
                         else:
-                            v_list = v.split(v.numel() // self._world_size)
-                            zero_state_dict["state"][param_idx][k] = v_list[self._local_rank].detach().clone()
+                            v_list = v.split(v.numel() // self._bucket_store.zero_world_size)
+                            zero_state_dict["state"][param_idx][k] = (
+                                v_list[self._bucket_store.zero_local_rank].detach().clone()
+                            )
 
         self.optim.load_state_dict(zero_state_dict)
 
     def state_dict_shard(self, max_shard_size: int = 1024) -> Iterator[Tuple[Dict, int]]:
         """Returns dictionaries containing a whole state of the module one by one. The max size of dictionary shard is specified by ``max_shard_size``.
            Only include the 'state' in state_dict.
 
@@ -900,24 +940,26 @@
                 if (group_id + 1) * len(pg) < param_idx:
                     continue
                 master_param = pg[param_idx - (group_id) * len(pg)]
                 working_param = self._param_store.master_to_working_param[id(master_param)]
 
             for k, v in states.items():
                 if isinstance(v, torch.Tensor) and k != "step":
-                    if self.moe_extra_dp_pg is not None and is_moe_tensor(v):
+                    if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(v):
                         state_tensor = [
-                            torch.zeros(v.shape, device=device, dtype=v.dtype) for _ in range(self.moe_extra_dp_pg_size)
+                            torch.zeros(v.shape, device=device, dtype=v.dtype)
+                            for _ in range(self._bucket_store.moe_extra_dp_pg_size)
                         ]
-                        dist.all_gather(state_tensor, v.to(device), group=self.moe_extra_dp_pg)
+                        dist.all_gather(state_tensor, v.to(device), group=self._bucket_store.moe_extra_dp_pg)
                     else:
                         state_tensor = [
-                            torch.zeros(v.shape, device=device, dtype=v.dtype) for _ in range(self._world_size)
+                            torch.zeros(v.shape, device=device, dtype=v.dtype)
+                            for _ in range(self._bucket_store.zero_world_size)
                         ]
-                        dist.all_gather(state_tensor, v.to(device), group=self.dp_pg)
+                        dist.all_gather(state_tensor, v.to(device), group=self._bucket_store.torch_pg)
                     state_tensor = (
                         torch.stack(state_tensor).view(-1)[: working_param.numel()].reshape_as(working_param).cpu()
                     )
                     current_block_size += state_tensor.numel()
                     current_block[k] = state_tensor
 
             if ret_block_size + current_block_size > max_shard_size and len(ret_block) > 0:
@@ -940,25 +982,44 @@
             p_id = id(p)
             if p_id in self._param_store.working_to_master_param:
                 master_param = self._param_store.working_to_master_param[p_id]
                 padding_size = self._param_store.get_param_padding_size(p)
                 working_param = p.data.view(-1)
                 if padding_size > 0:
                     working_param = torch.nn.functional.pad(working_param, [0, padding_size])
-                if self.moe_extra_dp_pg is not None and is_moe_tensor(p):
+                if self._bucket_store.moe_extra_dp_pg is not None and is_moe_tensor(p):
                     master_param.copy_(working_param.chunk(self.extra_dp_pg_size)[self.extra_dp_pg_rank])
                 else:
-                    master_param.copy_(working_param.chunk(self._world_size)[self._local_rank])
+                    master_param.copy_(
+                        working_param.chunk(self._bucket_store.zero_world_size)[self._bucket_store.zero_local_rank]
+                    )
         if hasattr(self, "master_moe_params"):
             for master_moe_param, working_moe_param in zip(self.master_moe_params, self.working_moe_params):
                 master_moe_param.copy_(working_moe_param)
 
+    def remove_hooks(self) -> None:
+        """remove the registered hooks
+
+        Args:
+            plugin (LowLevelZeroPlugin): the plugin to bound this method.
+        """
+        for group_id in range(self.num_param_groups):
+            param_group = self._working_param_groups[group_id]
+            for param in param_group:
+                if param.requires_grad:
+                    assert hasattr(param, "_grad_handle")
+                    param._grad_handle.remove()
+                    delattr(param, "_grad_handle")
+
     def get_working_to_master_map(self) -> Dict[int, torch.Tensor]:
         return self._param_store.working_to_master_param
 
     def get_master_to_working_map(self) -> Dict[int, torch.Tensor]:
         if hasattr(self, "moe_master_to_working_map"):
             return {
                 **self._param_store.master_to_working_param,
                 **self.moe_master_to_working_map,
             }
         return self._param_store.master_to_working_param
+
+    def get_param_padding_map(self) -> Dict[int, torch.Tensor]:
+        return self._param_store.get_padding_map()
```

### Comparing `colossalai-nightly-2024.5.4/colossalai/zero/wrapper.py` & `colossalai-nightly-2024.6.1/colossalai/zero/wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/PKG-INFO` & `colossalai-nightly-2024.6.1/colossalai_nightly.egg-info/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: colossalai-nightly
-Version: 2024.5.4
+Version: 2024.6.1
 Summary: An integrated large-scale model training system with efficient parallelization techniques
 Home-page: https://www.colossalai.org
 License: Apache Software License 2.0
 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/discussions
 Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/issues
 Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io
@@ -32,14 +32,15 @@
         
         
            | [English](README.md) | [](docs/README-zh-Hans.md) |
         
         </div>
         
         ## Latest News
+        * [2024/05] [Large AI Models Inference Speed Doubled, Colossal-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference)
         * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
         * [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)
         * [2024/03] [314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
         * [2024/03] [Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models](https://hpc-ai.com/blog/open-sora-v1.0)
         * [2024/03] [Open-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/open-sora)
         * [2024/01] [Inference Performance Improved by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer)
         * [2024/01] [Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b)
@@ -82,19 +83,17 @@
              <li><a href="#GPT-2-Single">GPT-2</a></li>
              <li><a href="#PaLM-Single">PaLM</a></li>
            </ul>
          </li>
          <li>
            <a href="#Inference">Inference</a>
            <ul>
+             <li><a href="#Colossal-Inference">Colossal-Inference: Large AI  Models Inference Speed Doubled</a></li>
              <li><a href="#Grok-1">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>
              <li><a href="#SwiftInfer">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>
-             <li><a href="#GPT-3-Inference">GPT-3</a></li>
-             <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
-             <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
            </ul>
          </li>
          <li>
            <a href="#Installation">Installation</a>
            <ul>
              <li><a href="#PyPI">PyPI</a></li>
              <li><a href="#Install-From-Source">Install From Source</a></li>
@@ -384,56 +383,52 @@
         
         - 34x larger model size on the same hardware
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         
         ## Inference
+        ### Colossal-Inference
+        <p align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-1.png" width=1000/>
+        </p>
+        
+        <p align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-2.png" width=1000/>
+        </p>
+        
+         - Large AI models inference speed doubled, compared to the offline inference performance of vLLM in some cases.
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/inference)
+        [[blog]](https://hpc-ai.com/blog/colossal-inference)
+        
         ### Grok-1
         <p id="Grok-1" align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg" width=600/>
         </p>
         
          - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.
         
         [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)
         [[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
         [[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)
         [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)
         
+        ### SwiftInfer
         <p id="SwiftInfer" align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg" width=800/>
         </p>
         
         - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations
         
-        <p id="GPT-3-Inference" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference_GPT-3.jpg" width=800/>
-        </p>
-        
-        - [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference acceleration on the same hardware
-        
-        <p id="OPT-Serving" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20serving.png" width=600/>
-        </p>
-        
-        - [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service): Try 175-billion-parameter OPT online services
-        
-        <p id="BLOOM-Inference" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20Inference.PNG" width=800/>
-        </p>
-        
-        - [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom): Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10 times.
-        
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         ## Installation
         
         Requirements:
-        - PyTorch >= 1.11 and PyTorch <= 2.1
+        - PyTorch >= 2.1
         - Python >= 3.7
         - CUDA >= 11.0
         - [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)
         - Linux OS
         
         If you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.5.4 Summary: An
+Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.6.1 Summary: An
 integrated large-scale model training system with efficient parallelization
 techniques Home-page: https://www.colossalai.org License: Apache Software
 License 2.0 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/
 discussions Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/
 issues Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io Project-URL:
 Github, https://github.com/hpcaitech/ColossalAI Description: # Colossal-AI
@@ -22,46 +22,47 @@
  (https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://
 huggingface.co/hpcai-tech) [![slack badge](https://img.shields.io/badge/Slack-
 join-blueviolet?logo=slack&)](https://github.com/hpcaitech/public_assets/tree/
  main/colossalai/contact/slack) [![WeChat badge](https://img.shields.io/badge/
 --green?logo=wechat&)](https://raw.githubusercontent.com/hpcaitech/
 public_assets/main/colossalai/img/WeChat.png) | [English](README.md) | []
                           (docs/README-zh-Hans.md) |
-## Latest News * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open
-Source with Single-Shot 16-Second Video Generation and 720p Resolution](https:/
-/hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-
-video-generation-and-720p-resolution-in-open-source) * [2024/04] [Most cost-
-effective solutions for inference, fine-tuning and pretraining, tailored to
-LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-
-inference-fine-tuning-and-pretraining-tailored-to-llama3-series) * [2024/03]
-[314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and
-Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-
-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-
-use-pytorchhuggingface-version-is-here) * [2024/03] [Open-Sora: Revealing
-Complete Model Parameters, Training Details, and Everything for Sora-like Video
-Generation Models](https://hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-
-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to
-Nearly a Million](https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference
-Performance Improved by 46%, Open Source Solution Breaks the Length Limit of
-LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-
-SwiftInfer) * [2024/01] [Construct Refined 13B Private Model With Just $5000
-USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/
-colossal-llama-2-13b) * [2023/11] [Enhanced MoE Parallelism, Open-source MoE
-Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/
-enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-
-efficient) * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars
-Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-
-Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-
-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-
-large-models-open-source-and-commercial-free-domain-specific-llm-solution) *
-[2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%]
-(https://www.hpc-ai.tech/blog/70b-llama2-training) * [2023/07] [HPC-AI Tech
-Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-
-tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-
-business-growth) ## Table of Contents
+## Latest News * [2024/05] [Large AI Models Inference Speed Doubled, Colossal-
+Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference) *
+[2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-
+Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/
+open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-
+and-720p-resolution-in-open-source) * [2024/04] [Most cost-effective solutions
+for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://
+hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-
+pretraining-tailored-to-llama3-series) * [2024/03] [314 Billion Parameter Grok-
+1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace
+version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-
+inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-
+version-is-here) * [2024/03] [Open-Sora: Revealing Complete Model Parameters,
+Training Details, and Everything for Sora-like Video Generation Models](https:/
+/hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-SoraSora Replication
+Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million]
+(https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference Performance Improved
+by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round
+Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer) * [2024/01]
+[Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI
+Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b) * [2023/11]
+[Enhanced MoE Parallelism, Open-source MoE Model Training Can Be 9 Times More
+Efficient](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-
+moe-model-training-can-be-9-times-more-efficient) * [2023/09] [One Half-Day of
+Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large
+Models, Open-Source and Commercial-Free Domain-Specific LLM Solution](https://
+www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-
+yields-similar-results-to-mainstream-large-models-open-source-and-commercial-
+free-domain-specific-llm-solution) * [2023/09] [70 Billion Parameter LLaMA2
+Model Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-
+training) * [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding]
+(https://www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-
+funding-to-fuel-team-expansion-and-business-growth) ## Table of Contents
     * _W_h_y_ _C_o_l_o_s_s_a_l_-_A_I
     * _F_e_a_t_u_r_e_s
     * _C_o_l_o_s_s_a_l_-_A_I_ _f_o_r_ _R_e_a_l_ _W_o_r_l_d_ _A_p_p_l_i_c_a_t_i_o_n_s
           o _O_p_e_n_-_S_o_r_a_:_ _R_e_v_e_a_l_i_n_g_ _C_o_m_p_l_e_t_e_ _M_o_d_e_l_ _P_a_r_a_m_e_t_e_r_s_,_ _T_r_a_i_n_i_n_g_ _D_e_t_a_i_l_s_,
             _a_n_d_ _E_v_e_r_y_t_h_i_n_g_ _f_o_r_ _S_o_r_a_-_l_i_k_e_ _V_i_d_e_o_ _G_e_n_e_r_a_t_i_o_n_ _M_o_d_e_l_s
           o _C_o_l_o_s_s_a_l_-_L_L_a_M_A_-_2_:_ _O_n_e_ _H_a_l_f_-_D_a_y_ _o_f_ _T_r_a_i_n_i_n_g_ _U_s_i_n_g_ _a_ _F_e_w_ _H_u_n_d_r_e_d
             _D_o_l_l_a_r_s_ _Y_i_e_l_d_s_ _S_i_m_i_l_a_r_ _R_e_s_u_l_t_s_ _t_o_ _M_a_i_n_s_t_r_e_a_m_ _L_a_r_g_e_ _M_o_d_e_l_s_,_ _O_p_e_n_-
@@ -80,20 +81,18 @@
           o _O_P_T
           o _V_i_T
           o _R_e_c_o_m_m_e_n_d_a_t_i_o_n_ _S_y_s_t_e_m_ _M_o_d_e_l_s
     * _S_i_n_g_l_e_ _G_P_U_ _T_r_a_i_n_i_n_g_ _D_e_m_o
           o _G_P_T_-_2
           o _P_a_L_M
     * _I_n_f_e_r_e_n_c_e
+          o _C_o_l_o_s_s_a_l_-_I_n_f_e_r_e_n_c_e_:_ _L_a_r_g_e_ _A_I_ _M_o_d_e_l_s_ _I_n_f_e_r_e_n_c_e_ _S_p_e_e_d_ _D_o_u_b_l_e_d
           o _G_r_o_k_-_1_:_ _3_1_4_B_ _m_o_d_e_l_ _o_f_ _P_y_T_o_r_c_h_ _+_ _H_u_g_g_i_n_g_F_a_c_e_ _I_n_f_e_r_e_n_c_e
           o _S_w_i_f_t_I_n_f_e_r_:_B_r_e_a_k_s_ _t_h_e_ _L_e_n_g_t_h_ _L_i_m_i_t_ _o_f_ _L_L_M_ _f_o_r_ _M_u_l_t_i_-_R_o_u_n_d
             _C_o_n_v_e_r_s_a_t_i_o_n_s_ _w_i_t_h_ _4_6_%_ _A_c_c_e_l_e_r_a_t_i_o_n
-          o _G_P_T_-_3
-          o _O_P_T_-_1_7_5_B_ _O_n_l_i_n_e_ _S_e_r_v_i_n_g_ _f_o_r_ _T_e_x_t_ _G_e_n_e_r_a_t_i_o_n
-          o _1_7_6_B_ _B_L_O_O_M
     * _I_n_s_t_a_l_l_a_t_i_o_n
           o _P_y_P_I
           o _I_n_s_t_a_l_l_ _F_r_o_m_ _S_o_u_r_c_e
     * _U_s_e_ _D_o_c_k_e_r
     * _C_o_m_m_u_n_i_t_y
     * _C_o_n_t_r_i_b_u_t_i_n_g
     * _C_i_t_e_ _U_s
@@ -283,72 +282,66 @@
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 GPT2-NVME.png]
 - 120x larger model size on the same hardware (RTX 3080) ### PaLM
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 PaLM-GPU1.png]
 - 34x larger model size on the same hardware
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Inference ### Grok-1
+## Inference ### Colossal-Inference
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                    inference/colossal-inference-v1-1.png]
+[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
+                    inference/colossal-inference-v1-2.png]
+- Large AI models inference speed doubled, compared to the offline inference
+performance of vLLM in some cases. [[code]](https://github.com/hpcaitech/
+ColossalAI/tree/main/colossalai/inference) [[blog]](https://hpc-ai.com/blog/
+colossal-inference) ### Grok-1
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                          images/grok-1-inference.jpg]
 - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use
 Python + PyTorch + HuggingFace version for Inference. [[code]](https://
 github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1) [[blog]]
 (https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-
 3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here) [
 [HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/
 grok-1) [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/
-models/colossalai/grok-1-pytorch/summary)
+models/colossalai/grok-1-pytorch/summary) ### SwiftInfer
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 SwiftInfer.jpg]
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance
 improved by 46%, open source solution breaks the length limit of LLM for multi-
 round conversations
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             inference_GPT-3.jpg]
-- [Energon-AI](https://github.com/hpcaitech/EnergonAI): 50% inference
-acceleration on the same hardware
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                             BLOOM%20serving.png]
-- [OPT Serving](https://colossalai.org/docs/advanced_tutorials/opt_service):
-Try 175-billion-parameter OPT online services
-[https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
-                            BLOOM%20Inference.PNG]
-- [BLOOM](https://github.com/hpcaitech/EnergonAI/tree/main/examples/bloom):
-Reduce hardware deployment costs of 176-billion-parameter BLOOM by more than 10
-times.
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Installation Requirements: - PyTorch >= 1.11 and PyTorch <= 2.1 - Python >=
-3.7 - CUDA >= 11.0 - [NVIDIA GPU Compute Capability](https://
-developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher) - Linux OS If
-you encounter any problem with installation, you may want to raise an [issue]
-(https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.
-### Install from PyPI You can easily install Colossal-AI with the following
-command. **By default, we do not build PyTorch extensions during
-installation.** ```bash pip install colossalai ``` **Note: only Linux is
-supported for now.** However, if you want to build the PyTorch extensions
-during installation, you can set `BUILD_EXT=1`. ```bash BUILD_EXT=1 pip install
-colossalai ``` **Otherwise, CUDA kernels will be built during runtime when you
-actually need them.** We also keep releasing the nightly version to PyPI every
-week. This allows you to access the unreleased features and bug fixes in the
-main branch. Installation can be made via ```bash pip install colossalai-
-nightly ``` ### Download From Source > The version of Colossal-AI will be in
-line with the main branch of the repository. Feel free to raise an issue if you
-encounter any problems. :) ```shell git clone https://github.com/hpcaitech/
-ColossalAI.git cd ColossalAI # install colossalai pip install . ``` By default,
-we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.
-If you want to install and enable CUDA kernel fusion (compulsory installation
-when using fused optimizer): ```shell BUILD_EXT=1 pip install . ``` For Users
-with CUDA 10.2, you can still build ColossalAI from source. However, you need
-to manually download the cub library and copy it to the corresponding
-directory. ```bash # clone the repository git clone https://github.com/
-hpcaitech/ColossalAI.git cd ColossalAI # download the cub library wget https://
-github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip cp -r cub-
-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ # install
-BUILD_EXT=1 pip install . ```
+## Installation Requirements: - PyTorch >= 2.1 - Python >= 3.7 - CUDA >= 11.0 -
+[NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0
+(V100/RTX20 and higher) - Linux OS If you encounter any problem with
+installation, you may want to raise an [issue](https://github.com/hpcaitech/
+ColossalAI/issues/new/choose) in this repository. ### Install from PyPI You can
+easily install Colossal-AI with the following command. **By default, we do not
+build PyTorch extensions during installation.** ```bash pip install colossalai
+``` **Note: only Linux is supported for now.** However, if you want to build
+the PyTorch extensions during installation, you can set `BUILD_EXT=1`. ```bash
+BUILD_EXT=1 pip install colossalai ``` **Otherwise, CUDA kernels will be built
+during runtime when you actually need them.** We also keep releasing the
+nightly version to PyPI every week. This allows you to access the unreleased
+features and bug fixes in the main branch. Installation can be made via ```bash
+pip install colossalai-nightly ``` ### Download From Source > The version of
+Colossal-AI will be in line with the main branch of the repository. Feel free
+to raise an issue if you encounter any problems. :) ```shell git clone https://
+github.com/hpcaitech/ColossalAI.git cd ColossalAI # install colossalai pip
+install . ``` By default, we do not compile CUDA/C++ kernels. ColossalAI will
+build them during runtime. If you want to install and enable CUDA kernel fusion
+(compulsory installation when using fused optimizer): ```shell BUILD_EXT=1 pip
+install . ``` For Users with CUDA 10.2, you can still build ColossalAI from
+source. However, you need to manually download the cub library and copy it to
+the corresponding directory. ```bash # clone the repository git clone https://
+github.com/hpcaitech/ColossalAI.git cd ColossalAI # download the cub library
+wget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip unzip 1.8.0.zip
+cp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/ #
+install BUILD_EXT=1 pip install . ```
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
 ## Use Docker ### Pull from DockerHub You can directly pull the docker image
 from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The
 image is automatically uploaded upon release. ### Build On Your Own Run the
 following command to build a docker image from Dockerfile provided. > Building
 Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker
 Runtime as the default when doing `docker build`. More details can be found
```

### Comparing `colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/SOURCES.txt` & `colossalai-nightly-2024.6.1/colossalai_nightly.egg-info/SOURCES.txt`

 * *Files 5% similar despite different names*

```diff
@@ -271,113 +271,141 @@
 colossalai/fx/tracer/meta_patch/patched_module/convolution.py
 colossalai/fx/tracer/meta_patch/patched_module/embedding.py
 colossalai/fx/tracer/meta_patch/patched_module/linear.py
 colossalai/fx/tracer/meta_patch/patched_module/normalization.py
 colossalai/fx/tracer/meta_patch/patched_module/pooling.py
 colossalai/fx/tracer/meta_patch/patched_module/rnn.py
 colossalai/inference/__init__.py
-colossalai/inference/engine/__init__.py
-colossalai/inference/engine/engine.py
-colossalai/inference/engine/microbatch_manager.py
-colossalai/inference/engine/modeling/__init__.py
-colossalai/inference/engine/modeling/_utils.py
-colossalai/inference/engine/modeling/bloom.py
-colossalai/inference/engine/modeling/chatglm2.py
-colossalai/inference/engine/modeling/llama.py
-colossalai/inference/engine/policies/__init__.py
-colossalai/inference/engine/policies/bloom.py
-colossalai/inference/engine/policies/chatglm2.py
-colossalai/inference/engine/policies/llama.py
+colossalai/inference/batch_bucket.py
+colossalai/inference/config.py
+colossalai/inference/flash_decoding_utils.py
+colossalai/inference/graph_runner.py
+colossalai/inference/logit_processors.py
+colossalai/inference/sampler.py
+colossalai/inference/struct.py
+colossalai/inference/utils.py
+colossalai/inference/core/__init__.py
+colossalai/inference/core/async_engine.py
+colossalai/inference/core/engine.py
+colossalai/inference/core/plugin.py
+colossalai/inference/core/request_handler.py
+colossalai/inference/core/rpc_engine.py
 colossalai/inference/kv_cache/__init__.py
-colossalai/inference/kv_cache/batch_infer_state.py
+colossalai/inference/kv_cache/block_cache.py
 colossalai/inference/kv_cache/kvcache_manager.py
-colossalai/inference/quant/__init__.py
-colossalai/inference/quant/gptq/__init__.py
-colossalai/inference/quant/gptq/gptq_manager.py
-colossalai/inference/quant/gptq/cai_gptq/__init__.py
-colossalai/inference/quant/gptq/cai_gptq/cai_quant_linear.py
-colossalai/inference/quant/gptq/cai_gptq/gptq_op.py
-colossalai/inference/quant/smoothquant/__init__.py
-colossalai/inference/quant/smoothquant/models/__init__.py
-colossalai/inference/quant/smoothquant/models/base_model.py
-colossalai/inference/quant/smoothquant/models/linear.py
-colossalai/inference/quant/smoothquant/models/llama.py
-colossalai/inference/quant/smoothquant/models/parallel_linear.py
+colossalai/inference/modeling/__init__.py
+colossalai/inference/modeling/layers/__init__.py
+colossalai/inference/modeling/layers/attention.py
+colossalai/inference/modeling/layers/baichuan_tp_linear.py
+colossalai/inference/modeling/models/__init__.py
+colossalai/inference/modeling/models/glide_llama.py
+colossalai/inference/modeling/models/nopadding_baichuan.py
+colossalai/inference/modeling/models/nopadding_llama.py
+colossalai/inference/modeling/policy/__init__.py
+colossalai/inference/modeling/policy/glide_llama.py
+colossalai/inference/modeling/policy/nopadding_baichuan.py
+colossalai/inference/modeling/policy/nopadding_llama.py
+colossalai/inference/server/__init__.py
+colossalai/inference/server/api_server.py
+colossalai/inference/server/chat_service.py
+colossalai/inference/server/completion_service.py
+colossalai/inference/server/utils.py
+colossalai/inference/spec/__init__.py
+colossalai/inference/spec/drafter.py
+colossalai/inference/spec/struct.py
 colossalai/interface/__init__.py
 colossalai/interface/model.py
 colossalai/interface/optimizer.py
 colossalai/interface/pretrained.py
 colossalai/kernel/__init__.py
 colossalai/kernel/kernel_loader.py
 colossalai/kernel/extensions/__init__.py
 colossalai/kernel/extensions/base_extension.py
 colossalai/kernel/extensions/cpp_extension.py
 colossalai/kernel/extensions/cuda_extension.py
 colossalai/kernel/extensions/triton_extension.py
 colossalai/kernel/extensions/utils.py
-colossalai/kernel/extensions/cpu_adam/__init__.py
-colossalai/kernel/extensions/cpu_adam/cpu_adam_arm.py
-colossalai/kernel/extensions/cpu_adam/cpu_adam_x86.py
 colossalai/kernel/extensions/csrc/__init__.py
-colossalai/kernel/extensions/csrc/scaled_softmax.py
-colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.cpp
-colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.h
-colossalai/kernel/extensions/csrc/cuda/colossal_C_frontend.cpp
-colossalai/kernel/extensions/csrc/cuda/compat.h
-colossalai/kernel/extensions/csrc/cuda/cpu_adam.cpp
-colossalai/kernel/extensions/csrc/cuda/cpu_adam.h
-colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda.cpp
-colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda_kernel.cu
-colossalai/kernel/extensions/csrc/cuda/moe_cuda.cpp
-colossalai/kernel/extensions/csrc/cuda/moe_cuda_kernel.cu
-colossalai/kernel/extensions/csrc/cuda/multi_tensor_adam.cu
-colossalai/kernel/extensions/csrc/cuda/multi_tensor_apply.cuh
-colossalai/kernel/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
-colossalai/kernel/extensions/csrc/cuda/multi_tensor_lamb.cu
-colossalai/kernel/extensions/csrc/cuda/multi_tensor_scale_kernel.cu
-colossalai/kernel/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
-colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.cpp
-colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.h
-colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
-colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
-colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
-colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
-colossalai/kernel/extensions/csrc/cuda/type_shim.h
-colossalai/kernel/extensions/csrc/cuda/include/block_reduce.h
-colossalai/kernel/extensions/flash_attention/__init__.py
-colossalai/kernel/extensions/flash_attention/flash_attention_dao_cuda.py
-colossalai/kernel/extensions/flash_attention/flash_attention_npu.py
-colossalai/kernel/extensions/flash_attention/flash_attention_sdpa_cuda.py
-colossalai/kernel/extensions/layernorm/__init__.py
-colossalai/kernel/extensions/layernorm/layernorm_cuda.py
-colossalai/kernel/extensions/moe/__init__.py
-colossalai/kernel/extensions/moe/moe_cuda.py
-colossalai/kernel/extensions/optimizer/__init__.py
-colossalai/kernel/extensions/optimizer/fused_optimizer_cuda.py
-colossalai/kernel/extensions/softmax/__init__.py
-colossalai/kernel/extensions/softmax/scaled_masked_softmax_cuda.py
-colossalai/kernel/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
+colossalai/kernel/extensions/csrc/common/data_type.h
+colossalai/kernel/extensions/csrc/common/micros.h
+colossalai/kernel/extensions/csrc/common/mp_type_traits.h
+colossalai/kernel/extensions/csrc/common/target.h
+colossalai/kernel/extensions/csrc/common/vec_type_traits.h
+colossalai/kernel/extensions/csrc/funcs/binary_functor.h
+colossalai/kernel/extensions/csrc/funcs/cast_functor.h
+colossalai/kernel/extensions/csrc/funcs/reduce_function.h
+colossalai/kernel/extensions/csrc/funcs/ternary_functor.h
+colossalai/kernel/extensions/csrc/funcs/unary_functor.h
+colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.cpp
+colossalai/kernel/extensions/csrc/kernel/arm/cpu_adam_arm.h
+colossalai/kernel/extensions/csrc/kernel/cuda/activation_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/convert_fp8_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/layer_norm_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/moe_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_apply.cuh
+colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/rms_layernorm_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu
+colossalai/kernel/extensions/csrc/kernel/cuda/attention/attention_utils.h
+colossalai/kernel/extensions/csrc/kernel/cuda/utils/gpu_launch_config.h
+colossalai/kernel/extensions/csrc/kernel/cuda/utils/micros.h
+colossalai/kernel/extensions/csrc/kernel/cuda/utils/nvgpu_dev_info.h
+colossalai/kernel/extensions/csrc/kernel/cuda/utils/vec_copy.h
+colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.cpp
+colossalai/kernel/extensions/csrc/kernel/x86/cpu_adam.h
+colossalai/kernel/extensions/pybind/__init__.py
+colossalai/kernel/extensions/pybind/cpu_adam/__init__.py
+colossalai/kernel/extensions/pybind/cpu_adam/cpu_adam_arm.py
+colossalai/kernel/extensions/pybind/cpu_adam/cpu_adam_x86.py
+colossalai/kernel/extensions/pybind/flash_attention/__init__.py
+colossalai/kernel/extensions/pybind/flash_attention/flash_attention_dao_cuda.py
+colossalai/kernel/extensions/pybind/flash_attention/flash_attention_npu.py
+colossalai/kernel/extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py
+colossalai/kernel/extensions/pybind/inference/__init__.py
+colossalai/kernel/extensions/pybind/inference/inference.cpp
+colossalai/kernel/extensions/pybind/inference/inference_ops_cuda.py
+colossalai/kernel/extensions/pybind/layernorm/__init__.py
+colossalai/kernel/extensions/pybind/layernorm/layer_norm.cpp
+colossalai/kernel/extensions/pybind/layernorm/layernorm_cuda.py
+colossalai/kernel/extensions/pybind/moe/__init__.py
+colossalai/kernel/extensions/pybind/moe/moe.cpp
+colossalai/kernel/extensions/pybind/moe/moe_cuda.py
+colossalai/kernel/extensions/pybind/optimizer/__init__.py
+colossalai/kernel/extensions/pybind/optimizer/fused_optimizer_cuda.py
+colossalai/kernel/extensions/pybind/optimizer/optimizer.cpp
+colossalai/kernel/extensions/pybind/softmax/__init__.py
+colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax.cpp
+colossalai/kernel/extensions/pybind/softmax/scaled_masked_softmax_cuda.py
+colossalai/kernel/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.cpp
+colossalai/kernel/extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py
 colossalai/kernel/jit/__init__.py
 colossalai/kernel/jit/bias_dropout_add.py
 colossalai/kernel/jit/bias_gelu.py
 colossalai/kernel/jit/option.py
 colossalai/kernel/triton/__init__.py
-colossalai/kernel/triton/context_attention.py
-colossalai/kernel/triton/copy_kv_cache_dest.py
-colossalai/kernel/triton/custom_autotune.py
+colossalai/kernel/triton/context_attn_unpad.py
 colossalai/kernel/triton/flash_decoding.py
-colossalai/kernel/triton/fused_layernorm.py
-colossalai/kernel/triton/gptq_triton.py
-colossalai/kernel/triton/int8_rotary_embedding_kernel.py
+colossalai/kernel/triton/fused_rotary_embedding.py
+colossalai/kernel/triton/kvcache_copy.py
 colossalai/kernel/triton/llama_act_combine_kernel.py
+colossalai/kernel/triton/no_pad_rotary_embedding.py
 colossalai/kernel/triton/qkv_matmul_kernel.py
-colossalai/kernel/triton/self_attention_nofusion.py
-colossalai/kernel/triton/smooth_attention.py
+colossalai/kernel/triton/rms_layernorm.py
+colossalai/kernel/triton/rotary_cache_copy.py
 colossalai/kernel/triton/softmax.py
-colossalai/kernel/triton/token_attention_kernel.py
 colossalai/lazy/__init__.py
 colossalai/lazy/construction.py
 colossalai/lazy/lazy_init.py
 colossalai/lazy/pretrained.py
 colossalai/legacy/__init__.py
 colossalai/legacy/constants.py
 colossalai/legacy/core.py
@@ -643,18 +671,25 @@
 colossalai/nn/lr_scheduler/delayed.py
 colossalai/nn/lr_scheduler/linear.py
 colossalai/nn/lr_scheduler/multistep.py
 colossalai/nn/lr_scheduler/onecycle.py
 colossalai/nn/lr_scheduler/poly.py
 colossalai/nn/lr_scheduler/torch.py
 colossalai/nn/optimizer/__init__.py
+colossalai/nn/optimizer/adafactor.py
+colossalai/nn/optimizer/came.py
 colossalai/nn/optimizer/cpu_adam.py
+colossalai/nn/optimizer/distributed_adafactor.py
+colossalai/nn/optimizer/distributed_came.py
+colossalai/nn/optimizer/distributed_galore.py
+colossalai/nn/optimizer/distributed_lamb.py
 colossalai/nn/optimizer/fused_adam.py
 colossalai/nn/optimizer/fused_lamb.py
 colossalai/nn/optimizer/fused_sgd.py
+colossalai/nn/optimizer/galore.py
 colossalai/nn/optimizer/hybrid_adam.py
 colossalai/nn/optimizer/lamb.py
 colossalai/nn/optimizer/lars.py
 colossalai/nn/optimizer/nvme_optimizer.py
 colossalai/pipeline/__init__.py
 colossalai/pipeline/p2p.py
 colossalai/pipeline/stage_manager.py
@@ -688,14 +723,15 @@
 colossalai/shardformer/modeling/falcon.py
 colossalai/shardformer/modeling/gpt2.py
 colossalai/shardformer/modeling/gptj.py
 colossalai/shardformer/modeling/jit.py
 colossalai/shardformer/modeling/llama.py
 colossalai/shardformer/modeling/mistral.py
 colossalai/shardformer/modeling/opt.py
+colossalai/shardformer/modeling/qwen2.py
 colossalai/shardformer/modeling/sam.py
 colossalai/shardformer/modeling/t5.py
 colossalai/shardformer/modeling/vit.py
 colossalai/shardformer/modeling/whisper.py
 colossalai/shardformer/modeling/chatglm2_6b/__init__.py
 colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py
 colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py
@@ -708,14 +744,15 @@
 colossalai/shardformer/policies/chatglm2.py
 colossalai/shardformer/policies/falcon.py
 colossalai/shardformer/policies/gpt2.py
 colossalai/shardformer/policies/gptj.py
 colossalai/shardformer/policies/llama.py
 colossalai/shardformer/policies/mistral.py
 colossalai/shardformer/policies/opt.py
+colossalai/shardformer/policies/qwen2.py
 colossalai/shardformer/policies/sam.py
 colossalai/shardformer/policies/t5.py
 colossalai/shardformer/policies/vit.py
 colossalai/shardformer/policies/whisper.py
 colossalai/shardformer/shard/__init__.py
 colossalai/shardformer/shard/grad_ckpt_config.py
 colossalai/shardformer/shard/shard_config.py
@@ -804,68 +841,89 @@
 examples/language/performance_evaluator.py
 extensions/__init__.py
 extensions/base_extension.py
 extensions/cpp_extension.py
 extensions/cuda_extension.py
 extensions/triton_extension.py
 extensions/utils.py
-extensions/cpu_adam/__init__.py
-extensions/cpu_adam/cpu_adam_arm.py
-extensions/cpu_adam/cpu_adam_x86.py
 extensions/csrc/__init__.py
-extensions/csrc/scaled_softmax.py
-extensions/csrc/arm/cpu_adam_arm.cpp
-extensions/csrc/arm/cpu_adam_arm.h
-extensions/csrc/cuda/colossal_C_frontend.cpp
-extensions/csrc/cuda/compat.h
-extensions/csrc/cuda/cpu_adam.cpp
-extensions/csrc/cuda/cpu_adam.h
-extensions/csrc/cuda/layer_norm_cuda.cpp
-extensions/csrc/cuda/layer_norm_cuda_kernel.cu
-extensions/csrc/cuda/moe_cuda.cpp
-extensions/csrc/cuda/moe_cuda_kernel.cu
-extensions/csrc/cuda/multi_tensor_adam.cu
-extensions/csrc/cuda/multi_tensor_apply.cuh
-extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
-extensions/csrc/cuda/multi_tensor_lamb.cu
-extensions/csrc/cuda/multi_tensor_scale_kernel.cu
-extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
-extensions/csrc/cuda/scaled_masked_softmax.cpp
-extensions/csrc/cuda/scaled_masked_softmax.h
-extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
-extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
-extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
-extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
-extensions/csrc/cuda/type_shim.h
-extensions/csrc/cuda/include/block_reduce.h
-extensions/flash_attention/__init__.py
-extensions/flash_attention/flash_attention_dao_cuda.py
-extensions/flash_attention/flash_attention_npu.py
-extensions/flash_attention/flash_attention_sdpa_cuda.py
-extensions/layernorm/__init__.py
-extensions/layernorm/layernorm_cuda.py
-extensions/moe/__init__.py
-extensions/moe/moe_cuda.py
-extensions/optimizer/__init__.py
-extensions/optimizer/fused_optimizer_cuda.py
-extensions/softmax/__init__.py
-extensions/softmax/scaled_masked_softmax_cuda.py
-extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
-requirements/requirements-infer.txt
+extensions/csrc/common/data_type.h
+extensions/csrc/common/micros.h
+extensions/csrc/common/mp_type_traits.h
+extensions/csrc/common/target.h
+extensions/csrc/common/vec_type_traits.h
+extensions/csrc/funcs/binary_functor.h
+extensions/csrc/funcs/cast_functor.h
+extensions/csrc/funcs/reduce_function.h
+extensions/csrc/funcs/ternary_functor.h
+extensions/csrc/funcs/unary_functor.h
+extensions/csrc/kernel/arm/cpu_adam_arm.cpp
+extensions/csrc/kernel/arm/cpu_adam_arm.h
+extensions/csrc/kernel/cuda/activation_kernel.cu
+extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.cu
+extensions/csrc/kernel/cuda/convert_fp8_kernel.cu
+extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.cu
+extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.cu
+extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.cu
+extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.cu
+extensions/csrc/kernel/cuda/layer_norm_kernel.cu
+extensions/csrc/kernel/cuda/moe_kernel.cu
+extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu
+extensions/csrc/kernel/cuda/multi_tensor_apply.cuh
+extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu
+extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu
+extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu
+extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu
+extensions/csrc/kernel/cuda/rms_layernorm_kernel.cu
+extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu
+extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu
+extensions/csrc/kernel/cuda/attention/attention_utils.h
+extensions/csrc/kernel/cuda/utils/gpu_launch_config.h
+extensions/csrc/kernel/cuda/utils/micros.h
+extensions/csrc/kernel/cuda/utils/nvgpu_dev_info.h
+extensions/csrc/kernel/cuda/utils/vec_copy.h
+extensions/csrc/kernel/x86/cpu_adam.cpp
+extensions/csrc/kernel/x86/cpu_adam.h
+extensions/pybind/__init__.py
+extensions/pybind/cpu_adam/__init__.py
+extensions/pybind/cpu_adam/cpu_adam_arm.py
+extensions/pybind/cpu_adam/cpu_adam_x86.py
+extensions/pybind/flash_attention/__init__.py
+extensions/pybind/flash_attention/flash_attention_dao_cuda.py
+extensions/pybind/flash_attention/flash_attention_npu.py
+extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py
+extensions/pybind/inference/__init__.py
+extensions/pybind/inference/inference.cpp
+extensions/pybind/inference/inference_ops_cuda.py
+extensions/pybind/layernorm/__init__.py
+extensions/pybind/layernorm/layer_norm.cpp
+extensions/pybind/layernorm/layernorm_cuda.py
+extensions/pybind/moe/__init__.py
+extensions/pybind/moe/moe.cpp
+extensions/pybind/moe/moe_cuda.py
+extensions/pybind/optimizer/__init__.py
+extensions/pybind/optimizer/fused_optimizer_cuda.py
+extensions/pybind/optimizer/optimizer.cpp
+extensions/pybind/softmax/__init__.py
+extensions/pybind/softmax/scaled_masked_softmax.cpp
+extensions/pybind/softmax/scaled_masked_softmax_cuda.py
+extensions/pybind/softmax/scaled_upper_triang_masked_softmax.cpp
+extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py
 requirements/requirements-test.txt
 requirements/requirements.txt
 tests/kit/__init__.py
 tests/kit/model_zoo/__init__.py
 tests/kit/model_zoo/executor.py
 tests/kit/model_zoo/registry.py
 tests/kit/model_zoo/custom/__init__.py
 tests/kit/model_zoo/custom/base.py
 tests/kit/model_zoo/custom/hanging_param_model.py
 tests/kit/model_zoo/custom/nested_model.py
 tests/kit/model_zoo/custom/repeated_computed_layers.py
+tests/kit/model_zoo/custom/simple_mlp.py
 tests/kit/model_zoo/custom/simple_net.py
 tests/kit/model_zoo/diffusers/__init__.py
 tests/kit/model_zoo/diffusers/diffusers.py
 tests/kit/model_zoo/timm/__init__.py
 tests/kit/model_zoo/timm/timm.py
 tests/kit/model_zoo/torchaudio/__init__.py
 tests/kit/model_zoo/torchaudio/torchaudio.py
@@ -881,14 +939,15 @@
 tests/kit/model_zoo/transformers/chatglm2.py
 tests/kit/model_zoo/transformers/falcon.py
 tests/kit/model_zoo/transformers/gpt.py
 tests/kit/model_zoo/transformers/gptj.py
 tests/kit/model_zoo/transformers/llama.py
 tests/kit/model_zoo/transformers/mistral.py
 tests/kit/model_zoo/transformers/opt.py
+tests/kit/model_zoo/transformers/qwen2.py
 tests/kit/model_zoo/transformers/sam.py
 tests/kit/model_zoo/transformers/t5.py
 tests/kit/model_zoo/transformers/vit.py
 tests/kit/model_zoo/transformers/whisper.py
 tests/test_analyzer/__init__.py
 tests/test_analyzer/test_fx/__init__.py
 tests/test_analyzer/test_fx/test_bias_addition.py
@@ -943,14 +1002,43 @@
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py
+tests/test_infer/__init__.py
+tests/test_infer/_utils.py
+tests/test_infer/test_batch_bucket.py
+tests/test_infer/test_config_and_struct.py
+tests/test_infer/test_continuous_batching.py
+tests/test_infer/test_cuda_graph.py
+tests/test_infer/test_drafter.py
+tests/test_infer/test_inference_engine.py
+tests/test_infer/test_kvcache_manager.py
+tests/test_infer/test_request_handler.py
+tests/test_infer/test_rpc_engine.py
+tests/test_infer/test_kernels/__init__.py
+tests/test_infer/test_kernels/cuda/__init__.py
+tests/test_infer/test_kernels/cuda/test_convert_fp8.py
+tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py
+tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py
+tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py
+tests/test_infer/test_kernels/cuda/test_rms_layernorm.py
+tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py
+tests/test_infer/test_kernels/cuda/test_silu_and_mul.py
+tests/test_infer/test_kernels/triton/__init__.py
+tests/test_infer/test_kernels/triton/kernel_utils.py
+tests/test_infer/test_kernels/triton/test_context_attn_unpad.py
+tests/test_infer/test_kernels/triton/test_decoding_attn.py
+tests/test_infer/test_kernels/triton/test_fused_rotary_embedding.py
+tests/test_infer/test_kernels/triton/test_kvcache_copy.py
+tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py
+tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py
+tests/test_infer/test_kernels/triton/test_xine_copy.py
 tests/test_shardformer/__init__.py
 tests/test_shardformer/test_flash_attention.py
 tests/test_shardformer/test_shard_utils.py
 tests/test_shardformer/test_with_torch_ddp.py
 tests/test_shardformer/test_model/__init__.py
 tests/test_shardformer/test_model/_utils.py
 tests/test_shardformer/test_model/test_shard_bert.py
@@ -959,11 +1047,12 @@
 tests/test_shardformer/test_model/test_shard_chatglm2.py
 tests/test_shardformer/test_model/test_shard_falcon.py
 tests/test_shardformer/test_model/test_shard_gpt2.py
 tests/test_shardformer/test_model/test_shard_gptj.py
 tests/test_shardformer/test_model/test_shard_llama.py
 tests/test_shardformer/test_model/test_shard_mistral.py
 tests/test_shardformer/test_model/test_shard_opt.py
+tests/test_shardformer/test_model/test_shard_qwen2.py
 tests/test_shardformer/test_model/test_shard_sam.py
 tests/test_shardformer/test_model/test_shard_t5.py
 tests/test_shardformer/test_model/test_shard_vit.py
 tests/test_shardformer/test_model/test_shard_whisper.py
```

### Comparing `colossalai-nightly-2024.5.4/examples/language/data_utils.py` & `colossalai-nightly-2024.6.1/examples/language/data_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/examples/language/model_utils.py` & `colossalai-nightly-2024.6.1/examples/language/model_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/examples/language/performance_evaluator.py` & `colossalai-nightly-2024.6.1/examples/language/performance_evaluator.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 from time import time
 from typing import Optional
 
 import torch
 import torch.distributed as dist
 from torch import Tensor
+from torch.profiler import ProfilerActivity, profile, schedule, tensorboard_trace_handler
 
 from colossalai.accelerator import get_accelerator
 from colossalai.cluster import DistCoordinator
 
 
 def divide(x: float, y: float) -> float:
     if y == 0:
@@ -23,14 +24,41 @@
         return x
     tensor = torch.tensor([x], device=get_accelerator().get_current_device())
     dist.all_reduce(tensor)
     tensor = tensor / world_size
     return tensor.item()
 
 
+def get_profile_context(enable_flag, warmup_steps, active_steps, save_dir):
+    class DummyProfiler:
+        def __init__(self):
+            self.step_number = 0
+
+        def step(self):
+            self.step_number += 1
+
+        def __enter__(self):
+            return self
+
+        def __exit__(self, exc_type, exc_value, traceback):
+            pass
+
+    if enable_flag:
+        return profile(
+            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
+            schedule=schedule(wait=0, warmup=warmup_steps, active=active_steps),
+            on_trace_ready=tensorboard_trace_handler(save_dir),
+            record_shapes=True,
+            profile_memory=True,
+            with_stack=True,
+        )
+    else:
+        return DummyProfiler()
+
+
 class Timer:
     def __init__(self) -> None:
         self.start_time: Optional[float] = None
         self.duration: float = 0.0
 
     def start(self) -> None:
         self.start_time = time()
```

### Comparing `colossalai-nightly-2024.5.4/extensions/__init__.py` & `colossalai-nightly-2024.6.1/extensions/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,32 +1,39 @@
-from .cpu_adam import CpuAdamArmExtension, CpuAdamX86Extension
-from .flash_attention import FlashAttentionDaoCudaExtension, FlashAttentionNpuExtension, FlashAttentionSdpaCudaExtension
-from .layernorm import LayerNormCudaExtension
-from .moe import MoeCudaExtension
-from .optimizer import FusedOptimizerCudaExtension
-from .softmax import ScaledMaskedSoftmaxCudaExtension, ScaledUpperTriangleMaskedSoftmaxCudaExtension
+from .pybind.cpu_adam import CpuAdamArmExtension, CpuAdamX86Extension
+from .pybind.flash_attention import (
+    FlashAttentionDaoCudaExtension,
+    FlashAttentionNpuExtension,
+    FlashAttentionSdpaCudaExtension,
+)
+from .pybind.inference import InferenceOpsCudaExtension
+from .pybind.layernorm import LayerNormCudaExtension
+from .pybind.moe import MoeCudaExtension
+from .pybind.optimizer import FusedOptimizerCudaExtension
+from .pybind.softmax import ScaledMaskedSoftmaxCudaExtension, ScaledUpperTriangleMaskedSoftmaxCudaExtension
 
 ALL_EXTENSIONS = [
     CpuAdamArmExtension,
     CpuAdamX86Extension,
     LayerNormCudaExtension,
     MoeCudaExtension,
     FusedOptimizerCudaExtension,
+    InferenceOpsCudaExtension,
     ScaledMaskedSoftmaxCudaExtension,
     ScaledUpperTriangleMaskedSoftmaxCudaExtension,
     FlashAttentionDaoCudaExtension,
     FlashAttentionSdpaCudaExtension,
     FlashAttentionNpuExtension,
 ]
 
 __all__ = [
     "CpuAdamArmExtension",
     "CpuAdamX86Extension",
     "LayerNormCudaExtension",
     "MoeCudaExtension",
     "FusedOptimizerCudaExtension",
+    "InferenceOpsCudaExtension",
     "ScaledMaskedSoftmaxCudaExtension",
     "ScaledUpperTriangleMaskedSoftmaxCudaExtension",
     "FlashAttentionDaoCudaExtension",
     "FlashAttentionSdpaCudaExtension",
     "FlashAttentionNpuExtension",
 ]
```

### Comparing `colossalai-nightly-2024.5.4/extensions/base_extension.py` & `colossalai-nightly-2024.6.1/extensions/base_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/extensions/cpp_extension.py` & `colossalai-nightly-2024.6.1/extensions/cpp_extension.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,14 +21,17 @@
         self.prebuilt_module_path = "colossalai._C"
         self.prebuilt_import_path = f"{self.prebuilt_module_path}.{self.name}"
         self.version_dependent_macros = ["-DVERSION_GE_1_1", "-DVERSION_GE_1_3", "-DVERSION_GE_1_5"]
 
     def csrc_abs_path(self, path):
         return os.path.join(self.relative_to_abs_path("csrc"), path)
 
+    def pybind_abs_path(self, path):
+        return os.path.join(self.relative_to_abs_path("pybind"), path)
+
     def relative_to_abs_path(self, code_path: str) -> str:
         """
         This function takes in a path relative to the colossalai root directory and return the absolute path.
         """
 
         # get the current file path
         # iteratively check the parent directory
@@ -112,14 +115,15 @@
         """
 
     @abstractmethod
     def include_dirs(self) -> List[str]:
         """
         This function should return a list of include files for extensions.
         """
+        return [self.csrc_abs_path("")]
 
     @abstractmethod
     def cxx_flags(self) -> List[str]:
         """
         This function should return a list of cxx compilation flags for extensions.
         """
```

### Comparing `colossalai-nightly-2024.5.4/extensions/cpu_adam/cpu_adam_x86.py` & `colossalai-nightly-2024.6.1/extensions/pybind/cpu_adam/cpu_adam_x86.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import platform
 
-from ..cuda_extension import _CudaExtension
-from ..utils import append_nvcc_threads
+from ...cuda_extension import _CudaExtension
+from ...utils import append_nvcc_threads
 
 
 class CpuAdamX86Extension(_CudaExtension):
     def __init__(self):
         super().__init__(name="cpu_adam_x86")
 
     def is_available(self) -> bool:
@@ -17,21 +17,18 @@
             arch == "x86_64"
         ), f"[extension] The {self.name} kernel requires the CPU architecture to be x86_64 but got {arch}"
         super().assert_compatible()
 
     # necessary 4 functions
     def sources_files(self):
         ret = [
-            self.csrc_abs_path("cuda/cpu_adam.cpp"),
+            self.csrc_abs_path("kernel/x86/cpu_adam.cpp"),
         ]
         return ret
 
-    def include_dirs(self):
-        return [self.csrc_abs_path("includes"), self.get_cuda_home_include()]
-
     def cxx_flags(self):
         extra_cxx_flags = [
             "-std=c++14",
             "-std=c++17",
             "-lcudart",
             "-lcublas",
             "-g",
@@ -46,9 +43,9 @@
             "-std=c++14",
             "-std=c++17",
             "-U__CUDA_NO_HALF_OPERATORS__",
             "-U__CUDA_NO_HALF_CONVERSIONS__",
             "-U__CUDA_NO_HALF2_OPERATORS__",
             "-DTHRUST_IGNORE_CUB_VERSION_CHECK",
         ]
-        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags
+        ret = ["-O3", "--use_fast_math"] + self.version_dependent_macros + extra_cuda_flags + super().nvcc_flags()
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.cpp` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/arm/cpu_adam_arm.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.h` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/arm/cpu_adam_arm.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/colossal_C_frontend.cpp` & `colossalai-nightly-2024.6.1/extensions/pybind/optimizer/optimizer.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.cpp` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/x86/cpu_adam.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.h` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/x86/cpu_adam.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda.cpp` & `colossalai-nightly-2024.6.1/extensions/pybind/layernorm/layer_norm.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
  *     with minor changes. */
 
 #include <torch/extension.h>
 
 #include <cassert>
 #include <vector>
 
-#include "compat.h"
+#include "common/micros.h"
 
 namespace {
 
 void compute_n1_n2(at::Tensor input, at::IntArrayRef normalized_shape, int &n1,
                    int &n2) {
   int idiff = input.ndimension() - normalized_shape.size();
   n2 = 1;
```

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda_kernel.cu` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/layer_norm_kernel.cu`

 * *Files 6% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 #include <cuda.h>
 #include <cuda_runtime.h>
 
 #include "ATen/ATen.h"
 #include "ATen/AccumulateType.h"
 #include "ATen/cuda/CUDAContext.h"
 #include "ATen/cuda/DeviceUtils.cuh"
-#include "type_shim.h"
+#include "common/micros.h"
 
 template <typename U>
 __device__ void cuWelfordOnlineSum(const U curr, U& mu, U& sigma2, U& count) {
   count = count + U(1);
   U delta = curr - mu;
   U lmean = mu + delta / count;
   mu = lmean;
@@ -602,19 +602,19 @@
 #else
                      at::IntList normalized_shape,
 #endif
                      at::Tensor* gamma, at::Tensor* beta, double epsilon) {
   using namespace at;
   DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
       input->scalar_type(), output->scalar_type(), "cuda_layer_norm_kernel",
-      HostApplyLayerNorm(output->DATA_PTR<scalar_t_out>(),
-                         mean->DATA_PTR<float>(), invvar->DATA_PTR<float>(),
-                         input->DATA_PTR<scalar_t_in>(), n1, n2, epsilon,
-                         gamma != NULL ? gamma->DATA_PTR<scalar_t_out>() : NULL,
-                         beta != NULL ? beta->DATA_PTR<scalar_t_out>() : NULL);)
+      HostApplyLayerNorm(output->data_ptr<scalar_t_out>(),
+                         mean->data_ptr<float>(), invvar->data_ptr<float>(),
+                         input->data_ptr<scalar_t_in>(), n1, n2, epsilon,
+                         gamma != NULL ? gamma->data_ptr<scalar_t_out>() : NULL,
+                         beta != NULL ? beta->data_ptr<scalar_t_out>() : NULL);)
 }
 
 template <typename T, typename U, typename V>
 void HostLayerNormGradient(const V* dout, const U* mean, const U* invvar,
                            at::Tensor* input, int n1, int n2, const V* gamma,
                            const V* beta, double epsilon, T* grad_input,
                            V* grad_gamma, V* grad_beta) {
@@ -629,33 +629,33 @@
         2 * sizeof(U) * threads2.y * threads2.y * (threads2.x + 1);
     const int nshared2_b = threads2.x * threads2.y * sizeof(U);
     const int nshared2 = nshared2_a > nshared2_b ? nshared2_a : nshared2_b;
     at::Tensor part_grad_gamma = at::empty(
         {part_size, n2}, input->options().dtype(at::ScalarType::Float));
     at::Tensor part_grad_beta = at::empty_like(part_grad_gamma);
     cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
-        dout, input->DATA_PTR<T>(), n1, n2, mean, invvar, U(epsilon),
-        part_grad_gamma.DATA_PTR<U>(), part_grad_beta.DATA_PTR<U>());
+        dout, input->data_ptr<T>(), n1, n2, mean, invvar, U(epsilon),
+        part_grad_gamma.data_ptr<U>(), part_grad_beta.data_ptr<U>());
 
     const dim3 threads3(32, 8, 1);
     const dim3 blocks3((n2 + threads2.x - 1) / threads2.x, 1, 1);
     const int nshared3 = threads3.x * threads3.y * sizeof(U);
     cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
-        part_grad_gamma.DATA_PTR<U>(), part_grad_beta.DATA_PTR<U>(), part_size,
+        part_grad_gamma.data_ptr<U>(), part_grad_beta.data_ptr<U>(), part_size,
         n1, n2, grad_gamma, grad_beta);
   }
 
   // compute grad_input
   const uint64_t maxGridY =
       at::cuda::getCurrentDeviceProperties()->maxGridSize[1];
   const dim3 blocks1(1, std::min((uint64_t)n1, maxGridY), 1);
   const dim3 threads1(32, 4, 1);
   int nshared = threads1.y > 1 ? threads1.y * threads1.x * sizeof(U) : 0;
   cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
-      dout, input->DATA_PTR<T>(), n1, n2, mean, invvar, U(epsilon), gamma,
+      dout, input->data_ptr<T>(), n1, n2, mean, invvar, U(epsilon), gamma,
       grad_input);
 }
 
 void cuda_layer_norm_gradient(at::Tensor* dout, at::Tensor* mean,
                               at::Tensor* invvar, at::Tensor* input, int n1,
                               int n2,
 #ifdef VERSION_GE_1_1
@@ -667,17 +667,17 @@
                               double epsilon, at::Tensor* grad_input,
                               at::Tensor* grad_gamma, at::Tensor* grad_beta) {
   using namespace at;
   DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
       input->scalar_type(), gamma->scalar_type(),
       "cuda_layer_norm_gradient_kernel",
       HostLayerNormGradient(
-          dout->DATA_PTR<scalar_t_out>(), mean->DATA_PTR<float>(),
-          invvar->DATA_PTR<float>(), input, n1, n2,
+          dout->data_ptr<scalar_t_out>(), mean->data_ptr<float>(),
+          invvar->data_ptr<float>(), input, n1, n2,
           // TMJ pass NULL argument for gamma, beta, grad_gamma and grad_beta
           // if gamma Tensor is NULL on input.
-          gamma != NULL ? gamma->DATA_PTR<scalar_t_out>() : NULL,
-          gamma != NULL ? beta->DATA_PTR<scalar_t_out>() : NULL, epsilon,
-          grad_input->DATA_PTR<scalar_t_in>(),
-          gamma != NULL ? grad_gamma->DATA_PTR<scalar_t_out>() : NULL,
-          gamma != NULL ? grad_beta->DATA_PTR<scalar_t_out>() : NULL);)
+          gamma != NULL ? gamma->data_ptr<scalar_t_out>() : NULL,
+          gamma != NULL ? beta->data_ptr<scalar_t_out>() : NULL, epsilon,
+          grad_input->data_ptr<scalar_t_in>(),
+          gamma != NULL ? grad_gamma->data_ptr<scalar_t_out>() : NULL,
+          gamma != NULL ? grad_beta->data_ptr<scalar_t_out>() : NULL);)
 }
```

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda.cpp` & `colossalai-nightly-2024.6.1/extensions/pybind/moe/moe.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda_kernel.cu` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/moe_kernel.cu`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,17 @@
 #include <cuda.h>
 #include <cuda_fp16.h>
 #include <torch/extension.h>
 
 #include <cub/cub.cuh>
 
-#include "block_reduce.h"
+#include "funcs/reduce_function.h"
+
+using colossalAI::funcs::block_reduce;
+using colossalAI::funcs::ReduceType;
 
 template <typename T, int block_size, int pack_size>
 __device__ void moe_dpch_one_fwd(T *src_row, T *dst_row, const int cols) {
   assert(cols % pack_size == 0);
   const int bpack_size = block_size * pack_size;
 
   typedef cub::BlockLoad<T, block_size, pack_size, cub::BLOCK_LOAD_VECTORIZE>
@@ -153,16 +156,15 @@
     for (int i = 0; i < pack_size; ++i) {
       thread_sum += grad[i] * tokens[i];
       grad[i] *= weight;
     }
 
     BlockStore(ts_store).Store(src_row + idx, grad);
   }
-
-  blockReduce<ReduceType::kSum, 1>(&thread_sum);
+  block_reduce<float, ReduceType::kSum, 1>(&thread_sum);
 
   if (threadIdx.x == 0) *weight_grad = static_cast<T>(thread_sum);
 }
 
 template <typename T, int block_size, int pack_size>
 __device__ void moe_cb_two_fwd(T *src_row1, T *src_row2, T *dst_row,
                                const T weight1, const T weight2,
@@ -226,15 +228,15 @@
       sgrad2[i] = weight2 * grad[i];
     }
 
     BlockStore(ts_store).Store(src_row1 + idx, sgrad1);
     BlockStore(ts_store).Store(src_row2 + idx, sgrad2);
   }
 
-  blockReduce<ReduceType::kSum, 2>(thread_sum);
+  block_reduce<float, ReduceType::kSum, 2>(thread_sum);
 
   if (threadIdx.x == 0)
     *weight_grad1 = static_cast<T>(thread_sum[0]);
   else if (threadIdx.x == 1)
     *weight_grad2 = static_cast<T>(thread_sum[1]);
 }
 
@@ -533,15 +535,15 @@
     cumsum_kernel<1024, 2><<<e, 1024>>>(inputs, outputs, s, e);
   else
     cumsum_kernel<1024, 4><<<e, 1024>>>(inputs, outputs, s, e);
 }
 
 // API FUNCTIONS --------------------------------
 
-#define DISPATCH_FLOAT_AND_HALF(TYPE, NAME, ...)                       \
+#define DISPATCH_FLOAT_AND_HALF_MOE(TYPE, NAME, ...)                   \
   switch (TYPE) {                                                      \
     case at::ScalarType::Float: {                                      \
       using scalar_t = float;                                          \
       __VA_ARGS__;                                                     \
       break;                                                           \
     }                                                                  \
     case at::ScalarType::Half: {                                       \
@@ -559,41 +561,41 @@
                                         torch::Tensor dest_idx) {
   assert(h % 16 == 0);
   auto res = torch::zeros(
       {ec, h},
       torch::dtype(batch_tokens.dtype()).device(batch_tokens.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF(
+  DISPATCH_FLOAT_AND_HALF_MOE(
       batch_tokens.scalar_type(), "moe dispatch forward",
       moe_dpch_fwd_launch<scalar_t>(
-          batch_tokens.data<scalar_t>(), res.data<scalar_t>(),
-          mask[0].data<int>(), k == 1 ? nullptr : mask[1].data<int>(),
-          dest_idx[0].data<int>(),
-          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, h));
+          batch_tokens.data_ptr<scalar_t>(), res.data_ptr<scalar_t>(),
+          mask[0].data_ptr<int>(), k == 1 ? nullptr : mask[1].data_ptr<int>(),
+          dest_idx[0].data_ptr<int>(),
+          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, h));
 
   return res;
 }
 
 torch::Tensor moe_dispatch_cuda_backward(int s, int ec, int h,
                                          torch::Tensor expert_grad,
                                          torch::Tensor mask,
                                          torch::Tensor dest_idx) {
   assert(h % 16 == 0);
   auto res = torch::zeros(
       {s, h}, torch::dtype(expert_grad.dtype()).device(expert_grad.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF(
+  DISPATCH_FLOAT_AND_HALF_MOE(
       expert_grad.scalar_type(), "moe dispatch backward",
       moe_dpch_bwd_launch<scalar_t>(
-          res.data<scalar_t>(), expert_grad.data<scalar_t>(),
-          mask[0].data<int>(), k == 1 ? nullptr : mask[1].data<int>(),
-          dest_idx[0].data<int>(),
-          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, h));
+          res.data_ptr<scalar_t>(), expert_grad.data_ptr<scalar_t>(),
+          mask[0].data_ptr<int>(), k == 1 ? nullptr : mask[1].data_ptr<int>(),
+          dest_idx[0].data_ptr<int>(),
+          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, h));
 
   return res;
 }
 
 torch::Tensor moe_combine_cuda_forward(int s, int e, int c, int h,
                                        torch::Tensor expert_tokens,
                                        torch::Tensor logits, torch::Tensor mask,
@@ -602,21 +604,21 @@
   assert(expert_tokens.dtype() == logits.dtype());
 
   auto res = torch::zeros(
       {s, h},
       torch::dtype(expert_tokens.dtype()).device(expert_tokens.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF(
+  DISPATCH_FLOAT_AND_HALF_MOE(
       expert_tokens.scalar_type(), "moe combine forward",
       moe_cb_fwd_launch<scalar_t>(
-          expert_tokens.data<scalar_t>(), res.data<scalar_t>(),
-          logits.data<scalar_t>(), mask[0].data<int>(),
-          k == 1 ? nullptr : mask[1].data<int>(), dest_idx[0].data<int>(),
-          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, e, c,
+          expert_tokens.data_ptr<scalar_t>(), res.data_ptr<scalar_t>(),
+          logits.data_ptr<scalar_t>(), mask[0].data_ptr<int>(),
+          k == 1 ? nullptr : mask[1].data_ptr<int>(), dest_idx[0].data_ptr<int>(),
+          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, e, c,
           h));
 
   return res;
 }
 
 std::vector<torch::Tensor> moe_combine_cuda_backward(
     int s, int e, int c, int h, torch::Tensor tokens_grad,
@@ -629,31 +631,31 @@
   auto egrad = torch::zeros(
            {e * c, h},
            torch::dtype(tokens_grad.dtype()).device(tokens_grad.device())),
        wgrad = torch::zeros(
            {s, e}, torch::dtype(logits.dtype()).device(logits.device()));
   auto k = mask.size(0);
 
-  DISPATCH_FLOAT_AND_HALF(
+  DISPATCH_FLOAT_AND_HALF_MOE(
       tokens_grad.scalar_type(), "moe combine backward",
       moe_cb_bwd_launch<scalar_t>(
-          tokens_grad.data<scalar_t>(), egrad.data<scalar_t>(),
-          expert_tokens.data<scalar_t>(), logits.data<scalar_t>(),
-          wgrad.data<scalar_t>(), mask[0].data<int>(),
-          k == 1 ? nullptr : mask[1].data<int>(), dest_idx[0].data<int>(),
-          k == 1 ? dest_idx[0].data<int>() : dest_idx[1].data<int>(), s, e, c,
+          tokens_grad.data_ptr<scalar_t>(), egrad.data_ptr<scalar_t>(),
+          expert_tokens.data_ptr<scalar_t>(), logits.data_ptr<scalar_t>(),
+          wgrad.data_ptr<scalar_t>(), mask[0].data_ptr<int>(),
+          k == 1 ? nullptr : mask[1].data_ptr<int>(), dest_idx[0].data_ptr<int>(),
+          k == 1 ? dest_idx[0].data_ptr<int>() : dest_idx[1].data_ptr<int>(), s, e, c,
           h));
 
   return {egrad, wgrad};
 }
 
 torch::Tensor cumsum_sub_one_in_dim0(torch::Tensor mask) {
   assert(mask.dim() == 2);
   assert(mask.dtype() == torch::kInt32);
 
   const int s = mask.size(0), e = mask.size(1);
   auto res =
       torch::empty({s, e}, torch::dtype(torch::kInt32).device(mask.device()));
-  cumsum_launch(mask.data<int>(), res.data<int>(), s, e);
+  cumsum_launch(mask.data_ptr<int>(), res.data_ptr<int>(), s, e);
 
   return res;
 }
```

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_adam.cu` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 #include <ATen/cuda/Exceptions.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "type_shim.h"
+#include "common/micros.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 typedef enum {
   ADAM_MODE_0 = 0,  // L2 regularization mode
   ADAM_MODE_1 = 1   // Decoupled weight decay mode(AdamW)
```

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_apply.cuh` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_apply.cuh`

 * *Files 1% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 #include <assert.h>
 #include <c10/cuda/CUDAGuard.h>
 
-#include "compat.h"
+#include "common/micros.h"
 
 // #include <iostream>
 
 // This header is the one-stop shop for all your multi-tensor apply needs.
 
 // TODO:  Kernel arg size limit may be <4KB for some other cards (ie Jetson)
 constexpr int depth_to_max_tensors[5] = {110, 64, 48, 36, 30};
@@ -100,15 +100,15 @@
       bool tensors_full = (loc_tensor_info == depth_to_max_tensors[depth - 1] &&
                            chunk == chunks_this_tensor - 1);
       bool blocks_full = (loc_block_info == depth_to_max_blocks[depth - 1]);
       bool last_chunk = (t == ntensors - 1 && chunk == chunks_this_tensor - 1);
       if (tensors_full || blocks_full || last_chunk) {
         // using accscalar_t = acc_type<scalar_t, true>;
         multi_tensor_apply_kernel<<<loc_block_info, block_size, 0, stream>>>(
-            chunk_size, noop_flag.DATA_PTR<int>(), tl, callable, args...);
+            chunk_size, noop_flag.data_ptr<int>(), tl, callable, args...);
 
         AT_CUDA_CHECK(cudaGetLastError());
 
         // Reset.  The control flow possibilities here make my brain hurt.
         loc_block_info = 0;
         if (chunk == chunks_this_tensor - 1) {
           // std::cout << "Hit case 1 " << cond1 << " " << cond2 << " " << cond3
```

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu`

 * *Files 15% similar despite different names*

```diff
@@ -7,19 +7,107 @@
 #include <c10/cuda/CUDAGuard.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "type_shim.h"
+#include "common/micros.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
+
+template <typename T>
+__device__ __forceinline__ T reduce_block_into_lanes(
+    T* x, T val, int lanes = 1,
+    bool share_result = false)  // lanes is intended to be <= 32.
+{
+  int tid = threadIdx.x + threadIdx.y * blockDim.x;
+  int blockSize =
+      blockDim.x * blockDim.y;  // blockSize is intended to be a multiple of 32.
+
+  if (blockSize >= 64) {
+    x[tid] = val;
+    __syncthreads();
+  }
+
+#pragma unroll
+  for (int i = (blockSize >> 1); i >= 64; i >>= 1) {
+    if (tid < i) x[tid] = x[tid] + x[tid + i];
+    __syncthreads();
+  }
+
+  T final;
+
+  if (tid < 32) {
+    if (blockSize >= 64)
+      final = x[tid] + x[tid + 32];
+    else
+      final = val;
+      // __SYNCWARP();
+
+#pragma unroll
+    for (int i = 16; i >= lanes; i >>= 1)
+      final = final + __shfl_down_sync(0xffffffff, final, i);
+  }
+
+  if (share_result) {
+    if (tid < lanes) x[tid] = final;  // EpilogueOp
+    // Make sure the smem result is visible to all warps.
+    __syncthreads();
+  }
+
+  return final;
+}
+
+template <typename T>
+__device__ __forceinline__ T reduce_block_into_lanes_max_op(
+    T* x, T val, int lanes = 1,
+    bool share_result = false)  // lanes is intended to be <= 32.
+{
+  int tid = threadIdx.x + threadIdx.y * blockDim.x;
+  int blockSize =
+      blockDim.x * blockDim.y;  // blockSize is intended to be a multiple of 32.
+
+  if (blockSize >= 64) {
+    x[tid] = val;
+    __syncthreads();
+  }
+
+#pragma unroll
+  for (int i = (blockSize >> 1); i >= 64; i >>= 1) {
+    if (tid < i) x[tid] = fmaxf(fabsf(x[tid]), fabsf(x[tid + i]));
+    __syncthreads();
+  }
+
+  T final;
+
+  if (tid < 32) {
+    if (blockSize >= 64)
+      final = fmaxf(fabsf(x[tid]), fabsf(x[tid + 32]));
+    else
+      final = val;
+      // __SYNCWARP();
+
+#pragma unroll
+    for (int i = 16; i >= lanes; i >>= 1)
+      final =
+          fmaxf(fabsf(final), fabsf(__shfl_down_sync(0xffffffff, final, i)));
+  }
+
+  if (share_result) {
+    if (tid < lanes) x[tid] = final;  // EpilogueOp
+    // Make sure the smem result is visible to all warps.
+    __syncthreads();
+  }
+
+  return final;
+}
+
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
 }
 
 template <typename T>
 __device__ __forceinline__ void load_store(T *dst, T *src, int dst_offset,
@@ -285,32 +373,32 @@
     ret_per_tensor = at::empty({0}, float_options);
   }
 
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "multi_tensor_l2norm_cuda",
       multi_tensor_apply<1>(
           BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-          L2NormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
-          per_tensor ? output_per_tensor.DATA_PTR<float>() : nullptr,
+          L2NormFunctor<scalar_t_0>(), output.data_ptr<float>(),
+          per_tensor ? output_per_tensor.data_ptr<float>() : nullptr,
           per_tensor, max_chunks_per_tensor);)
 
   AT_CUDA_CHECK(cudaGetLastError());
   // AT_CUDA_CHECK(cudaDeviceSynchronize());
 
   // This involves one more small kernel launches, but will be negligible end to
   // end. I could get rid of these by hacking the functor + multi tensor harness
   // with persistence logic, but keeping it simple for now
   auto ret = at::empty({1}, output.options());
   const at::cuda::OptionalCUDAGuard device_guard(device_of(output));
   auto stream = at::cuda::getCurrentCUDAStream();
   cleanup<<<per_tensor ? ntensors : 1, 512, 0, stream>>>(
-      output.DATA_PTR<float>(),
-      per_tensor ? output_per_tensor.DATA_PTR<float>() : nullptr,
-      ret.DATA_PTR<float>(),
-      per_tensor ? ret_per_tensor.DATA_PTR<float>() : nullptr, per_tensor,
+      output.data_ptr<float>(),
+      per_tensor ? output_per_tensor.data_ptr<float>() : nullptr,
+      ret.data_ptr<float>(),
+      per_tensor ? ret_per_tensor.data_ptr<float>() : nullptr, per_tensor,
       max_chunks_per_tensor);
 
   return std::tuple<at::Tensor, at::Tensor>(ret, ret_per_tensor);
 }
 
 // Compute and update grad norm
 // Here use a per tensor norm, and blend new norm(n) and old norm(gn) by
@@ -345,23 +433,23 @@
       at::zeros({ntensors * max_chunks_per_tensor}, float_options);
 
   if (norm_type == 0) {
     DISPATCH_FLOAT_AND_HALF(
         tensor_lists[0][0].scalar_type(), 0, "multi_tensor_maxnorm_cuda",
         multi_tensor_apply<1>(
             BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-            MaxNormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
-            output_per_tensor.DATA_PTR<float>(), true, max_chunks_per_tensor);)
+            MaxNormFunctor<scalar_t_0>(), output.data_ptr<float>(),
+            output_per_tensor.data_ptr<float>(), true, max_chunks_per_tensor);)
   } else {
     DISPATCH_FLOAT_AND_HALF(
         tensor_lists[0][0].scalar_type(), 0, "multi_tensor_l2norm_cuda",
         multi_tensor_apply<1>(
             BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
-            L2NormFunctor<scalar_t_0>(), output.DATA_PTR<float>(),
-            output_per_tensor.DATA_PTR<float>(), true, max_chunks_per_tensor);)
+            L2NormFunctor<scalar_t_0>(), output.data_ptr<float>(),
+            output_per_tensor.data_ptr<float>(), true, max_chunks_per_tensor);)
   }
   AT_CUDA_CHECK(cudaGetLastError());
 
   // AT_CUDA_CHECK(cudaDeviceSynchronize());
 
   // This involves one more small kernel launches, but will be negligible end to
   // end. I could get rid of these by hacking the functor + multi tensor harness
@@ -370,13 +458,13 @@
 
   // Adding the following device guard since it happens sometimes that the
   // tensors are on one device and the cuda stream is on another device which
   // results in ILLEGAL MEM ACCESS error.
   const at::cuda::OptionalCUDAGuard device_guard(device_of(output));
   auto stream = at::cuda::getCurrentCUDAStream();
   cleanup_v2<<<ntensors, 512, 0, stream>>>(
-      output.DATA_PTR<float>(), output_per_tensor.DATA_PTR<float>(),
-      ret.DATA_PTR<float>(), out.DATA_PTR<float>(), true, max_chunks_per_tensor,
+      output.data_ptr<float>(), output_per_tensor.data_ptr<float>(),
+      ret.data_ptr<float>(), out.data_ptr<float>(), true, max_chunks_per_tensor,
       norm_type, alpha, beta);
 
   return;
 }
```

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_lamb.cu` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 #include <ATen/cuda/Exceptions.h>
 // Another possibility:
 // #include <torch/all.h>
 
 #include <assert.h>
 
 #include "multi_tensor_apply.cuh"
-#include "type_shim.h"
+#include "common/micros.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
@@ -329,26 +329,26 @@
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "lamb_stage_1",
       multi_tensor_apply<4>(BLOCK_SIZE, chunk_size, noop_flag, tensor_lists,
                             LAMBStage1Functor<scalar_t_0>(), beta1, beta2,
                             beta3,  // 1-beta1 or 1 depends on averaging mode
                             bias_correction1, bias_correction2, epsilon,
                             (adamMode_t)mode, weight_decay,
-                            global_grad_norm.DATA_PTR<float>(), max_grad_norm);)
+                            global_grad_norm.data_ptr<float>(), max_grad_norm);)
 
   // Compute update norms
   auto update_norm_tuple =
       multi_tensor_l2norm_cuda(chunk_size, noop_flag, grad_list, true);
 
   std::vector<std::vector<at::Tensor>> grad_param_list(
       tensor_lists.begin(), tensor_lists.begin() + 2);
 
   DISPATCH_FLOAT_AND_HALF(
       tensor_lists[0][0].scalar_type(), 0, "lamb_stage_2",
       multi_tensor_apply<2>(BLOCK_SIZE, chunk_size, noop_flag, grad_param_list,
                             LAMBStage2Functor<scalar_t_0>(),
-                            std::get<1>(param_norm_tuple).DATA_PTR<float>(),
-                            std::get<1>(update_norm_tuple).DATA_PTR<float>(),
+                            std::get<1>(param_norm_tuple).data_ptr<float>(),
+                            std::get<1>(update_norm_tuple).data_ptr<float>(),
                             lr, weight_decay, use_nvlamb);)
 
   AT_CUDA_CHECK(cudaGetLastError());
 }
```

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_scale_kernel.cu` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 // #include <torch/all.h>
 
 #include <assert.h>
 // Stringstream is a big hammer, but I want to rely on operator<< for dtype.
 #include <sstream>
 
 #include "multi_tensor_apply.cuh"
-#include "type_shim.h"
+#include "common/micros.h"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 template <typename T>
 __device__ __forceinline__ bool is_aligned(T *p) {
   return ((uint64_t)p) % (ILP * sizeof(T)) == 0;
```

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 #include <ATen/ATen.h>
 #include <ATen/AccumulateType.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <ATen/cuda/Exceptions.h>
 #include <assert.h>
 #include <cuda_runtime.h>
 
-#include "compat.h"
+#include "common/micros.h"
 #include "multi_tensor_apply.cuh"
 
 #define BLOCK_SIZE 512
 #define ILP 4
 
 /**
  * Perform fused SGD on multiple buffers
```

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax.h` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu`

 * *Files 9% similar despite different names*

```diff
@@ -1,103 +1,34 @@
 /*This code from NVIDIA Megatron:
  *     with minor changes. */
 
-#pragma once
+#include <ATen/ATen.h>
+#include <ATen/cuda/CUDAContext.h>
+#include <cuda.h>
+#include <cuda_fp16.h>
+#include <cuda_profiler_api.h>
+#include <cuda_runtime.h>
+#include <torch/extension.h>
 
 #include <assert.h>
 #include <c10/macros/Macros.h>
-#include <cuda_fp16.h>
-#include <stdint.h>
-
 #include <cfloat>
 #include <limits>
 
-namespace {
-
-template <typename Datatype, int ELEMENTS_PER_LDG>
-__device__ __inline__ void copy_vector(Datatype *dst, const Datatype *src);
-
-template <>
-__device__ __inline__ void copy_vector<c10::BFloat16, 1>(
-    c10::BFloat16 *dst, const c10::BFloat16 *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<c10::BFloat16, 4>(
-    c10::BFloat16 *dst, const c10::BFloat16 *src) {
-  *((float2 *)dst) = *((float2 *)src);
-}
+#include "common/micros.h"
+#include "utils/vec_copy.h"
+#include "funcs/reduce_function.h"
+#include "funcs/unary_functor.h"
+
+using colossalAI::funcs::UnaryOpFunctor;
+using colossalAI::funcs::UnaryOpType;
+using colossalAI::funcs::warp_reduce;
+using colossalAI::funcs::ReduceType;
+using colossalAI::cuda::utils::copy;
 
-template <>
-__device__ __inline__ void copy_vector<c10::Half, 1>(c10::Half *dst,
-                                                     const c10::Half *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<c10::Half, 4>(c10::Half *dst,
-                                                     const c10::Half *src) {
-  *((float2 *)dst) = *((float2 *)src);
-}
-
-template <>
-__device__ __inline__ void copy_vector<uint8_t, 1>(uint8_t *dst,
-                                                   const uint8_t *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<uint8_t, 4>(uint8_t *dst,
-                                                   const uint8_t *src) {
-  *((half2 *)dst) = *((half2 *)src);
-}
-
-int log2_ceil(int value) {
-  int log2_value = 0;
-  while ((1 << log2_value) < value) ++log2_value;
-  return log2_value;
-}
-
-template <typename T>
-struct Add {
-  __device__ __forceinline__ T operator()(T a, T b) const { return a + b; }
-};
-
-template <typename T>
-struct Max {
-  __device__ __forceinline__ T operator()(T a, T b) const {
-    return a < b ? b : a;
-  }
-};
-
-template <typename T>
-__device__ __forceinline__ T
-WARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,
-                     unsigned int mask = 0xffffffff) {
-#if CUDA_VERSION >= 9000
-  return __shfl_xor_sync(mask, value, laneMask, width);
-#else
-  return __shfl_xor(value, laneMask, width);
-#endif
-}
-
-template <typename acc_t, int WARP_BATCH, int WARP_SIZE,
-          template <typename> class ReduceOp>
-__device__ __forceinline__ void warp_reduce(acc_t *sum) {
-  ReduceOp<acc_t> r;
-#pragma unroll
-  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
-#pragma unroll
-    for (int i = 0; i < WARP_BATCH; ++i) {
-      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);
-      sum[i] = r(sum[i], b);
-    }
-  }
-}
 
 /*
  * Extended softmax (from native aten pytorch) with following additional
  * features 1) input scaling 2) Explicit masking
  */
 template <typename input_t, typename output_t, typename acc_t,
           int log2_elements>
@@ -152,16 +83,16 @@
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
 
       if (element_index < batch_element_count) {
         int itr_idx = i * element_count + it * WARP_SIZE;
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(temp_data, src + itr_idx);
-        copy_vector<uint8_t, ELEMENTS_PER_LDG_STG>(temp_mask, mask + itr_idx);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(src + itr_idx, temp_data);
+        copy<uint8_t, ELEMENTS_PER_LDG_STG>(mask + itr_idx, temp_mask);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (temp_mask[element] != 1) {
             elements[i][it + element] = (acc_t)temp_data[element] * scale;
           } else {
             elements[i][it + element] = -10000.0;
@@ -183,42 +114,42 @@
     max_value[i] = elements[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       max_value[i] =
           (max_value[i] > elements[i][it]) ? max_value[i] : elements[i][it];
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Max>(max_value);
+  warp_reduce<acc_t,ReduceType::kMax,WARP_BATCH,WARP_SIZE>(max_value);
 
   acc_t sum[WARP_BATCH]{0.0f};
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; ++it) {
       elements[i][it] = std::exp((elements[i][it] - max_value[i]));
       sum[i] += elements[i][it];
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
+  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
 
   // store result
   output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < element_count) {
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] = elements[i][it + element] / sum[i];
         }
-        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
-            dst + i * element_count + it * WARP_SIZE, out);
+        copy<output_t, ELEMENTS_PER_LDG_STG>(
+          out,  dst + i * element_count + it * WARP_SIZE);
       } else {
         break;
       }
     }
   }
 }
 
@@ -265,18 +196,18 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     int batch_element_count = (i >= local_batches) ? 0 : element_count;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < batch_element_count) {
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
-            temp_grad, grad + i * element_count + it * WARP_SIZE);
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
-            temp_output, output + i * element_count + it * WARP_SIZE);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(
+            grad + i * element_count + it * WARP_SIZE, temp_grad);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(
+            output + i * element_count + it * WARP_SIZE, temp_output);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           output_reg[i][it + element] = (acc_t)temp_output[element];
         }
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
@@ -292,15 +223,15 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     sum[i] = grad_reg[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       sum[i] += grad_reg[i][it];
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
+  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
 
 // store result
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
@@ -310,25 +241,25 @@
         output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] =
               (output_t)(scale * (grad_reg[i][it + element] -
                                   output_reg[i][it + element] * sum[i]));
         }
-        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
-            gradInput + i * element_count + it * WARP_SIZE, out);
+        copy<output_t, ELEMENTS_PER_LDG_STG>(
+          out, gradInput + i * element_count + it * WARP_SIZE);
       }
     }
   }
 }
-}  // end of anonymous namespace
+
 
 int get_batch_per_block(int query_seq_len, int key_seq_len, int batches,
                         int attn_heads) {
-  int log2_elements = log2_ceil(key_seq_len);
+  int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
   const int next_power_of_two = 1 << log2_elements;
 
   int warp_size =
       (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
   int batches_per_warp = (next_power_of_two <= 128) ? 2 : 1;
 
   constexpr int threads_per_block = 128;
@@ -345,15 +276,15 @@
                                             int query_seq_len, int key_seq_len,
                                             int batches, int attn_heads,
                                             int pad_batches) {
   TORCH_INTERNAL_ASSERT(key_seq_len >= 0 && key_seq_len <= 2048);
   if (key_seq_len == 0) {
     return;
   } else {
-    int log2_elements = log2_ceil(key_seq_len);
+    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
     const int next_power_of_two = 1 << log2_elements;
     int batch_count = batches * attn_heads * query_seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_forward.
     int warp_size =
         (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
@@ -445,15 +376,15 @@
                                              const acc_t scale,
                                              int query_seq_len, int key_seq_len,
                                              int batches, int attn_heads) {
   TORCH_INTERNAL_ASSERT(key_seq_len >= 0 && key_seq_len <= 2048);
   if (key_seq_len == 0) {
     return;
   } else {
-    int log2_elements = log2_ceil(key_seq_len);
+    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(key_seq_len);
     const int next_power_of_two = 1 << log2_elements;
     int batch_count = batches * attn_heads * query_seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_backward.
     int warp_size =
         (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;
@@ -532,7 +463,71 @@
                 grad_input, grad, output, scale, batch_count, key_seq_len);
         break;
       default:
         break;
     }
   }
 }
+
+torch::Tensor fwd_cuda(torch::Tensor const& input, torch::Tensor const& mask,
+                       float scale_factor) {
+  // input is a 4d tensor with dimensions [batches, attn_heads, seq_len,
+  // seq_len]
+  const int batches = input.size(0);
+  const int pad_batches = mask.size(0);
+  const int attn_heads = input.size(1);
+  const int query_seq_len = input.size(2);
+  const int key_seq_len = input.size(3);
+  TORCH_INTERNAL_ASSERT(key_seq_len <= 2048);
+  TORCH_INTERNAL_ASSERT(query_seq_len > 1);
+  TORCH_INTERNAL_ASSERT(pad_batches == 1 || pad_batches == batches);
+  TORCH_INTERNAL_ASSERT(mask.size(1) == 1);
+  TORCH_INTERNAL_ASSERT(mask.size(2) == query_seq_len);
+  TORCH_INTERNAL_ASSERT(mask.size(3) == key_seq_len);
+
+  // Output
+  auto act_options = input.options().requires_grad(false);
+  torch::Tensor softmax_results = torch::empty(
+      {batches, attn_heads, query_seq_len, key_seq_len}, act_options);
+
+  // Softmax Intermediate Result Ptr
+  void* input_ptr = static_cast<void*>(input.data_ptr());
+  void* mask_ptr = static_cast<void*>(mask.data_ptr());
+  void* softmax_results_ptr = static_cast<void*>(softmax_results.data_ptr());
+
+  DISPATCH_HALF_AND_BFLOAT(
+      input.scalar_type(), "dispatch_scaled_masked_softmax_forward",
+      dispatch_scaled_masked_softmax_forward<scalar_t, scalar_t, float>(
+          reinterpret_cast<scalar_t*>(softmax_results_ptr),
+          reinterpret_cast<const scalar_t*>(input_ptr),
+          reinterpret_cast<const uint8_t*>(mask_ptr), scale_factor,
+          query_seq_len, key_seq_len, batches, attn_heads, pad_batches););
+  return softmax_results;
+}
+
+torch::Tensor bwd_cuda(torch::Tensor const& output_grads_,
+                       torch::Tensor const& softmax_results_,
+                       float scale_factor) {
+  auto output_grads = output_grads_.contiguous();
+  auto softmax_results = softmax_results_.contiguous();
+
+  // output grads is a 4d tensor with dimensions [batches, attn_heads, seq_len,
+  // seq_len]
+  const int batches = output_grads.size(0);
+  const int attn_heads = output_grads.size(1);
+  const int query_seq_len = output_grads.size(2);
+  const int key_seq_len = output_grads.size(3);
+
+  void* output_grads_ptr = static_cast<void*>(output_grads.data_ptr());
+
+  // Softmax Grad
+  DISPATCH_HALF_AND_BFLOAT(
+      output_grads_.scalar_type(), "dispatch_scaled_masked_softmax_backward",
+      dispatch_scaled_masked_softmax_backward<scalar_t, scalar_t, float>(
+          reinterpret_cast<scalar_t*>(output_grads_ptr),
+          reinterpret_cast<scalar_t*>(output_grads_ptr),
+          reinterpret_cast<scalar_t const*>(softmax_results.data_ptr()),
+          scale_factor, query_seq_len, key_seq_len, batches, attn_heads););
+
+  // backward pass is completely in-place
+  return output_grads;
+}
```

### Comparing `colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h` & `colossalai-nightly-2024.6.1/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu`

 * *Files 10% similar despite different names*

```diff
@@ -1,128 +1,34 @@
 /*This code from NVIDIA Megatron:
  *     with minor changes. */
 
-#pragma once
-
+#include <ATen/ATen.h>
+#include <ATen/cuda/CUDAContext.h>
+#include <cuda.h>
+#include <cuda_fp16.h>
+#include <cuda_profiler_api.h>
+#include <cuda_runtime.h>
+#include <torch/extension.h>
 #include <assert.h>
 #include <c10/macros/Macros.h>
-#include <cuda_fp16.h>
 #include <stdint.h>
-
 #include <cfloat>
 #include <limits>
 
-namespace {
-
-template <typename Datatype, int ELEMENTS_PER_LDG>
-__device__ __inline__ void copy_vector(Datatype *dst, const Datatype *src);
-
-template <>
-__device__ __inline__ void copy_vector<c10::BFloat16, 1>(
-    c10::BFloat16 *dst, const c10::BFloat16 *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<c10::BFloat16, 4>(
-    c10::BFloat16 *dst, const c10::BFloat16 *src) {
-  *((float2 *)dst) = *((float2 *)src);
-}
-
-template <>
-__device__ __inline__ void copy_vector<c10::Half, 1>(c10::Half *dst,
-                                                     const c10::Half *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<c10::Half, 4>(c10::Half *dst,
-                                                     const c10::Half *src) {
-  *((float2 *)dst) = *((float2 *)src);
-}
-
-template <>
-__device__ __inline__ void copy_vector<uint8_t, 1>(uint8_t *dst,
-                                                   const uint8_t *src) {
-  *dst = *src;
-}
-
-template <>
-__device__ __inline__ void copy_vector<uint8_t, 4>(uint8_t *dst,
-                                                   const uint8_t *src) {
-  *((half2 *)dst) = *((half2 *)src);
-}
-
-template <typename Datatype, int ELEMENTS_PER_LDG>
-__device__ __inline__ void copy_zero_vector(Datatype *dst);
-
-template <>
-__device__ __inline__ void copy_zero_vector<c10::BFloat16, 1>(
-    c10::BFloat16 *dst) {
-  *dst = 0.0;
-}
-
-template <>
-__device__ __inline__ void copy_zero_vector<c10::BFloat16, 4>(
-    c10::BFloat16 *dst) {
-  *((float2 *)dst) = make_float2(0.0f, 0.0f);
-}
-
-template <>
-__device__ __inline__ void copy_zero_vector<c10::Half, 1>(c10::Half *dst) {
-  *dst = 0.0;
-}
-
-template <>
-__device__ __inline__ void copy_zero_vector<c10::Half, 4>(c10::Half *dst) {
-  *((float2 *)dst) = make_float2(0.0f, 0.0f);
-}
-
-int log2_ceil(int value) {
-  int log2_value = 0;
-  while ((1 << log2_value) < value) ++log2_value;
-  return log2_value;
-}
-
-template <typename T>
-struct Add {
-  __device__ __forceinline__ T operator()(T a, T b) const { return a + b; }
-};
-
-template <typename T>
-struct Max {
-  __device__ __forceinline__ T operator()(T a, T b) const {
-    return a < b ? b : a;
-  }
-};
-
-template <typename T>
-__device__ __forceinline__ T
-WARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,
-                     unsigned int mask = 0xffffffff) {
-#if CUDA_VERSION >= 9000
-  return __shfl_xor_sync(mask, value, laneMask, width);
-#else
-  return __shfl_xor(value, laneMask, width);
-#endif
-}
-
-template <typename acc_t, int WARP_BATCH, int WARP_SIZE,
-          template <typename> class ReduceOp>
-__device__ __forceinline__ void warp_reduce(acc_t *sum) {
-  ReduceOp<acc_t> r;
-#pragma unroll
-  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
-#pragma unroll
-    for (int i = 0; i < WARP_BATCH; ++i) {
-      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);
-      sum[i] = r(sum[i], b);
-    }
-  }
-}
+#include "common/micros.h"
+#include "utils/vec_copy.h"
+#include "funcs/reduce_function.h"
+#include "funcs/unary_functor.h"
+
+using colossalAI::funcs::UnaryOpFunctor;
+using colossalAI::funcs::UnaryOpType;
+using colossalAI::funcs::warp_reduce;
+using colossalAI::funcs::ReduceType;
+using colossalAI::cuda::utils::copy;
+using colossalAI::cuda::utils::copy_zero;
 
 /*
  * Extended softmax (from native aten pytorch) with following additional
  * features 1) input scaling 2) Implicit time (diagonal masking)
  */
 template <typename input_t, typename output_t, typename acc_t,
           int log2_elements>
@@ -165,16 +71,16 @@
     int batch_element_count = (i >= local_batches) ? 0 : local_seq;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
 
       if (element_index < batch_element_count) {
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
-            temp_data, src + i * element_count * stride + it * WARP_SIZE);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(
+            src + i * element_count * stride + it * WARP_SIZE, temp_data);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if ((element_index + element) < batch_element_count) {
             elements[i][it + element] = (acc_t)temp_data[element] * scale;
           } else {
             elements[i][it + element] = -std::numeric_limits<acc_t>::infinity();
@@ -196,28 +102,29 @@
     max_value[i] = elements[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       max_value[i] =
           (max_value[i] > elements[i][it]) ? max_value[i] : elements[i][it];
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Max>(max_value);
+  warp_reduce<acc_t,ReduceType::kMax,WARP_BATCH,WARP_SIZE>(max_value);
 
   acc_t sum[WARP_BATCH]{0.0f};
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; ++it) {
       if (it < warp_iteration_limit) {
         elements[i][it] = std::exp((elements[i][it] - max_value[i]));
         sum[i] += elements[i][it];
       }
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
+  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
+
 
   // store result
   output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
@@ -229,18 +136,18 @@
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (element_index + element < local_seq) {
             out[element] = elements[i][it + element] / sum[i];
           } else {
             out[element] = 0;
           }
         }
-        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
-            dst + i * element_count * stride + it * WARP_SIZE, out);
+        copy<output_t, ELEMENTS_PER_LDG_STG>(
+            out, dst + i * element_count * stride + it * WARP_SIZE);
       } else if (element_index < element_count) {
-        copy_zero_vector<output_t, ELEMENTS_PER_LDG_STG>(
+        copy_zero<output_t, ELEMENTS_PER_LDG_STG>(
             dst + i * element_count * stride + it * WARP_SIZE);
       } else {
         break;
       }
     }
   }
 }
@@ -288,18 +195,18 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     int batch_element_count = (i >= local_batches) ? 0 : local_seq;
 
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
       int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;
       if (element_index < batch_element_count) {
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
-            temp_grad, grad + i * element_count * stride + it * WARP_SIZE);
-        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(
-            temp_output, output + i * element_count * stride + it * WARP_SIZE);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(
+            grad + i * element_count * stride + it * WARP_SIZE, temp_grad);
+        copy<input_t, ELEMENTS_PER_LDG_STG>(
+            output + i * element_count * stride + it * WARP_SIZE, temp_output);
 
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           if (element_index + element < batch_element_count) {
             output_reg[i][it + element] = (acc_t)temp_output[element];
           }
         }
@@ -319,15 +226,15 @@
   for (int i = 0; i < WARP_BATCH; ++i) {
     sum[i] = grad_reg[i][0];
 #pragma unroll
     for (int it = 1; it < WARP_ITERATIONS; ++it) {
       sum[i] += grad_reg[i][it];
     }
   }
-  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);
+  warp_reduce<acc_t,ReduceType::kSum,WARP_BATCH,WARP_SIZE>(sum);
 
 // store result
 #pragma unroll
   for (int i = 0; i < WARP_BATCH; ++i) {
     if (i >= local_batches) break;
 #pragma unroll
     for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {
@@ -337,32 +244,30 @@
         output_t out[ELEMENTS_PER_LDG_STG];
 #pragma unroll
         for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {
           out[element] =
               (output_t)(scale * (grad_reg[i][it + element] -
                                   output_reg[i][it + element] * sum[i]));
         }
-        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(
-            gradInput + i * element_count * stride + it * WARP_SIZE, out);
+        copy<output_t, ELEMENTS_PER_LDG_STG>(
+            out, gradInput + i * element_count * stride + it * WARP_SIZE);
       }
     }
   }
 }
 
-}  // end of anonymous namespace
-
 template <typename input_t, typename output_t, typename acc_t>
 void dispatch_scaled_upper_triang_masked_softmax_forward(
     output_t *dst, const input_t *src, const input_t scale,
     int softmax_elements, int softmax_elements_stride, int attn_batches) {
   TORCH_INTERNAL_ASSERT(softmax_elements >= 0 && softmax_elements <= 2048);
   if (softmax_elements == 0) {
     return;
   } else {
-    int log2_elements = log2_ceil(softmax_elements);
+    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(softmax_elements);
     const int next_power_of_two = 1 << log2_elements;
     int seq_len = softmax_elements;
     int batch_count = attn_batches * seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_forward.
     int warp_size =
@@ -479,15 +384,15 @@
     output_t *grad_input, input_t *grad, const input_t *output,
     const acc_t scale, int softmax_elements, int softmax_elements_stride,
     int attn_batches) {
   TORCH_INTERNAL_ASSERT(softmax_elements >= 0 && softmax_elements <= 2048);
   if (softmax_elements == 0) {
     return;
   } else {
-    int log2_elements = log2_ceil(softmax_elements);
+    int log2_elements = UnaryOpFunctor<int, int, UnaryOpType::kLog2Ceil>()(softmax_elements);
     const int next_power_of_two = 1 << log2_elements;
     int seq_len = softmax_elements;
     int batch_count = attn_batches * seq_len;
 
     // This value must match the WARP_SIZE constexpr value computed inside
     // softmax_warp_backward.
     int warp_size =
@@ -594,7 +499,65 @@
                 softmax_elements_stride, softmax_elements);
         break;
       default:
         break;
     }
   }
 }
+
+
+
+
+torch::Tensor fwd_cuda(torch::Tensor const& input, float scale_factor) {
+  // input is a 3d tensor with dimensions [attn_batches, seq_len, seq_len]
+  const int attn_batches = input.size(0);
+  const int seq_len = input.size(1);
+  TORCH_INTERNAL_ASSERT(seq_len <= 2048);
+
+  // Output
+  auto act_options = input.options().requires_grad(false);
+  torch::Tensor softmax_results =
+      torch::empty({attn_batches, seq_len, seq_len}, act_options);
+
+  // Softmax Intermediate Result Ptr
+  void* input_ptr = static_cast<void*>(input.data_ptr());
+  void* softmax_results_ptr = static_cast<void*>(softmax_results.data_ptr());
+
+  DISPATCH_HALF_AND_BFLOAT(
+      input.scalar_type(),
+      "dispatch_scaled_upper_triang_masked_softmax_forward",
+      dispatch_scaled_upper_triang_masked_softmax_forward<scalar_t, scalar_t,
+                                                          float>(
+          reinterpret_cast<scalar_t*>(softmax_results_ptr),
+          reinterpret_cast<const scalar_t*>(input_ptr), scale_factor, seq_len,
+          seq_len, attn_batches););
+  return softmax_results;
+}
+
+torch::Tensor bwd_cuda(torch::Tensor const& output_grads_,
+                       torch::Tensor const& softmax_results_,
+                       float scale_factor) {
+  auto output_grads = output_grads_.contiguous();
+  auto softmax_results = softmax_results_.contiguous();
+
+  // output grads is a 3d tensor with dimensions [attn_batches, seq_len,
+  // seq_len]
+  const int attn_batches = output_grads.size(0);
+  const int seq_len = output_grads.size(1);
+  TORCH_INTERNAL_ASSERT(output_grads.size(1) == output_grads.size(2));
+
+  void* output_grads_ptr = static_cast<void*>(output_grads.data_ptr());
+
+  // Softmax Grad
+  DISPATCH_HALF_AND_BFLOAT(
+      output_grads_.scalar_type(),
+      "dispatch_scaled_upper_triang_masked_softmax_backward",
+      dispatch_scaled_upper_triang_masked_softmax_backward<scalar_t, scalar_t,
+                                                           float>(
+          reinterpret_cast<scalar_t*>(output_grads_ptr),
+          reinterpret_cast<scalar_t*>(output_grads_ptr),
+          reinterpret_cast<scalar_t const*>(softmax_results.data_ptr()),
+          scale_factor, seq_len, seq_len, attn_batches););
+
+  // backward pass is completely in-place
+  return output_grads;
+}
```

### Comparing `colossalai-nightly-2024.5.4/extensions/cuda_extension.py` & `colossalai-nightly-2024.6.1/extensions/cuda_extension.py`

 * *Files 13% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 
 class _CudaExtension(_CppExtension):
     @abstractmethod
     def nvcc_flags(self) -> List[str]:
         """
         This function should return a list of nvcc compilation flags for extensions.
         """
+        return ["-DCOLOSSAL_WITH_CUDA"]
 
     def is_available(self) -> bool:
         # cuda extension can only be built if cuda is available
         try:
             import torch
 
             cuda_available = torch.cuda.is_available()
@@ -49,14 +50,20 @@
         from torch.utils.cpp_extension import CUDA_HOME
 
         if CUDA_HOME is None:
             raise RuntimeError("CUDA_HOME is None, please set CUDA_HOME to compile C++/CUDA kernels in ColossalAI.")
         cuda_include = os.path.join(CUDA_HOME, "include")
         return cuda_include
 
+    def include_dirs(self) -> List[str]:
+        """
+        This function should return a list of include files for extensions.
+        """
+        return super().include_dirs() + [self.get_cuda_home_include()]
+
     def build_jit(self) -> None:
         from torch.utils.cpp_extension import CUDA_HOME, load
 
         set_cuda_arch_list(CUDA_HOME)
 
         # get build dir
         build_directory = _Extension.get_jit_extension_folder_path()
```

### Comparing `colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_dao_cuda.py` & `colossalai-nightly-2024.6.1/extensions/pybind/flash_attention/flash_attention_dao_cuda.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ..base_extension import _Extension
+from ...base_extension import _Extension
 
 
 class FlashAttentionDaoCudaExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_dao_cuda", support_aot=False, support_jit=False, priority=10)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_npu.py` & `colossalai-nightly-2024.6.1/extensions/pybind/flash_attention/flash_attention_npu.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ..base_extension import _Extension
+from ...base_extension import _Extension
 
 
 class FlashAttentionNpuExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_npu", support_aot=False, support_jit=False)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_sdpa_cuda.py` & `colossalai-nightly-2024.6.1/extensions/pybind/flash_attention/flash_attention_sdpa_cuda.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from ..base_extension import _Extension
+from ...base_extension import _Extension
 
 
 class FlashAttentionSdpaCudaExtension(_Extension):
     def __init__(self):
         super().__init__(name="flash_attention_sdpa_cuda", support_aot=False, support_jit=False)
 
     def is_available(self) -> bool:
```

### Comparing `colossalai-nightly-2024.5.4/extensions/layernorm/layernorm_cuda.py` & `colossalai-nightly-2024.6.1/extensions/pybind/layernorm/layernorm_cuda.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,24 +1,26 @@
-from ..cuda_extension import _CudaExtension
-from ..utils import append_nvcc_threads, get_cuda_cc_flag
+from ...cuda_extension import _CudaExtension
+from ...utils import append_nvcc_threads, get_cuda_cc_flag
 
 
 class LayerNormCudaExtension(_CudaExtension):
     def __init__(self):
         super().__init__(name="layernorm_cuda")
 
     def sources_files(self):
-        ret = [self.csrc_abs_path(fname) for fname in ["cuda/layer_norm_cuda.cpp", "cuda/layer_norm_cuda_kernel.cu"]]
+        ret = [self.csrc_abs_path(fname) for fname in ["kernel/cuda/layer_norm_kernel.cu"]] + [
+            self.pybind_abs_path("layernorm/layer_norm.cpp")
+        ]
         return ret
 
     def include_dirs(self):
-        ret = [self.get_cuda_home_include()]
+        ret = [self.get_cuda_home_include()] + [self.csrc_abs_path("")]
         return ret
 
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
         extra_cuda_flags = ["-maxrregcount=50"]
         extra_cuda_flags.extend(get_cuda_cc_flag())
-        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags + self.version_dependent_macros
+        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags + self.version_dependent_macros + super().nvcc_flags()
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.4/extensions/moe/moe_cuda.py` & `colossalai-nightly-2024.6.1/extensions/pybind/softmax/scaled_upper_triangle_masked_softmax_cuda.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,29 +1,30 @@
-from ..cuda_extension import _CudaExtension
-from ..utils import append_nvcc_threads, get_cuda_cc_flag
+from ...cuda_extension import _CudaExtension
+from ...utils import append_nvcc_threads, get_cuda_cc_flag
 
 
-class MoeCudaExtension(_CudaExtension):
+class ScaledUpperTriangleMaskedSoftmaxCudaExtension(_CudaExtension):
     def __init__(self):
-        super().__init__(name="moe_cuda")
-
-    def include_dirs(self):
-        ret = [self.csrc_abs_path("cuda/include"), self.get_cuda_home_include()]
-        return ret
+        super().__init__(name="scaled_upper_triangle_masked_softmax_cuda")
 
     def sources_files(self):
-        ret = [self.csrc_abs_path(fname) for fname in ["cuda/moe_cuda.cpp", "cuda/moe_cuda_kernel.cu"]]
+        ret = [
+            self.csrc_abs_path(fname)
+            for fname in [
+                "kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu",
+            ]
+        ] + [self.pybind_abs_path("softmax/scaled_upper_triang_masked_softmax.cpp")]
         return ret
 
     def cxx_flags(self):
         return ["-O3"] + self.version_dependent_macros
 
     def nvcc_flags(self):
         extra_cuda_flags = [
             "-U__CUDA_NO_HALF_OPERATORS__",
             "-U__CUDA_NO_HALF_CONVERSIONS__",
             "--expt-relaxed-constexpr",
             "--expt-extended-lambda",
         ]
         extra_cuda_flags.extend(get_cuda_cc_flag())
-        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags
+        ret = ["-O3", "--use_fast_math"] + extra_cuda_flags + super().nvcc_flags()
         return append_nvcc_threads(ret)
```

### Comparing `colossalai-nightly-2024.5.4/extensions/triton_extension.py` & `colossalai-nightly-2024.6.1/extensions/triton_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/extensions/utils.py` & `colossalai-nightly-2024.6.1/extensions/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/setup.py` & `colossalai-nightly-2024.6.1/setup.py`

 * *Files 1% similar despite different names*

```diff
@@ -90,15 +90,15 @@
         raise RuntimeError("[extension] Could not find any kernel compatible with the current environment.")
     else:
         op_name_list = ", ".join(op_names)
         print(f"[extension] Building extensions{op_name_list}")
 else:
     ext_modules = []
 
-version = "2024.05.04"
+version = "2024.06.01"
 package_name = "colossalai-nightly"
 
 setup(
     name=package_name,
     version=version,
     packages=find_packages(
         exclude=(
@@ -107,15 +107,14 @@
             "docker",
             "tests",
             "docs",
             "examples",
             "tests",
             "scripts",
             "requirements",
-            "extensions",
             "*.egg-info",
         ),
     ),
     description="An integrated large-scale model training system with efficient parallelization techniques",
     long_description=fetch_readme(),
     long_description_content_type="text/markdown",
     license="Apache Software License 2.0",
```

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/__init__.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/base.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/hanging_param_model.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/hanging_param_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/nested_model.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/nested_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/repeated_computed_layers.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/repeated_computed_layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/simple_net.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/custom/simple_net.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/diffusers.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/diffusers/diffusers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/executor.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/executor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/registry.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/timm.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/timm/timm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/torchaudio.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchaudio/torchaudio.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/torchrec.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchrec/torchrec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/torchvision.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/torchvision/torchvision.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/albert.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/albert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bert.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/bert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/blip2.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/blip2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bloom.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/bloom.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/chatglm2.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/falcon.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/falcon.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/gpt.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/gpt.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/gptj.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/gptj.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/llama.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/llama.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/mistral.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/mistral.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/opt.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/opt.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/sam.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/sam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/t5.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/t5.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/vit.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/vit.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/whisper.py` & `colossalai-nightly-2024.6.1/tests/kit/model_zoo/transformers/whisper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_bias_addition.py` & `colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/test_bias_addition.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_mod_dir.py` & `colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/test_mod_dir.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_nested_ckpt.py` & `colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/test_nested_ckpt.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_shape_prop.py` & `colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/test_shape_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_symbolic_profile.py` & `colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/test_symbolic_profile.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/zoo.py` & `colossalai-nightly-2024.6.1/tests/test_analyzer/test_fx/zoo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_aten.py` & `colossalai-nightly-2024.6.1/tests/test_analyzer/test_subclasses/test_aten.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_flop_tensor.py` & `colossalai-nightly-2024.6.1/tests/test_analyzer/test_subclasses/test_flop_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_meta_mode.py` & `colossalai-nightly-2024.6.1/tests/test_analyzer/test_subclasses/test_meta_mode.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_node_converting_pass.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_pass/test_node_converting_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py` & `colossalai-nightly-2024.6.1/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_flash_attention.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_flash_attention.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/_utils.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/_utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -7,19 +7,22 @@
 from torch import Tensor
 from torch import distributed as dist
 from torch.distributed import ProcessGroup
 from torch.nn import Module
 from torch.optim import Adam, Optimizer
 from torch.testing import assert_close
 
+from colossalai.accelerator import get_accelerator
 from colossalai.booster import Booster
-from colossalai.booster.plugin import HybridParallelPlugin
+from colossalai.booster.plugin import HybridParallelPlugin, LowLevelZeroPlugin
 from colossalai.booster.plugin.hybrid_parallel_plugin import HybridParallelModule
 from colossalai.checkpoint_io.utils import gather_distributed_param
 from colossalai.lazy import LazyInitContext
+from colossalai.nn.optimizer import GaLoreAdamW8bit
+from colossalai.nn.optimizer.galore import get_galore_param_groups
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer import ShardConfig, ShardFormer
 from colossalai.shardformer._utils import getattr_
 from colossalai.shardformer.policies.auto_policy import Policy
 from colossalai.tensor.d_tensor.api import is_customized_distributed_tensor, is_distributed_tensor
 from colossalai.tensor.padded_tensor.api import is_padded_tensor, to_unpadded_tensor
 
@@ -109,28 +112,47 @@
         assert k in shard_sd, f"{name} {k} not in sharded model"
         shard_v = shard_sd[k]
         assert v.shape == shard_v.shape, f"{name} {k} shape mismatch, {v.shape} vs {shard_v.shape}"
         assert v.dtype == shard_v.dtype, f"{name} {k} dtype mismatch, {v.dtype} vs {shard_v.dtype}"
         assert torch.equal(v, shard_v), f"{name} {k} value mismatch"
 
 
-def build_model_from_hybrid_plugin(model_fn: Callable, loss_fn: Callable, test_config: Dict[str, Any]):
+def build_model_from_hybrid_plugin(
+    model_fn: Callable, loss_fn: Callable, test_config: Dict[str, Any], optim_class=Adam, sharded_optim_class=Adam
+):
     use_lazy_init = False
     if "use_lazy_init" in test_config:
         use_lazy_init = test_config.pop("use_lazy_init")
 
     ctx = LazyInitContext() if use_lazy_init else nullcontext()
     with ctx:
         org_model = model_fn()
         sharded_model = copy.deepcopy(org_model)
     if use_lazy_init:
         ctx.materialize(org_model)
     org_model = org_model.cuda()
-    org_optimizer = Adam(org_model.parameters(), lr=1e-3)
-    sharded_optimizer = Adam(sharded_model.parameters(), lr=1e-3)
+    if optim_class == GaLoreAdamW8bit:
+        # Disable clipping and block-wise quantization
+        org_optimizer = optim_class(
+            get_galore_param_groups(org_model, weight_decay=0, rank=4),
+            lr=1e-3,
+            percentile_clipping=101,
+            block_wise=False,
+            min_8bit_size=1e10,
+        )
+        sharded_optimizer = sharded_optim_class(
+            get_galore_param_groups(sharded_model, weight_decay=0, rank=4),
+            lr=1e-3,
+            percentile_clipping=101,
+            block_wise=False,
+            min_8bit_size=1e10,
+        )
+    else:
+        org_optimizer = optim_class(org_model.parameters(), lr=1e-3)
+        sharded_optimizer = sharded_optim_class(sharded_model.parameters(), lr=1e-3)
     criterion = loss_fn
 
     plugin = HybridParallelPlugin(**test_config)
     booster = Booster(plugin=plugin)
 
     sharded_model, sharded_optimizer, criterion, _, _ = booster.boost(sharded_model, sharded_optimizer, criterion)
     return (
@@ -139,14 +161,40 @@
         sharded_model,
         sharded_optimizer,
         criterion,
         booster,
     )
 
 
+def build_model_from_low_level_zero_plugin(
+    model_fn: Callable, loss_fn: Callable, test_config: Dict[str, Any], optim_class=Adam, sharded_optim_class=Adam
+):
+    use_lazy_init = False
+    if "use_lazy_init" in test_config:
+        use_lazy_init = test_config.pop("use_lazy_init")
+
+    ctx = LazyInitContext() if use_lazy_init else nullcontext()
+    with ctx:
+        org_model = model_fn()
+        sharded_model = copy.deepcopy(org_model)
+    if use_lazy_init:
+        ctx.materialize(org_model)
+
+    org_model = org_model.cuda()
+    org_optimizer = optim_class(org_model.parameters(), lr=1e-3)
+    sharded_optimizer = sharded_optim_class(sharded_model.parameters(), lr=1e-3)
+    criterion = loss_fn
+
+    plugin = LowLevelZeroPlugin(**test_config)
+    booster = Booster(plugin=plugin)
+
+    sharded_model, sharded_optimizer, criterion, _, _ = booster.boost(sharded_model, sharded_optimizer, criterion)
+    return org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster
+
+
 def run_forward_backward_with_hybrid_plugin(
     org_model: Module,
     sharded_model: Module,
     sharded_optimizer: Optimizer,
     data_gen_fn: Callable,
     output_transform_fn: Callable,
     criterion: Callable,
@@ -205,14 +253,52 @@
     org_output = org_model(**unshard_test_data)
     org_loss = criterion(org_output)
     org_loss.backward()
 
     return org_loss, org_output, sharded_loss, sharded_output
 
 
+def run_forward_backward_with_low_level_zero_plugin(
+    org_model: Module,
+    sharded_model: Module,
+    sharded_optimizer: Optimizer,
+    data_gen_fn: Callable,
+    output_transform_fn: Callable,
+    criterion: Callable,
+    booster: Booster,
+):
+    get_accelerator().get_current_device()
+    org_model.cuda()
+    sharded_model.cuda()
+
+    def _criterion(outputs, inputs):
+        outputs = output_transform_fn(outputs)
+        loss = criterion(outputs)
+        return loss
+
+    data = data_gen_fn()
+
+    # data = {
+    #     k: v.to(device) if torch.is_tensor(v) or "Tensor" in v.__class__.__name__ else v for k, v in data.items()
+    # }
+    data = {k: v.cuda() for k, v in data.items()}
+
+    sharded_model.train()
+    sharded_output = sharded_model(**data)
+    sharded_loss = criterion(sharded_output)
+    sharded_optimizer.backward(sharded_loss)
+
+    org_model.train()
+    org_output = org_model(**data)
+    org_loss = criterion(org_output)
+    org_loss.backward()
+
+    return org_loss, org_output, sharded_loss, sharded_output
+
+
 def check_output_hidden_state(
     org_output: Tensor,
     sharded_output: Tensor,
     stage_manager: Optional[PipelineStageManager] = None,
     atol: float = 1e-5,
     rtol: float = 1e-3,
 ):
@@ -308,14 +394,17 @@
     rtol: float = 1e-3,
     verbose: bool = False,
 ):
     for suffix in layer_suffix:
         org_grad = getattr_(org_model, suffix).weight.grad
         shard_grad = getattr_(sharded_model, suffix).weight.grad
         shard_weight = getattr_(sharded_model, suffix).weight
+        # if verbose and dist.get_rank() == 0:
+        #     print("shard_weight", shard_weight)
+        #     print("org_grad", org_grad)
         if is_distributed_tensor(shard_weight) or is_customized_distributed_tensor(shard_weight):
             shard_grad_list = [torch.zeros_like(shard_grad).to("cuda") for _ in range(dist.get_world_size(tp_group))]
             dist.all_gather(shard_grad_list, shard_grad, tp_group)
             shard_grad = torch.cat(shard_grad_list, dim=dim)
 
         # embedding may be resized when using tensor parallel
         if shard_grad.shape[0] > org_grad.shape[0]:
```

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_bert.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_bert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_blip2.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_blip2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_bloom.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_bloom.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_chatglm2.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_falcon.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_falcon.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gpt2.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_gpt2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gptj.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_gptj.py`

 * *Files 2% similar despite different names*

```diff
@@ -236,28 +236,26 @@
     clear_layout_converter()
     torch.cuda.empty_cache()
 
 
 def check_gptj(rank, world_size, port):
     disable_existing_loggers()
     colossalai.launch(
-        config={},
         rank=rank,
         world_size=world_size,
         host="localhost",
         port=port,
         backend="nccl",
     )
     run_gptj_test()
 
 
 def check_gptj_3d(rank, world_size, port):
     disable_existing_loggers()
     colossalai.launch(
-        config={},
         rank=rank,
         world_size=world_size,
         host="localhost",
         port=port,
         backend="nccl",
     )
     run_gptj_3d_test()
```

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_llama.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_llama.py`

 * *Files 0% similar despite different names*

```diff
@@ -60,15 +60,17 @@
         booster.plugin.zero_stage in [1, 2]
         and booster.plugin.shard_config.enable_sequence_parallelism
         and booster.plugin.shard_config.sequence_parallelism_mode == "all_to_all"
     ):
         for p1, p2 in zip(llama_model.parameters(), sharded_optimizer._master_param_groups_of_current_rank[0]):
             working_p = sharded_optimizer._param_store.master_to_working_param[id(p2)]
             grads = sharded_optimizer._grad_store.get_partitioned_gradients_by_param_id(0, id(working_p))
-            grad_index = 0 if sharded_optimizer._partition_grads else sharded_optimizer._local_rank
+            grad_index = (
+                0 if sharded_optimizer._grad_store._partition_grads else sharded_optimizer._bucket_store.zero_local_rank
+            )
             grad = grads[grad_index]
             sharded_grad = p1.grad.view(-1).chunk(dist.get_world_size())[dist.get_rank()]
             assert_close(sharded_grad, grad[: sharded_grad.shape[0]], atol=5e-3, rtol=5e-3, check_dtype=False)
 
     # Save gradient tensors for comparison between the original model and the sharded model before optimizer step.
     grads_to_check = {}
     if (stage_manager is None or stage_manager.is_first_stage(ignore_chunk=True)) and booster.plugin.zero_stage == 0:
```

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_mistral.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_mistral.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_opt.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_opt.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_sam.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_sam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_t5.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_t5.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_vit.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_vit.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_whisper.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_model/test_shard_whisper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_shard_utils.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_shard_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.5.4/tests/test_shardformer/test_with_torch_ddp.py` & `colossalai-nightly-2024.6.1/tests/test_shardformer/test_with_torch_ddp.py`

 * *Files identical despite different names*

