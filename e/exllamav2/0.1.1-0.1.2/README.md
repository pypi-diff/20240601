# Comparing `tmp/exllamav2-0.1.1-py3-none-any.whl.zip` & `tmp/exllamav2-0.1.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,157 +1,157 @@
-Zip file size: 245009 bytes, number of entries: 155
--rw-r--r--  2.0 unx      482 b- defN 24-May-27 16:52 exllamav2/__init__.py
--rw-r--r--  2.0 unx    23883 b- defN 24-May-27 16:52 exllamav2/architecture.py
--rw-r--r--  2.0 unx    39138 b- defN 24-May-27 16:52 exllamav2/attn.py
--rw-r--r--  2.0 unx    16389 b- defN 24-May-27 16:52 exllamav2/cache.py
--rw-r--r--  2.0 unx     2001 b- defN 24-May-27 16:52 exllamav2/compat.py
--rw-r--r--  2.0 unx    12574 b- defN 24-May-27 16:52 exllamav2/config.py
--rw-r--r--  2.0 unx     4545 b- defN 24-May-27 16:52 exllamav2/embedding.py
--rw-r--r--  2.0 unx    12302 b- defN 24-May-27 16:52 exllamav2/ext.py
--rw-r--r--  2.0 unx     6399 b- defN 24-May-27 16:52 exllamav2/fasttensors.py
--rw-r--r--  2.0 unx     3969 b- defN 24-May-27 16:52 exllamav2/headnorm.py
--rw-r--r--  2.0 unx     3502 b- defN 24-May-27 16:52 exllamav2/layernorm.py
--rw-r--r--  2.0 unx    11159 b- defN 24-May-27 16:52 exllamav2/linear.py
--rw-r--r--  2.0 unx     6408 b- defN 24-May-27 16:52 exllamav2/lora.py
--rw-r--r--  2.0 unx    12405 b- defN 24-May-27 16:52 exllamav2/mlp.py
--rw-r--r--  2.0 unx    31876 b- defN 24-May-27 16:52 exllamav2/model.py
--rw-r--r--  2.0 unx     4931 b- defN 24-May-27 16:52 exllamav2/model_init.py
--rw-r--r--  2.0 unx     7100 b- defN 24-May-27 16:52 exllamav2/module.py
--rw-r--r--  2.0 unx    14288 b- defN 24-May-27 16:52 exllamav2/moe_mlp.py
--rw-r--r--  2.0 unx     5262 b- defN 24-May-27 16:52 exllamav2/parallel_decoder.py
--rw-r--r--  2.0 unx     3172 b- defN 24-May-27 16:52 exllamav2/pos_embedding.py
--rw-r--r--  2.0 unx     3646 b- defN 24-May-27 16:52 exllamav2/rmsnorm.py
--rw-r--r--  2.0 unx     6187 b- defN 24-May-27 16:52 exllamav2/util.py
--rw-r--r--  2.0 unx       21 b- defN 24-May-27 16:52 exllamav2/version.py
--rw-r--r--  2.0 unx      337 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/config.h
--rw-r--r--  2.0 unx     4378 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_bindings.cpp
--rw-r--r--  2.0 unx     8037 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_cache.cpp
--rw-r--r--  2.0 unx     1105 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_cache.h
--rw-r--r--  2.0 unx     1455 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_gemm.cpp
--rw-r--r--  2.0 unx      161 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_gemm.h
--rw-r--r--  2.0 unx     2505 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_hadamard.cpp
--rw-r--r--  2.0 unx       82 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_hadamard.h
--rw-r--r--  2.0 unx     3158 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_norm.cpp
--rw-r--r--  2.0 unx      639 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_norm.h
--rw-r--r--  2.0 unx     6577 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qattn.cpp
--rw-r--r--  2.0 unx     1764 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qattn.h
--rw-r--r--  2.0 unx     5665 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qmatrix.cpp
--rw-r--r--  2.0 unx      818 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qmatrix.h
--rw-r--r--  2.0 unx     8846 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qmlp.cpp
--rw-r--r--  2.0 unx     2274 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qmlp.h
--rw-r--r--  2.0 unx     5533 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_quant.cpp
--rw-r--r--  2.0 unx      895 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_quant.h
--rw-r--r--  2.0 unx     1339 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_rope.cpp
--rw-r--r--  2.0 unx      186 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_rope.h
--rw-r--r--  2.0 unx      344 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_safetensors.cpp
--rw-r--r--  2.0 unx       29 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_safetensors.h
--rw-r--r--  2.0 unx    10976 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_sampling.cpp
--rw-r--r--  2.0 unx     1379 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_sampling.h
--rw-r--r--  2.0 unx     1275 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/avx2_target.h
--rw-r--r--  2.0 unx    24414 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/avx_mathfun.h
--rw-r--r--  2.0 unx     1299 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/generator.cpp
--rw-r--r--  2.0 unx      274 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/generator.h
--rw-r--r--  2.0 unx     1627 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/profiling.cpp
--rw-r--r--  2.0 unx      362 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/profiling.h
--rw-r--r--  2.0 unx     2265 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/quantize_func.cpp
--rw-r--r--  2.0 unx      611 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/quantize_func.h
--rw-r--r--  2.0 unx     8308 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/safetensors.cpp
--rw-r--r--  2.0 unx      801 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/safetensors.h
--rw-r--r--  2.0 unx    20446 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/sampling.cpp
--rw-r--r--  2.0 unx     2283 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/sampling.h
--rw-r--r--  2.0 unx     3089 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/sampling_avx2.cpp
--rw-r--r--  2.0 unx      334 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/sampling_avx2.h
--rw-r--r--  2.0 unx     2204 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/util.h
--rw-r--r--  2.0 unx    13564 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/cache.cu
--rw-r--r--  2.0 unx     1658 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/cache.cuh
--rw-r--r--  2.0 unx     2831 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/compat.cuh
--rw-r--r--  2.0 unx     2604 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/h_add.cu
--rw-r--r--  2.0 unx      265 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/h_add.cuh
--rw-r--r--  2.0 unx     7202 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/h_gemm.cu
--rw-r--r--  2.0 unx      667 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/h_gemm.cuh
--rw-r--r--  2.0 unx     3160 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/head_norm.cu
--rw-r--r--  2.0 unx      329 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/head_norm.cuh
--rw-r--r--  2.0 unx     4970 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/layer_norm.cu
--rw-r--r--  2.0 unx      302 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/layer_norm.cuh
--rw-r--r--  2.0 unx      935 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/lora.cu
--rw-r--r--  2.0 unx      463 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/lora.cuh
--rw-r--r--  2.0 unx     4293 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/matrix_view.cuh
--rw-r--r--  2.0 unx     7287 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/pack_tensor.cu
--rw-r--r--  2.0 unx      549 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/pack_tensor.cuh
--rw-r--r--  2.0 unx     6642 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_attn.cu
--rw-r--r--  2.0 unx     2392 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_attn.cuh
--rw-r--r--  2.0 unx     8809 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_gemm.cu
--rw-r--r--  2.0 unx      614 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_gemm.cuh
--rw-r--r--  2.0 unx     1705 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_gemm_autotune.cuh
--rw-r--r--  2.0 unx    19672 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_gemm_kernel.cuh
--rw-r--r--  2.0 unx     7516 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq.cuh
--rw-r--r--  2.0 unx    21853 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_matrix.cu
--rw-r--r--  2.0 unx     1910 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_matrix.cuh
--rw-r--r--  2.0 unx     8195 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_mlp.cu
--rw-r--r--  2.0 unx     2924 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_mlp.cuh
--rw-r--r--  2.0 unx     5762 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_mlp_activation.cuh
--rw-r--r--  2.0 unx     7296 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_mlp_softmax.cuh
--rw-r--r--  2.0 unx     9975 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quantize.cu
--rw-r--r--  2.0 unx     1303 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quantize.cuh
--rw-r--r--  2.0 unx     3834 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/rms_norm.cu
--rw-r--r--  2.0 unx      277 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/rms_norm.cuh
--rw-r--r--  2.0 unx     7215 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/rope.cu
--rw-r--r--  2.0 unx      736 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/rope.cuh
--rw-r--r--  2.0 unx      624 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/util.cu
--rw-r--r--  2.0 unx     3345 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/util.cuh
--rw-r--r--  2.0 unx     1313 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cu
--rw-r--r--  2.0 unx     8080 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cuh
--rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1a.cu
--rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1b.cu
--rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2a.cu
--rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2b.cu
--rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3a.cu
--rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3b.cu
--rw-r--r--  2.0 unx      634 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_1.cu
--rw-r--r--  2.0 unx      634 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_2.cu
--rw-r--r--  2.0 unx      634 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_3.cu
--rw-r--r--  2.0 unx     2881 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_2.cuh
--rw-r--r--  2.0 unx     5782 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_3.cuh
--rw-r--r--  2.0 unx     5755 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_4.cuh
--rw-r--r--  2.0 unx     7342 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_5.cuh
--rw-r--r--  2.0 unx     4530 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_6.cuh
--rw-r--r--  2.0 unx      643 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_8.cuh
--rw-r--r--  2.0 unx     1367 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_util.cuh
--rw-r--r--  2.0 unx      513 b- defN 24-May-27 16:52 exllamav2/generator/__init__.py
--rw-r--r--  2.0 unx    12853 b- defN 24-May-27 16:52 exllamav2/generator/base.py
--rw-r--r--  2.0 unx    80867 b- defN 24-May-27 16:52 exllamav2/generator/dynamic.py
--rw-r--r--  2.0 unx     2687 b- defN 24-May-27 16:52 exllamav2/generator/dynamic_async.py
--rw-r--r--  2.0 unx      497 b- defN 24-May-27 16:52 exllamav2/generator/hooks.py
--rw-r--r--  2.0 unx     2370 b- defN 24-May-27 16:52 exllamav2/generator/ngram.py
--rw-r--r--  2.0 unx    10380 b- defN 24-May-27 16:52 exllamav2/generator/sampler.py
--rw-r--r--  2.0 unx    39172 b- defN 24-May-27 16:52 exllamav2/generator/streaming.py
--rw-r--r--  2.0 unx      242 b- defN 24-May-27 16:52 exllamav2/generator/filters/__init__.py
--rw-r--r--  2.0 unx      756 b- defN 24-May-27 16:52 exllamav2/generator/filters/base.py
--rw-r--r--  2.0 unx     1866 b- defN 24-May-27 16:52 exllamav2/generator/filters/prefix.py
--rw-r--r--  2.0 unx     3965 b- defN 24-May-27 16:52 exllamav2/generator/filters/select.py
--rw-r--r--  2.0 unx     4346 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard.py
--rw-r--r--  2.0 unx        1 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_1.txt
--rw-r--r--  2.0 unx    10099 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_100.txt
--rw-r--r--  2.0 unx    13571 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_116.txt
--rw-r--r--  2.0 unx    24491 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_156.txt
--rw-r--r--  2.0 unx    29755 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_172.txt
--rw-r--r--  2.0 unx    35531 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_188.txt
--rw-r--r--  2.0 unx    55931 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_236.txt
--rw-r--r--  2.0 unx    59779 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_244.txt
--rw-r--r--  2.0 unx   183612 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_428.txt
--rw-r--r--  2.0 unx     2755 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_52.txt
--rw-r--r--  2.0 unx     8555 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_92.txt
--rw-r--r--  2.0 unx    58982 b- defN 24-May-27 16:52 exllamav2/hadamard/primes.txt
--rw-r--r--  2.0 unx      107 b- defN 24-May-27 16:52 exllamav2/server/__init__.py
--rw-r--r--  2.0 unx     1623 b- defN 24-May-27 16:52 exllamav2/server/websocket.py
--rw-r--r--  2.0 unx    10260 b- defN 24-May-27 16:52 exllamav2/server/websocket_actions.py
--rw-r--r--  2.0 unx      217 b- defN 24-May-27 16:52 exllamav2/tokenizer/__init__.py
--rw-r--r--  2.0 unx     2157 b- defN 24-May-27 16:52 exllamav2/tokenizer/base.py
--rw-r--r--  2.0 unx     3028 b- defN 24-May-27 16:52 exllamav2/tokenizer/hf.py
--rw-r--r--  2.0 unx     1972 b- defN 24-May-27 16:52 exllamav2/tokenizer/spm.py
--rw-r--r--  2.0 unx    23709 b- defN 24-May-27 16:52 exllamav2/tokenizer/tokenizer.py
--rw-r--r--  2.0 unx     1035 b- defN 24-May-27 16:53 exllamav2-0.1.1.dist-info/LICENSE
--rw-r--r--  2.0 unx      421 b- defN 24-May-27 16:53 exllamav2-0.1.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-27 16:53 exllamav2-0.1.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 24-May-27 16:53 exllamav2-0.1.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    14370 b- defN 24-May-27 16:53 exllamav2-0.1.1.dist-info/RECORD
-155 files, 1289727 bytes uncompressed, 221983 bytes compressed:  82.8%
+Zip file size: 246923 bytes, number of entries: 155
+-rw-r--r--  2.0 unx      482 b- defN 24-Jun-01 17:57 exllamav2/__init__.py
+-rw-r--r--  2.0 unx    25250 b- defN 24-Jun-01 17:57 exllamav2/architecture.py
+-rw-r--r--  2.0 unx    41386 b- defN 24-Jun-01 17:57 exllamav2/attn.py
+-rw-r--r--  2.0 unx    17150 b- defN 24-Jun-01 17:57 exllamav2/cache.py
+-rw-r--r--  2.0 unx     2001 b- defN 24-Jun-01 17:57 exllamav2/compat.py
+-rw-r--r--  2.0 unx    13140 b- defN 24-Jun-01 17:57 exllamav2/config.py
+-rw-r--r--  2.0 unx     4876 b- defN 24-Jun-01 17:57 exllamav2/embedding.py
+-rw-r--r--  2.0 unx    12303 b- defN 24-Jun-01 17:57 exllamav2/ext.py
+-rw-r--r--  2.0 unx     6399 b- defN 24-Jun-01 17:57 exllamav2/fasttensors.py
+-rw-r--r--  2.0 unx     3995 b- defN 24-Jun-01 17:57 exllamav2/headnorm.py
+-rw-r--r--  2.0 unx     3528 b- defN 24-Jun-01 17:57 exllamav2/layernorm.py
+-rw-r--r--  2.0 unx    11185 b- defN 24-Jun-01 17:57 exllamav2/linear.py
+-rw-r--r--  2.0 unx     6408 b- defN 24-Jun-01 17:57 exllamav2/lora.py
+-rw-r--r--  2.0 unx    12410 b- defN 24-Jun-01 17:57 exllamav2/mlp.py
+-rw-r--r--  2.0 unx    31876 b- defN 24-Jun-01 17:57 exllamav2/model.py
+-rw-r--r--  2.0 unx     5097 b- defN 24-Jun-01 17:57 exllamav2/model_init.py
+-rw-r--r--  2.0 unx     7100 b- defN 24-Jun-01 17:57 exllamav2/module.py
+-rw-r--r--  2.0 unx    14314 b- defN 24-Jun-01 17:57 exllamav2/moe_mlp.py
+-rw-r--r--  2.0 unx     5288 b- defN 24-Jun-01 17:57 exllamav2/parallel_decoder.py
+-rw-r--r--  2.0 unx     3422 b- defN 24-Jun-01 17:57 exllamav2/pos_embedding.py
+-rw-r--r--  2.0 unx     3671 b- defN 24-Jun-01 17:57 exllamav2/rmsnorm.py
+-rw-r--r--  2.0 unx     6187 b- defN 24-Jun-01 17:57 exllamav2/util.py
+-rw-r--r--  2.0 unx       21 b- defN 24-Jun-01 17:57 exllamav2/version.py
+-rw-r--r--  2.0 unx      417 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/config.h
+-rw-r--r--  2.0 unx     4378 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_bindings.cpp
+-rw-r--r--  2.0 unx     8041 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_cache.cpp
+-rw-r--r--  2.0 unx     1105 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_cache.h
+-rw-r--r--  2.0 unx     1455 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_gemm.cpp
+-rw-r--r--  2.0 unx      161 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_gemm.h
+-rw-r--r--  2.0 unx     2505 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_hadamard.cpp
+-rw-r--r--  2.0 unx       82 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_hadamard.h
+-rw-r--r--  2.0 unx     3158 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_norm.cpp
+-rw-r--r--  2.0 unx      639 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_norm.h
+-rw-r--r--  2.0 unx     6577 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_qattn.cpp
+-rw-r--r--  2.0 unx     1764 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_qattn.h
+-rw-r--r--  2.0 unx     5665 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_qmatrix.cpp
+-rw-r--r--  2.0 unx      818 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_qmatrix.h
+-rw-r--r--  2.0 unx     8846 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_qmlp.cpp
+-rw-r--r--  2.0 unx     2274 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_qmlp.h
+-rw-r--r--  2.0 unx     5533 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_quant.cpp
+-rw-r--r--  2.0 unx      895 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_quant.h
+-rw-r--r--  2.0 unx     1339 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_rope.cpp
+-rw-r--r--  2.0 unx      186 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_rope.h
+-rw-r--r--  2.0 unx      344 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_safetensors.cpp
+-rw-r--r--  2.0 unx       29 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_safetensors.h
+-rw-r--r--  2.0 unx    10976 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_sampling.cpp
+-rw-r--r--  2.0 unx     1379 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/ext_sampling.h
+-rw-r--r--  2.0 unx     1275 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/avx2_target.h
+-rw-r--r--  2.0 unx    24414 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/avx_mathfun.h
+-rw-r--r--  2.0 unx     1299 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/generator.cpp
+-rw-r--r--  2.0 unx      274 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/generator.h
+-rw-r--r--  2.0 unx     1627 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/profiling.cpp
+-rw-r--r--  2.0 unx      362 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/profiling.h
+-rw-r--r--  2.0 unx     2265 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/quantize_func.cpp
+-rw-r--r--  2.0 unx      611 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/quantize_func.h
+-rw-r--r--  2.0 unx     8308 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/safetensors.cpp
+-rw-r--r--  2.0 unx      801 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/safetensors.h
+-rw-r--r--  2.0 unx    20446 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/sampling.cpp
+-rw-r--r--  2.0 unx     2283 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/sampling.h
+-rw-r--r--  2.0 unx     3089 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/sampling_avx2.cpp
+-rw-r--r--  2.0 unx      334 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/sampling_avx2.h
+-rw-r--r--  2.0 unx     2204 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cpp/util.h
+-rw-r--r--  2.0 unx    14080 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/cache.cu
+-rw-r--r--  2.0 unx     1658 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/cache.cuh
+-rw-r--r--  2.0 unx     2831 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/compat.cuh
+-rw-r--r--  2.0 unx     2604 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/h_add.cu
+-rw-r--r--  2.0 unx      265 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/h_add.cuh
+-rw-r--r--  2.0 unx     7202 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/h_gemm.cu
+-rw-r--r--  2.0 unx      667 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/h_gemm.cuh
+-rw-r--r--  2.0 unx     3160 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/head_norm.cu
+-rw-r--r--  2.0 unx      329 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/head_norm.cuh
+-rw-r--r--  2.0 unx     4970 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/layer_norm.cu
+-rw-r--r--  2.0 unx      302 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/layer_norm.cuh
+-rw-r--r--  2.0 unx      935 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/lora.cu
+-rw-r--r--  2.0 unx      463 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/lora.cuh
+-rw-r--r--  2.0 unx     4293 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/matrix_view.cuh
+-rw-r--r--  2.0 unx     7287 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/pack_tensor.cu
+-rw-r--r--  2.0 unx      549 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/pack_tensor.cuh
+-rw-r--r--  2.0 unx     6642 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_attn.cu
+-rw-r--r--  2.0 unx     2392 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_attn.cuh
+-rw-r--r--  2.0 unx     8809 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_gemm.cu
+-rw-r--r--  2.0 unx      614 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_gemm.cuh
+-rw-r--r--  2.0 unx     1705 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_gemm_autotune.cuh
+-rw-r--r--  2.0 unx    19672 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_gemm_kernel.cuh
+-rw-r--r--  2.0 unx     7516 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq.cuh
+-rw-r--r--  2.0 unx    21853 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_matrix.cu
+-rw-r--r--  2.0 unx     1910 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_matrix.cuh
+-rw-r--r--  2.0 unx     8195 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_mlp.cu
+-rw-r--r--  2.0 unx     2924 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_mlp.cuh
+-rw-r--r--  2.0 unx     5762 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_mlp_activation.cuh
+-rw-r--r--  2.0 unx     7296 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/q_mlp_softmax.cuh
+-rw-r--r--  2.0 unx     9975 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/quantize.cu
+-rw-r--r--  2.0 unx     1303 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/quantize.cuh
+-rw-r--r--  2.0 unx     3834 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/rms_norm.cu
+-rw-r--r--  2.0 unx      277 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/rms_norm.cuh
+-rw-r--r--  2.0 unx     7215 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/rope.cu
+-rw-r--r--  2.0 unx      736 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/rope.cuh
+-rw-r--r--  2.0 unx      624 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/util.cu
+-rw-r--r--  2.0 unx     3345 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/util.cuh
+-rw-r--r--  2.0 unx     1313 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cu
+-rw-r--r--  2.0 unx     8080 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cuh
+-rw-r--r--  2.0 unx      694 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1a.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1b.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2a.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2b.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3a.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3b.cu
+-rw-r--r--  2.0 unx      634 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_1.cu
+-rw-r--r--  2.0 unx      634 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_2.cu
+-rw-r--r--  2.0 unx      634 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_3.cu
+-rw-r--r--  2.0 unx     2881 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/quant/qdq_2.cuh
+-rw-r--r--  2.0 unx     5782 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/quant/qdq_3.cuh
+-rw-r--r--  2.0 unx     5755 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/quant/qdq_4.cuh
+-rw-r--r--  2.0 unx     7342 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/quant/qdq_5.cuh
+-rw-r--r--  2.0 unx     4530 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/quant/qdq_6.cuh
+-rw-r--r--  2.0 unx      643 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/quant/qdq_8.cuh
+-rw-r--r--  2.0 unx     1367 b- defN 24-Jun-01 17:57 exllamav2/exllamav2_ext/cuda/quant/qdq_util.cuh
+-rw-r--r--  2.0 unx      513 b- defN 24-Jun-01 17:57 exllamav2/generator/__init__.py
+-rw-r--r--  2.0 unx    12853 b- defN 24-Jun-01 17:57 exllamav2/generator/base.py
+-rw-r--r--  2.0 unx    82375 b- defN 24-Jun-01 17:57 exllamav2/generator/dynamic.py
+-rw-r--r--  2.0 unx     2687 b- defN 24-Jun-01 17:57 exllamav2/generator/dynamic_async.py
+-rw-r--r--  2.0 unx      497 b- defN 24-Jun-01 17:57 exllamav2/generator/hooks.py
+-rw-r--r--  2.0 unx     2370 b- defN 24-Jun-01 17:57 exllamav2/generator/ngram.py
+-rw-r--r--  2.0 unx    11181 b- defN 24-Jun-01 17:57 exllamav2/generator/sampler.py
+-rw-r--r--  2.0 unx    39172 b- defN 24-Jun-01 17:57 exllamav2/generator/streaming.py
+-rw-r--r--  2.0 unx      242 b- defN 24-Jun-01 17:57 exllamav2/generator/filters/__init__.py
+-rw-r--r--  2.0 unx      756 b- defN 24-Jun-01 17:57 exllamav2/generator/filters/base.py
+-rw-r--r--  2.0 unx     1866 b- defN 24-Jun-01 17:57 exllamav2/generator/filters/prefix.py
+-rw-r--r--  2.0 unx     3965 b- defN 24-Jun-01 17:57 exllamav2/generator/filters/select.py
+-rw-r--r--  2.0 unx     4346 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard.py
+-rw-r--r--  2.0 unx        1 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard_1.txt
+-rw-r--r--  2.0 unx    10099 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard_100.txt
+-rw-r--r--  2.0 unx    13571 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard_116.txt
+-rw-r--r--  2.0 unx    24491 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard_156.txt
+-rw-r--r--  2.0 unx    29755 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard_172.txt
+-rw-r--r--  2.0 unx    35531 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard_188.txt
+-rw-r--r--  2.0 unx    55931 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard_236.txt
+-rw-r--r--  2.0 unx    59779 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard_244.txt
+-rw-r--r--  2.0 unx   183612 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard_428.txt
+-rw-r--r--  2.0 unx     2755 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard_52.txt
+-rw-r--r--  2.0 unx     8555 b- defN 24-Jun-01 17:57 exllamav2/hadamard/hadamard_92.txt
+-rw-r--r--  2.0 unx    58982 b- defN 24-Jun-01 17:57 exllamav2/hadamard/primes.txt
+-rw-r--r--  2.0 unx      107 b- defN 24-Jun-01 17:57 exllamav2/server/__init__.py
+-rw-r--r--  2.0 unx     1623 b- defN 24-Jun-01 17:57 exllamav2/server/websocket.py
+-rw-r--r--  2.0 unx    10260 b- defN 24-Jun-01 17:57 exllamav2/server/websocket_actions.py
+-rw-r--r--  2.0 unx      217 b- defN 24-Jun-01 17:57 exllamav2/tokenizer/__init__.py
+-rw-r--r--  2.0 unx     2157 b- defN 24-Jun-01 17:57 exllamav2/tokenizer/base.py
+-rw-r--r--  2.0 unx     3028 b- defN 24-Jun-01 17:57 exllamav2/tokenizer/hf.py
+-rw-r--r--  2.0 unx     1972 b- defN 24-Jun-01 17:57 exllamav2/tokenizer/spm.py
+-rw-r--r--  2.0 unx    23709 b- defN 24-Jun-01 17:57 exllamav2/tokenizer/tokenizer.py
+-rw-r--r--  2.0 unx     1035 b- defN 24-Jun-01 17:57 exllamav2-0.1.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx      421 b- defN 24-Jun-01 17:57 exllamav2-0.1.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Jun-01 17:57 exllamav2-0.1.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 24-Jun-01 17:57 exllamav2-0.1.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    14370 b- defN 24-Jun-01 17:57 exllamav2-0.1.2.dist-info/RECORD
+155 files, 1298486 bytes uncompressed, 223897 bytes compressed:  82.8%
```

## zipnote {}

```diff
@@ -444,23 +444,23 @@
 
 Filename: exllamav2/tokenizer/spm.py
 Comment: 
 
 Filename: exllamav2/tokenizer/tokenizer.py
 Comment: 
 
-Filename: exllamav2-0.1.1.dist-info/LICENSE
+Filename: exllamav2-0.1.2.dist-info/LICENSE
 Comment: 
 
-Filename: exllamav2-0.1.1.dist-info/METADATA
+Filename: exllamav2-0.1.2.dist-info/METADATA
 Comment: 
 
-Filename: exllamav2-0.1.1.dist-info/WHEEL
+Filename: exllamav2-0.1.2.dist-info/WHEEL
 Comment: 
 
-Filename: exllamav2-0.1.1.dist-info/top_level.txt
+Filename: exllamav2-0.1.2.dist-info/top_level.txt
 Comment: 
 
-Filename: exllamav2-0.1.1.dist-info/RECORD
+Filename: exllamav2-0.1.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## exllamav2/architecture.py

```diff
@@ -94,14 +94,15 @@
 
         self.fused_mlp_key_12 = None
         self.fused_mlp_key_3 = None
         self.learned_pos_emb_key = None
 
         self.default_inner_dim_mult = None
         self.orig_weights_transposed = False
+        self.logit_scale_basedim = False
 
         # Mistral
 
         if arch_string == "MistralForCausalLM":
             arch_recognized = True
             self.layer_keys += \
                 layer_keys_llama_norms + \
@@ -546,14 +547,49 @@
             self.fused_qkv_key = "c_attn"
             self.mqa = False
             self.learned_pos_emb_key = "model.wpe"
             self.scale_attn_weights = True
             self.default_inner_dim_mult = 4
             self.orig_weights_transposed = True
 
+        # MiniCPM
+
+        if arch_string == "MiniCPMForCausalLM":
+            arch_recognized = True
+            self.layer_keys += \
+                layer_keys_llama_norms + \
+                layer_keys_llama_attn + \
+                layer_keys_llama_mlp
+            self.expect_keys += \
+                expect_keys_llama
+            self.norm_eps_key = "rms_norm_eps"
+            self.attention_bias_qkv = False
+            self.attention_bias_o = False
+            self.mlp_bias = False
+            self.mlp_gate = True
+            self.mlp_key_gate = ".mlp.gate_proj"
+            self.mlp_key_up = ".mlp.up_proj"
+            self.mlp_key_down = ".mlp.down_proj"
+            self.mlp_act_func = "silu"
+            self.is_moe = False
+            self.norm = "rmsnorm"
+            self.lm_head_key = "lm_head"
+            self.normalize_embeddings = False
+            self.norm_key_1 = ".input_layernorm"
+            self.norm_key_2 = ".post_attention_layernorm"
+            self.norm_constant_bias = 0
+            self.parallel_decoder_blocks = False
+            self.requires_bos = False
+            self.rope_style = RopeStyle.NEOX
+            self.keymap = None
+            self.fused_qkv_key = None
+            self.mqa = False
+            self.scale_attn_weights = False
+            self.logit_scale_basedim = True
+
         # Llama (default + fallback)
 
         if arch_string != "LlamaForCausalLM" and not arch_recognized:
             print(f" !! Warning, unknown architecture: {arch_string}")
             print(f" !! Loading as LlamaForCausalLM")
             self.arch_string = "LlamaForCausalLM"
         if not arch_recognized:
```

## exllamav2/attn.py

```diff
@@ -197,44 +197,69 @@
 
 
     class PagedParams(Params):
 
         block_index: torch.Tensor
         cache_seqlens: torch.Tensor
         page_size: int
+        is_sequential: bool
+        first_index: int
 
         def __init__(
             self,
             batch_size: int,
             block_index: torch.Tensor,
             cache_seqlens: torch.Tensor,
-            page_size: int
+            page_size: int,
+            q_len: int = 0
         ):
             super().__init__(
                 batch_size = batch_size,
                 paged = True
             )
 
             self.block_index = block_index
             self.cache_seqlens = cache_seqlens
             self.page_size = page_size
 
+            self.is_sequential = False
+            assert self.block_index.device.type == "cpu"
+            assert self.cache_seqlens.device.type == "cpu"
+            assert q_len > 0
+            if self.block_index.shape[0] == 1:
+                vi0 = self.cache_seqlens[0].item()
+                vi1 = vi0 + q_len
+                vp0 = vi0 // page_size
+                vp1 = (vi1 - 1) // page_size
+                for i in range(vp0 + 1, vp1 + 1):
+                    if self.block_index[0, i].item() != self.block_index[0, i - 1].item() + 1:
+                        break
+                else:
+                    self.is_sequential = True
+                    self.first_index = self.block_index[0, vp0].item() * page_size + vi0 - vp0 * page_size
+                    self.cache_seqlens_after = self.cache_seqlens + q_len
+
         def get_attn_mask(self, device):
             raise NotImplementedError()
 
         def get_block_index(self, device) -> torch.Tensor:
             if self.block_index.device != device:
                 self.block_index = safe_move_tensor(self.block_index, device)
             return self.block_index
 
         def get_cache_seqlens(self, device) -> torch.Tensor:
             if self.cache_seqlens.device != device:
                 self.cache_seqlens = safe_move_tensor(self.cache_seqlens, device)
             return self.cache_seqlens
 
+        def get_cache_seqlens_after(self, device) -> torch.Tensor:
+            if self.cache_seqlens_after.device != device:
+                self.cache_seqlens_after = safe_move_tensor(self.cache_seqlens_after, device)
+            return self.cache_seqlens_after
+
 
     def __init__(self,
                  model: ExLlamaV2,
                  key: str,
                  layer_idx: int,
                  has_norm: bool = True,
                  has_residual: bool = True):
@@ -265,15 +290,15 @@
         f_c = f_b + cfg.num_key_value_heads * cfg.head_dim
         f_d = f_c + cfg.num_key_value_heads * cfg.head_dim
         f_key = (key + ".self_attn." + cfg.arch.fused_qkv_key) if cfg.arch.fused_qkv_key else None
 
         self.q_proj = ExLlamaV2Linear(model, key + ".self_attn.q_proj", hidden_size, cfg.num_attention_heads * cfg.head_dim, cfg.arch.attention_bias_qkv, f_key = f_key, f_beg = f_a, f_end = f_b)
         self.k_proj = ExLlamaV2Linear(model, key + ".self_attn.k_proj", hidden_size, cfg.num_key_value_heads * cfg.head_dim, cfg.arch.attention_bias_qkv, f_key = f_key, f_beg = f_b, f_end = f_c)
         self.v_proj = ExLlamaV2Linear(model, key + ".self_attn.v_proj", hidden_size, cfg.num_key_value_heads * cfg.head_dim, cfg.arch.attention_bias_qkv, f_key = f_key, f_beg = f_c, f_end = f_d)
-        self.o_proj = ExLlamaV2Linear(model, key + ".self_attn.o_proj", cfg.num_attention_heads * cfg.head_dim, hidden_size, cfg.arch.attention_bias_o)
+        self.o_proj = ExLlamaV2Linear(model, key + ".self_attn.o_proj", cfg.num_attention_heads * cfg.head_dim, hidden_size, cfg.arch.attention_bias_o, prescale = cfg.scale_depth)
 
         if cfg.use_qk_norm:
             self.q_norm = ExLlamaV2HeadNorm(model, key + ".self_attn.q_norm", cfg.num_attention_heads, cfg.head_dim)
             self.k_norm = ExLlamaV2HeadNorm(model, key + ".self_attn.k_norm", cfg.num_key_value_heads, cfg.head_dim)
         else:
             self.q_norm = None
             self.k_norm = None
@@ -306,16 +331,19 @@
         if self.input_layernorm is not None: numel += self.input_layernorm.numel()
         if self.q_norm is not None: numel += self.q_norm.numel()
         if self.k_norm is not None: numel += self.k_norm.numel()
 
         return numel
 
 
+    @torch.inference_mode
     def load(self):
 
+        cfg = self.model.config
+
         if self.input_layernorm is not None: self.input_layernorm.load()
         self.q_proj.load()
         self.k_proj.load()
         self.v_proj.load()
         self.o_proj.load()
         if self.q_norm is not None: self.q_norm.load()
         if self.k_norm is not None: self.k_norm.load()
@@ -327,15 +355,15 @@
             device_tensors = self.model.get_device_tensors(self.device_idx)
             device_tensors.begin_scratch_alloc()
             self.temp_state = device_tensors.get_scratch_slice(self.temp_state_size())
             # self.temp_q = device_tensors.get_scratch_slice(self.temp_q_size())
             # self.temp_k = device_tensors.get_scratch_slice(self.temp_k_size())
             # self.temp_v = device_tensors.get_scratch_slice(self.temp_v_size())
             self.temp_dq = device_tensors.get_scratch_slice(self.temp_dq_size())
-            # self.temp_kv = device_tensors.get_scratch_slice(self.temp_kv_size()) if self.model.config.num_attention_heads != self.model.config.num_key_value_heads else None
+            # self.temp_kv = device_tensors.get_scratch_slice(self.temp_kv_size()) if cfg.num_attention_heads != cfg.num_key_value_heads else None
 
             if self.has_norm:
                 norm_weight = self.input_layernorm.weight if self.input_layernorm.weight is not None else none_tensor
                 norm_bias = self.input_layernorm.bias if self.input_layernorm.bias is not None else none_tensor
                 is_rms = isinstance(self.input_layernorm, ExLlamaV2RMSNorm)
                 eps = self.input_layernorm.variance_epsilon
             else:
@@ -364,22 +392,22 @@
                 self.v_proj.q_handle,
                 self.o_proj.q_handle,
                 self.temp_state,
                 # self.temp_q,
                 # self.temp_k,
                 # self.temp_v,
                 self.temp_dq,
-                self.model.config.max_input_len * self.model.config.max_batch_size,
-                self.model.config.hidden_size,
-                self.model.config.num_attention_heads,
-                self.model.config.num_key_value_heads,
-                self.model.config.head_dim,
-                self.model.config.max_seq_len,
+                cfg.max_input_len * cfg.max_batch_size,
+                cfg.hidden_size,
+                cfg.num_attention_heads,
+                cfg.num_key_value_heads,
+                cfg.head_dim,
+                cfg.max_seq_len,
                 self.has_residual,
-                self.model.config.arch.rope_style.value,
+                cfg.arch.rope_style.value,
                 q_norm,
                 k_norm
             )
 
 
     def unload(self):
         if self.q_handle is not None:
@@ -430,59 +458,65 @@
                self.temp_dq_size() + \
                self.temp_kv_size()
                # self.temp_attn_size() +  # Accounted for separately in model.set_device_map()
 
 
     def temp_state_size(self):
 
-        return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.num_attention_heads * self.model.config.head_dim * 2 + 128
+        cfg = self.model.config
+        return cfg.max_input_len * cfg.max_batch_size * cfg.num_attention_heads * cfg.head_dim * 2 + 128
 
 
     def temp_q_size(self):
 
-        return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.num_attention_heads * self.model.config.head_dim * 2 + 128
+        cfg = self.model.config
+        return cfg.max_input_len * cfg.max_batch_size * cfg.num_attention_heads * cfg.head_dim * 2 + 128
 
 
     def temp_k_size(self):
 
-        return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.num_key_value_heads * self.model.config.head_dim * 2 + 128
+        cfg = self.model.config
+        return cfg.max_input_len * cfg.max_batch_size * cfg.num_key_value_heads * cfg.head_dim * 2 + 128
 
 
     def temp_v_size(self):
 
-        return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.num_key_value_heads * self.model.config.head_dim * 2 + 128
+        cfg = self.model.config
+        return cfg.max_input_len * cfg.max_batch_size * cfg.num_key_value_heads * cfg.head_dim * 2 + 128
 
 
     def temp_dq_size(self):
 
         return max(self.q_proj.temp_dq_size(),
                    self.k_proj.temp_dq_size(),
                    self.v_proj.temp_dq_size(),
                    self.o_proj.temp_dq_size())
 
 
     def temp_kv_size(self):
 
-        if self.model.config.num_key_value_heads == self.model.config.num_attention_heads: return 0
-        return 2 * self.model.config.max_seq_len * self.model.config.max_batch_size * self.model.config.num_attention_heads * self.model.config.head_dim * 2 + 128
+        cfg = self.model.config
+        if cfg.num_key_value_heads == cfg.num_attention_heads: return 0
+        return 2 * cfg.max_seq_len * cfg.max_batch_size * cfg.num_attention_heads * cfg.head_dim * 2 + 128
 
 
     def temp_attn_size(self):
         global has_flash_attn
         global has_xformers
 
-        att_max = min(self.model.config.max_attention_size, self.model.config.max_seq_len ** 2)
+        cfg = self.model.config
+        att_max = min(cfg.max_attention_size, cfg.max_seq_len ** 2)
 
-        if (has_flash_attn and not self.model.config.no_flash_attn) or (has_xformers and not self.model.config.no_xformers) :
+        if (has_flash_attn and not cfg.no_flash_attn) or (has_xformers and not cfg.no_xformers) :
             #in sm>=80 devices, xformers uses the same memory as flash_attn
             #todo: due to the different implementions. in sm<80 devices, xformers uses less memory than it in sm>=80. There may still be room for optimization.
-            eff = self.model.config.max_attention_size ** 0.5 / 190  # based on supposed memory savings listed in flash-attn repo + some fudging
+            eff = cfg.max_attention_size ** 0.5 / 190  # based on supposed memory savings listed in flash-attn repo + some fudging
             att_max //= eff
 
-        return 2 * att_max * self.model.config.num_attention_heads * 2 + 128
+        return 2 * att_max * cfg.num_attention_heads * 2 + 128
 
 
     def set_device_idx(self, idx):
         super().set_device_idx(idx)
 
         if self.input_layernorm is not None: self.input_layernorm.set_device_idx(idx)
         self.q_proj.set_device_idx(idx)
@@ -509,30 +543,39 @@
                       attn_params: ExLlamaV2Attention.PagedParams | None = None,
                       loras: list[ExLlamaV2Lora] | None = None,
                       **kwargs) -> torch.Tensor:
 
         is_q = self.q_handle is not None
         cfg = self.model.config
         constants = self.model.get_device_tensors(self.device_idx, scratch = is_q)
-
         page_size = attn_params.page_size
-
         batch_size, q_len, _ = hidden_states.shape
-
         cache_seqlens = attn_params.get_cache_seqlens(self.device())
         block_table = attn_params.get_block_index(self.device())
 
-        k_cache, v_cache = cache.get_kv_state(self.layer_idx, batch_size, 0, 1, page_size, cache_seqlens, block_table)
-        k_cache = k_cache.view(k_cache.shape[1] // page_size, page_size, k_cache.shape[2], k_cache.shape[3])
-        v_cache = v_cache.view(v_cache.shape[1] // page_size, page_size, v_cache.shape[2], v_cache.shape[3])
+        # TODO: We only need keys/values when preprocess_only == True, so we could skip q projection and attention.
+        #   Would need custom kernel to update paged cache if not calling flash_attn_with_kvcache
+        # skip_attn = kwargs.get("kv_only")
+
+        # TODO: Potentially we could emulate paged cache when in Q4 mode, since that requires copying the active part
+        #   of the current cache layer anyway. Test if block diagonal masking works with lower-right aligned mask.
+
+        k_cache_f, v_cache_f = cache.get_kv_state(self.layer_idx, batch_size, 0, 1, page_size, cache_seqlens, block_table)
+        k_cache = k_cache_f.view(k_cache_f.shape[1] // page_size, page_size, k_cache_f.shape[2], k_cache_f.shape[3])
+        v_cache = v_cache_f.view(v_cache_f.shape[1] // page_size, page_size, v_cache_f.shape[2], v_cache_f.shape[3])
 
         if is_q:
             q = torch.empty((batch_size, q_len, cfg.num_attention_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
-            k = torch.empty((batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
-            v = torch.empty((batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
+            if attn_params.is_sequential:
+                assert batch_size == 1
+                k = k_cache_f[:, attn_params.first_index : attn_params.first_index + q_len, :, :]
+                v = v_cache_f[:, attn_params.first_index : attn_params.first_index + q_len, :, :]
+            else:
+                k = torch.empty((batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
+                v = torch.empty((batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
 
             if loras is None or self.temp_lora_size == 0:
                 pass_loras = []
                 pass_lora_temp = none_tensor
             else:
                 pass_loras = [id(x) for x in loras]
                 pass_lora_temp = torch.empty((self.temp_lora_size,), dtype = torch.half, device = hidden_states.device)
@@ -572,22 +615,34 @@
                         constants.cos,
                         0,
                         heads,
                         cfg.head_dim,
                         attn_params.get_cache_seqlens(self.device()),
                         cfg.arch.rope_style == RopeStyle.NEOX
                     )
+            if attn_params.is_sequential:
+                k_ = k_cache_f[:, attn_params.first_index : attn_params.first_index + q_len, :, :]
+                v_ = v_cache_f[:, attn_params.first_index : attn_params.first_index + q_len, :, :]
+                k_.copy_(k)
+                v_.copy_(v)
+
+        if attn_params.is_sequential:
+            k = None
+            v = None
+            cache_seqlens_a = attn_params.get_cache_seqlens_after(self.device())
+        else:
+            cache_seqlens_a = cache_seqlens
 
         attn_output = flash_attn_with_kvcache(
             q = q,
             k = k,
             v = v,
             k_cache = k_cache,
             v_cache = v_cache,
-            cache_seqlens = cache_seqlens,
+            cache_seqlens = cache_seqlens_a,
             block_table = block_table,
             causal = True
         )
         attn_output = attn_output.view((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
 
         cache.store_kv_state(self.layer_idx, batch_size, 0, q_len, page_size, cache_seqlens, block_table)
```

## exllamav2/cache.py

```diff
@@ -350,14 +350,29 @@
                  copy_from: ExLlamaV2Cache_Q4 | None = None,
                  lazy: bool = False):
 
         super().__init__(model, batch_size, max_seq_len, torch.uint8, 2, True)
 
         self.create_state_tensors(copy_from, lazy)
 
+        # Models with small key/value dims need to to quantize/dequantize in multi-token blocks
+
+        Q_CACHE_BLOCKSIZE_Q = 512
+        kv_dim = model.config.num_key_value_heads * model.config.head_dim
+        if kv_dim < Q_CACHE_BLOCKSIZE_Q:
+            self.q_block = Q_CACHE_BLOCKSIZE_Q // kv_dim
+            assert self.q_block * kv_dim == Q_CACHE_BLOCKSIZE_Q, \
+                f"Cannot create Q4 cache. " + \
+                f"{Q_CACHE_BLOCKSIZE_Q} does not split into blocks of num_key_value_heads * head_dim"
+        else:
+            self.q_block = 1
+            assert kv_dim % Q_CACHE_BLOCKSIZE_Q == 0, \
+                f"Cannot create Q4 cache. " + \
+                f"num_key_value_heads * head_dim does not split into blocks of {Q_CACHE_BLOCKSIZE_Q}"
+
         # Create temp FP16 tensors for accessing Q4 layers
 
         self.temp_tensors = {}
         if not lazy:
             for device in self.model.get_cache_devices(): self.touch_device(device)
 
 
@@ -376,52 +391,76 @@
                      width: int,
                      page_size: int = 0,
                      cache_seqlens: torch.Tensor | None = None,
                      block_table: torch.Tensor | None = None) -> (torch.Tensor, torch.Tensor):
 
         device = self.model.cache_map[layer_idx]
         temp_key_state, temp_value_state = self.temp_tensors[device]
-        if width > 0: ext_c.q4_to_fp16_kv(self.key_states[layer_idx],
-                                          temp_key_state,
-                                          self.key_scales[layer_idx],
-                                          self.value_states[layer_idx],
-                                          temp_value_state,
-                                          self.value_scales[layer_idx],
-                                          batch_size,
-                                          offset,
-                                          width,
-                                          page_size,
-                                          cache_seqlens if cache_seqlens is not None else none_tensor,
-                                          block_table if block_table is not None else none_tensor)
+        if width == 0: return temp_key_state, temp_value_state
+
+        if self.q_block > 1 and not page_size:
+            a = offset
+            b = offset + width
+            a = a // self.q_block * self.q_block
+            b = (b + self.q_block - 1) // self.q_block * self.q_block
+            offset = a
+            width = b - a
+
+        ext_c.q4_to_fp16_kv(
+            self.key_states[layer_idx],
+            temp_key_state,
+            self.key_scales[layer_idx],
+            self.value_states[layer_idx],
+            temp_value_state,
+            self.value_scales[layer_idx],
+            batch_size,
+            offset,
+            width,
+            page_size,
+            cache_seqlens if cache_seqlens is not None else none_tensor,
+            block_table if block_table is not None else none_tensor
+        )
         return temp_key_state, temp_value_state
 
 
     def store_kv_state(self,
                        layer_idx: int,
                        batch_size: int,
                        offset: int,
                        width: int,
                        page_size: int = 0,
                        cache_seqlens: torch.Tensor | None = None,
                        block_table: torch.Tensor | None = None):
 
+        if width == 0: return
+
+        if self.q_block > 1 and not page_size:
+            a = offset
+            b = offset + width
+            a = a // self.q_block * self.q_block
+            b = (b + self.q_block - 1) // self.q_block * self.q_block
+            offset = a
+            width = b - a
+
         device = self.model.cache_map[layer_idx]
         temp_key_state, temp_value_state = self.temp_tensors[device]
-        if width > 0: ext_c.fp16_to_q4_kv(temp_key_state,
-                                          self.key_states[layer_idx],
-                                          self.key_scales[layer_idx],
-                                          temp_value_state,
-                                          self.value_states[layer_idx],
-                                          self.value_scales[layer_idx],
-                                          batch_size,
-                                          offset,
-                                          width,
-                                          page_size,
-                                          cache_seqlens if cache_seqlens is not None else none_tensor,
-                                          block_table if block_table is not None else none_tensor)
+        ext_c.fp16_to_q4_kv(
+            temp_key_state,
+            self.key_states[layer_idx],
+            self.key_scales[layer_idx],
+            temp_value_state,
+            self.value_states[layer_idx],
+            self.value_scales[layer_idx],
+            batch_size,
+            offset,
+            width,
+            page_size,
+            cache_seqlens if cache_seqlens is not None else none_tensor,
+            block_table if block_table is not None else none_tensor
+        )
 
 
     def footprint(self) -> list[int]:
 
         fp = []
         for layer in self.key_states + self.value_states:
             dev = layer.device.index
```

## exllamav2/config.py

```diff
@@ -1,15 +1,16 @@
 from __future__ import annotations
+
 import torch
+import math
 from exllamav2.fasttensors import STFile
 from exllamav2.architecture import ExLlamaV2ArchParams
 import os, glob, json
 from typing import Any, Dict, List, TypeVar, Union, cast
 
-
 T = TypeVar('T')
 no_default = object()
 
 def read(input_dict: dict[str, Any], expected_type: type, keys: str | list[str], default = no_default) -> T:
 
     if isinstance(keys, str): keys = [keys]
 
@@ -91,14 +92,16 @@
     scale_short_factor: list[float] | None
     alt_rope_method: str | None
     original_max_seq_len: int
     head_dim: int
     num_experts: int | None
     num_experts_per_token: int | None
     logit_scale: float
+    scale_depth: float
+    scale_emb: float
     use_qk_norm: bool
 
     checkpoint_fused_mlp: bool
 
 
     def __init__(self,
                  model_dir: str | None = None):
@@ -220,17 +223,27 @@
         else:
             default_intermediate_size = no_default
 
         self.intermediate_size = read(read_config, int, ["intermediate_size", "ffn_config->ffn_hidden_size", "n_inner"], default_intermediate_size)
         self.num_experts = read(read_config, int, ["num_local_experts", "ffn_config->moe_num_experts"], None)
         self.num_experts_per_token = read(read_config, int,["num_experts_per_tok", "ffn_config->moe_top_k"], None)
 
-        # Logit scale
+        # Logit/embedding/residual scale
 
         self.logit_scale = read(read_config, float, "logit_scale", 1)
+        if self.arch.logit_scale_basedim:
+            dim_model_base = read(read_config, int, "dim_model_base", self.hidden_size)
+            self.logit_scale /= (self.hidden_size / dim_model_base)
+
+        self.scale_emb = read(read_config, float, "scale_emb", 1)
+        scale_depth = read(read_config, float, "scale_depth", None)
+        if scale_depth is None:
+            self.scale_depth = 1
+        else:
+            self.scale_depth = scale_depth / math.sqrt(self.num_hidden_layers)
 
         # Positional embeddings
 
         self.rotary_embedding_base = read(read_config, float, ["rope_theta", "attn_config->rope_theta"], 10000.0)
 
         self.max_seq_len = read(read_config, int,["max_sequence_length",
                                                   "model_max_length",
```

## exllamav2/embedding.py

```diff
@@ -24,38 +24,45 @@
                  key: str):
         super().__init__(model, key)
 
         self.native_vocab_size = None
         self.embedding = None
 
 
+    @torch.inference_mode
     def load(self):
 
         vocab_size = self.model.config.vocab_size
         hidden_size = self.model.config.hidden_size
         pad_token_id = self.model.config.pad_token_id
 
         w = self.load_weight()
         assert isinstance(w, nn.Parameter)
-        w.pin_memory()
+        # TODO: Figure out why pinning this tensor allocates GPU memory??
+        # w.pin_memory()
         self.native_vocab_size = w.shape[0]
 
         self.embedding = nn.Embedding(vocab_size, hidden_size, pad_token_id, device = "meta")
+        if self.model.config.scale_emb != 1:
+            w *= self.model.config.scale_emb
         self.embedding.weight = w
 
 
     def unload(self):
 
         del self.embedding
         self.embedding = None
 
 
     def get_weight(self) -> torch.Tensor:
 
-        return self.embedding.weight.data
+        if self.model.config.scale_emb != 1:
+            return self.embedding.weight.data / self.model.config.scale_emb
+        else:
+            return self.embedding.weight.data
 
 
     def weight_footprint(self) -> int:
 
         vocab_size = self.model.config.vocab_size
         hidden_size = self.model.config.hidden_size
         # kv_size = self.model.config.num_key_value_heads * self.model.config.head_dim
```

## exllamav2/ext.py

```diff
@@ -122,15 +122,15 @@
             # Possible locations for MSVC, in order of preference
 
             program_files_x64 = os.environ["ProgramW6432"]
             program_files_x86 = os.environ["ProgramFiles(x86)"]
 
             msvc_dirs = \
             [
-                a + "\\Microsoft Visual Studio\\" + b + "\\" + c + "\\VC\Tools\\MSVC\\"
+                a + "\\Microsoft Visual Studio\\" + b + "\\" + c + "\\VC\\Tools\\MSVC\\"
                 for b in ["2022", "2019", "2017"]
                 for a in [program_files_x64, program_files_x86]
                 for c in ["BuildTools", "Community", "Professional", "Enterprise", "Preview"]
             ]
 
             for msvc_dir in msvc_dirs:
                 if not os.path.exists(msvc_dir): continue
```

## exllamav2/headnorm.py

```diff
@@ -33,14 +33,15 @@
         self.bias = None
         self.variance_epsilon = self.model.config.norm_eps
 
         self.head_dim = head_dim
         self.num_heads = num_heads
 
 
+    @torch.inference_mode
     def load(self):
 
         w = self.load_weight()
 
         if isinstance(w, tuple):
             weight = w[0]
             bias = w[1]
```

## exllamav2/layernorm.py

```diff
@@ -25,14 +25,15 @@
 
         self.layernorm = None
         self.weight = None
         self.bias = None
         self.variance_epsilon = 1e-6
 
 
+    @torch.inference_mode
     def load(self):
 
         w = self.load_weight()
 
         if isinstance(w, tuple):
             weight = w[0]
             bias = w[1]
```

## exllamav2/linear.py

```diff
@@ -85,14 +85,15 @@
         self.f_key = f_key
         self.f_beg = f_beg
         self.f_end = f_end
 
         self.assumed_footprint = in_features * (out_features + self.padding) * 2 + 128
 
 
+    @torch.inference_mode
     def load(self,
              w: dict | nn.Parameter | tuple | None = None,
              device_tensors: bool = True):
 
         if self.f_key: w = self.load_weight_fused(self.f_key, self.f_beg, self.f_end, self.in_features, self.out_features)
         if w is None: w = self.load_weight()
```

## exllamav2/mlp.py

```diff
@@ -58,15 +58,15 @@
                 self.post_attention_layernorm = ExLlamaV2LayerNorm(model, key + cfg.arch.norm_key_2)
             elif cfg.arch.norm == "rmsnorm":
                 self.post_attention_layernorm = ExLlamaV2RMSNorm(model, key + cfg.arch.norm_key_2)
         else:
             self.post_attention_layernorm = None
 
         self.up_proj = ExLlamaV2Linear(model, key + cfg.arch.mlp_key_up, cfg.hidden_size, cfg.intermediate_size, self.model.config.arch.mlp_bias, f_key = f_key, f_beg = f_b, f_end = f_c)
-        self.down_proj = ExLlamaV2Linear(model, key + cfg.arch.mlp_key_down, cfg.intermediate_size, cfg.hidden_size, self.model.config.arch.mlp_bias)
+        self.down_proj = ExLlamaV2Linear(model, key + cfg.arch.mlp_key_down, cfg.intermediate_size, cfg.hidden_size, self.model.config.arch.mlp_bias, prescale = cfg.scale_depth)
 
         self.submodules = [self.up_proj,
                            self.down_proj]
         if self.has_norm:
             self.submodules += [self.post_attention_layernorm]
 
         if cfg.arch.mlp_gate:
@@ -86,14 +86,15 @@
 
         if self.post_attention_layernorm is not None:
             numel += self.post_attention_layernorm.numel()
 
         return numel
 
 
+    @torch.inference_mode
     def load(self):
 
         cfg = self.model.config
 
         if self.post_attention_layernorm is not None:
             self.post_attention_layernorm.load()
 
@@ -177,34 +178,38 @@
                self.temp_a_size() + \
                self.temp_b_size() + \
                self.temp_dq_size()
 
 
     def scratch_space(self) -> int:
 
-        assert self.model.config.intermediate_size >= self.model.config.hidden_size
+        cfg = self.model.config
+        assert cfg.intermediate_size >= cfg.hidden_size
         return self.temp_state_size() + \
                self.temp_a_size() + \
                self.temp_b_size() + \
                self.temp_dq_size()
 
 
     def temp_state_size(self) -> int:
 
-        return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.hidden_size * 2 + 128
+        cfg = self.model.config
+        return cfg.max_input_len * cfg.max_batch_size * cfg.hidden_size * 2 + 128
 
 
     def temp_a_size(self) -> int:
 
-        return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.intermediate_size * 2 + 128
+        cfg = self.model.config
+        return cfg.max_input_len * cfg.max_batch_size * cfg.intermediate_size * 2 + 128
 
 
     def temp_b_size(self) -> int:
 
-        return self.model.config.max_input_len * self.model.config.max_batch_size * self.model.config.intermediate_size * 2 + 128
+        cfg = self.model.config
+        return cfg.max_input_len * cfg.max_batch_size * cfg.intermediate_size * 2 + 128
 
 
     def temp_dq_size(self) -> int:
 
         return max(0 if self.gate_proj is None else self.gate_proj.temp_dq_size(),
                    self.up_proj.temp_dq_size(),
                    self.down_proj.temp_dq_size())
@@ -252,32 +257,34 @@
                       cache = None,
                       attn_params = None,
                       past_len = None,
                       intermediates: bool = False,
                       loras: list[ExLlamaV2Lora] | None = None,
                       **kwargs) -> torch.Tensor | dict[str: torch.Tensor]:
 
+        cfg = self.model.config
+
         residual = hidden_states
         post_norm = self.post_attention_layernorm.forward(hidden_states) \
             if self.has_norm else hidden_states
 
         if self.gate_proj is not None:
             gate = self.gate_proj.forward(post_norm, loras = loras)
-            if self.model.config.arch.mlp_act_func == "silu":
+            if cfg.arch.mlp_act_func == "silu":
                 y = F.silu(gate)
-            elif self.model.config.arch.mlp_act_func == "gelu":
+            elif cfg.arch.mlp_act_func == "gelu":
                 y = F.gelu(gate)
             up = self.up_proj.forward(post_norm, loras = loras)
             y *= up
             y.clamp_(min = -65504.0, max = 65504.0)
         else:
             up = self.up_proj.forward(post_norm, loras = loras)
-            if self.model.config.arch.mlp_act_func == "silu":
+            if cfg.arch.mlp_act_func == "silu":
                 y = F.silu(up)
-            elif self.model.config.arch.mlp_act_func == "gelu":
+            elif cfg.arch.mlp_act_func == "gelu":
                 y = F.gelu(up)
 
         down = self.down_proj.forward(y, loras = loras)
         hidden_states = down + residual if self.has_residual else down
 
         if intermediates:
             return {"post_norm": post_norm,
```

## exllamav2/model_init.py

```diff
@@ -69,15 +69,17 @@
 
 def init(args,
          quiet: bool = False,
          allow_auto_split: bool = False,
          skip_load: bool = False,
          benchmark: bool = False,
          max_batch_size: int = None,
-         max_output_len: int = None):
+         max_input_len: int = None,
+         max_output_len: int = None,
+         progress: bool = False):
 
     # Create config
 
     config = ExLlamaV2Config()
     config.model_dir = args.model_dir
     config.fasttensors = hasattr(args, "fast_safetensors") and args.fast_safetensors
     config.prepare()
@@ -89,14 +91,15 @@
     if args.rope_alpha: config.scale_alpha_value = args.rope_alpha
     config.no_flash_attn = args.no_flash_attn
     config.no_xformers = args.no_xformers
     if args.experts_per_token: config.num_experts_per_token = args.experts_per_token
 
     if max_batch_size: config.max_batch_size = max_batch_size
     config.max_output_len = max_output_len
+    if max_input_len: config.max_input_len = max_input_len
 
     # Set low-mem options
 
     if args.low_mem: config.set_low_mem()
     if args.load_q4: config.load_in_q4 = True
 
     # Load model
@@ -106,17 +109,17 @@
     model = ExLlamaV2(config)
 
     split = None
     if args.gpu_split and args.gpu_split != "auto":
         split = [float(alloc) for alloc in args.gpu_split.split(",")]
 
     if args.gpu_split != "auto" and not skip_load:
-        if not quiet: print(" -- Loading model...")
+        if not quiet and not progress: print(" -- Loading model...")
         t = time.time()
-        model.load(split)
+        model.load(split, progress = progress)
         t = time.time() - t
         if benchmark and not quiet:
             print(f" -- Loaded model in {t:.4f} seconds")
     else:
         assert allow_auto_split, "Auto split not allowed."
 
     # Load tokenizer
```

## exllamav2/moe_mlp.py

```diff
@@ -89,14 +89,15 @@
 
 
     def numel(self) -> int:
 
         return sum(l.numel() for l in self.w1 + self.w2 + self.w3)
 
 
+    @torch.inference_mode
     def load(self):
 
         self.post_attention_layernorm.load()
         self.gate.load()
         for e in range(self.num_experts):
             self.w1[e].load()
             self.w2[e].load()
```

## exllamav2/parallel_decoder.py

```diff
@@ -46,14 +46,15 @@
     def numel(self) -> int:
 
         return self.attn.numel() + \
                self.mlp.numel() + \
                self.input_layernorm.numel()
 
 
+    @torch.inference_mode
     def load(self):
 
         self.input_layernorm.load()
         self.attn.load()
         self.mlp.load()
```

## exllamav2/pos_embedding.py

```diff
@@ -22,14 +22,15 @@
                  key: str):
         super().__init__(model, key)
 
         self.native_ctx_size = model.config.max_seq_len
         self.embedding = None
 
 
+    @torch.inference_mode
     def load(self):
 
         w = self.load_weight()
         assert isinstance(w, nn.Parameter)
         self.native_ctx_size = w.shape[0]
         assert self.model.config.max_seq_len <= self.native_ctx_size, \
             f"Learned positional embeddings cannot be extended past native size of {self.native_ctx_size}."
@@ -81,24 +82,27 @@
             hidden_states[:] += emb_slice
 
         else:
 
             bsz, q_len, dim = hidden_states.shape
             for b in range(bsz):
 
-                if attn_params.past_len is not None:
-                    past_len = attn_params.past_len
-                else:
-                    assert attn_params.past_lens is not None
-                    past_len = attn_params.past_lens[b]
-
-                if attn_params.position_offsets is not None:
-                    offset = attn_params.position_offsets[b].item()
-                else:
+                if isinstance(attn_params, ExLlamaV2Attention.PagedParams):
+                    past_len = attn_params.cache_seqlens[b]
                     offset = 0
+                else:
+                    if attn_params.past_len is not None:
+                        past_len = attn_params.past_len
+                    else:
+                        assert attn_params.past_lens is not None
+                        past_len = attn_params.past_lens[b]
+                    if attn_params.position_offsets is not None:
+                        offset = attn_params.position_offsets[b].item()
+                    else:
+                        offset = 0
 
                 slice_a = past_len + offset
                 slice_b = past_len + q_len + offset
                 assert slice_b > 0
                 target_a = 0
                 target_b = q_len
```

## exllamav2/rmsnorm.py

```diff
@@ -21,17 +21,17 @@
         super().__init__(model, key)
 
         self.weight = None
         self.bias = None
         self.variance_epsilon = 1e-6
 
 
+    @torch.inference_mode
     def load(self):
 
-
         w = self.load_weight()
 
         if isinstance(w, tuple):
             self.weight = w[0]
             self.bias = w[1]
         else:
             self.weight = w
```

## exllamav2/version.py

```diff
@@ -1 +1 @@
-__version__ = "0.1.1"
+__version__ = "0.1.2"
```

## exllamav2/exllamav2_ext/config.h

```diff
@@ -11,8 +11,11 @@
 #define QMODE_5BIT 1
 #define QMODE_6BIT 1
 #define QMODE_8BIT 0
 
 #define USE_AVX2
 //#define PROFILING
 
+#define Q_CACHE_BLOCKSIZE_Q 512
+#define Q_CACHE_SUPER_BLOCKSIZE_Q (128 * 1024)
+
 #endif
```

## exllamav2/exllamav2_ext/ext_cache.cpp

```diff
@@ -109,15 +109,15 @@
     if (page_size)
     {
         int dim = k_in.size(2) * k_in.size(3);
         batch_size = block_table.size(0);
         int pages_per_seq = block_table.size(1);
 
         TORCH_CHECK_SHAPES(cache_seqlens, 0, block_table, 0, 1);
-        TORCH_CHECK(dim % 256 == 0, "(num_kv_heads * head_dim) must be divisible by 256");
+//        TORCH_CHECK(dim % 256 == 0, "(num_kv_heads * head_dim) must be divisible by 256");
 
         array_fp16_to_q4_kv_paged_cuda
         (
             (const half*) k_in.data_ptr(),
             (unsigned char*) k_out.data_ptr(),
             (half*) k_scales.data_ptr(),
             (const half*) v_in.data_ptr(),
@@ -193,15 +193,15 @@
     if (page_size)
     {
         int dim = k_out.size(2) * k_out.size(3);
         batch_size = block_table.size(0);
         int pages_per_seq = block_table.size(1);
 
         TORCH_CHECK_SHAPES(cache_seqlens, 0, block_table, 0, 1);
-        TORCH_CHECK(dim % 256 == 0, "(num_kv_heads * head_dim) must be divisible by 256");
+//        TORCH_CHECK(dim % 256 == 0, "(num_kv_heads * head_dim) must be divisible by 256");
 
         array_q4_to_fp16_kv_paged_cuda
         (
             (const unsigned char*) k_in.data_ptr(),
             (const half*) k_scales.data_ptr(),
             (half*) k_out.data_ptr(),
             (const unsigned char*) v_in.data_ptr(),
```

## exllamav2/exllamav2_ext/cuda/cache.cu

```diff
@@ -1,16 +1,17 @@
 #include "cache.cuh"
 
 #include "quant/qdq_util.cuh"
 #include "util.cuh"
 #include "compat.cuh"
+#include "../config.h"
 
 #define THREADS 32
-#define BLOCKSIZE_Q 512
-#define SUPER_BLOCKSIZE_Q (128 * 1024)
+#define BLOCKSIZE_Q Q_CACHE_BLOCKSIZE_Q
+#define SUPER_BLOCKSIZE_Q Q_CACHE_SUPER_BLOCKSIZE_Q
 #define THREADS_Q (BLOCKSIZE_Q / 2)
 #define HADAMARD_Q4
 
 // The upper 8 bits of FP16 are equivalent to FP8 E5M2.
 //
 // The range of values typically cached seem to be in the range of +/- 16, with an exponent component (with bias) up to
 // about 20. Empirically, the MSE over the whole range of observed values in the K/V cache works out the same for E4M3
@@ -216,14 +217,23 @@
     int y = blockIdx.z >> 1;
 
     int page = block_table[pages_per_seq * y + x];
     int seqlen = cache_seqlens[y];
     int vx_a = page_size * x;
     int px_a = seqlen - vx_a;
     int px_b = px_a + q_len;
+
+    if (dim < BLOCKSIZE_Q)
+    {
+        int g = BLOCKSIZE_Q / dim;
+//        if (px_a > 0) DBGI4(px_a, px_b, px_a / g * g, DIVIDE(px_b, g) * g);
+        px_a = px_a / g * g;
+        px_b = DIVIDE(px_b, g) * g;
+    }
+
     px_a = max(px_a, 0);
     px_b = min(px_b, page_size);
 
     int block_a = (page * page_size + px_a) * dim;
     int block_b = (page * page_size + px_b) * dim;
 
     for (int i = block_a; i < block_b; i += SUPER_BLOCKSIZE_Q)
@@ -411,16 +421,27 @@
     const half* scales = kv ? v_scales : k_scales;
     half* out = kv ? v_out : k_out;
 
     int x = blockIdx.x;
     int y = blockIdx.z >> 1;
     int page = block_table[pages_per_seq * y + x];
     int seqlen = cache_seqlens[y];
+    if (!seqlen) return;
+
     int vx_a = page_size * x;
     int vx_b = min(vx_a + page_size, seqlen);
+
+    if (dim < BLOCKSIZE_Q)
+    {
+        int g = BLOCKSIZE_Q / dim;
+//        if (vx_a > 0) DBGI4(vx_a, vx_b, vx_a / g * g, DIVIDE(vx_b, g) * g);
+        vx_a = vx_a / g * g;
+        vx_b = DIVIDE(vx_b, g) * g;
+    }
+
     int vnum = max(vx_b - vx_a, 0);
     int block_a = (page * page_size) * dim;
     int block_b = (page * page_size + vnum) * dim;
 
     for (int i = block_a; i < block_b; i += SUPER_BLOCKSIZE_Q)
     {
         int j = i + blockIdx.y * BLOCKSIZE_Q;
```

## exllamav2/generator/dynamic.py

```diff
@@ -11,17 +11,17 @@
 import torch
 import random
 import numpy as np
 import time
 import threading
 import pprint
 from collections import deque
-import queue
 import hashlib
 from dataclasses import dataclass
+# import xxhash
 # from line_profiler import profile
 
 # TODO:
 #  - ExLlamaV2StreamingGenerator wrapper
 #  - Input embeddings
 #  - Faster hash algorithm (Murmur?)
 
@@ -30,14 +30,25 @@
 def _tensor_blake2b_checksum(tensor: torch.Tensor, prev_hash: bytes | None) -> bytes:
     hasher = hashlib.blake2b(digest_size = 16)
     if prev_hash is not None:
         hasher.update(prev_hash)
     hasher.update(tensor.numpy().tobytes())
     return hasher.digest()
 
+# xxhasher = xxhash.xxh128()
+# def _tensor_xxhash128_checksum(tensor: torch.Tensor, prev_hash: bytes | None) -> bytes:
+#     global xxhasher
+#     xxhasher.reset()
+#     if prev_hash is not None:
+#         xxhasher.update(prev_hash)
+#     xxhasher.update(tensor.numpy().tobytes())
+#     return xxhasher.digest()
+
+_tensor_hash_checksum = _tensor_blake2b_checksum
+
 _uniquehash = 0
 def _randomhash():
     global _uniquehash
     _uniquehash += 1
     return _uniquehash.to_bytes(16, byteorder = 'big')
 
 @dataclass
@@ -315,14 +326,16 @@
                 draft_cache.batch_size == cache.batch_size, \
                 "Cache and draft cache must be same dimensions"
             assert draft_model.config.max_seq_len >= model.config.max_seq_len, \
                 "Draft model seq len must be >= model seq len"
 
         if paged:
             assert_paged_attn()
+            assert not cfg.no_flash_attn, \
+                "Paged mode requires flash-attn, but flash-attn is disabled in model config."
 
         assert not isinstance(cache, ExLlamaV2Cache_8bit), \
             "Dynamic generator does not currently work with 8-bit cache. Use either FP16 or Q4."
 
         model_max_q = cfg.max_batch_size * cfg.max_input_len
         req_max_q = max_q_size * max_batch_size
         assert req_max_q <= model_max_q, \
@@ -337,15 +350,15 @@
             self.max_seq_len = max_seq_len
         else:
             self.max_seq_len = model.config.max_seq_len
 
         # Initialize cache/page table
 
         self.paged = paged
-        self.page_size = PAGED_PAGE_SIZE if paged else self.max_seq_len
+        self.page_size = PAGED_PAGE_SIZE if paged else self.cache.max_seq_len
 
         assert cache.batch_size == 1, \
             f"DynamicGenerator requires cache to have batch_size = 1"
 
         assert self.cache.max_seq_len % PAGED_PAGE_SIZE == 0, \
             f"cache.max_seq_len must be multiple of {PAGED_PAGE_SIZE}, received {cache.max_seq_len}"
         self.max_pages = max(cache.max_seq_len // self.page_size, 1)
@@ -474,14 +487,15 @@
         decode_special_tokens: bool = False,
         stop_conditions: list[int | str] | None = None,
         add_bos: bool = False,
         abort_event: threading.Event | None = None,
         completion_only: bool = False,
         filters: list[list[ExLlamaV2Filter]] | list[ExLlamaV2Filter] | None = None,
         filter_prefer_eos: bool = False,
+        return_last_results: bool = False,
         **kwargs
     ):
         """
         Generate one or more completions.
 
         :param prompt:
             If this argument is a list, its length determines the batch size, and the output will be a list of strings
@@ -529,16 +543,20 @@
         :param filters:
             (List of) list of ExLlamaV2Filters to apply during generation. Each prompt in a batch needs
             its own filter list, or a value of None to disable filters for individual prompts.
 
         :param filter_prefer_eos:
             If True, always sample the tokenizer's defined EOS token as soon as it's allowed by the filters
 
+        :param return_last_results:
+            If True, returns the last results dict for each job
+
         :return:
-            Completion(s) (str or list[str] depending on the type of the input prompt argument)
+            Completion(s): (str or list[str] depending on the type of the input prompt argument)
+            Optionally, last results: (dict or list[dict] depending on the type of the input prompt argument)
         """
 
         order = {}
         if isinstance(prompt, list):
             prompts = prompt
         else:
             prompts = [prompt]
@@ -589,35 +607,42 @@
 
             serial = self.enqueue(job)
             order[serial] = idx
 
         # Collect outputs until all jobs finish
 
         completions = [""] * batch_size
+        last_results = [None] * batch_size
 
         while self.num_remaining_jobs():
             results = self.iterate()
             for r in results:
                 idx = order[r["serial"]]
                 if r["stage"] == "streaming":
                     text = r.get("text", "")
                     completions[idx] += text
+                if r["eos"]:
+                    last_results[idx] = r
             if abort_event is not None and abort_event.is_set():
                 self.clear_queue()
                 return None
 
         # Return results
 
         if not completion_only:
             completions = [(p if isinstance(p, str) else p[0]) + c for p, c in zip(prompts, completions)]
 
-        if isinstance(prompt, list):
-            return completions
+        if not isinstance(prompt, list):
+            completions = completions[0]
+            last_results = last_results[0]
+
+        if return_last_results:
+            return completions, last_results
         else:
-            return completions[0]
+            return completions
 
 
     def print_page_list(self, short: bool = True):
         for cp in self.all_pages:
             if cp.phash in self.referenced_pages:
                 assert cp.ref_count > 0
                 ref = str(cp.ref_count) if cp.ref_count < 10 else "+"
@@ -682,15 +707,15 @@
             #         for page in seq.allocated_pages:
             #             spos2 = min(spos + self.page_size, seq.kv_position)
             #             ids = seq.sequence_ids.torch()[:, spos:spos2]
             #             assert page.kv_position >= ids.shape[-1]
             #             if ids.shape[-1] > 0:
             #                 assert page.prev_hash == prev_hash, "bad prev_hash " + str(job) + " -> " + str(page)
             #             if ids.shape[-1] == self.page_size:
-            #                 phash = _tensor_blake2b_checksum(ids, prev_hash)
+            #                 phash = _tensor_hash_checksum(ids, prev_hash)
             #                 assert page.phash == phash, "bad phash " + str(job) + " -> " + str(page)
             #                 prev_hash = phash
             #             spos = spos2
 
 
         except Exception as ex:
             print(ex)
@@ -751,19 +776,21 @@
 
         # assert all(
         #     cache_seqlens[i].item() + q_len <= block_index.shape[-1] * self.page_size
         #     for i in range(batch_size)
         # )
 
         if self.paged:
+
             return ExLlamaV2Attention.PagedParams(
                 batch_size = batch_size,
                 block_index = block_index,
                 cache_seqlens = cache_seqlens,
-                page_size = self.page_size
+                page_size = self.page_size,
+                q_len = q_len
             )
         else:
             assert cache_seqlens.shape[0] == 1
             return ExLlamaV2Attention.Params(
                 batch_size = 1,
                 seq_len = q_len,
                 past_len = cache_seqlens[0].item()
@@ -779,24 +806,26 @@
 
             # Job has started
             {
                 "job": ExLlamaV2DynamicJob  - reference to job
                 "stage": "started"
                 "identifier":  - optional identifier
                 "serial": int  - job serial number
+                "eos": bool  - always False at this stage
             }
 
             # Prefill is underway
             {
                 "job": ExLlamaV2DynamicJob  - reference to job
                 "stage": "prefill"
                 "curr_progress": int  - prompt tokens ingested so far
                 "max_progress": int  - total prompt tokens to ingest
                 "identifier":  - optional identifier
                 "serial": int   - job serial number
+                "eos": bool  - always False at this stage
             }
 
             # Generation is underway
             {
                 "job": ExLlamaV2DynamicJob  - reference to job
                 "stage": "streaming"
                 "identifier":  - optional identifier
@@ -1602,15 +1631,15 @@
                 else:
                     last_hash = None
 
                 page_ids = seq.sequence_ids.torch_slice(page_before * page_size, page_after * page_size)
                 # assert page.sequence.shape[-1] == self.generator.page_size
                 # assert torch.all(page_ids == page.sequence)
                 # assert page_ids.shape[-1] == self.generator.page_size
-                new_hash = _tensor_blake2b_checksum(page_ids, last_hash)
+                new_hash = _tensor_hash_checksum(page_ids, last_hash)
 
                 # If another referenced page has the same hash, switch to referencing that instead
 
                 if new_hash in self.generator.referenced_pages:
                     new_serial = page.access_serial
                     page.sub_ref()
                     page = self.generator.referenced_pages[new_hash]
@@ -1627,14 +1656,17 @@
                         up = self.generator.unreferenced_pages[new_hash]
                         up.clear()
 
                     # Update the hash
 
                     page.update_hash(new_hash)
 
+                # if page_after >= len(seq.allocated_pages):
+                #     pass
+
                 page = seq.allocated_pages[page_after]
                 page.prev_hash = new_hash
                 page.can_revert = False
 
         # Stream output
 
         def emit(
@@ -1657,26 +1689,26 @@
 
             if emit_held:
                 if self.held_text != "":
                     self.full_completion += self.held_text
                     r.update({ "text": self.held_text })
                     self.held_text = ""
                 if self.held_tokens:
-                    r.update({ "token_ids": self.held_tokens.torch() })
+                    r.update({ "token_ids": self.held_tokens.torch().clone() })
                     self.held_tokens.clear()
                 if self.held_probs:
-                    r.update({ "token_probs": self.held_probs.torch() })
+                    r.update({ "token_probs": self.held_probs.torch().clone() })
                     self.held_probs.clear()
                 if self.held_k_tokens:
-                    r.update({ "top_k_tokens": self.held_k_tokens.torch() })
-                    r.update({ "top_k_probs": self.held_k_probs.torch() })
+                    r.update({ "top_k_tokens": self.held_k_tokens.torch().clone() })
+                    r.update({ "top_k_probs": self.held_k_probs.torch().clone() })
                     self.held_k_tokens.clear()
                     self.held_k_probs.clear()
                 if self.held_logits:
-                    r.update({ "logits": self.held_logits.torch() })
+                    r.update({ "logits": self.held_logits.torch().clone() })
                     self.held_logits.clear()
 
             if suppressed_text:
                 r.update({ "suppressed_text": suppressed_text })
                 r.update({ "suppressed_tokens": suppressed_tokens.torch() })
 
             if emit_eos:
@@ -1849,28 +1881,33 @@
         all_unique_pages = 0
 
         for seq in self.sequences:
 
             seq.page_hashes = []
 
             max_len = len(seq.sequence_ids) + self.max_new_tokens
+            if self.prefix_token:
+                max_len += 1
             context_pages = (len(seq.sequence_ids) - 1) // page_size
             total_pages = (max_len + page_size - 1) // page_size
 
             r_hash = None
             for i in range(context_pages):
                 page_ids = seq.sequence_ids.torch_slice(i * page_size, (i + 1) * page_size)
                 assert page_ids.shape[-1] == self.generator.page_size
-                r_hash = _tensor_blake2b_checksum(page_ids, r_hash)
+                r_hash = _tensor_hash_checksum(page_ids, r_hash)
                 seq.page_hashes.append(r_hash)
                 all_unique_hashes.add(r_hash)
 
             seq.new_unique_pages = total_pages - context_pages
             all_unique_pages += seq.new_unique_pages
 
+            # seq.context_pages = context_pages
+            # seq.total_pages = total_pages
+
         self.all_unique_hashes = list(all_unique_hashes)
 
         # Make sure the request can potentially fit
 
         total_pages = len(self.all_unique_hashes) + seq.new_unique_pages
         assert total_pages <= self.generator.max_pages, \
             f"Job requires {total_pages} pages (only {self.generator.max_pages} available) and cannot " + \
@@ -2081,34 +2118,33 @@
         page_size = self.generator.page_size
 
         for seq in self.sequences:
 
             seq.allocated_pages = []
             available_pages = None
 
-            self.generator.access_serial += 1
-            new_serial = self.generator.access_serial
-
             # Allocate whole pages
 
             for h in seq.page_hashes:
 
+                self.generator.access_serial += 1
+
                 # Find matching referenced page
 
                 rp = self.generator.referenced_pages.get(h)
                 if rp:
-                    rp.add_ref(new_serial)
+                    rp.add_ref(self.generator.access_serial)
                     seq.allocated_pages.append(rp)
 
                 # If possible, reuse an unreferenced page with matching hash
 
                 else:
                     up = self.generator.unreferenced_pages.get(h)
                     if up:
-                        up.add_ref(new_serial)
+                        up.add_ref(self.generator.access_serial)
                         seq.allocated_pages.append(up)
 
                     # No matching pages
 
                     else:
 
                         # Get list of unreferenced pages in order of oldest to newest
@@ -2124,21 +2160,23 @@
                         # assert all((p.phash in self.generator.unreferenced_pages) for p in available_pages)
                         # assert all((p.phash not in self.generator.referenced_pages) for p in available_pages)
                         # assert all(p.ref_count == 0 for p in available_pages)
 
                         # Allocate oldest unreferenced page
 
                         np = available_pages.popleft()
-                        np.add_ref_clear(new_serial, h)
+                        np.add_ref_clear(self.generator.access_serial, h)
                         seq.allocated_pages.append(np)
 
             # Allocate unique pages
 
             for npi in range(seq.new_unique_pages):
 
+                self.generator.access_serial += 1
+
                 # Get list of unreferenced pages in order of oldest to newest
 
                 if available_pages is None:
                     available_pages = list(self.generator.unreferenced_pages.values())
                     available_pages.sort(key = lambda x: x.access_serial)
                     available_pages = deque(available_pages)
                 else:
@@ -2146,15 +2184,15 @@
                         available_pages.popleft()
 
                 # assert all((p.phash in self.generator.unreferenced_pages) for p in available_pages)
                 # assert all((p.phash not in self.generator.referenced_pages) for p in available_pages)
                 # assert all(p.ref_count == 0 for p in available_pages)
 
                 np = available_pages.popleft()
-                np.add_ref_unique(new_serial)
+                np.add_ref_unique(self.generator.access_serial)
                 seq.allocated_pages.append(np)
 
             # Advance cache over prefilled pages
 
             for page in seq.allocated_pages:
                 if page.kv_position == page_size:
                     seq.kv_position += page_size
```

## exllamav2/generator/sampler.py

```diff
@@ -87,14 +87,34 @@
             if self.token_bias is None:
                 padding = -tokenizer.config.vocab_size % 32
                 self.token_bias = torch.zeros((tokenizer.config.vocab_size + padding,), dtype = torch.float)
 
             self.token_bias[tokens] = float("-inf")
 
 
+        def allow_tokens(
+            self,
+            tokenizer: ExLlamaV2Tokenizer,
+            tokens: list[int | str]
+        ):
+            """Utility function to set/update the logit bias, disallowing all but specific tokens in the supplied list"""
+
+            if self.token_bias is None:
+                padding = -tokenizer.config.vocab_size % 32
+                self.token_bias = torch.full((tokenizer.config.vocab_size + padding,), float("-inf"), dtype = torch.float)
+
+            for t in tokens:
+                if isinstance(t, int):
+                    self.token_bias[t] = 0.0
+                elif isinstance(t, str):
+                    self.token_bias[tokenizer.single_id(t)] = 0.0
+                else:
+                    raise ValueError("Incorrect type in allow_tokens list")
+
+
     @staticmethod
     # @profile
     def sample(
         logits: torch.tensor,
         settings: Settings,
         sequence_ids: torch.tensor,
         random: float,
```

## Comparing `exllamav2-0.1.1.dist-info/LICENSE` & `exllamav2-0.1.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `exllamav2-0.1.1.dist-info/RECORD` & `exllamav2-0.1.2.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -1,33 +1,33 @@
 exllamav2/__init__.py,sha256=DMqLKtqqEDBywO-Coct6oOI3dKZ2h5sXBgV_5QmNPfA,482
-exllamav2/architecture.py,sha256=Tppq2SDuNazhJAt8ZYLjLNPT7kLRqsDMDgXaeeFvhAY,23883
-exllamav2/attn.py,sha256=Ep_tCyZ-BUEXWPlN2vUJAIvhe7ibDZd_qBzc_tPMIYw,39138
-exllamav2/cache.py,sha256=qrv2e2LKrJs7HrkDOzV59BKqzPiflYtPz0gUkICrEX0,16389
+exllamav2/architecture.py,sha256=PlCHXABIdh2HLXRQ-nUKRcrix60L9CPEZxfziCNS2WA,25250
+exllamav2/attn.py,sha256=gQnKFJ735KU6Y3v3kZVOT6BC3QV1qU4Zt49xjwJEaJs,41386
+exllamav2/cache.py,sha256=cZopRpQG7r-79YCsig21tDCnGa0vl8wH69uxB7R0lG8,17150
 exllamav2/compat.py,sha256=6uS0zlyU2vP7Lm9fKuKFRB2Tnoz2bH0otQjYSFSs0j4,2001
-exllamav2/config.py,sha256=F_LgDI1xcRRr9HYGNXM009pxGTpc3vuoP918jj255o4,12574
-exllamav2/embedding.py,sha256=jP7-FxDSl5m3qR_XG2Pb90d9h1hlEe8kj6di2axnQXM,4545
-exllamav2/ext.py,sha256=T9PQnqgkDli8bPVKZJb5lvx4_-P4e0Wn8qOZ4xdFcCQ,12302
+exllamav2/config.py,sha256=4I9ujFG-2nIL9_zbs0tsoBbY1wCYeiZSoGS3rJVwEA8,13140
+exllamav2/embedding.py,sha256=PffK6htimNIUzrWfSJHu1YY3CMkJ2mVmCREQXlTupa0,4876
+exllamav2/ext.py,sha256=enRuaw7OwaDxRiR6EcoHtFwAYi4MxlgvCYos7Dhth9g,12303
 exllamav2/fasttensors.py,sha256=48uGS8ZvRINWDZKBGY9CyvBXtAIGvig8aqhiG0NCzPE,6399
-exllamav2/headnorm.py,sha256=MujxZqqBOc6Nh-1supwtbTZAp0gJqZsV3CXVtIRKGsg,3969
-exllamav2/layernorm.py,sha256=FWZLmRp_HmrpsabTGtkMvTB8K3tzYVyrvxVRRpsbxR0,3502
-exllamav2/linear.py,sha256=Lko2aQ06axN7NB_NDERoBo9sxwLgAxh44ZjU53qW8cQ,11159
+exllamav2/headnorm.py,sha256=4WNUy08TgijzxVhialcAt8AsRoZQlXam4HQ4iteIzBw,3995
+exllamav2/layernorm.py,sha256=ovqIIenLA78NE_KxYbxdDch1-dY8_26OexMSOmBAvqI,3528
+exllamav2/linear.py,sha256=m9cyTRQryi5yp4FxBQ1Ib72i7vKf4bcKTExZvu6JRuU,11185
 exllamav2/lora.py,sha256=0diPvhoQZtu04XgmkKVnybWiJcvJwboNAd534jLnytE,6408
-exllamav2/mlp.py,sha256=8OIowovzn0DgSpkheBi4_0DGVDd9Reu8oImxxwengwI,12405
+exllamav2/mlp.py,sha256=tZViDMWc2wxpUR0yG7_3bcP0ZugMnT4yu-JZ_39aYZY,12410
 exllamav2/model.py,sha256=HSM_ktlQHFYLzmGfJ2TAgXLkXajziH22QPA8IdI0AkU,31876
-exllamav2/model_init.py,sha256=8TnYA9J9CubITMGFInBDGLYouY9S2zHTMgxHm_ORIXg,4931
+exllamav2/model_init.py,sha256=UoHGbzNwHdXtX_KFgZ5P7dqki0uN3VPjnVTWSpkubvk,5097
 exllamav2/module.py,sha256=xJ-BcES9OVXHv56BEy4VVRxyr7NzqXWV1S2FfPXqh3o,7100
-exllamav2/moe_mlp.py,sha256=MbBmpKSqxrx_J4iRuwQ5tY9W7BZYV9UCFOs7lxkLs7E,14288
-exllamav2/parallel_decoder.py,sha256=kuUEUnDTITez_S2z3RRofg_DeZT75VhbLhoQvEY1G48,5262
-exllamav2/pos_embedding.py,sha256=FOIr10SIBgzbTDeLXmNTJ9PTfYFDemYHT1gOzAmjiBs,3172
-exllamav2/rmsnorm.py,sha256=AnHr1_NHYSgMZqZEq3kV7HCM4sf981rek1Ok1xF8zEY,3646
+exllamav2/moe_mlp.py,sha256=NKwNmcue7-GsAKJjUUI5ZYYJ4HAVUnnlFHfhWgRGomg,14314
+exllamav2/parallel_decoder.py,sha256=_6o7FE19OAjA3JObnZ0tobxcQ_EuIbtnZBVzV-5qKEY,5288
+exllamav2/pos_embedding.py,sha256=MpUM1-ituNJcYTfTeriLhmXyYDe0UAiScCkUeLv3cIM,3422
+exllamav2/rmsnorm.py,sha256=gnKaa6E5lvo9AD6r6x_pjMV2OhLB_16wnwcuo5GLYwM,3671
 exllamav2/util.py,sha256=Nxs_As0K7toR8cNuZeicxfhmdJ9PbWjxEaaRaud-L7E,6187
-exllamav2/version.py,sha256=8oAxKUG747GUokmxjkrWejyJa5yPNEsoJDlXxoedxTw,21
-exllamav2/exllamav2_ext/config.h,sha256=EDy0nMntf9sCDb37o8Hz156uxGmDdmIersSaOyj_BnE,337
+exllamav2/version.py,sha256=K5SiDdEGYMpdqXThrqwTqECJJBOQNTQDrnpc2K5mzKs,21
+exllamav2/exllamav2_ext/config.h,sha256=y6YugEFr3PtOSsCVMlzP_Fac_ZvERJ--ZBrcgOjJDCA,417
 exllamav2/exllamav2_ext/ext_bindings.cpp,sha256=NN2asv7tDnlbnUsgEpLA1W9VQh2yU--wL1q1HtAwNnY,4378
-exllamav2/exllamav2_ext/ext_cache.cpp,sha256=q39bqJCgW9WoGn4tOPmbT0LQBKgQlCzof1g-cLkgpvs,8037
+exllamav2/exllamav2_ext/ext_cache.cpp,sha256=lZxPW8lDmfs8604VXJ-CE7n5bjViGiYDV2T3Nz1tAt4,8041
 exllamav2/exllamav2_ext/ext_cache.h,sha256=PRXrgDyAIyjvnkOT0jdGBypgjXHr7cYHoRFMBl3qZ3M,1105
 exllamav2/exllamav2_ext/ext_gemm.cpp,sha256=YAM7OXhVugvhI7U4YUZtGxTLoDjzhpaqKA6lff-RidA,1455
 exllamav2/exllamav2_ext/ext_gemm.h,sha256=ttylcKe8XcitDFhd-6FgRyck9rPVYzy9xt5GHXggfuA,161
 exllamav2/exllamav2_ext/ext_hadamard.cpp,sha256=wUgHLHANiHRNy_5hXddvZ1gfYcbu1apj2rjWAZo2_x4,2505
 exllamav2/exllamav2_ext/ext_hadamard.h,sha256=w1XQdKN3EJedVY-zptOPvRgBc_4sMsSbAk_haBNFkG8,82
 exllamav2/exllamav2_ext/ext_norm.cpp,sha256=GhI9kxdXLVOnF5kq7orvf6sfiCfg7sjU89VwxAnbAFc,3158
 exllamav2/exllamav2_ext/ext_norm.h,sha256=dwHEbHAeymi5-0YXhxPQ8aCwHhCYrF7qgCyqWi2vaPs,639
@@ -56,15 +56,15 @@
 exllamav2/exllamav2_ext/cpp/safetensors.cpp,sha256=wJkfd7_Su4jKUhNjBH2ZOKznmrGsgy_05X_8LxosqZ0,8308
 exllamav2/exllamav2_ext/cpp/safetensors.h,sha256=jl2f5bcdt5GySYCuAAlbowquck68tlMnSZ1HU2dH9Gg,801
 exllamav2/exllamav2_ext/cpp/sampling.cpp,sha256=e6aXc_JyMorrPv9bYQw3Y0x-VuQCE-FChkGabsYy5_s,20446
 exllamav2/exllamav2_ext/cpp/sampling.h,sha256=Nh1FZ-l2HWe5f2a-03IpJjmaEMcP6FKdVfvQrDCIOqc,2283
 exllamav2/exllamav2_ext/cpp/sampling_avx2.cpp,sha256=ArDeIEjdG3wf6syC8RHtq2CQ5xnZxelPbxFcm1mydI4,3089
 exllamav2/exllamav2_ext/cpp/sampling_avx2.h,sha256=8bZHIcrY7jC0ca2EanR6va46fiAGIT98JKdsUY1QsW4,334
 exllamav2/exllamav2_ext/cpp/util.h,sha256=M7CBgwNltaa7H_dKN0WpVbE1jtnbxDQPg9M3W908av0,2204
-exllamav2/exllamav2_ext/cuda/cache.cu,sha256=ozgfDUaDRUusRLTR0K060tP76bhUPUm5KNsnvpWDC8A,13564
+exllamav2/exllamav2_ext/cuda/cache.cu,sha256=SjwLOtBUVj5jJM3uZuebvUDpStWR7VTAesVKKoLNBvk,14080
 exllamav2/exllamav2_ext/cuda/cache.cuh,sha256=YSq6j6AeLbpVcggkcjhacq-DrWlKYkix3HFN3WO6YOY,1658
 exllamav2/exllamav2_ext/cuda/compat.cuh,sha256=zua49pQI0UnYKF3wO7POzz7L29PD6o55DM0awWSK-oY,2831
 exllamav2/exllamav2_ext/cuda/h_add.cu,sha256=XXS6F2abr9oQUBYbAKWG5ddE2XsQlCSZ1je7_AKEyic,2604
 exllamav2/exllamav2_ext/cuda/h_add.cuh,sha256=5-6EliRD6dBScpmlO-DrrP7Wm3Xk5Bm4EEs_a6-iUcw,265
 exllamav2/exllamav2_ext/cuda/h_gemm.cu,sha256=zamYBBUKcdyUzlp9V8RtbFyb-pOhpfB-lECaX5Z8TzU,7202
 exllamav2/exllamav2_ext/cuda/h_gemm.cuh,sha256=-nlLG5p7dfyUltraxSv6BL6_mrATaolbMHjj0fU5Chc,667
 exllamav2/exllamav2_ext/cuda/head_norm.cu,sha256=hQ46TxdScwtm6dwlwLHpXzpfpmwXHYQftoRkmS_zK58,3160
@@ -113,19 +113,19 @@
 exllamav2/exllamav2_ext/cuda/quant/qdq_4.cuh,sha256=2TP9bFuf6ZN1eWA398JgdLipnUfDhZOoo7d1WkJIpaM,5755
 exllamav2/exllamav2_ext/cuda/quant/qdq_5.cuh,sha256=lc4Hjp-vTdbOw02TlDy9YmkAgd8YRTKwhUtu4anUewI,7342
 exllamav2/exllamav2_ext/cuda/quant/qdq_6.cuh,sha256=jX7EYGh2EELTteVaqWkhQ6O5nvWIHpbRyw0hwpAc4no,4530
 exllamav2/exllamav2_ext/cuda/quant/qdq_8.cuh,sha256=YvJHf4kOHMOoXbI-hoRZt-Q0I36P7n7FCsKn--4dWQE,643
 exllamav2/exllamav2_ext/cuda/quant/qdq_util.cuh,sha256=_U-x5oEmtKL5ulsdcg2-CsZsYiCISH2hRD-qgLPYtB4,1367
 exllamav2/generator/__init__.py,sha256=0fXrRbpo5aYJB3oTY_ORJNev03nqynVuGeI2kfZsFMQ,513
 exllamav2/generator/base.py,sha256=OQCbX28f4ZB-sW1E_asL2hARpAq4uW8wfYlaf_jFqSQ,12853
-exllamav2/generator/dynamic.py,sha256=pSEzS2OLJfnXZWTxJWhTq-KY-swwNqwjU1Qa0WRk-NA,80867
+exllamav2/generator/dynamic.py,sha256=gk3BaqtE29PkBXF3xSJW6GCSuCwRSf6lcknYNEYte8I,82375
 exllamav2/generator/dynamic_async.py,sha256=qXAlWhbMvgHt5CnE3wUKt2HkPJr2jQE0ccKSoJQDs6A,2687
 exllamav2/generator/hooks.py,sha256=oxkTKys3iUtpQby97fb8ktlwm1LBPT6wz9GAGEF4yhU,497
 exllamav2/generator/ngram.py,sha256=jp94y816c3d0Ac4RuGQmNhfOOaaf46uZSA_OZccsPFA,2370
-exllamav2/generator/sampler.py,sha256=rJuFIyaQnh6hwEvJscucWCOsCiPdejKXFY4h5zFg3Xs,10380
+exllamav2/generator/sampler.py,sha256=nBRkC-XjBOHQDdTldI_biWc3WAIgUpIEqaL63YOjxTg,11181
 exllamav2/generator/streaming.py,sha256=4t86BBBE86TBlTmE_Ii2dfxbZx_TX3J9lbrS1AhIS5M,39172
 exllamav2/generator/filters/__init__.py,sha256=O-_cLgKh9gN-n8wboZbOmwbmruU6jRolz7wNg85CUBY,242
 exllamav2/generator/filters/base.py,sha256=eBFcaYvBlHThqqClRZaKyl0tLwXrrSrXyj-kggz1m84,756
 exllamav2/generator/filters/prefix.py,sha256=5hQX7fxAtAniZEGYolYUB7m1iI8Nh1OMMop81IA9Jbg,1866
 exllamav2/generator/filters/select.py,sha256=XLhU1MZA1_PLbJj-uupIsPH_NJ_JhM93rgcUBMUGWhI,3965
 exllamav2/hadamard/hadamard.py,sha256=ZBNmxT5951EnrSjh27590tkHdIoLqq7Q4F_qwIbbPbc,4346
 exllamav2/hadamard/hadamard_1.txt,sha256=oxjCQhbe_iBv7rc-9b4AAz-pxKdNC5Z_ZTKibKWQbTs,1
@@ -144,12 +144,12 @@
 exllamav2/server/websocket.py,sha256=v3dxGQr6Hhtt5UhNd2GuSQzAhicwWGO61uYA9Q9854Q,1623
 exllamav2/server/websocket_actions.py,sha256=gcYNpfmnMvFYeQQwxf6G67TtcPMJfV4GZHzDJm5kqhA,10260
 exllamav2/tokenizer/__init__.py,sha256=1-nke-DAloiKZbcuiVqkJGB0hiURq3V184pXza-1ep4,217
 exllamav2/tokenizer/base.py,sha256=63BtYpnPK8NpD4QKSRTcV5eVDXlb06_N6rraxh5CrbM,2157
 exllamav2/tokenizer/hf.py,sha256=HaWxaxzpzKRzKENCGyIwNpkvkSAEZ7Q29mHNssouHm4,3028
 exllamav2/tokenizer/spm.py,sha256=WkmFxy7--Ex-h2Xs5NaV5QnkuMSebQN7XYOu7xb4EmE,1972
 exllamav2/tokenizer/tokenizer.py,sha256=pvM8k90d1gFnWX9roMbTcKbE8LjxG-5_JetRZUTWKew,23709
-exllamav2-0.1.1.dist-info/LICENSE,sha256=GEfg4GmBQu1DR8FEGp-oHI-93USx2LvNXjZH-ZF1nX8,1035
-exllamav2-0.1.1.dist-info/METADATA,sha256=DpUwDcaXXy080blEBl_Ad3iGFrBbrAazdLW5x1vY2EQ,421
-exllamav2-0.1.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-exllamav2-0.1.1.dist-info/top_level.txt,sha256=zHcNv9GI63f6FSZ1gBE6rZUoFLz6jTBuX2RjL8_6H0g,10
-exllamav2-0.1.1.dist-info/RECORD,,
+exllamav2-0.1.2.dist-info/LICENSE,sha256=GEfg4GmBQu1DR8FEGp-oHI-93USx2LvNXjZH-ZF1nX8,1035
+exllamav2-0.1.2.dist-info/METADATA,sha256=FAhBFhpIXfz7GKkoqORE5P-tR2ZlVIpAUJwmw5X6SI8,421
+exllamav2-0.1.2.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+exllamav2-0.1.2.dist-info/top_level.txt,sha256=zHcNv9GI63f6FSZ1gBE6rZUoFLz6jTBuX2RjL8_6H0g,10
+exllamav2-0.1.2.dist-info/RECORD,,
```

